<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ib.bsb.br/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ib.bsb.br/" rel="alternate" type="text/html" /><updated>2025-05-18T19:14:34+00:00</updated><id>https://ib.bsb.br/feed.xml</id><title type="html">infoBAG</title><entry><title type="html">sharing files with `miniserve` HTTP server and `netbird`</title><link href="https://ib.bsb.br/sharing-files-with-miniserve-http-server-and-netbird/" rel="alternate" type="text/html" title="sharing files with `miniserve` HTTP server and `netbird`" /><published>2025-05-18T00:00:00+00:00</published><updated>2025-05-18T13:08:07+00:00</updated><id>https://ib.bsb.br/sharing-files-with-miniserve-http-server-and-netbird</id><content type="html" xml:base="https://ib.bsb.br/sharing-files-with-miniserve-http-server-and-netbird/"><![CDATA[<h3 id="1-introduction-to-miniserve">1. Introduction to <code class="language-plaintext highlighter-rouge">miniserve</code></h3>

<p>As detailed in its <code class="language-plaintext highlighter-rouge">README.md</code>, <code class="language-plaintext highlighter-rouge">miniserve</code> is a “CLI tool to serve files and dirs over HTTP.” It’s designed to be small, self-contained, and cross-platform, making it ideal for quickly sharing files. Built in Rust, it offers good performance. Key features include serving single files or entire directories, MIME type handling, authentication, folder downloads, file uploading, directory creation, themes, QR code support, TLS, and read-only WebDAV support.</p>

<h3 id="2-prerequisites">2. Prerequisites</h3>

<ul>
  <li><strong>RK3588 Device</strong>: An ARM64 device (e.g., Orange Pi 5, Rock 5B).</li>
  <li><strong>Operating System</strong>: Debian Bullseye (or compatible) installed.</li>
  <li><strong>Basic Linux Knowledge</strong>: Command line, package installation, file editing.</li>
  <li><strong>Root/Sudo Access</strong>.</li>
  <li><strong>Internet Connectivity</strong>.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">netbird</code> Account</strong>: You’ll need a <code class="language-plaintext highlighter-rouge">netbird.io</code> account.</li>
</ul>

<hr />

<h3 id="3-phase-1-rk3588-debian-bullseye-preparation">3. Phase 1: RK3588 Debian Bullseye Preparation</h3>

<p>Ensure your Debian Bullseye system is ready.</p>

<h4 id="system-update">System Update</h4>
<p>Log in via SSH and run:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt full-upgrade <span class="nt">-y</span>
<span class="nb">sudo </span>apt autoremove <span class="nt">-y</span>
<span class="nb">sudo </span>apt clean
<span class="c"># Consider a reboot if kernel updates occurred: sudo reboot</span>
</code></pre></div></div>

<h4 id="essential-tools">Essential Tools</h4>
<p>Install common utilities if not already present:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> curl wget git ca-certificates gnupg
</code></pre></div></div>

<hr />

<h3 id="4-phase-2-netbird-installation-and-configuration">4. Phase 2: <code class="language-plaintext highlighter-rouge">netbird</code> Installation and Configuration</h3>

<p><code class="language-plaintext highlighter-rouge">netbird</code> creates a secure peer-to-peer VPN.</p>

<h4 id="install-netbird-client">Install <code class="language-plaintext highlighter-rouge">netbird</code> Client</h4>
<p>The recommended way to install <code class="language-plaintext highlighter-rouge">netbird</code> on Linux is using their installation script:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-fsSL</span> https://pkgs.netbird.io/install.sh | <span class="nb">sudo </span>bash
</code></pre></div></div>
<p>This script will typically detect your OS (Debian), add the necessary repository and GPG key, and install the <code class="language-plaintext highlighter-rouge">netbird</code> package.</p>

<p>Alternatively, for manual installation, refer to the <a href="https://netbird.io/docs/installation/overview">official Netbird documentation</a>, as steps can change.</p>

<h4 id="log-in-to-netbird">Log in to <code class="language-plaintext highlighter-rouge">netbird</code></h4>
<p>Once installed, connect your RK3588 to your <code class="language-plaintext highlighter-rouge">netbird</code> network:</p>

<ul>
  <li><strong>Method 1: Interactive Login (if you have easy browser access):</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>netbird up
</code></pre></div>    </div>
    <p>This will provide a URL. Open it in a browser on any device, log in to your <code class="language-plaintext highlighter-rouge">netbird</code> account, and authorize the RK3588.</p>
  </li>
  <li><strong>Method 2: Setup Key (Recommended for headless servers):</strong>
    <ol>
      <li>In your <code class="language-plaintext highlighter-rouge">netbird</code> admin dashboard (app.netbird.io), go to “Setup Keys.”</li>
      <li>Create a new key (reusable or one-time, as needed). Copy the generated key.</li>
      <li>On your RK3588, run:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>netbird up <span class="nt">--setup-key</span> YOUR_COPIED_SETUP_KEY
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ul>

<h4 id="verify-netbird-connection--identify-ip">Verify <code class="language-plaintext highlighter-rouge">netbird</code> Connection &amp; Identify IP</h4>
<p>Check the <code class="language-plaintext highlighter-rouge">netbird</code> status on your RK3588:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>netbird status
</code></pre></div></div>
<p>This command will show if the client is connected and will display its <code class="language-plaintext highlighter-rouge">netbird</code> IP address (e.g., <code class="language-plaintext highlighter-rouge">100.x.y.z</code>). Note this IP.
You can also use:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip addr show netbird0 <span class="c"># Or the interface name shown by 'netbird status'</span>
</code></pre></div></div>

<h4 id="enable-netbird-service">Enable <code class="language-plaintext highlighter-rouge">netbird</code> Service</h4>
<p>Ensure <code class="language-plaintext highlighter-rouge">netbird</code> starts on boot:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> netbird
</code></pre></div></div>

<hr />

<h3 id="5-phase-3-miniserve-installation-on-arm64">5. Phase 3: <code class="language-plaintext highlighter-rouge">miniserve</code> Installation on ARM64</h3>

<h4 id="option-a-using-pre-compiled-binaries-recommended">Option A: Using Pre-compiled Binaries (Recommended)</h4>
<p><code class="language-plaintext highlighter-rouge">miniserve</code> provides pre-built <code class="language-plaintext highlighter-rouge">aarch64-unknown-linux-musl</code> (statically linked, good portability) or <code class="language-plaintext highlighter-rouge">aarch64-unknown-linux-gnu</code> binaries.</p>

<ol>
  <li><strong>Find the Latest Release:</strong> Go to <code class="language-plaintext highlighter-rouge">https://github.com/svenstaro/miniserve/releases</code>.</li>
  <li><strong>Download the ARM64 Binary:</strong>
On your RK3588 (replace <code class="language-plaintext highlighter-rouge">&lt;VERSION&gt;</code> and adjust binary name if needed):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">VERSION</span><span class="o">=</span><span class="s2">"0.29.0"</span> <span class="c"># Check for the actual latest version!</span>
<span class="c"># Choose musl for static linking or gnu</span>
<span class="nv">BINARY_FILENAME</span><span class="o">=</span><span class="s2">"miniserve-</span><span class="k">${</span><span class="nv">VERSION</span><span class="k">}</span><span class="s2">-aarch64-unknown-linux-musl"</span>
<span class="c"># BINARY_FILENAME="miniserve-${VERSION}-aarch64-unknown-linux-gnu"</span>

<span class="nb">cd</span> /tmp
wget <span class="s2">"https://github.com/svenstaro/miniserve/releases/download/v</span><span class="k">${</span><span class="nv">VERSION</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">BINARY_FILENAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">-O</span> miniserve-arm64
</code></pre></div>    </div>
  </li>
  <li><strong>Make Executable and Install:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod</span> +x miniserve-arm64
<span class="nb">sudo mv </span>miniserve-arm64 /usr/local/bin/miniserve
</code></pre></div>    </div>
  </li>
  <li><strong>Verify:</strong> <code class="language-plaintext highlighter-rouge">miniserve --version</code></li>
</ol>

<h4 id="option-b-building-from-source-with-cargo">Option B: Building from Source with <code class="language-plaintext highlighter-rouge">cargo</code></h4>
<ol>
  <li><strong>Install Rust &amp; Build Tools:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--proto</span> <span class="s1">'=https'</span> <span class="nt">--tlsv1</span>.2 <span class="nt">-sSf</span> https://sh.rustup.rs | sh
<span class="nb">source</span> <span class="s2">"</span><span class="nv">$HOME</span><span class="s2">/.cargo/env"</span> <span class="c"># Or re-login/open new terminal</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> build-essential pkg-config libssl-dev <span class="c"># For GNU target</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Install <code class="language-plaintext highlighter-rouge">miniserve</code>:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cargo <span class="nb">install</span> <span class="nt">--locked</span> miniserve
</code></pre></div>    </div>
    <p>The binary will be in <code class="language-plaintext highlighter-rouge">$HOME/.cargo/bin/miniserve</code>. Ensure this is in your <code class="language-plaintext highlighter-rouge">PATH</code> or move the binary to <code class="language-plaintext highlighter-rouge">/usr/local/bin/</code>.</p>
  </li>
  <li><strong>Verify:</strong> <code class="language-plaintext highlighter-rouge">$HOME/.cargo/bin/miniserve --version</code> (or <code class="language-plaintext highlighter-rouge">miniserve --version</code> if in PATH).</li>
</ol>

<h4 id="option-c-using-docker">Option C: Using Docker</h4>
<p><code class="language-plaintext highlighter-rouge">miniserve</code> offers multi-arch Docker images.</p>
<ol>
  <li><strong>Install Docker:</strong>
The simplest way is often the convenience script:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-fsSL</span> https://get.docker.com <span class="nt">-o</span> get-docker.sh
<span class="nb">sudo </span>sh get-docker.sh
<span class="nb">sudo </span>usermod <span class="nt">-aG</span> docker your_user <span class="c"># Add your user to docker group, re-login after</span>
</code></pre></div>    </div>
    <p>Alternatively, follow manual instructions from the <a href="https://docs.docker.com/engine/install/debian/">Docker Debian documentation</a>.</p>
  </li>
  <li><strong>Run <code class="language-plaintext highlighter-rouge">miniserve</code>:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Replace /host/path/to/share with the actual path on your RK3588</span>
docker run <span class="nt">-d</span> <span class="nt">--name</span> my-miniserve <span class="se">\</span>
  <span class="nt">-v</span> /host/path/to/share:/data:ro <span class="se">\</span>
  <span class="nt">-p</span> &lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;:8080 <span class="se">\</span>
  <span class="nt">--restart</span> unless-stopped <span class="se">\</span>
  docker.io/svenstaro/miniserve /data <span class="nt">-p</span> 8080
</code></pre></div>    </div>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">-d</code>: Run detached.</li>
      <li><code class="language-plaintext highlighter-rouge">--name my-miniserve</code>: Name the container.</li>
      <li><code class="language-plaintext highlighter-rouge">-v /host/path/to/share:/data:ro</code>: Mounts your host directory read-only (<code class="language-plaintext highlighter-rouge">:ro</code>) into the container. Remove <code class="language-plaintext highlighter-rouge">:ro</code> for uploads.</li>
      <li><code class="language-plaintext highlighter-rouge">-p &lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;:8080</code>: Binds <code class="language-plaintext highlighter-rouge">miniserve</code>’s port <code class="language-plaintext highlighter-rouge">8080</code> inside the container to a specific <code class="language-plaintext highlighter-rouge">&lt;PORT&gt;</code> on your RK3588’s <code class="language-plaintext highlighter-rouge">&lt;YOUR_NETBIRD_IP&gt;</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">--restart unless-stopped</code>: For persistence.</li>
      <li><code class="language-plaintext highlighter-rouge">/data -p 8080</code>: Tells <code class="language-plaintext highlighter-rouge">miniserve</code> inside the container to serve <code class="language-plaintext highlighter-rouge">/data</code> on port <code class="language-plaintext highlighter-rouge">8080</code>.</li>
      <li><strong>Note on Volume Permissions:</strong> If <code class="language-plaintext highlighter-rouge">miniserve</code> inside Docker (often runs as non-root) can’t read <code class="language-plaintext highlighter-rouge">/host/path/to/share</code>, ensure the host path has appropriate read permissions for the user/group ID <code class="language-plaintext highlighter-rouge">miniserve</code> runs as in the container, or explore Docker’s user mapping options.</li>
    </ul>
  </li>
</ol>

<p><em>(The rest of this tutorial focuses on non-Docker systemd setup for <code class="language-plaintext highlighter-rouge">miniserve</code>)</em></p>

<hr />

<h3 id="6-phase-4-running-miniserve">6. Phase 4: Running <code class="language-plaintext highlighter-rouge">miniserve</code></h3>

<h4 id="create-a-directory-to-serve">Create a Directory to Serve</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /srv/miniserve_data <span class="c"># Or your preferred location</span>
<span class="nb">echo</span> <span class="s2">"Hello from miniserve on RK3588 via Netbird!"</span> <span class="o">&gt;</span> /srv/miniserve_data/index.html
<span class="nb">sudo chown</span> <span class="nt">-R</span> your_user:your_user /srv/miniserve_data <span class="c"># Change 'your_user' if needed</span>
</code></pre></div></div>

<h4 id="manual-test-run-binding-to-netbird-ip">Manual Test Run (Binding to <code class="language-plaintext highlighter-rouge">netbird</code> IP)</h4>
<p>Replace <code class="language-plaintext highlighter-rouge">&lt;YOUR_NETBIRD_IP&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;PORT&gt;</code> (e.g., 8080).</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>miniserve <span class="nt">-i</span> &lt;YOUR_NETBIRD_IP&gt; <span class="nt">-p</span> &lt;PORT&gt; /srv/miniserve_data
</code></pre></div></div>
<p>Test access from another <code class="language-plaintext highlighter-rouge">netbird</code> device: <code class="language-plaintext highlighter-rouge">http://&lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;</code>. Press <code class="language-plaintext highlighter-rouge">CTRL+C</code> to stop.</p>

<h4 id="setting-up-miniserve-as-a-systemd-service">Setting up <code class="language-plaintext highlighter-rouge">miniserve</code> as a <code class="language-plaintext highlighter-rouge">systemd</code> Service</h4>

<ol>
  <li><strong>Obtain and Place the <code class="language-plaintext highlighter-rouge">systemd</code> Unit File:</strong>
Create <code class="language-plaintext highlighter-rouge">/etc/systemd/system/miniserve@.service</code> (ensure <code class="language-plaintext highlighter-rouge">miniserve</code> binary path is correct):
    <div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">miniserve for %i</span>
<span class="py">After</span><span class="p">=</span><span class="s">network-online.target netbird.service</span>
<span class="py">Wants</span><span class="p">=</span><span class="s">network-online.target netbird.service</span>

<span class="nn">[Service]</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/local/bin/miniserve -- %I # Verify this path to miniserve</span>

<span class="c"># Security Hardening
</span><span class="py">IPAccounting</span><span class="p">=</span><span class="s">yes</span>
<span class="c"># IPAddressAllow/Deny are removed here as miniserve will bind to a specific IP.
# If miniserve were to bind 0.0.0.0, you'd use IPAddressAllow for Netbird subnet.
</span><span class="py">DynamicUser</span><span class="p">=</span><span class="s">yes</span>
<span class="py">PrivateTmp</span><span class="p">=</span><span class="s">yes</span>
<span class="py">PrivateUsers</span><span class="p">=</span><span class="s">yes</span>
<span class="py">PrivateDevices</span><span class="p">=</span><span class="s">yes</span>
<span class="py">NoNewPrivileges</span><span class="p">=</span><span class="s">true</span>
<span class="py">ProtectSystem</span><span class="p">=</span><span class="s">strict</span>
<span class="py">ProtectHome</span><span class="p">=</span><span class="s">read-only # Change to 'no' or use a dedicated user if serving from /home</span>
<span class="py">ProtectClock</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectControlGroups</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectKernelLogs</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectKernelModules</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectKernelTunables</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectProc</span><span class="p">=</span><span class="s">invisible</span>
<span class="py">CapabilityBoundingSet</span><span class="p">=</span><span class="s">CAP_NET_BIND_SERVICE CAP_DAC_READ_SEARCH</span>

<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">multi-user.target</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Dedicated User for <code class="language-plaintext highlighter-rouge">miniserve</code> (Optional but Recommended):</strong>
If <code class="language-plaintext highlighter-rouge">DynamicUser=yes</code> is problematic for permissions, or you prefer explicit control:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>groupadd <span class="nt">--system</span> miniserve-runner
<span class="nb">sudo </span>useradd <span class="nt">--system</span> <span class="nt">-g</span> miniserve-runner <span class="nt">-d</span> /var/empty <span class="nt">-s</span> /bin/false miniserve-runner
</code></pre></div>    </div>
    <p>Then, in your systemd override (next step), you’ll add <code class="language-plaintext highlighter-rouge">User=miniserve-runner</code> and <code class="language-plaintext highlighter-rouge">Group=miniserve-runner</code>.</p>
  </li>
  <li><strong>Determine and Escape the Path to Serve:</strong>
Example: For <code class="language-plaintext highlighter-rouge">/srv/miniserve_data</code>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemd-escape /srv/miniserve_data
</code></pre></div>    </div>
    <p>This might output <code class="language-plaintext highlighter-rouge">srv-miniserve_data</code>. This is your <code class="language-plaintext highlighter-rouge">&lt;escaped-path&gt;</code>.</p>
  </li>
  <li><strong>Create <code class="language-plaintext highlighter-rouge">systemd</code> Override File for Custom Options:</strong>
Use <code class="language-plaintext highlighter-rouge">sudo systemctl edit miniserve@&lt;escaped-path&gt;.service</code>. Add:
    <div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Service]</span>
<span class="c"># If using a dedicated user:
# User=miniserve-runner
# Group=miniserve-runner
</span>
<span class="c"># Clear default ExecStart
</span><span class="py">ExecStart</span><span class="p">=</span>
<span class="c"># Set our custom ExecStart. Replace &lt;YOUR_NETBIRD_IP&gt; and &lt;PORT&gt;.
# Add other miniserve flags as needed (e.g., --auth, -u, --tls-cert).
</span><span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/local/bin/miniserve -i &lt;YOUR_NETBIRD_IP&gt; -p &lt;PORT&gt; --title "RK3588 Files" -- %I</span>
</code></pre></div>    </div>
    <p>Ensure <code class="language-plaintext highlighter-rouge">/usr/local/bin/miniserve</code> is the correct absolute path to your <code class="language-plaintext highlighter-rouge">miniserve</code> binary.</p>
  </li>
  <li><strong>File System Permissions for the Served Directory:</strong>
The user <code class="language-plaintext highlighter-rouge">miniserve</code> runs as (either dynamic or <code class="language-plaintext highlighter-rouge">miniserve-runner</code>) needs read access to <code class="language-plaintext highlighter-rouge">/srv/miniserve_data</code> and its contents.
If using <code class="language-plaintext highlighter-rouge">miniserve-runner</code>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chown</span> <span class="nt">-R</span> miniserve-runner:miniserve-runner /srv/miniserve_data
<span class="nb">sudo chmod</span> <span class="nt">-R</span> <span class="nv">u</span><span class="o">=</span>rX,g<span class="o">=</span>rX,o-rwx /srv/miniserve_data <span class="c"># Read/execute for user/group</span>
</code></pre></div>    </div>
    <p>If <code class="language-plaintext highlighter-rouge">DynamicUser=yes</code> and serving from outside standard system paths (like <code class="language-plaintext highlighter-rouge">/srv</code>), you might need <code class="language-plaintext highlighter-rouge">setfacl</code> for more granular permissions if <code class="language-plaintext highlighter-rouge">chown</code> is not desired, or ensure the path is world-readable (less secure).</p>
  </li>
  <li><strong>Reload <code class="language-plaintext highlighter-rouge">systemd</code>, Enable, and Start the Service:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl daemon-reload
<span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> miniserve@&lt;escaped-path&gt;.service
</code></pre></div>    </div>
  </li>
  <li><strong>Check Service Status:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl status miniserve@&lt;escaped-path&gt;.service
<span class="nb">sudo </span>journalctl <span class="nt">-u</span> miniserve@&lt;escaped-path&gt;.service <span class="nt">-f</span>
</code></pre></div>    </div>
  </li>
</ol>

<hr />

<h3 id="7-phase-5-accessing-miniserve-and-other-services-via-netbird">7. Phase 5: Accessing <code class="language-plaintext highlighter-rouge">miniserve</code> and Other Services via <code class="language-plaintext highlighter-rouge">netbird</code></h3>

<h4 id="accessing-miniserve">Accessing <code class="language-plaintext highlighter-rouge">miniserve</code></h4>
<p>From another device on your <code class="language-plaintext highlighter-rouge">netbird</code> network, use your browser:
<code class="language-plaintext highlighter-rouge">http://&lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;</code> (or <code class="language-plaintext highlighter-rouge">https://</code> if you configured TLS).</p>

<h4 id="accessing-ssh-ftp-etc">Accessing SSH, FTP, etc.</h4>
<p>Services like SSH on your RK3588 are now accessible via its <code class="language-plaintext highlighter-rouge">netbird</code> IP from other peers in your <code class="language-plaintext highlighter-rouge">netbird</code> network:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh your_rk3588_user@&lt;YOUR_NETBIRD_IP&gt;
</code></pre></div></div>
<p>Similarly for FTP or other services configured to listen on the <code class="language-plaintext highlighter-rouge">netbird</code> IP or <code class="language-plaintext highlighter-rouge">0.0.0.0</code>.</p>

<hr />

<h3 id="8-phase-6-advanced-miniserve-configuration--security">8. Phase 6: Advanced <code class="language-plaintext highlighter-rouge">miniserve</code> Configuration &amp; Security</h3>

<h4 id="key-miniserve-features">Key <code class="language-plaintext highlighter-rouge">miniserve</code> Features</h4>
<p>Consult <code class="language-plaintext highlighter-rouge">miniserve --help</code> and the <code class="language-plaintext highlighter-rouge">README.md</code>.</p>
<ul>
  <li><strong>Authentication:</strong> <code class="language-plaintext highlighter-rouge">--auth username:password</code> or <code class="language-plaintext highlighter-rouge">--auth-file /path/auth.txt</code>. Highly recommended.
Example for systemd override: <code class="language-plaintext highlighter-rouge">ExecStart=... --auth "admin:$(openssl passwd -1 'securepass')" -- %I</code></li>
  <li><strong>TLS (HTTPS):</strong> <code class="language-plaintext highlighter-rouge">--tls-cert /path/cert.pem --tls-key /path/key.pem</code>.
See “TLS Certificate Generation with SAN” below.</li>
  <li><strong>File Uploads:</strong> <code class="language-plaintext highlighter-rouge">-u</code> or <code class="language-plaintext highlighter-rouge">-u /allowed/subdir</code>. Use with caution regarding permissions.</li>
  <li><strong>Directory Creation:</strong> <code class="language-plaintext highlighter-rouge">--mkdir</code> (requires uploads).</li>
  <li><strong>WebDAV:</strong> <code class="language-plaintext highlighter-rouge">--enable-webdav</code> for read-only WebDAV access. This can be a good alternative to FTP for file management over HTTP.</li>
</ul>

<h4 id="tls-certificate-generation-with-san">TLS Certificate Generation with SAN</h4>
<p>For self-signed certificates to work well with modern browsers (even over <code class="language-plaintext highlighter-rouge">netbird</code>), include a Subject Alternative Name (SAN) for the IP.</p>
<ol>
  <li>Create a directory for certs: <code class="language-plaintext highlighter-rouge">sudo mkdir -p /etc/miniserve/tls; cd /etc/miniserve/tls</code></li>
  <li>Generate key and cert (replace <code class="language-plaintext highlighter-rouge">&lt;YOUR_NETBIRD_IP&gt;</code>):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>openssl req <span class="nt">-x509</span> <span class="nt">-newkey</span> rsa:4096 <span class="nt">-keyout</span> key.pem <span class="nt">-out</span> cert.pem <span class="se">\</span>
  <span class="nt">-sha256</span> <span class="nt">-days</span> 3650 <span class="nt">-nodes</span> <span class="se">\</span>
  <span class="nt">-subj</span> <span class="s2">"/CN=&lt;YOUR_NETBIRD_IP&gt;"</span> <span class="se">\</span>
  <span class="nt">-addext</span> <span class="s2">"subjectAltName = IP:&lt;YOUR_NETBIRD_IP&gt;"</span>
</code></pre></div>    </div>
  </li>
  <li>Set permissions (if <code class="language-plaintext highlighter-rouge">miniserve</code> runs as <code class="language-plaintext highlighter-rouge">miniserve-runner</code>):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chown</span> <span class="nt">-R</span> miniserve-runner:miniserve-runner /etc/miniserve/tls
<span class="nb">sudo chmod </span>600 /etc/miniserve/tls/key.pem
<span class="nb">sudo chmod </span>644 /etc/miniserve/tls/cert.pem
</code></pre></div>    </div>
  </li>
  <li>Update your systemd override <code class="language-plaintext highlighter-rouge">ExecStart</code> with:
<code class="language-plaintext highlighter-rouge">--tls-cert /etc/miniserve/tls/cert.pem --tls-key /etc/miniserve/tls/key.pem</code></li>
  <li>Access via <code class="language-plaintext highlighter-rouge">https://&lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;</code>. You’ll need to accept the self-signed certificate warning in your browser.</li>
</ol>

<h4 id="firewall-ufw-on-rk3588">Firewall (UFW on RK3588)</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> ufw
<span class="nb">sudo </span>ufw default deny incoming
<span class="nb">sudo </span>ufw default allow outgoing
<span class="nb">sudo </span>ufw allow ssh <span class="c"># Essential for remote access</span>
<span class="c"># Allow miniserve port ONLY from Netbird interface (e.g., netbird0)</span>
<span class="nb">sudo </span>ufw allow <span class="k">in </span>on netbird0 to any port &lt;PORT&gt; proto tcp 
<span class="nb">sudo </span>ufw <span class="nb">enable
sudo </span>ufw status verbose
</code></pre></div></div>

<h4 id="netbird-access-controls"><code class="language-plaintext highlighter-rouge">netbird</code> Access Controls</h4>
<p>Utilize <code class="language-plaintext highlighter-rouge">netbird</code>’s dashboard to create access policies, restricting which peers can connect to your RK3588 and on which ports/protocols for fine-grained security.</p>

<h4 id="regular-updates">Regular Updates</h4>
<p>Keep Debian, <code class="language-plaintext highlighter-rouge">netbird</code>, and <code class="language-plaintext highlighter-rouge">miniserve</code> updated.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt full-upgrade <span class="nt">-y</span>
<span class="c"># For miniserve from cargo: cargo install --locked miniserve</span>
<span class="c"># For Netbird: usually updates via apt if repo was added.</span>
</code></pre></div></div>

<hr />

<h3 id="9-phase-7-troubleshooting">9. Phase 7: Troubleshooting</h3>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">miniserve</code> Service Issues:</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">sudo systemctl status miniserve@&lt;escaped-path&gt;.service</code></li>
      <li><code class="language-plaintext highlighter-rouge">sudo journalctl -u miniserve@&lt;escaped-path&gt;.service -n 100 --no-pager --follow</code></li>
      <li>Check <code class="language-plaintext highlighter-rouge">miniserve</code> binary path and permissions.</li>
      <li>Manually run the <code class="language-plaintext highlighter-rouge">ExecStart</code> command from the systemd unit (as the correct user if specified) to see direct errors.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">netbird</code> Connectivity:</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">sudo netbird status</code> on all relevant peers.</li>
      <li>Ping <code class="language-plaintext highlighter-rouge">netbird</code> IPs between peers.</li>
      <li>Check <code class="language-plaintext highlighter-rouge">netbird</code> admin dashboard for peer status and access rules.</li>
    </ul>
  </li>
  <li><strong>Firewall:</strong>
    <ul>
      <li>Temporarily <code class="language-plaintext highlighter-rouge">sudo ufw disable</code> to isolate firewall issues. If it works, your UFW rules need adjustment. Re-enable UFW.</li>
      <li>Check UFW logs: <code class="language-plaintext highlighter-rouge">sudo less /var/log/ufw.log</code>.</li>
    </ul>
  </li>
  <li><strong>Permissions:</strong> Double-check that the user <code class="language-plaintext highlighter-rouge">miniserve</code> runs as has read (and write, if uploads enabled) permissions for the target directories.</li>
</ul>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">bash script to setup ‘wifish’ on Linux</title><link href="https://ib.bsb.br/bash-script-to-setup-wifish-on-linux/" rel="alternate" type="text/html" title="bash script to setup ‘wifish’ on Linux" /><published>2025-05-17T00:00:00+00:00</published><updated>2025-05-17T22:44:46+00:00</updated><id>https://ib.bsb.br/bash-script-to-setup-wifish-on-linux</id><content type="html" xml:base="https://ib.bsb.br/bash-script-to-setup-wifish-on-linux/"><![CDATA[<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="#!/bin/bash
# This script automates the setup of &#39;wifish&#39; on a Debian Bullseye system.
# Version 2: Improved backup, idempotency, and interface detection.
#
# IMPORTANT: This script makes certain assumptions:
# 1. It should be run as root.
# 2. It attempts to auto-detect the Wi-Fi interface. If detection fails or is
#    incorrect, the default &quot;wlan0&quot; is used, or you may need to edit the script
#    or use wifish with the -i flag.
# 3. The active wpa_supplicant configuration file is assumed to be
#    &#39;/etc/wpa_supplicant/wpa_supplicant.conf&#39;. If different, this needs to be changed.
# 4. It will attempt to identify the primary non-root user (UID 1000 or SUDO_USER)
#    to add to the &#39;netdev&#39; group.
#
# Review these assumptions and the script content before execution.

set -e # Exit immediately if a command exits with a non-zero status.

# --- Configuration ---
DEFAULT_WIFI_INTERFACE=&quot;wlan0&quot;
WPA_SUPPLICANT_CONF_FILE=&quot;/etc/wpa_supplicant/wpa_supplicant.conf&quot;
WIFI_INTERFACE=&quot;&quot; # Will be auto-detected or fall back to DEFAULT_WIFI_INTERFACE

# Determine the primary non-root user to add to the &#39;netdev&#39; group.
PRIMARY_USER=&quot;&quot;
if [ -n &quot;$SUDO_USER&quot; ] &amp;&amp; [ &quot;$SUDO_USER&quot; != &quot;root&quot; ]; then # Prefer SUDO_USER if available and not root
    PRIMARY_USER=&quot;$SUDO_USER&quot;
elif command -v id &gt;/dev/null &amp;&amp; id -u 1000 &amp;&gt;/dev/null; then # Try UID 1000
    PRIMARY_USER_UID1000=$(id -un 1000)
    if [ &quot;$PRIMARY_USER_UID1000&quot; != &quot;root&quot; ]; then # Ensure user 1000 is not root
        PRIMARY_USER=&quot;$PRIMARY_USER_UID1000&quot;
    fi
fi
# Fallback to parsing /etc/passwd if PRIMARY_USER is still not set
if [ -z &quot;$PRIMARY_USER&quot; ]; then
    PRIMARY_USER=$(awk -F: &#39;($3&gt;=1000 &amp;&amp; $1 != &quot;root&quot; &amp;&amp; $1 != &quot;&quot; &amp;&amp; $1 != &quot;nobody&quot;){print $1; exit}&#39; /etc/passwd)
fi


# --- Helper Functions ---
log_action() {
    echo &quot;[INFO] $1&quot;
}

log_warning() {
    echo &quot;[WARNING] $1&quot;
}

log_error() {
    echo &quot;[ERROR] $1&quot; &gt;&amp;2
}

# --- Wi-Fi Interface Detection ---
detect_wifi_interface() {
    log_action &quot;Attempting to auto-detect Wi-Fi interface...&quot;
    local detected_iface=&quot;&quot;
    # Try with &#39;iw dev&#39; first, common for Wi-Fi specific tools
    if command -v iw &gt;/dev/null; then
        detected_iface=$(iw dev | awk &#39;$1==&quot;Interface&quot;{print $2; exit}&#39;)
    fi

    # If &#39;iw dev&#39; didn&#39;t yield a result, try with &#39;ip link&#39; for wlan*
    if [ -z &quot;$detected_iface&quot; ] &amp;&amp; command -v ip &gt;/dev/null; then
        detected_iface=$(ip -o link show type wlan | awk -F&#39;: &#39; &#39;{print $2; exit}&#39; | awk &#39;{print $1}&#39;)
    fi

    # If still no result, try &#39;ip link&#39; for wlp* (common alternative naming)
    if [ -z &quot;$detected_iface&quot; ] &amp;&amp; command -v ip &gt;/dev/null; then
        detected_iface=$(ip -o link show | grep -Eo &#39;wlp[0-9]+s[0-9]+&#39; | head -n1)
    fi

    if [ -n &quot;$detected_iface&quot; ]; then
        WIFI_INTERFACE=&quot;$detected_iface&quot;
        log_action &quot;Detected Wi-Fi interface: $WIFI_INTERFACE. Using this.&quot;
    else
        WIFI_INTERFACE=&quot;$DEFAULT_WIFI_INTERFACE&quot;
        log_warning &quot;Could not auto-detect Wi-Fi interface. Using default: $WIFI_INTERFACE.&quot;
        log_warning &quot;If this is incorrect, review script or use wifish with the &#39;-i &lt;your_interface&gt;&#39; option.&quot;
    fi
}


# --- Main Script ---

# Check if running as root
if [ &quot;$(id -u)&quot; -ne 0 ]; then
  log_error &quot;This script must be run as root. Please use &#39;sudo $0&#39; or run as the root user.&quot;
  exit 1
fi

detect_wifi_interface # Call detection function

log_action &quot;Starting wifish setup script (v2)...&quot;
log_action &quot;Using Wi-Fi Interface: ${WIFI_INTERFACE}&quot;
log_action &quot;Targeting wpa_supplicant config: ${WPA_SUPPLICANT_CONF_FILE}&quot;

if [ -n &quot;$PRIMARY_USER&quot; ]; then
    log_action &quot;Primary non-root user identified for &#39;netdev&#39; group: ${PRIMARY_USER}&quot;
else
    log_warning &quot;Could not automatically detect a primary non-root user. Manual &#39;usermod -a -G netdev youruser&#39; might be required.&quot;
fi
echo &quot;---------------------------------------------------------------------&quot;

# Phase 1: Prerequisites and Getting wifish
log_action &quot;Phase 1: Installing prerequisites and getting wifish...&quot;
log_action &quot;Updating package lists (apt update)...&quot;
if ! apt update -qq; then
    log_error &quot;apt update failed. Please check your network connection and apt sources.&quot;
    exit 1
fi

log_action &quot;Installing gawk, dialog, git...&quot;
if ! apt install -y gawk dialog git; then
    log_error &quot;Failed to install required packages. Please check apt output for errors.&quot;
    exit 1
fi

TEMP_WIFISH_DIR=&quot;/tmp/bougyman-wifish_install_temp_$(date +%Y%m%d%H%M%S)&quot;
log_action &quot;Cloning wifish repository to $TEMP_WIFISH_DIR...&quot;
rm -rf &quot;$TEMP_WIFISH_DIR&quot; # Clean up if exists
if ! git clone https://github.com/bougyman/wifish.git &quot;$TEMP_WIFISH_DIR&quot;; then
    log_error &quot;Failed to clone wifish repository. Check internet connection and git.&quot;
    exit 1
fi

log_action &quot;Setting execute permissions for scripts in repository...&quot;
(
    cd &quot;$TEMP_WIFISH_DIR&quot;
    chmod +x wifish install.sh test/test.sh
    chmod +x sv/wpa_supplicant/run sv/wpa_supplicant/log/run
)
log_action &quot;Phase 1 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 2: Configuring wpa_supplicant
log_action &quot;Phase 2: Configuring wpa_supplicant...&quot;
log_action &quot;Ensuring wpa_supplicant configuration file directory exists: $(dirname &quot;$WPA_SUPPLICANT_CONF_FILE&quot;)&quot;
mkdir -p &quot;$(dirname &quot;$WPA_SUPPLICANT_CONF_FILE&quot;)&quot;

if [ ! -f &quot;$WPA_SUPPLICANT_CONF_FILE&quot; ]; then
    log_action &quot;Creating empty wpa_supplicant configuration file: $WPA_SUPPLICANT_CONF_FILE&quot;
    touch &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
fi

TIMESTAMP=$(date +%Y%m%d%H%M%S)
BACKUP_FILE=&quot;${WPA_SUPPLICANT_CONF_FILE}.${TIMESTAMP}.bak&quot;
log_action &quot;Backing up current $WPA_SUPPLICANT_CONF_FILE to $BACKUP_FILE...&quot;
if cp &quot;$WPA_SUPPLICANT_CONF_FILE&quot; &quot;$BACKUP_FILE&quot;; then
    log_action &quot;Backup successful: $BACKUP_FILE&quot;
else
    log_warning &quot;Failed to create backup of $WPA_SUPPLICANT_CONF_FILE. Proceeding with caution.&quot;
fi

log_action &quot;Configuring &#39;ctrl_interface&#39; and &#39;update_config&#39; in $WPA_SUPPLICANT_CONF_FILE for idempotency...&quot;

# Comment out any existing ctrl_interface lines to avoid conflicts
sed -i -E &#39;s/^[[:space:]]*ctrl_interface=.*$/#&amp; (old_ctrl_interface, commented by wifish_setup.sh)/&#39; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
# Add the correct ctrl_interface line if it&#39;s not already present (uncommented)
if ! grep -qFx &quot;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&quot; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;; then
    log_action &quot;Adding &#39;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&#39; to $WPA_SUPPLICANT_CONF_FILE.&quot;
    echo &quot;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&quot; &gt;&gt; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
else
    log_action &quot;&#39;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&#39; already present or re-added.&quot;
fi

# Comment out any existing update_config lines
sed -i -E &#39;s/^[[:space:]]*update_config=.*$/#&amp; (old_update_config, commented by wifish_setup.sh)/&#39; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
# Add the correct update_config line if it&#39;s not already present (uncommented)
if ! grep -qFx &quot;update_config=1&quot; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;; then
    log_action &quot;Adding &#39;update_config=1&#39; to $WPA_SUPPLICANT_CONF_FILE.&quot;
    echo &quot;update_config=1&quot; &gt;&gt; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
else
    log_action &quot;&#39;update_config=1&#39; already present or re-added.&quot;
fi

log_action &quot;Securing $WPA_SUPPLICANT_CONF_FILE permissions (chmod 600)...&quot;
chmod 600 &quot;$WPA_SUPPLICANT_CONF_FILE&quot;

USER_MODIFIED_LOGIN_MESSAGE=&quot;&quot;
if [ -n &quot;$PRIMARY_USER&quot; ]; then
    log_action &quot;Ensuring &#39;netdev&#39; group exists and adding user &#39;$PRIMARY_USER&#39; to it...&quot;
    if ! getent group netdev &gt;/dev/null; then
        log_action &quot;Group &#39;netdev&#39; does not exist. Creating it...&quot;
        if ! groupadd --system netdev; then # Use --system for system groups if appropriate
            log_warning &quot;Could not create group &#39;netdev&#39;. Manual creation (groupadd netdev) might be needed.&quot;
        else
            log_action &quot;Group &#39;netdev&#39; created.&quot;
        fi
    fi
    # Check if user is already in group to avoid unnecessary usermod message
    if ! groups &quot;$PRIMARY_USER&quot; | grep -q &#39;\bnetdev\b&#39;; then
        if ! usermod -a -G netdev &quot;$PRIMARY_USER&quot;; then
             log_warning &quot;Failed to add user &#39;$PRIMARY_USER&#39; to &#39;netdev&#39; group. Check permissions or do it manually.&quot;
        else
            USER_MODIFIED_LOGIN_MESSAGE=&quot;User &#39;$PRIMARY_USER&#39; has been added to the &#39;netdev&#39; group. IMPORTANT: &#39;$PRIMARY_USER&#39; MUST log out and log back in for this change to take effect.&quot;
            log_action &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
        fi
    else
        log_action &quot;User &#39;$PRIMARY_USER&#39; is already a member of the &#39;netdev&#39; group.&quot;
        USER_MODIFIED_LOGIN_MESSAGE=&quot;User &#39;$PRIMARY_USER&#39; is a member of &#39;netdev&#39;. If this membership is recent, a logout/login might still be needed for all services to recognize it.&quot;

    fi
else
    USER_MODIFIED_LOGIN_MESSAGE=&quot;IMPORTANT: Could not automatically determine a primary non-root user. Please manually add your regular user to the &#39;netdev&#39; group (e.g., &#39;sudo usermod -a -G netdev yourusername&#39;) and then log out and log back in.&quot;
    log_warning &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
fi

log_action &quot;Attempting to restart wpa_supplicant for interface &#39;$WIFI_INTERFACE&#39;...&quot;
WPA_RESTARTED_MANUALLY_MSG=&quot;If wpa_supplicant was restarted, it should pick up the new configuration. If not, a manual restart of wpa_supplicant or a system reboot might be necessary.&quot;

SERVICE_NAME=&quot;wpa_supplicant@${WIFI_INTERFACE}.service&quot;
SERVICE_EXISTS=false
if systemctl list-unit-files | grep -q &quot;^${SERVICE_NAME}&quot;; then # More precise grep
    SERVICE_EXISTS=true
fi

if [ &quot;$SERVICE_EXISTS&quot; = true ]; then
    log_action &quot;${SERVICE_NAME} found.&quot;
    if systemctl is-active --quiet &quot;${SERVICE_NAME}&quot;; then
        log_action &quot;Restarting ${SERVICE_NAME} via systemd...&quot;
        if ! systemctl restart &quot;${SERVICE_NAME}&quot;; then
            log_warning &quot;Failed to restart ${SERVICE_NAME}. Check &#39;systemctl status ${SERVICE_NAME}&#39; and &#39;journalctl -u ${SERVICE_NAME}&#39;.&quot;
        fi
    else
        log_action &quot;${SERVICE_NAME} exists but is not active. Attempting to enable and start it...&quot;
        if ! systemctl enable &quot;${SERVICE_NAME}&quot; --now; then
             log_warning &quot;Failed to enable and start ${SERVICE_NAME}. Check status and journal.&quot;
        fi
    fi
elif [ -f &quot;/etc/network/interfaces&quot; ] &amp;&amp; grep -q &quot;iface ${WIFI_INTERFACE} inet&quot; /etc/network/interfaces; then # More specific grep
    log_action &quot;Attempting to restart network interface ${WIFI_INTERFACE} via ifdown/ifup...&quot;
    ifdown &quot;${WIFI_INTERFACE}&quot; &gt;/dev/null 2&gt;&amp;1 || true # Ignore errors if already down
    if ! ifup &quot;${WIFI_INTERFACE}&quot;; then
        log_warning &quot;ifup ${WIFI_INTERFACE} failed. Manual network reconfiguration might be needed. Check /etc/network/interfaces configuration.&quot;
    fi
else
    WPA_RESTARTED_MANUALLY_MSG=&quot;Could not determine how wpa_supplicant is managed for &#39;${WIFI_INTERFACE}&#39; (no specific systemd service or /etc/network/interfaces entry found). A manual restart of wpa_supplicant or a system reboot might be necessary to apply configuration changes.&quot;
    log_warning &quot;$WPA_RESTARTED_MANUALLY_MSG&quot;
fi
log_action &quot;Phase 2 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 3: Implementing wifish (System-Wide Installation)
log_action &quot;Phase 3: Installing wifish system-wide...&quot;
(
    cd &quot;$TEMP_WIFISH_DIR&quot;
    log_action &quot;Running install.sh from wifish repository (current directory: $(pwd))...&quot;
    if ! ./install.sh; then
        log_error &quot;wifish install.sh script failed. Please check output for errors.&quot;
        log_action &quot;Cleaning up temporary directory $TEMP_WIFISH_DIR...&quot;
        rm -rf &quot;$TEMP_WIFISH_DIR&quot;
        exit 1
    fi
)
log_action &quot;wifish&#39;s install.sh completed.&quot;
if [ -f &quot;$TEMP_WIFISH_DIR/test/test.sh&quot; ]; then
    log_action &quot;The wifish repository includes a test script: $TEMP_WIFISH_DIR/test/test.sh&quot;
    log_action &quot;You can explore running it manually from the &#39;$TEMP_WIFISH_DIR&#39; directory if desired (e.g., &#39;./test/test.sh&#39;).&quot;
    log_action &quot;Note: This test script typically mocks wpa_cli and runs its own checks.&quot;
fi
log_action &quot;Phase 3 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 4: Using wifish (Information for the user)
log_action &quot;Phase 4: Information on using wifish...&quot;
log_action &quot;wifish should now be installed and available as the &#39;wifish&#39; command.&quot;
log_action &quot;Example usage (as the non-root user, AFTER they have logged back in if their group membership was changed):&quot;
log_action &quot;  wifish list&quot;
log_action &quot;  wifish menu&quot;
log_action &quot;  wifish connect \&quot;Your_Network_SSID\&quot;&quot;
log_action &quot; &quot;
log_action &quot;If wifish needs to target a specific interface (detected/defaulted to &#39;${WIFI_INTERFACE}&#39;), use:&quot;
log_action &quot;  wifish -i ${WIFI_INTERFACE} menu&quot;
log_action &quot;or set the environment variable for the session:&quot;
log_action &quot;  export WPA_CLI_INTERFACE=${WIFI_INTERFACE}  (unset with &#39;unset WPA_CLI_INTERFACE&#39;)&quot;
log_action &quot; &quot;
log_action &quot;IMPORTANT NOTE ON IP ADDRESS:&quot;
log_action &quot;wifish handles the Wi-Fi connection (association). After connecting,&quot;
log_action &quot;your system still needs an IP address to access the internet.&quot;
log_action &quot;If not configured automatically, run as root (or with sudo):&quot;
log_action &quot;  dhclient ${WIFI_INTERFACE}&quot;
log_action &quot;(Or use another DHCP client like &#39;dhcpcd5&#39; if installed and preferred, or check &#39;systemd-networkd&#39; if active).&quot;
log_action &quot;Phase 4 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Final messages
log_action &quot;wifish setup script finished.&quot;
if [ -n &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot; ]; then
    log_action &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
fi
log_action &quot;$WPA_RESTARTED_MANUALLY_MSG&quot;
log_action &quot; &quot;
log_action &quot;To test, AFTER the designated user (&#39;${PRIMARY_USER:-your regular user}&#39;) has potentially logged out and back in:&quot;
log_action &quot;1. Open a new terminal as that user.&quot;
log_action &quot;2. Run &#39;wpa_cli status&#39;. It should show connection details without needing sudo.&quot;
log_action &quot;3. Run &#39;wifish list&#39; or &#39;wifish menu&#39;.&quot;
log_action &quot; &quot;
log_action &quot;The script used Wi-Fi interface &#39;${WIFI_INTERFACE}&#39;. If this was incorrect,&quot;
log_action &quot;use wifish with the &#39;-i &lt;your_actual_interface&gt;&#39; flag.&quot;

log_action &quot;Cleaning up temporary directory $TEMP_WIFISH_DIR...&quot;
rm -rf &quot;$TEMP_WIFISH_DIR&quot;

log_action &quot;Setup complete. Backup of wpa_supplicant config (if it existed) is at: $BACKUP_FILE&quot;
echo &quot;---------------------------------------------------------------------&quot;

exit 0" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">#!/bin/bash
# This script automates the setup of &#39;wifish&#39; on a Debian Bullseye system.
# Version 2: Improved backup, idempotency, and interface detection.
#
# IMPORTANT: This script makes certain assumptions:
# 1. It should be run as root.
# 2. It attempts to auto-detect the Wi-Fi interface. If detection fails or is
#    incorrect, the default &quot;wlan0&quot; is used, or you may need to edit the script
#    or use wifish with the -i flag.
# 3. The active wpa_supplicant configuration file is assumed to be
#    &#39;/etc/wpa_supplicant/wpa_supplicant.conf&#39;. If different, this needs to be changed.
# 4. It will attempt to identify the primary non-root user (UID 1000 or SUDO_USER)
#    to add to the &#39;netdev&#39; group.
#
# Review these assumptions and the script content before execution.

set -e # Exit immediately if a command exits with a non-zero status.

# --- Configuration ---
DEFAULT_WIFI_INTERFACE=&quot;wlan0&quot;
WPA_SUPPLICANT_CONF_FILE=&quot;/etc/wpa_supplicant/wpa_supplicant.conf&quot;
WIFI_INTERFACE=&quot;&quot; # Will be auto-detected or fall back to DEFAULT_WIFI_INTERFACE

# Determine the primary non-root user to add to the &#39;netdev&#39; group.
PRIMARY_USER=&quot;&quot;
if [ -n &quot;$SUDO_USER&quot; ] &amp;&amp; [ &quot;$SUDO_USER&quot; != &quot;root&quot; ]; then # Prefer SUDO_USER if available and not root
    PRIMARY_USER=&quot;$SUDO_USER&quot;
elif command -v id &gt;/dev/null &amp;&amp; id -u 1000 &amp;&gt;/dev/null; then # Try UID 1000
    PRIMARY_USER_UID1000=$(id -un 1000)
    if [ &quot;$PRIMARY_USER_UID1000&quot; != &quot;root&quot; ]; then # Ensure user 1000 is not root
        PRIMARY_USER=&quot;$PRIMARY_USER_UID1000&quot;
    fi
fi
# Fallback to parsing /etc/passwd if PRIMARY_USER is still not set
if [ -z &quot;$PRIMARY_USER&quot; ]; then
    PRIMARY_USER=$(awk -F: &#39;($3&gt;=1000 &amp;&amp; $1 != &quot;root&quot; &amp;&amp; $1 != &quot;&quot; &amp;&amp; $1 != &quot;nobody&quot;){print $1; exit}&#39; /etc/passwd)
fi


# --- Helper Functions ---
log_action() {
    echo &quot;[INFO] $1&quot;
}

log_warning() {
    echo &quot;[WARNING] $1&quot;
}

log_error() {
    echo &quot;[ERROR] $1&quot; &gt;&amp;2
}

# --- Wi-Fi Interface Detection ---
detect_wifi_interface() {
    log_action &quot;Attempting to auto-detect Wi-Fi interface...&quot;
    local detected_iface=&quot;&quot;
    # Try with &#39;iw dev&#39; first, common for Wi-Fi specific tools
    if command -v iw &gt;/dev/null; then
        detected_iface=$(iw dev | awk &#39;$1==&quot;Interface&quot;{print $2; exit}&#39;)
    fi

    # If &#39;iw dev&#39; didn&#39;t yield a result, try with &#39;ip link&#39; for wlan*
    if [ -z &quot;$detected_iface&quot; ] &amp;&amp; command -v ip &gt;/dev/null; then
        detected_iface=$(ip -o link show type wlan | awk -F&#39;: &#39; &#39;{print $2; exit}&#39; | awk &#39;{print $1}&#39;)
    fi

    # If still no result, try &#39;ip link&#39; for wlp* (common alternative naming)
    if [ -z &quot;$detected_iface&quot; ] &amp;&amp; command -v ip &gt;/dev/null; then
        detected_iface=$(ip -o link show | grep -Eo &#39;wlp[0-9]+s[0-9]+&#39; | head -n1)
    fi

    if [ -n &quot;$detected_iface&quot; ]; then
        WIFI_INTERFACE=&quot;$detected_iface&quot;
        log_action &quot;Detected Wi-Fi interface: $WIFI_INTERFACE. Using this.&quot;
    else
        WIFI_INTERFACE=&quot;$DEFAULT_WIFI_INTERFACE&quot;
        log_warning &quot;Could not auto-detect Wi-Fi interface. Using default: $WIFI_INTERFACE.&quot;
        log_warning &quot;If this is incorrect, review script or use wifish with the &#39;-i &lt;your_interface&gt;&#39; option.&quot;
    fi
}


# --- Main Script ---

# Check if running as root
if [ &quot;$(id -u)&quot; -ne 0 ]; then
  log_error &quot;This script must be run as root. Please use &#39;sudo $0&#39; or run as the root user.&quot;
  exit 1
fi

detect_wifi_interface # Call detection function

log_action &quot;Starting wifish setup script (v2)...&quot;
log_action &quot;Using Wi-Fi Interface: ${WIFI_INTERFACE}&quot;
log_action &quot;Targeting wpa_supplicant config: ${WPA_SUPPLICANT_CONF_FILE}&quot;

if [ -n &quot;$PRIMARY_USER&quot; ]; then
    log_action &quot;Primary non-root user identified for &#39;netdev&#39; group: ${PRIMARY_USER}&quot;
else
    log_warning &quot;Could not automatically detect a primary non-root user. Manual &#39;usermod -a -G netdev youruser&#39; might be required.&quot;
fi
echo &quot;---------------------------------------------------------------------&quot;

# Phase 1: Prerequisites and Getting wifish
log_action &quot;Phase 1: Installing prerequisites and getting wifish...&quot;
log_action &quot;Updating package lists (apt update)...&quot;
if ! apt update -qq; then
    log_error &quot;apt update failed. Please check your network connection and apt sources.&quot;
    exit 1
fi

log_action &quot;Installing gawk, dialog, git...&quot;
if ! apt install -y gawk dialog git; then
    log_error &quot;Failed to install required packages. Please check apt output for errors.&quot;
    exit 1
fi

TEMP_WIFISH_DIR=&quot;/tmp/bougyman-wifish_install_temp_$(date +%Y%m%d%H%M%S)&quot;
log_action &quot;Cloning wifish repository to $TEMP_WIFISH_DIR...&quot;
rm -rf &quot;$TEMP_WIFISH_DIR&quot; # Clean up if exists
if ! git clone https://github.com/bougyman/wifish.git &quot;$TEMP_WIFISH_DIR&quot;; then
    log_error &quot;Failed to clone wifish repository. Check internet connection and git.&quot;
    exit 1
fi

log_action &quot;Setting execute permissions for scripts in repository...&quot;
(
    cd &quot;$TEMP_WIFISH_DIR&quot;
    chmod +x wifish install.sh test/test.sh
    chmod +x sv/wpa_supplicant/run sv/wpa_supplicant/log/run
)
log_action &quot;Phase 1 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 2: Configuring wpa_supplicant
log_action &quot;Phase 2: Configuring wpa_supplicant...&quot;
log_action &quot;Ensuring wpa_supplicant configuration file directory exists: $(dirname &quot;$WPA_SUPPLICANT_CONF_FILE&quot;)&quot;
mkdir -p &quot;$(dirname &quot;$WPA_SUPPLICANT_CONF_FILE&quot;)&quot;

if [ ! -f &quot;$WPA_SUPPLICANT_CONF_FILE&quot; ]; then
    log_action &quot;Creating empty wpa_supplicant configuration file: $WPA_SUPPLICANT_CONF_FILE&quot;
    touch &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
fi

TIMESTAMP=$(date +%Y%m%d%H%M%S)
BACKUP_FILE=&quot;${WPA_SUPPLICANT_CONF_FILE}.${TIMESTAMP}.bak&quot;
log_action &quot;Backing up current $WPA_SUPPLICANT_CONF_FILE to $BACKUP_FILE...&quot;
if cp &quot;$WPA_SUPPLICANT_CONF_FILE&quot; &quot;$BACKUP_FILE&quot;; then
    log_action &quot;Backup successful: $BACKUP_FILE&quot;
else
    log_warning &quot;Failed to create backup of $WPA_SUPPLICANT_CONF_FILE. Proceeding with caution.&quot;
fi

log_action &quot;Configuring &#39;ctrl_interface&#39; and &#39;update_config&#39; in $WPA_SUPPLICANT_CONF_FILE for idempotency...&quot;

# Comment out any existing ctrl_interface lines to avoid conflicts
sed -i -E &#39;s/^[[:space:]]*ctrl_interface=.*$/#&amp; (old_ctrl_interface, commented by wifish_setup.sh)/&#39; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
# Add the correct ctrl_interface line if it&#39;s not already present (uncommented)
if ! grep -qFx &quot;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&quot; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;; then
    log_action &quot;Adding &#39;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&#39; to $WPA_SUPPLICANT_CONF_FILE.&quot;
    echo &quot;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&quot; &gt;&gt; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
else
    log_action &quot;&#39;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&#39; already present or re-added.&quot;
fi

# Comment out any existing update_config lines
sed -i -E &#39;s/^[[:space:]]*update_config=.*$/#&amp; (old_update_config, commented by wifish_setup.sh)/&#39; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
# Add the correct update_config line if it&#39;s not already present (uncommented)
if ! grep -qFx &quot;update_config=1&quot; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;; then
    log_action &quot;Adding &#39;update_config=1&#39; to $WPA_SUPPLICANT_CONF_FILE.&quot;
    echo &quot;update_config=1&quot; &gt;&gt; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
else
    log_action &quot;&#39;update_config=1&#39; already present or re-added.&quot;
fi

log_action &quot;Securing $WPA_SUPPLICANT_CONF_FILE permissions (chmod 600)...&quot;
chmod 600 &quot;$WPA_SUPPLICANT_CONF_FILE&quot;

USER_MODIFIED_LOGIN_MESSAGE=&quot;&quot;
if [ -n &quot;$PRIMARY_USER&quot; ]; then
    log_action &quot;Ensuring &#39;netdev&#39; group exists and adding user &#39;$PRIMARY_USER&#39; to it...&quot;
    if ! getent group netdev &gt;/dev/null; then
        log_action &quot;Group &#39;netdev&#39; does not exist. Creating it...&quot;
        if ! groupadd --system netdev; then # Use --system for system groups if appropriate
            log_warning &quot;Could not create group &#39;netdev&#39;. Manual creation (groupadd netdev) might be needed.&quot;
        else
            log_action &quot;Group &#39;netdev&#39; created.&quot;
        fi
    fi
    # Check if user is already in group to avoid unnecessary usermod message
    if ! groups &quot;$PRIMARY_USER&quot; | grep -q &#39;\bnetdev\b&#39;; then
        if ! usermod -a -G netdev &quot;$PRIMARY_USER&quot;; then
             log_warning &quot;Failed to add user &#39;$PRIMARY_USER&#39; to &#39;netdev&#39; group. Check permissions or do it manually.&quot;
        else
            USER_MODIFIED_LOGIN_MESSAGE=&quot;User &#39;$PRIMARY_USER&#39; has been added to the &#39;netdev&#39; group. IMPORTANT: &#39;$PRIMARY_USER&#39; MUST log out and log back in for this change to take effect.&quot;
            log_action &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
        fi
    else
        log_action &quot;User &#39;$PRIMARY_USER&#39; is already a member of the &#39;netdev&#39; group.&quot;
        USER_MODIFIED_LOGIN_MESSAGE=&quot;User &#39;$PRIMARY_USER&#39; is a member of &#39;netdev&#39;. If this membership is recent, a logout/login might still be needed for all services to recognize it.&quot;

    fi
else
    USER_MODIFIED_LOGIN_MESSAGE=&quot;IMPORTANT: Could not automatically determine a primary non-root user. Please manually add your regular user to the &#39;netdev&#39; group (e.g., &#39;sudo usermod -a -G netdev yourusername&#39;) and then log out and log back in.&quot;
    log_warning &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
fi

log_action &quot;Attempting to restart wpa_supplicant for interface &#39;$WIFI_INTERFACE&#39;...&quot;
WPA_RESTARTED_MANUALLY_MSG=&quot;If wpa_supplicant was restarted, it should pick up the new configuration. If not, a manual restart of wpa_supplicant or a system reboot might be necessary.&quot;

SERVICE_NAME=&quot;wpa_supplicant@${WIFI_INTERFACE}.service&quot;
SERVICE_EXISTS=false
if systemctl list-unit-files | grep -q &quot;^${SERVICE_NAME}&quot;; then # More precise grep
    SERVICE_EXISTS=true
fi

if [ &quot;$SERVICE_EXISTS&quot; = true ]; then
    log_action &quot;${SERVICE_NAME} found.&quot;
    if systemctl is-active --quiet &quot;${SERVICE_NAME}&quot;; then
        log_action &quot;Restarting ${SERVICE_NAME} via systemd...&quot;
        if ! systemctl restart &quot;${SERVICE_NAME}&quot;; then
            log_warning &quot;Failed to restart ${SERVICE_NAME}. Check &#39;systemctl status ${SERVICE_NAME}&#39; and &#39;journalctl -u ${SERVICE_NAME}&#39;.&quot;
        fi
    else
        log_action &quot;${SERVICE_NAME} exists but is not active. Attempting to enable and start it...&quot;
        if ! systemctl enable &quot;${SERVICE_NAME}&quot; --now; then
             log_warning &quot;Failed to enable and start ${SERVICE_NAME}. Check status and journal.&quot;
        fi
    fi
elif [ -f &quot;/etc/network/interfaces&quot; ] &amp;&amp; grep -q &quot;iface ${WIFI_INTERFACE} inet&quot; /etc/network/interfaces; then # More specific grep
    log_action &quot;Attempting to restart network interface ${WIFI_INTERFACE} via ifdown/ifup...&quot;
    ifdown &quot;${WIFI_INTERFACE}&quot; &gt;/dev/null 2&gt;&amp;1 || true # Ignore errors if already down
    if ! ifup &quot;${WIFI_INTERFACE}&quot;; then
        log_warning &quot;ifup ${WIFI_INTERFACE} failed. Manual network reconfiguration might be needed. Check /etc/network/interfaces configuration.&quot;
    fi
else
    WPA_RESTARTED_MANUALLY_MSG=&quot;Could not determine how wpa_supplicant is managed for &#39;${WIFI_INTERFACE}&#39; (no specific systemd service or /etc/network/interfaces entry found). A manual restart of wpa_supplicant or a system reboot might be necessary to apply configuration changes.&quot;
    log_warning &quot;$WPA_RESTARTED_MANUALLY_MSG&quot;
fi
log_action &quot;Phase 2 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 3: Implementing wifish (System-Wide Installation)
log_action &quot;Phase 3: Installing wifish system-wide...&quot;
(
    cd &quot;$TEMP_WIFISH_DIR&quot;
    log_action &quot;Running install.sh from wifish repository (current directory: $(pwd))...&quot;
    if ! ./install.sh; then
        log_error &quot;wifish install.sh script failed. Please check output for errors.&quot;
        log_action &quot;Cleaning up temporary directory $TEMP_WIFISH_DIR...&quot;
        rm -rf &quot;$TEMP_WIFISH_DIR&quot;
        exit 1
    fi
)
log_action &quot;wifish&#39;s install.sh completed.&quot;
if [ -f &quot;$TEMP_WIFISH_DIR/test/test.sh&quot; ]; then
    log_action &quot;The wifish repository includes a test script: $TEMP_WIFISH_DIR/test/test.sh&quot;
    log_action &quot;You can explore running it manually from the &#39;$TEMP_WIFISH_DIR&#39; directory if desired (e.g., &#39;./test/test.sh&#39;).&quot;
    log_action &quot;Note: This test script typically mocks wpa_cli and runs its own checks.&quot;
fi
log_action &quot;Phase 3 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 4: Using wifish (Information for the user)
log_action &quot;Phase 4: Information on using wifish...&quot;
log_action &quot;wifish should now be installed and available as the &#39;wifish&#39; command.&quot;
log_action &quot;Example usage (as the non-root user, AFTER they have logged back in if their group membership was changed):&quot;
log_action &quot;  wifish list&quot;
log_action &quot;  wifish menu&quot;
log_action &quot;  wifish connect \&quot;Your_Network_SSID\&quot;&quot;
log_action &quot; &quot;
log_action &quot;If wifish needs to target a specific interface (detected/defaulted to &#39;${WIFI_INTERFACE}&#39;), use:&quot;
log_action &quot;  wifish -i ${WIFI_INTERFACE} menu&quot;
log_action &quot;or set the environment variable for the session:&quot;
log_action &quot;  export WPA_CLI_INTERFACE=${WIFI_INTERFACE}  (unset with &#39;unset WPA_CLI_INTERFACE&#39;)&quot;
log_action &quot; &quot;
log_action &quot;IMPORTANT NOTE ON IP ADDRESS:&quot;
log_action &quot;wifish handles the Wi-Fi connection (association). After connecting,&quot;
log_action &quot;your system still needs an IP address to access the internet.&quot;
log_action &quot;If not configured automatically, run as root (or with sudo):&quot;
log_action &quot;  dhclient ${WIFI_INTERFACE}&quot;
log_action &quot;(Or use another DHCP client like &#39;dhcpcd5&#39; if installed and preferred, or check &#39;systemd-networkd&#39; if active).&quot;
log_action &quot;Phase 4 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Final messages
log_action &quot;wifish setup script finished.&quot;
if [ -n &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot; ]; then
    log_action &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
fi
log_action &quot;$WPA_RESTARTED_MANUALLY_MSG&quot;
log_action &quot; &quot;
log_action &quot;To test, AFTER the designated user (&#39;${PRIMARY_USER:-your regular user}&#39;) has potentially logged out and back in:&quot;
log_action &quot;1. Open a new terminal as that user.&quot;
log_action &quot;2. Run &#39;wpa_cli status&#39;. It should show connection details without needing sudo.&quot;
log_action &quot;3. Run &#39;wifish list&#39; or &#39;wifish menu&#39;.&quot;
log_action &quot; &quot;
log_action &quot;The script used Wi-Fi interface &#39;${WIFI_INTERFACE}&#39;. If this was incorrect,&quot;
log_action &quot;use wifish with the &#39;-i &lt;your_actual_interface&gt;&#39; flag.&quot;

log_action &quot;Cleaning up temporary directory $TEMP_WIFISH_DIR...&quot;
rm -rf &quot;$TEMP_WIFISH_DIR&quot;

log_action &quot;Setup complete. Backup of wpa_supplicant config (if it existed) is at: $BACKUP_FILE&quot;
echo &quot;---------------------------------------------------------------------&quot;

exit 0</code>
</section>]]></content><author><name></name></author><category term="scripts&gt;bash" /><category term="software&gt;linux" /></entry><entry><title type="html">czkawka dedup workflow</title><link href="https://ib.bsb.br/czkawka-dedup-workflow/" rel="alternate" type="text/html" title="czkawka dedup workflow" /><published>2025-05-17T00:00:00+00:00</published><updated>2025-05-17T22:29:37+00:00</updated><id>https://ib.bsb.br/czkawka-dedup-workflow</id><content type="html" xml:base="https://ib.bsb.br/czkawka-dedup-workflow/"><![CDATA[<ol>
  <li>
    <p><strong>!!! BACKUP YOUR ENTIRE TARGET DIRECTORY !!!</strong></p>
  </li>
  <li>
    <p>Carefully identify your <code class="language-plaintext highlighter-rouge">TARGET_DIR</code> (and any <code class="language-plaintext highlighter-rouge">-r</code> reference directories).</p>
  </li>
  <li><strong>Duplicate File Management (Czkawka CLI):</strong>
    <ul>
      <li><strong>Dry Run:</strong> <code class="language-plaintext highlighter-rouge">czkawka dup -d /your/target/dir [-r /your/reference/dir] -s HASH -D AEO --dry-run</code> (adjust <code class="language-plaintext highlighter-rouge">-m</code>, <code class="language-plaintext highlighter-rouge">-t</code>, <code class="language-plaintext highlighter-rouge">-D &lt;method&gt;</code> as needed).</li>
      <li>Review output carefully.</li>
      <li><strong>Actual Deletion:</strong>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>czkawka dup -d /mnt/my_data_drive/repository_to_clean -s HASH -t BLAKE3 -m 8192 -D AEO
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>Empty File Deletion (Czkawka CLI):</strong>
    <ul>
      <li>Find: <code class="language-plaintext highlighter-rouge">czkawka empty-files -d /your/target/dir</code></li>
      <li>Review.</li>
      <li>Delete: <code class="language-plaintext highlighter-rouge">czkawka empty-files -d /mnt/my_data_drive/repository_to_clean -D</code></li>
    </ul>
  </li>
  <li><strong>Empty Folder Deletion (Czkawka CLI):</strong>
    <ul>
      <li>Find: <code class="language-plaintext highlighter-rouge">czkawka empty-folders -d /your/target/dir</code></li>
      <li>Review.</li>
      <li>Delete: <code class="language-plaintext highlighter-rouge">czkawka empty-folders -d /mnt/my_data_drive/repository_to_clean -D</code></li>
    </ul>
  </li>
</ol>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">SBNB linux within Ubuntu Ansible Playbook</title><link href="https://ib.bsb.br/sbnb-ansible-ubuntu/" rel="alternate" type="text/html" title="SBNB linux within Ubuntu Ansible Playbook" /><published>2025-05-17T00:00:00+00:00</published><updated>2025-05-18T19:11:11+00:00</updated><id>https://ib.bsb.br/sbnb-ansible-ubuntu</id><content type="html" xml:base="https://ib.bsb.br/sbnb-ansible-ubuntu/"><![CDATA[<ol>
  <li><strong>Python 3 Installation:</strong> This playbook assumes the target Ubuntu 24.04 machine has Python 3 installed. Ansible is written in Python, and its modules are executed on the managed node (even if it’s <code class="language-plaintext highlighter-rouge">localhost</code>) using the system’s Python interpreter. Ubuntu 24.04 server typically includes Python 3 by default.</li>
  <li><strong>Provided Raw OS Image:</strong> The bootable raw operating system image (e.g., created by mkosi or similar tools) MUST be present at the location specified by <code class="language-plaintext highlighter-rouge">provided_raw_image_path</code> (default: <code class="language-plaintext highlighter-rouge">/root/IncusOS.raw</code>) on the target machine. This image should contain a complete filesystem ready to be booted.</li>
  <li><strong>Provided Incus Metadata Archive:</strong> An Incus metadata archive (typically <code class="language-plaintext highlighter-rouge">metadata.tar.xz</code>) MUST be present at the location specified by <code class="language-plaintext highlighter-rouge">provided_metadata_path</code> (default: <code class="language-plaintext highlighter-rouge">/root/metadata.tar.xz</code>) on the target machine. This file describes the image properties to Incus, such as architecture, creation date, and OS details, which are crucial for <code class="language-plaintext highlighter-rouge">incus image import</code>. (Alternatively, a <code class="language-plaintext highlighter-rouge">metadata.yaml</code> file can sometimes be used, depending on how the image and metadata were originally packaged, though this playbook assumes <code class="language-plaintext highlighter-rouge">metadata.tar.xz</code> as per the original context).</li>
</ol>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Incus metadata for SBNB Linux VM Image</span>
<span class="na">architecture</span><span class="pi">:</span> <span class="s2">"</span><span class="s">x86_64"</span>
<span class="na">creation_date</span><span class="pi">:</span> <span class="m">1747850911</span> <span class="c1"># Unix timestamp (seconds since epoch, UTC) for: Sat, 17 May 2025 12:48:31 GMT</span>
<span class="na">expiry_date</span><span class="pi">:</span> <span class="m">0</span> <span class="c1"># 0 means the image does not expire</span>
<span class="na">type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">virtual-machine"</span> <span class="c1"># Specifies that this image is for a virtual machine</span>

<span class="na">properties</span><span class="pi">:</span>
  <span class="na">os</span><span class="pi">:</span> <span class="s2">"</span><span class="s">SBNB</span><span class="nv"> </span><span class="s">Linux"</span>
  <span class="na">distribution</span><span class="pi">:</span> <span class="s2">"</span><span class="s">sbnb-linux"</span> <span class="c1"># Often lowercase version of os property</span>
  <span class="na">release</span><span class="pi">:</span> <span class="s2">"</span><span class="s">20250517"</span> <span class="c1"># Release identifier, YYYYMMDD format is common. Can be "rolling" or a specific version.</span>
  <span class="na">variant</span><span class="pi">:</span> <span class="s2">"</span><span class="s">default"</span>
  <span class="na">architecture</span><span class="pi">:</span> <span class="s2">"</span><span class="s">x86_64"</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">sbnb-linux-20250517"</span> <span class="c1"># A descriptive name: os-release-variant</span>
  <span class="na">description</span><span class="pi">:</span> <span class="s2">"</span><span class="s">SBNB</span><span class="nv"> </span><span class="s">Linux</span><span class="nv"> </span><span class="s">is</span><span class="nv"> </span><span class="s">a</span><span class="nv"> </span><span class="s">revolutionary</span><span class="nv"> </span><span class="s">minimalist</span><span class="nv"> </span><span class="s">Linux</span><span class="nv"> </span><span class="s">distribution</span><span class="nv"> </span><span class="s">designed</span><span class="nv"> </span><span class="s">to</span><span class="nv"> </span><span class="s">boot</span><span class="nv"> </span><span class="s">bare-metal</span><span class="nv"> </span><span class="s">servers</span><span class="nv"> </span><span class="s">and</span><span class="nv"> </span><span class="s">enable</span><span class="nv"> </span><span class="s">remote</span><span class="nv"> </span><span class="s">connections</span><span class="nv"> </span><span class="s">through</span><span class="nv"> </span><span class="s">fast</span><span class="nv"> </span><span class="s">tunnels.</span><span class="nv"> </span><span class="s">It</span><span class="nv"> </span><span class="s">is</span><span class="nv"> </span><span class="s">ideal</span><span class="nv"> </span><span class="s">for</span><span class="nv"> </span><span class="s">environments</span><span class="nv"> </span><span class="s">ranging</span><span class="nv"> </span><span class="s">from</span><span class="nv"> </span><span class="s">home</span><span class="nv"> </span><span class="s">labs</span><span class="nv"> </span><span class="s">to</span><span class="nv"> </span><span class="s">distributed</span><span class="nv"> </span><span class="s">data</span><span class="nv"> </span><span class="s">centers.</span><span class="nv"> </span><span class="s">SBNB</span><span class="nv"> </span><span class="s">Linux</span><span class="nv"> </span><span class="s">is</span><span class="nv"> </span><span class="s">simplified,</span><span class="nv"> </span><span class="s">automated,</span><span class="nv"> </span><span class="s">and</span><span class="nv"> </span><span class="s">resilient</span><span class="nv"> </span><span class="s">to</span><span class="nv"> </span><span class="s">power</span><span class="nv"> </span><span class="s">outages,</span><span class="nv"> </span><span class="s">supporting</span><span class="nv"> </span><span class="s">confidential</span><span class="nv"> </span><span class="s">computing</span><span class="nv"> </span><span class="s">to</span><span class="nv"> </span><span class="s">ensure</span><span class="nv"> </span><span class="s">secure</span><span class="nv"> </span><span class="s">operations</span><span class="nv"> </span><span class="s">in</span><span class="nv"> </span><span class="s">untrusted</span><span class="nv"> </span><span class="s">locations."</span>
  <span class="na">serial</span><span class="pi">:</span> <span class="s2">"</span><span class="s">"</span> <span class="c1"># Incus will populate this</span>
  <span class="na">requirements.secureboot</span><span class="pi">:</span> <span class="s2">"</span><span class="s">false"</span> <span class="c1"># SBNB Linux's build.yml makes sbnb.efi, implies UEFI. Assume no explicit Secure Boot support.</span>
  <span class="na">requirements.csm</span><span class="pi">:</span> <span class="s2">"</span><span class="s">false"</span> <span class="c1"># UEFI is expected.</span>

<span class="c1"># For VM images, the 'templates' section is typically not used for guest file templating</span>
<span class="c1"># as it is for containers. It can be omitted or left empty.</span>
<span class="na">templates</span><span class="pi">:</span> <span class="pi">{}</span>

<span class="c1"># The 'files' section lists files from the metadata archive itself that Incus processes.</span>
<span class="c1"># For basic VM images, this is usually empty.</span>
<span class="na">files</span><span class="pi">:</span> <span class="pi">[]</span>
</code></pre></div></div>

<h1 id="ansible-playbook-yml">ansible playbook yml</h1>

<section class="code-block-container" role="group" aria-label="Yaml Code Block" data-filename="yaml_code_block.yaml" data-code="- name: Setup Incus and Start VM from Provided Image
  hosts: localhost * Designed to be run directly on the target bare-metal Ubuntu 24.04.
                   * Change to your specific host or group name if targeting remote machines.
  connection: local * Uses the local connection plugin as we are targeting the machine Ansible is run on.
                    * Change to ‘ssh’ if connecting to remote hosts.
  become: yes * Most tasks require elevated (sudo) privileges for system-level changes like package
              * installation, Incus setup, and managing files in /root.
  vars:
    * Path on the target machine where the pre-existing raw OS image is located.
    * The /root/ path is used as an example assuming the image is placed there with root ownership.
    * Adjust if your image is located elsewhere and ensure appropriate read permissions for Ansible.
    provided_raw_image_path: “/root/IncusOS.raw”

    * Path on the target machine where the Incus metadata archive (e.g., metadata.tar.xz) is located.
    * This file is required by `incus image import` to understand the properties of the image.
    provided_metadata_path: “/root/metadata.tar.xz”

    * Temporary path where the .raw image will be converted to .qcow2 format before import.
    * This path should be writable by the user/process performing the qemu-img conversion.
    * Using /root/ here as other critical files are also assumed to be there.
    converted_qcow2_image_path: “/root/os-image.qcow2”

  tasks:
    - name: Check if provided IncusOS.raw image exists on target
      ansible.builtin.stat:
        path: “”
      register: raw_image_stat * Registers the result of the stat command into this variable.

    - name: Fail if IncusOS.raw image does not exist
      ansible.builtin.fail:
        msg: “Prerequisite failed: The raw image  does not exist on the target machine! Please ensure it is present before running this playbook.”
      when: not raw_image_stat.stat.exists * Fails if the ‘exists’ attribute from stat is false.

    - name: Check if provided metadata.tar.xz exists on target
      ansible.builtin.stat:
        path: “”
      register: metadata_stat

    - name: Fail if metadata.tar.xz does not exist
      ansible.builtin.fail:
        msg: “Prerequisite failed: The metadata archive  does not exist on the target machine! This is required for Incus image import.”
      when: not metadata_stat.stat.exists

    - name: Install essential system dependencies for Incus and image management
      ansible.builtin.apt:
        name:
          * binutils: Provides a collection of binary tools, including ‘ar’, ‘nm’, ‘objdump’, etc.
          *           While not directly used by every step here, it’s a common foundational package.
          - binutils
          * debian-archive-keyring: Contains GPG keys for verifying Debian/Ubuntu archive signatures.
          *                         Ensures authenticity of packages downloaded by apt.
          - debian-archive-keyring
          * qemu-utils: Provides the ‘qemu-img’ utility, which is essential for converting disk image
          *             formats (e.g., from .raw to .qcow2).
          - qemu-utils
          * Note: Incus itself, when installed via the Zabbly script, will pull in its own direct
          * dependencies such as liblxc, bridge-utils, etc.
        state: present * Ensures these packages are installed.
        update_cache: true * Runs ‘apt-get update’ before attempting to install packages.

    - name: Setup Incus (Install and Initialize)
      block: * Groups related tasks for better organization and error handling if needed.
        - name: Check if Incus command is already available
          ansible.builtin.command: incus —version
          register: incus_check * Stores the command’s output, including return code (rc).
          changed_when: false * This task doesn’t change system state, it’s a check.
          failed_when: false * Do not fail the playbook if incus is not found; we’ll install it.

        - name: Download and run Incus installation script (if Incus not found)
          ansible.builtin.shell: * Using shell module for commands involving pipes.
            * This command downloads the Zabbly script for Incus daily builds and executes it with sudo bash.
            * The Zabbly script typically adds a PPA/repository and installs the Incus package.
            cmd: “curl -s https://pkgs.zabbly.com/get/incus-daily | sudo bash”
            warn: false * Suppresses Ansible warnings about using shell for commands like curl.
          when: incus_check.rc != 0 * Only execute if the ‘incus —version’ command failed (rc != 0).

        - name: Check if Incus has been initialized
          ansible.builtin.command: incus profile show default
          * A successfully initialized Incus instance will have a ‘default’ profile.
          * If this command fails, it’s a strong indicator that `incus admin init` hasn’t completed.
          register: incus_init_check_before * Register before potential init
          changed_when: false
          failed_when: false * Don’t fail; use rc to decide if init is needed.

        - name: Initialize Incus daemon using auto configuration (if not already initialized)
          ansible.builtin.command: incus admin init —auto
          * The ‘—auto’ flag configures Incus with sensible defaults. This typically includes:
          * - A default storage pool (e.g., ZFS on a loop device if zfsutils-linux is installed,
          *   or a directory-based pool at /var/lib/incus/storage-pools/default otherwise).
          * - A default network bridge (e.g., `incusbr0`) providing NATed internet access to instances.
          * - Sets up the server for immediate use. For more granular control over storage or networking,
          *   `incus admin init` can be run interactively.
          when: incus_init_check_before.rc != 0 * Only run if the ‘incus profile show default’ command failed.
          changed_when: true * This command inherently changes system state if it runs the initialization.

        - name: Check if Incus socket exists after potential initialization
          ansible.builtin.stat:
            path: /var/lib/incus/unix.socket
          register: incus_socket_stat_after_init

        - name: Ensure Incus socket has 0666 permissions (replicating GHA behavior)
          ansible.builtin.file:
            path: /var/lib/incus/unix.socket
            mode: ‘0666’ * Sets read/write for owner, group, and others.
          * This task runs with ‘become: yes’ due to the play-level setting.
          * The original GHA workflow used ‘sudo chmod 666’.
          * Note: While `0666` permissions replicate the original workflow’s behavior, this is highly
          * permissive. In production environments, it’s strongly recommended to manage access to the
          * Incus socket via group membership (e.g., adding trusted users to the `incus` or `incus-admin`
          * group, which `incus admin init` might help configure or which can be done manually)
          * rather than world-writable permissions.
          when: incus_socket_stat_after_init.stat.exists and (incus_socket_stat_after_init.stat.issock or incus_socket_stat_after_init.stat.islnk)
          * The condition ensures the chmod is only attempted if the socket (or a symlink to it) exists.

    - name: Prepare and Import Provided Incus Image
      block:
        - name: Convert provided .raw image to .qcow2 format
          ansible.builtin.command:
            * qemu-img convert: Utility to convert disk images between formats.
            * -f raw: Specifies the source image format is raw.
            * -O qcow2: Specifies the output image format is QCOW2 (QEMU Copy On Write 2).
            * QCOW2 is a common format for VMs, supporting features like snapshots, thin provisioning,
            * and potentially better performance for some workloads compared to raw images.
            cmd: “qemu-img convert -f raw -O qcow2  ”
          changed_when: true * This command creates or overwrites the output qcow2 file.

        - name: Import converted qcow2 image into Incus
          ansible.builtin.command:
            * incus image import: Command to import an image into the Incus image store.
            * —alias incus-os: Assigns an alias ‘incus-os’ to the imported image for easy reference later.
            * : Path to the metadata archive (e.g., metadata.tar.xz).
            * : Path to the root filesystem image (now in qcow2 format).
            * If an image with the same alias or fingerprint already exists, this command might error
            * or behave differently based on Incus version. This playbook assumes a fresh import.
            cmd: “incus image import —alias incus-os  ”
          changed_when: true * This command adds a new image to the Incus store.

    - name: Create and Start Incus Virtual Machine
      block:
        - name: Create Incus VM ‘test-incus-os’ from the imported image
          ansible.builtin.command:
            cmd: &gt;
              incus create —quiet —vm incus-os test-incus-os
              -c security.secureboot=false
              -c limits.cpu=2
              -c limits.memory=2GiB
              -d root,size=50GiB
            * —quiet: Suppresses progress output.
            * —vm: Specifies that a virtual machine (not a container) should be created.
            * incus-os: Alias of the image to use (imported in the previous step).
            * test-incus-os: Name for the new VM instance.
            * Configuration options (-c key=value):
            *   security.secureboot=false: Disables Secure Boot for the VM. This is often necessary for
            *                            custom-built or generic images that may not have signed bootloaders
            *                            compatible with the host’s Secure Boot validation.
            *   limits.cpu=2: Allocates a maximum of 2 CPU cores to the VM.
            *   limits.memory=2GiB: Allocates 2 GiB of RAM to the VM.
            * Device options (-d device,properties):
            *   root,size=50GiB: Configures the root disk device, ensuring it has a size of 50 GiB.
            *                    Incus will typically expand the image’s filesystem to fill this size.
          changed_when: true * This command creates a new VM instance.

        - name: Add virtual TPM (Trusted Platform Module) device to ‘test-incus-os’
          ansible.builtin.command:
            cmd: incus config device add test-incus-os vtpm tpm
            * This adds a software-emulated TPM (vTPM) device named ‘vtpm’ of type ‘tpm’ to the VM.
            * A vTPM can be utilized by the guest OS for features like full-disk encryption (e.g., BitLocker, LUKS),
            * measured boot, or other security functionalities that rely on a TPM.
          changed_when: true * This command modifies the VM’s configuration.

        - name: Start the ‘test-incus-os’ VM
          ansible.builtin.command:
            cmd: incus start test-incus-os
          changed_when: true * This command changes the state of the VM to running.

        - name: Wait for the VM to become responsive
          ansible.builtin.command:
            * incus exec &lt;vm_name&gt; — &lt;command&gt;: Executes a command inside the specified VM.
            * /usr/bin/true: A simple command that does nothing and exits with status 0 if successful.
            * This is a common and lightweight way to check if the VM’s OS has booted sufficiently
            * to allow command execution via Incus.
            cmd: incus exec test-incus-os — /usr/bin/true
          register: vm_status * Stores the result of the command.
          until: vm_status.rc == 0 * Loop until the command executes successfully (return code 0).
          retries: 20 * Maximum number of retries.
          delay: 3 * Wait 3 seconds between retries (total wait time up to 60 seconds).
          changed_when: false * This task only checks status, doesn’t change system state.

        - name: Additional pause (1 minute) as per original workflow logic
          ansible.builtin.pause:
            minutes: 1
            * This pause might have been included in the original workflow to allow services
            * or applications inside the newly started VM to fully initialize before proceeding
            * with further tests or operations that might depend on those internal services.
          when: vm_status.rc == 0 * Only pause if the VM became responsive.

        - name: List Incus instances for final verification
          ansible.builtin.command: incus list
          register: incus_list_output
          changed_when: false

        - name: Display current Incus instances
          ansible.builtin.debug:
            var: incus_list_output.stdout_lines * Shows the standard output of ‘incus list’.

    - name: Cleanup temporary qcow2 image (optional step)
      ansible.builtin.file:
        path: “”
        state: absent * Ensures the file is removed.
      * This step is useful to free up disk space if the converted qcow2 image is no longer needed
      * after being imported into Incus’s storage pool (Incus makes its own copy).
      when: true * Set to ‘false’ or remove this task if you want to keep the qcow2 image for debugging.
      tags:
        - cleanup * Allows skipping this task with —skip-tags cleanup or running only it with —tags cleanup." data-download-link="" data-download-label="Download Yaml">
  <code class="language-yaml">- name: Setup Incus and Start VM from Provided Image
  hosts: localhost * Designed to be run directly on the target bare-metal Ubuntu 24.04.
                   * Change to your specific host or group name if targeting remote machines.
  connection: local * Uses the local connection plugin as we are targeting the machine Ansible is run on.
                    * Change to ‘ssh’ if connecting to remote hosts.
  become: yes * Most tasks require elevated (sudo) privileges for system-level changes like package
              * installation, Incus setup, and managing files in /root.
  vars:
    * Path on the target machine where the pre-existing raw OS image is located.
    * The /root/ path is used as an example assuming the image is placed there with root ownership.
    * Adjust if your image is located elsewhere and ensure appropriate read permissions for Ansible.
    provided_raw_image_path: “/root/IncusOS.raw”

    * Path on the target machine where the Incus metadata archive (e.g., metadata.tar.xz) is located.
    * This file is required by `incus image import` to understand the properties of the image.
    provided_metadata_path: “/root/metadata.tar.xz”

    * Temporary path where the .raw image will be converted to .qcow2 format before import.
    * This path should be writable by the user/process performing the qemu-img conversion.
    * Using /root/ here as other critical files are also assumed to be there.
    converted_qcow2_image_path: “/root/os-image.qcow2”

  tasks:
    - name: Check if provided IncusOS.raw image exists on target
      ansible.builtin.stat:
        path: “”
      register: raw_image_stat * Registers the result of the stat command into this variable.

    - name: Fail if IncusOS.raw image does not exist
      ansible.builtin.fail:
        msg: “Prerequisite failed: The raw image  does not exist on the target machine! Please ensure it is present before running this playbook.”
      when: not raw_image_stat.stat.exists * Fails if the ‘exists’ attribute from stat is false.

    - name: Check if provided metadata.tar.xz exists on target
      ansible.builtin.stat:
        path: “”
      register: metadata_stat

    - name: Fail if metadata.tar.xz does not exist
      ansible.builtin.fail:
        msg: “Prerequisite failed: The metadata archive  does not exist on the target machine! This is required for Incus image import.”
      when: not metadata_stat.stat.exists

    - name: Install essential system dependencies for Incus and image management
      ansible.builtin.apt:
        name:
          * binutils: Provides a collection of binary tools, including ‘ar’, ‘nm’, ‘objdump’, etc.
          *           While not directly used by every step here, it’s a common foundational package.
          - binutils
          * debian-archive-keyring: Contains GPG keys for verifying Debian/Ubuntu archive signatures.
          *                         Ensures authenticity of packages downloaded by apt.
          - debian-archive-keyring
          * qemu-utils: Provides the ‘qemu-img’ utility, which is essential for converting disk image
          *             formats (e.g., from .raw to .qcow2).
          - qemu-utils
          * Note: Incus itself, when installed via the Zabbly script, will pull in its own direct
          * dependencies such as liblxc, bridge-utils, etc.
        state: present * Ensures these packages are installed.
        update_cache: true * Runs ‘apt-get update’ before attempting to install packages.

    - name: Setup Incus (Install and Initialize)
      block: * Groups related tasks for better organization and error handling if needed.
        - name: Check if Incus command is already available
          ansible.builtin.command: incus —version
          register: incus_check * Stores the command’s output, including return code (rc).
          changed_when: false * This task doesn’t change system state, it’s a check.
          failed_when: false * Do not fail the playbook if incus is not found; we’ll install it.

        - name: Download and run Incus installation script (if Incus not found)
          ansible.builtin.shell: * Using shell module for commands involving pipes.
            * This command downloads the Zabbly script for Incus daily builds and executes it with sudo bash.
            * The Zabbly script typically adds a PPA/repository and installs the Incus package.
            cmd: “curl -s https://pkgs.zabbly.com/get/incus-daily | sudo bash”
            warn: false * Suppresses Ansible warnings about using shell for commands like curl.
          when: incus_check.rc != 0 * Only execute if the ‘incus —version’ command failed (rc != 0).

        - name: Check if Incus has been initialized
          ansible.builtin.command: incus profile show default
          * A successfully initialized Incus instance will have a ‘default’ profile.
          * If this command fails, it’s a strong indicator that `incus admin init` hasn’t completed.
          register: incus_init_check_before * Register before potential init
          changed_when: false
          failed_when: false * Don’t fail; use rc to decide if init is needed.

        - name: Initialize Incus daemon using auto configuration (if not already initialized)
          ansible.builtin.command: incus admin init —auto
          * The ‘—auto’ flag configures Incus with sensible defaults. This typically includes:
          * - A default storage pool (e.g., ZFS on a loop device if zfsutils-linux is installed,
          *   or a directory-based pool at /var/lib/incus/storage-pools/default otherwise).
          * - A default network bridge (e.g., `incusbr0`) providing NATed internet access to instances.
          * - Sets up the server for immediate use. For more granular control over storage or networking,
          *   `incus admin init` can be run interactively.
          when: incus_init_check_before.rc != 0 * Only run if the ‘incus profile show default’ command failed.
          changed_when: true * This command inherently changes system state if it runs the initialization.

        - name: Check if Incus socket exists after potential initialization
          ansible.builtin.stat:
            path: /var/lib/incus/unix.socket
          register: incus_socket_stat_after_init

        - name: Ensure Incus socket has 0666 permissions (replicating GHA behavior)
          ansible.builtin.file:
            path: /var/lib/incus/unix.socket
            mode: ‘0666’ * Sets read/write for owner, group, and others.
          * This task runs with ‘become: yes’ due to the play-level setting.
          * The original GHA workflow used ‘sudo chmod 666’.
          * Note: While `0666` permissions replicate the original workflow’s behavior, this is highly
          * permissive. In production environments, it’s strongly recommended to manage access to the
          * Incus socket via group membership (e.g., adding trusted users to the `incus` or `incus-admin`
          * group, which `incus admin init` might help configure or which can be done manually)
          * rather than world-writable permissions.
          when: incus_socket_stat_after_init.stat.exists and (incus_socket_stat_after_init.stat.issock or incus_socket_stat_after_init.stat.islnk)
          * The condition ensures the chmod is only attempted if the socket (or a symlink to it) exists.

    - name: Prepare and Import Provided Incus Image
      block:
        - name: Convert provided .raw image to .qcow2 format
          ansible.builtin.command:
            * qemu-img convert: Utility to convert disk images between formats.
            * -f raw: Specifies the source image format is raw.
            * -O qcow2: Specifies the output image format is QCOW2 (QEMU Copy On Write 2).
            * QCOW2 is a common format for VMs, supporting features like snapshots, thin provisioning,
            * and potentially better performance for some workloads compared to raw images.
            cmd: “qemu-img convert -f raw -O qcow2  ”
          changed_when: true * This command creates or overwrites the output qcow2 file.

        - name: Import converted qcow2 image into Incus
          ansible.builtin.command:
            * incus image import: Command to import an image into the Incus image store.
            * —alias incus-os: Assigns an alias ‘incus-os’ to the imported image for easy reference later.
            * : Path to the metadata archive (e.g., metadata.tar.xz).
            * : Path to the root filesystem image (now in qcow2 format).
            * If an image with the same alias or fingerprint already exists, this command might error
            * or behave differently based on Incus version. This playbook assumes a fresh import.
            cmd: “incus image import —alias incus-os  ”
          changed_when: true * This command adds a new image to the Incus store.

    - name: Create and Start Incus Virtual Machine
      block:
        - name: Create Incus VM ‘test-incus-os’ from the imported image
          ansible.builtin.command:
            cmd: &gt;
              incus create —quiet —vm incus-os test-incus-os
              -c security.secureboot=false
              -c limits.cpu=2
              -c limits.memory=2GiB
              -d root,size=50GiB
            * —quiet: Suppresses progress output.
            * —vm: Specifies that a virtual machine (not a container) should be created.
            * incus-os: Alias of the image to use (imported in the previous step).
            * test-incus-os: Name for the new VM instance.
            * Configuration options (-c key=value):
            *   security.secureboot=false: Disables Secure Boot for the VM. This is often necessary for
            *                            custom-built or generic images that may not have signed bootloaders
            *                            compatible with the host’s Secure Boot validation.
            *   limits.cpu=2: Allocates a maximum of 2 CPU cores to the VM.
            *   limits.memory=2GiB: Allocates 2 GiB of RAM to the VM.
            * Device options (-d device,properties):
            *   root,size=50GiB: Configures the root disk device, ensuring it has a size of 50 GiB.
            *                    Incus will typically expand the image’s filesystem to fill this size.
          changed_when: true * This command creates a new VM instance.

        - name: Add virtual TPM (Trusted Platform Module) device to ‘test-incus-os’
          ansible.builtin.command:
            cmd: incus config device add test-incus-os vtpm tpm
            * This adds a software-emulated TPM (vTPM) device named ‘vtpm’ of type ‘tpm’ to the VM.
            * A vTPM can be utilized by the guest OS for features like full-disk encryption (e.g., BitLocker, LUKS),
            * measured boot, or other security functionalities that rely on a TPM.
          changed_when: true * This command modifies the VM’s configuration.

        - name: Start the ‘test-incus-os’ VM
          ansible.builtin.command:
            cmd: incus start test-incus-os
          changed_when: true * This command changes the state of the VM to running.

        - name: Wait for the VM to become responsive
          ansible.builtin.command:
            * incus exec &lt;vm_name&gt; — &lt;command&gt;: Executes a command inside the specified VM.
            * /usr/bin/true: A simple command that does nothing and exits with status 0 if successful.
            * This is a common and lightweight way to check if the VM’s OS has booted sufficiently
            * to allow command execution via Incus.
            cmd: incus exec test-incus-os — /usr/bin/true
          register: vm_status * Stores the result of the command.
          until: vm_status.rc == 0 * Loop until the command executes successfully (return code 0).
          retries: 20 * Maximum number of retries.
          delay: 3 * Wait 3 seconds between retries (total wait time up to 60 seconds).
          changed_when: false * This task only checks status, doesn’t change system state.

        - name: Additional pause (1 minute) as per original workflow logic
          ansible.builtin.pause:
            minutes: 1
            * This pause might have been included in the original workflow to allow services
            * or applications inside the newly started VM to fully initialize before proceeding
            * with further tests or operations that might depend on those internal services.
          when: vm_status.rc == 0 * Only pause if the VM became responsive.

        - name: List Incus instances for final verification
          ansible.builtin.command: incus list
          register: incus_list_output
          changed_when: false

        - name: Display current Incus instances
          ansible.builtin.debug:
            var: incus_list_output.stdout_lines * Shows the standard output of ‘incus list’.

    - name: Cleanup temporary qcow2 image (optional step)
      ansible.builtin.file:
        path: “”
        state: absent * Ensures the file is removed.
      * This step is useful to free up disk space if the converted qcow2 image is no longer needed
      * after being imported into Incus’s storage pool (Incus makes its own copy).
      when: true * Set to ‘false’ or remove this task if you want to keep the qcow2 image for debugging.
      tags:
        - cleanup * Allows skipping this task with —skip-tags cleanup or running only it with —tags cleanup.</code>
</section>

<p><strong>Explanation of How to Use (Expanded):</strong></p>

<ol>
  <li>
    <p><strong>Save the Playbook:</strong> Save the content above into a file named, for example, <code class="language-plaintext highlighter-rouge">setup_incus_from_provided_image.yml</code> on the machine you’ll use to run Ansible (this could be the target Ubuntu 24.04 machine itself if <code class="language-plaintext highlighter-rouge">connection: local</code>).</p>
  </li>
  <li><strong>Prepare Prerequisites on the Target Ubuntu 24.04 Machine:</strong>
    <ul>
      <li><strong>Python 3:</strong> Ensure Python 3 is installed. For Ubuntu 24.04 Server, it’s typically present. You can check with <code class="language-plaintext highlighter-rouge">python3 —version</code>. If missing (unlikely for a server OS), install it: <code class="language-plaintext highlighter-rouge">sudo apt update &amp;&amp; sudo apt install python3</code>.</li>
      <li><strong>Raw OS Image:</strong> Place your bootable raw OS image (e.g., <code class="language-plaintext highlighter-rouge">IncusOS.raw</code>) at the exact path specified in the <code class="language-plaintext highlighter-rouge">provided_raw_image_path</code> variable (default: <code class="language-plaintext highlighter-rouge">/root/IncusOS.raw</code>). Ensure this file is readable by the root user (as the playbook uses <code class="language-plaintext highlighter-rouge">become: yes</code>).</li>
      <li><strong>Incus Metadata Archive:</strong> Place the corresponding Incus metadata archive (e.g., <code class="language-plaintext highlighter-rouge">metadata.tar.xz</code> or potentially a <code class="language-plaintext highlighter-rouge">metadata.yaml</code>) at the path specified in <code class="language-plaintext highlighter-rouge">provided_metadata_path</code> (default: <code class="language-plaintext highlighter-rouge">/root/metadata.tar.xz</code>). This file is crucial for Incus to understand the image’s properties.</li>
    </ul>
  </li>
  <li><strong>Install Ansible on the Control Machine (or Target if running locally):</strong>
    <ul>
      <li>If you are running the playbook directly on the target Ubuntu 24.04 machine (using <code class="language-plaintext highlighter-rouge">connection: local</code>), install Ansible on it:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> ansible
</code></pre></div>        </div>
      </li>
      <li>If you are running Ansible from a separate control node to manage the Ubuntu 24.04 machine remotely, ensure Ansible is installed on your control node.</li>
    </ul>
  </li>
  <li><strong>Run the Ansible Playbook:</strong>
    <ul>
      <li><strong>Locally on the Target Machine:</strong>
Navigate to the directory where you saved <code class="language-plaintext highlighter-rouge">setup_incus_from_provided_image.yml</code> and run:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ansible-playbook setup_incus_from_provided_image.yml
</code></pre></div>        </div>
        <p>You generally need <code class="language-plaintext highlighter-rouge">sudo</code> when using <code class="language-plaintext highlighter-rouge">connection: local</code> and <code class="language-plaintext highlighter-rouge">become: yes</code> because:
a.  The playbook performs privileged operations (package installs, service management, file operations in <code class="language-plaintext highlighter-rouge">/root</code>).
b.  <code class="language-plaintext highlighter-rouge">connection: local</code> means Ansible uses the privileges of the user executing <code class="language-plaintext highlighter-rouge">ansible-playbook</code>. If that user is not root, <code class="language-plaintext highlighter-rouge">become</code> will attempt to use <code class="language-plaintext highlighter-rouge">sudo</code> to elevate privileges.
If your regular user has passwordless <code class="language-plaintext highlighter-rouge">sudo</code> configured for all necessary commands, you might be able to run it without the <code class="language-plaintext highlighter-rouge">sudo</code> prefix, and Ansible’s <code class="language-plaintext highlighter-rouge">become</code> mechanism will handle the elevation.</p>
      </li>
      <li><strong>From a Remote Ansible Control Node:</strong>
If you’ve configured <code class="language-plaintext highlighter-rouge">hosts</code> in the playbook to point to your remote Ubuntu 24.04 machine (e.g., <code class="language-plaintext highlighter-rouge">myubuntuserver</code>) and have an Ansible inventory file (<code class="language-plaintext highlighter-rouge">your_inventory_file</code>) set up with SSH access to the target:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook <span class="nt">-i</span> your_inventory_file setup_incus_from_provided_image.yml
</code></pre></div>        </div>
        <p>Ensure the user specified in your inventory for the target host (e.g., <code class="language-plaintext highlighter-rouge">ansible_user=your_ssh_user</code>) has <code class="language-plaintext highlighter-rouge">sudo</code> privileges, as <code class="language-plaintext highlighter-rouge">become: yes</code> will be used on the remote host.</p>
      </li>
    </ul>
  </li>
  <li><strong>Useful Ansible Playbook Flags:</strong>
    <ul>
      <li><strong>Dry Run (Check Mode):</strong> To see what changes Ansible <em>would</em> make without actually executing them. This is highly recommended before running a new playbook for the first time.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ansible-playbook setup_incus_from_provided_image.yml —check
</code></pre></div>        </div>
      </li>
      <li><strong>Show Differences:</strong> To see the exact changes that would be made to files (useful with or without <code class="language-plaintext highlighter-rouge">—check</code>). This helps understand what content is being modified.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ansible-playbook setup_incus_from_provided_image.yml —diff
</code></pre></div>        </div>
      </li>
      <li><strong>Verbosity:</strong> Increase verbosity for more detailed output, which can be helpful for troubleshooting (e.g., <code class="language-plaintext highlighter-rouge">-v</code> for basic, <code class="language-plaintext highlighter-rouge">-vv</code> for more detail, <code class="language-plaintext highlighter-rouge">-vvv</code> for connection debug, <code class="language-plaintext highlighter-rouge">-vvvv</code> for even more).
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ansible-playbook setup_incus_from_provided_image.yml <span class="nt">-vv</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Tags:</strong> To run or skip specific parts of the playbook. For example, to skip the cleanup task:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ansible-playbook setup_incus_from_provided_image.yml —skip-tags cleanup
</code></pre></div>        </div>
        <p>Or to run <em>only</em> the cleanup task (assuming other tasks have completed successfully before):</p>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ansible-playbook setup_incus_from_provided_image.yml —tags cleanup
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>Troubleshooting Incus:</strong>
    <ul>
      <li>If you encounter issues with Incus services not starting correctly or VM misbehavior, checking the Incus daemon logs is a good first step:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>journalctl <span class="nt">-u</span> incus.service <span class="nt">-n</span> 100 —no-pager
</code></pre></div>        </div>
      </li>
      <li>For issues specific to an instance after it’s created:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>incus info test-incus-os —show-log
</code></pre></div>        </div>
      </li>
      <li>You can also try to access the console of the VM:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>incus console test-incus-os —type<span class="o">=</span>pty
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<p><strong>Summary of Key Changes and Simplifications (Expanded):</strong></p>

<ul>
  <li><strong>No Build Process:</strong> The most significant characteristic of this playbook is the complete removal of tasks related to source code checkout (Git), Go language setup, <code class="language-plaintext highlighter-rouge">pipx</code> and <code class="language-plaintext highlighter-rouge">mkosi</code> installation, and the <code class="language-plaintext highlighter-rouge">make</code> command for building the OS image. This is because the core assumption now is that <code class="language-plaintext highlighter-rouge">IncusOS.raw</code> is pre-built and provided directly on the target system.</li>
  <li><strong>Direct Image Usage:</strong> The playbook now directly consumes the user-provided <code class="language-plaintext highlighter-rouge">/root/IncusOS.raw</code> and an associated <code class="language-plaintext highlighter-rouge">/root/metadata.tar.xz</code> (or <code class="language-plaintext highlighter-rouge">metadata.yaml</code> if adapted). This makes the playbook much simpler if an image generation pipeline already exists separately.</li>
  <li><strong>Simplified Dependencies:</strong> The list of system packages installed via <code class="language-plaintext highlighter-rouge">apt</code> is reduced to only those essential for image conversion (<code class="language-plaintext highlighter-rouge">qemu-utils</code>) and general system health/repository access (<code class="language-plaintext highlighter-rouge">binutils</code>, <code class="language-plaintext highlighter-rouge">debian-archive-keyring</code>), as Incus’s own installation script (from Zabbly) handles its specific dependencies like <code class="language-plaintext highlighter-rouge">liblxc1</code>, <code class="language-plaintext highlighter-rouge">squashfs-tools</code>, etc.</li>
  <li><strong>Focus on Incus Setup and VM Lifecycle:</strong> The playbook’s primary operational focus shifts to robustly installing and initializing Incus, converting the provided raw image to the <code class="language-plaintext highlighter-rouge">qcow2</code> format (which offers benefits like thin provisioning and snapshot capabilities), importing this image into the Incus image store with a clear alias, and then proceeding with VM creation, specific configuration (like adding a vTPM for enhanced guest security), startup, and responsiveness checks.</li>
  <li><strong>Prerequisite Validation:</strong> Explicit <code class="language-plaintext highlighter-rouge">ansible.builtin.stat</code> tasks are included at the beginning to verify the existence of the crucial <code class="language-plaintext highlighter-rouge">IncusOS.raw</code> and <code class="language-plaintext highlighter-rouge">metadata.tar.xz</code> files. The playbook will fail early with a clear message if these prerequisites are not met, which significantly improves usability and aids in rapid error diagnosis.</li>
  <li><strong>Variable Simplification:</strong> The <code class="language-plaintext highlighter-rouge">vars</code> section is streamlined, primarily defining the paths to the pre-existing image and metadata files, making it easy for the user to configure these critical inputs.</li>
  <li><strong>Optional Cleanup:</strong> A tagged task for cleaning up the intermediate <code class="language-plaintext highlighter-rouge">os-image.qcow2</code> file is included. This is good practice as Incus makes its own copy of the image in its storage pool, so the temporary qcow2 file may no longer be needed and can be removed to save disk space. The use of tags gives the user fine-grained control over this step.</li>
</ul>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">TestDisk/PhotoRec to Recover Partitions/Files</title><link href="https://ib.bsb.br/testdiskphotorec-to-recover-partitionsfiles/" rel="alternate" type="text/html" title="TestDisk/PhotoRec to Recover Partitions/Files" /><published>2025-05-17T00:00:00+00:00</published><updated>2025-05-17T22:10:18+00:00</updated><id>https://ib.bsb.br/testdiskphotorec-to-recover-partitionsfiles</id><content type="html" xml:base="https://ib.bsb.br/testdiskphotorec-to-recover-partitionsfiles/"><![CDATA[<p><strong>Critically Important Disclaimers:</strong></p>

<ul>
  <li><strong>Data Loss Risk:</strong> Any operation on a damaged or corrupted drive carries an inherent risk of further data loss, especially if incorrect options are chosen. Proceed with extreme caution.</li>
  <li><strong>No Guarantee of Success:</strong> Data recovery is not guaranteed. Success depends on the nature and extent of the corruption and the physical health of the SD card.</li>
  <li><strong>Patience is Key:</strong> Some scanning processes, particularly <code class="language-plaintext highlighter-rouge">Deeper Search</code>, can take many hours, or even days for very large or slow/damaged drives. Do not interrupt them unnecessarily.</li>
  <li><strong>Command-Line Familiarity:</strong> This guide assumes a basic comfort level with the Linux command-line interface.</li>
</ul>

<hr />

<h3 id="phase-1-preparation--utmost-safety"><strong>Phase 1: Preparation &amp; Utmost Safety</strong></h3>

<p><strong>1. Install TestDisk:</strong>
If TestDisk is not already installed on your system, open a terminal and install it.</p>
<ul>
  <li>For Debian/Ubuntu-based systems (like Linaro):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>testdisk
</code></pre></div>    </div>
  </li>
  <li>For Fedora/CentOS/RHEL-based systems:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>dnf <span class="nb">install </span>testdisk
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>2. CRITICAL - Create a Disk Image (Highly Recommended):</strong>
Operating directly on a potentially failing or corrupted SD card can worsen the situation or lead to irreversible data loss. The <strong>safest approach</strong> is to create a bit-by-bit image of the SD card on another healthy storage device. You will then run TestDisk on this image file.</p>

<ul>
  <li><strong>Storage Requirement:</strong> You need enough free space on another drive (e.g., your <code class="language-plaintext highlighter-rouge">/mnt/mSATA</code> drive) to store the image. The image file will be the same size as your SD card (approximately 477.5 GiB).</li>
  <li><strong>Identify Your SD Card:</strong> Your logs confirm it as <code class="language-plaintext highlighter-rouge">/dev/sdb</code>. <strong>Double-check this device name (<code class="language-plaintext highlighter-rouge">lsblk</code> can help confirm) before proceeding to avoid imaging the wrong drive!</strong></li>
  <li><strong>Unmount SD Card Partitions (if any are mounted):</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>umount /dev/sdb<span class="k">*</span>
</code></pre></div>    </div>
    <p>(The <code class="language-plaintext highlighter-rouge">*</code> acts as a wildcard for any partitions like <code class="language-plaintext highlighter-rouge">/dev/sdb1</code>).</p>
  </li>
  <li><strong>Create the Image using <code class="language-plaintext highlighter-rouge">dd</code>:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo dd </span><span class="k">if</span><span class="o">=</span>/dev/sdb <span class="nv">of</span><span class="o">=</span>/mnt/mSATA/sdb_image.img <span class="nv">bs</span><span class="o">=</span>4M <span class="nv">status</span><span class="o">=</span>progress <span class="nv">conv</span><span class="o">=</span>noerror,sync
</code></pre></div>    </div>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">if=/dev/sdb</code>: <strong>Input File</strong> (your SD card). Verify this is correct!</li>
      <li><code class="language-plaintext highlighter-rouge">of=/mnt/mSATA/sdb_image.img</code>: <strong>Output File</strong> (the image). Choose a path and filename that makes sense for you.</li>
      <li><code class="language-plaintext highlighter-rouge">bs=4M</code>: Sets the block size, which can improve copying speed.</li>
      <li><code class="language-plaintext highlighter-rouge">status=progress</code>: Shows the progress of the <code class="language-plaintext highlighter-rouge">dd</code> command.</li>
      <li><code class="language-plaintext highlighter-rouge">conv=noerror,sync</code>: This is crucial. <code class="language-plaintext highlighter-rouge">noerror</code> tells <code class="language-plaintext highlighter-rouge">dd</code> to continue if it encounters read errors on the source drive. <code class="language-plaintext highlighter-rouge">sync</code> fills input blocks with zeros if there were read errors, ensuring the output image maintains correct offsets.</li>
      <li>This process will take a significant amount of time. Be patient.</li>
    </ul>

    <p><strong>If you successfully create and use an image, all subsequent <code class="language-plaintext highlighter-rouge">testdisk</code> commands in this guide should target the image file (e.g., <code class="language-plaintext highlighter-rouge">sudo testdisk /mnt/mSATA/sdb_image.img</code>) instead of <code class="language-plaintext highlighter-rouge">/dev/sdb</code>.</strong></p>
  </li>
</ul>

<p><strong>3. If Not Using an Image (Significantly Riskier):</strong>
If you choose to operate directly on <code class="language-plaintext highlighter-rouge">/dev/sdb</code> (not recommended):</p>
<ul>
  <li>Ensure all its partitions are unmounted: <code class="language-plaintext highlighter-rouge">sudo umount /dev/sdb*</code></li>
  <li>Be aware that any mistake or further drive degradation could lead to permanent data loss.</li>
</ul>

<hr />

<h3 id="phase-2-launching-testdisk--initial-setup"><strong>Phase 2: Launching TestDisk &amp; Initial Setup</strong></h3>

<p><strong>1. Launch TestDisk:</strong>
Open your terminal.</p>
<ul>
  <li>If working on the <strong>SD card directly</strong> (riskier):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>testdisk
</code></pre></div>    </div>
  </li>
  <li>If working on the <strong>disk image</strong> (recommended):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>testdisk /mnt/mSATA/sdb_image.img
</code></pre></div>    </div>
  </li>
</ul>

<p><strong>2. Log File Creation:</strong>
TestDisk will first ask about log file creation.</p>
<ul>
  <li>Use the arrow keys to select <code class="language-plaintext highlighter-rouge">[ Create ]</code> (to create a new log file). This is generally recommended for troubleshooting.</li>
  <li>Press <code class="language-plaintext highlighter-rouge">Enter</code> to confirm.</li>
</ul>

<p><strong>3. Disk Selection:</strong>
A list of detected storage media will be displayed.</p>
<ul>
  <li>Use the <code class="language-plaintext highlighter-rouge">Up/Down</code> arrow keys to navigate and highlight your target:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">/dev/sdb</code> (should show its size, approx. 477 GiB, and model “Storage Device”).</li>
      <li>Or, if using an image, <code class="language-plaintext highlighter-rouge">/mnt/mSATA/sdb_image.img</code>.</li>
    </ul>
  </li>
  <li><strong>Verify your selection carefully.</strong></li>
  <li>Ensure <code class="language-plaintext highlighter-rouge">[ Proceed ]</code> is highlighted at the bottom and press <code class="language-plaintext highlighter-rouge">Enter</code>.</li>
</ul>

<p><strong>4. Partition Table Type Selection:</strong>
TestDisk will attempt to auto-detect the partition table type. Your <code class="language-plaintext highlighter-rouge">fdisk -l</code> log indicated <code class="language-plaintext highlighter-rouge">Disklabel type: dos</code>, which means an MBR (Master Boot Record) partition table.</p>
<ul>
  <li>Even if TestDisk defaults to <code class="language-plaintext highlighter-rouge">[None]</code> due to severe corruption, manually select <code class="language-plaintext highlighter-rouge">[ Intel ] Intel/PC partition</code> (this is for MBR-style partition tables).</li>
  <li>Press <code class="language-plaintext highlighter-rouge">Enter</code>.</li>
</ul>

<hr />

<h3 id="phase-3-searching-for-lost-partitions"><strong>Phase 3: Searching for Lost Partitions</strong></h3>

<p><strong>1. Main Menu - Analyse:</strong>
You are now at TestDisk’s main menu.</p>
<ul>
  <li>Select <code class="language-plaintext highlighter-rouge">[ Analyse ]</code> (Analyse current partition structure and search for lost partitions).</li>
  <li>Press <code class="language-plaintext highlighter-rouge">Enter</code>.</li>
</ul>

<p><strong>2. Current Partition Structure &amp; Quick Search:</strong>
TestDisk will display the current partition structure it can read. Given the logs, this will likely show errors, “No partition found,” or the nonsensical partitions from your <code class="language-plaintext highlighter-rouge">fdisk</code> output.</p>
<ul>
  <li>Ensure <code class="language-plaintext highlighter-rouge">[ Quick Search ]</code> is highlighted at the bottom and press <code class="language-plaintext highlighter-rouge">Enter</code>.</li>
  <li>TestDisk might ask: “Search for partitions created under Windows Vista or later? (Y/N)”. This question helps TestDisk look for MBR partition signatures that might be placed according to newer standards or by specific operating systems. For exFAT (which your <code class="language-plaintext highlighter-rouge">lsblk -f</code> log indicated for <code class="language-plaintext highlighter-rouge">sdb</code>), answering <code class="language-plaintext highlighter-rouge">Y</code> (Yes) is generally a good starting point. Press <code class="language-plaintext highlighter-rouge">Y</code>.</li>
  <li>The Quick Search will begin. This may take some time.</li>
</ul>

<p><strong>3. Interpreting Quick Search Results:</strong>
Once the scan completes, TestDisk will list any potential partition candidates it found.</p>
<ul>
  <li><strong>Look for a partition that matches your expected exFAT partition:</strong>
    <ul>
      <li>It should be marked as <code class="language-plaintext highlighter-rouge">P</code> (Primary). Other types include <code class="language-plaintext highlighter-rouge">*</code> (Primary, bootable), <code class="language-plaintext highlighter-rouge">L</code> (Logical), <code class="language-plaintext highlighter-rouge">E</code> (Extended), <code class="language-plaintext highlighter-rouge">D</code> (Deleted).</li>
      <li>It should span a significant portion of your ~477.5 GiB card.</li>
      <li>The filesystem type might be identified as <code class="language-plaintext highlighter-rouge">HPFS/NTFS/exFAT</code> or similar. The partition label (e.g., “samsung500G”) might not be directly visible in this partition list but can be confirmed if you successfully list files.</li>
    </ul>
  </li>
  <li><strong>CRUCIAL STEP - Verify by Listing Files:</strong>
    <ul>
      <li>Use the <code class="language-plaintext highlighter-rouge">Up/Down</code> arrow keys to highlight a promising partition candidate.</li>
      <li>Press <code class="language-plaintext highlighter-rouge">P</code> on your keyboard. This attempts to list the files and folders within that found partition.</li>
      <li><strong>If you see your expected files and directory structure:</strong> This is a very positive sign! The partition and its filesystem are likely recoverable. Press <code class="language-plaintext highlighter-rouge">Q</code> to return to the partition list.</li>
      <li><strong>If the file listing is empty, shows garbage, or you get an error like “Can’t open filesystem”:</strong> This partition candidate is likely incorrect, or the filesystem within it is also severely damaged. Press <code class="language-plaintext highlighter-rouge">Q</code> to return to the partition list and try another candidate if available.</li>
    </ul>
  </li>
  <li><strong>If multiple candidates are found:</strong> Use your judgment. For a typical SD card used for storage, you’re usually looking for a single, large <code class="language-plaintext highlighter-rouge">P</code> (Primary) partition. Avoid small, overlapping, or ‘Extended’ partitions unless you specifically created such a layout. Check start/end sectors for plausibility.</li>
</ul>

<p><strong>4. Deeper Search (If Quick Search is Insufficient):</strong>
If <code class="language-plaintext highlighter-rouge">Quick Search</code> doesn’t find your main partition, or if listing files (<code class="language-plaintext highlighter-rouge">P</code>) doesn’t show your data for any found candidates:</p>
<ul>
  <li>Ensure you are on the screen listing the (Quick Search) found partitions (or the screen that says no partitions were found).</li>
  <li>Select <code class="language-plaintext highlighter-rouge">[ Deeper Search ]</code> from the options at the bottom.</li>
  <li>Press <code class="language-plaintext highlighter-rouge">Enter</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">Deeper Search</code> scans the drive much more thoroughly, sector by sector. <strong>This process will take considerably longer (potentially many hours).</strong> Be patient.</li>
  <li>Once <code class="language-plaintext highlighter-rouge">Deeper Search</code> completes, it will present another list of partition candidates. Again, for each promising candidate, highlight it and press <code class="language-plaintext highlighter-rouge">P</code> to list files and verify it’s your data. Press <code class="language-plaintext highlighter-rouge">Q</code> to return from the file listing.</li>
</ul>

<hr />

<h3 id="phase-4-recovering-data-by-copying-files-primary--safest-goal"><strong>Phase 4: Recovering Data by Copying Files (Primary &amp; Safest Goal)</strong></h3>

<p>This is the <strong>most recommended first step</strong> to retrieve your data, as it does not modify the source drive/image.</p>

<ol>
  <li><strong>Access File Listing:</strong>
    <ul>
      <li>After a <code class="language-plaintext highlighter-rouge">Quick Search</code> or <code class="language-plaintext highlighter-rouge">Deeper Search</code> has found a partition candidate, and you have verified with <code class="language-plaintext highlighter-rouge">P</code> that it lists your files correctly:
        <ul>
          <li>Ensure that correct partition is highlighted in the list.</li>
          <li>If you are not already viewing the files (i.e., you pressed <code class="language-plaintext highlighter-rouge">Q</code> to go back to the partition list), press <code class="language-plaintext highlighter-rouge">P</code> again to re-enter the file listing for that partition.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Navigate and Select Files/Folders:</strong>
    <ul>
      <li>Inside the file listing:
        <ul>
          <li>Use <code class="language-plaintext highlighter-rouge">Up/Down</code> arrow keys to navigate.</li>
          <li>Select <code class="language-plaintext highlighter-rouge">.</code> to stay in the current directory, <code class="language-plaintext highlighter-rouge">..</code> (or press the <code class="language-plaintext highlighter-rouge">Left</code> arrow key) to go to the parent directory.</li>
          <li>Press <code class="language-plaintext highlighter-rouge">Right</code> arrow key or <code class="language-plaintext highlighter-rouge">Enter</code> to enter a highlighted directory.</li>
        </ul>
      </li>
      <li><strong>To select items for copying:</strong>
        <ul>
          <li>Highlight a single file or folder you want to copy.</li>
          <li>Press <code class="language-plaintext highlighter-rouge">:</code> (colon) to select the currently highlighted item. Selected items usually change color.</li>
          <li>Repeat for other individual items if needed.</li>
          <li>To select <strong>all</strong> items in the current directory listing, press <code class="language-plaintext highlighter-rouge">a</code>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Copy Selected Files/Folders:</strong>
    <ul>
      <li>Once you have selected the desired file(s) or folder(s):
        <ul>
          <li>Press <code class="language-plaintext highlighter-rouge">C</code> (uppercase C) to initiate the copy process for the selected items.</li>
        </ul>
      </li>
      <li>TestDisk will then switch to a file browser showing your system’s <em>other</em> mounted filesystems. This is where you choose the <strong>destination</strong> for your recovered data.
        <ul>
          <li>Navigate to a safe location on a different drive (e.g., a folder like <code class="language-plaintext highlighter-rouge">/mnt/mSATA/recovered_data_from_sdcard/</code>). <strong>Do NOT save to the original SD card or its image!</strong></li>
          <li>Once you are in the correct destination directory in this browser view, press <code class="language-plaintext highlighter-rouge">C</code> (uppercase C) <strong>again</strong>. This confirms the destination and starts the actual copying process.</li>
        </ul>
      </li>
      <li>You should see “Copy done!” messages for successfully copied items. If errors occur, note them.</li>
      <li>Repeat this process (navigate, select, copy) for all data you need to recover.</li>
    </ul>
  </li>
  <li><strong>Exiting File Listing/Copy Mode:</strong>
    <ul>
      <li>Press <code class="language-plaintext highlighter-rouge">Q</code> to go back from the file listing to the partition list.</li>
    </ul>
  </li>
</ol>

<hr />

<h3 id="phase-5-attempting-to-write-partition-table-optional-secondary-riskier-goal"><strong>Phase 5: Attempting to Write Partition Table (Optional, Secondary, Riskier Goal)</strong></h3>

<p><strong>Only attempt this phase if:</strong></p>
<ul>
  <li>You have <strong>already successfully copied all your important files</strong> to a safe location using Phase 4.</li>
  <li>OR you are working on a <strong>disk image</strong> and are comfortable with the risk of potentially making the image unreadable if the wrong structure is written.</li>
  <li>This step attempts to repair the SD card’s (or image’s) partition table so it might be recognized by the system normally.</li>
</ul>

<ol>
  <li><strong>Select the Correct Partition(s) for Writing:</strong>
    <ul>
      <li>In the list of partitions found by TestDisk (after Quick or Deeper Search), ensure the partition(s) that correctly showed your files with <code class="language-plaintext highlighter-rouge">P</code> are highlighted.</li>
      <li>The status should typically be <code class="language-plaintext highlighter-rouge">P</code> (Primary). If TestDisk misidentified a partition type (e.g., a Linux partition as FAT), you could <em>cautiously</em> use <code class="language-plaintext highlighter-rouge">T</code> to change its type, but for a standard exFAT data drive, this is usually not necessary and best avoided unless you are certain.</li>
    </ul>
  </li>
  <li><strong>Proceed to Write:</strong>
    <ul>
      <li>Once you are confident in the selected partition structure, press <code class="language-plaintext highlighter-rouge">Enter</code> to continue from the partition list screen (if you are not already on the screen with the <code class="language-plaintext highlighter-rouge">[ Write ]</code> option).</li>
    </ul>
  </li>
  <li><strong>Write the Partition Table:</strong>
    <ul>
      <li>Select <code class="language-plaintext highlighter-rouge">[ Write ]</code> from the options at the bottom and press <code class="language-plaintext highlighter-rouge">Enter</code>.</li>
      <li>TestDisk will ask for confirmation: “Write partition table, confirm? (Y/N)”.</li>
      <li><strong>Think carefully.</strong> If you are sure, press <code class="language-plaintext highlighter-rouge">Y</code> to confirm and write the new partition table.</li>
    </ul>
  </li>
  <li><strong>Post-Write Action:</strong>
    <ul>
      <li>TestDisk will usually advise that you need to reboot your computer for the changes to take effect if you operated on <code class="language-plaintext highlighter-rouge">/dev/sdb</code> directly.</li>
      <li>Select <code class="language-plaintext highlighter-rouge">[ Quit ]</code> and exit TestDisk.</li>
    </ul>
  </li>
</ol>

<hr />

<h3 id="phase-6-post-recovery-actions"><strong>Phase 6: Post-Recovery Actions</strong></h3>

<ol>
  <li><strong>Verify Recovered Data:</strong>
    <ul>
      <li>Thoroughly check the files you copied (from Phase 4) to your recovery drive. Open various file types to ensure they are intact and not corrupted.</li>
    </ul>
  </li>
  <li><strong>If You Wrote a New Partition Table (Phase 5):</strong>
    <ul>
      <li><strong>If working on <code class="language-plaintext highlighter-rouge">/dev/sdb</code> directly:</strong> Reboot your computer.</li>
      <li>After rebooting (or if working on an image, you might try to mount it using loopback devices), check if the system now recognizes the SD card and its partition(s).</li>
      <li>If accessible, <strong>immediately back up any remaining data you couldn’t copy via TestDisk’s file copy, if any.</strong></li>
      <li><strong>Run <code class="language-plaintext highlighter-rouge">fsck</code> (Filesystem Check):</strong>
        <ul>
          <li>Identify the partition (e.g., <code class="language-plaintext highlighter-rouge">/dev/sdb1</code>).</li>
          <li>Unmount it if mounted: <code class="language-plaintext highlighter-rouge">sudo umount /dev/sdb1</code></li>
          <li>Run the filesystem check for exFAT:
            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>fsck.exfat /dev/sdb1
</code></pre></div>            </div>
            <p>(or <code class="language-plaintext highlighter-rouge">sudo exfatfsck /dev/sdb1</code> depending on your system’s exFAT utilities).</p>
          </li>
          <li>Follow any prompts. This can repair minor filesystem inconsistencies.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Future of the SD Card:</strong>
    <ul>
      <li><strong>If data recovery was successful but the card showed severe corruption:</strong> This SD card is highly suspect. Its reliability is questionable.</li>
      <li><strong>Consider a Full Reformat:</strong> After ensuring all data is safe, you might try a full (low-level) reformat of the SD card to see if it can be made stable. This involves deleting all partitions, creating a new partition table, and then formatting. Tools like <code class="language-plaintext highlighter-rouge">gparted</code> (GUI) or <code class="language-plaintext highlighter-rouge">fdisk</code>/<code class="language-plaintext highlighter-rouge">mkfs.exfat</code> (CLI) can be used.
        <ul>
          <li><strong>Example of zeroing out (ERASES EVERYTHING ON /dev/sdb - EXTREME CAUTION):</strong>
            <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># sudo dd if=/dev/zero of=/dev/sdb bs=4M status=progress</span>
</code></pre></div>            </div>
            <p><strong>TRIPLE CHECK <code class="language-plaintext highlighter-rouge">of=/dev/sdb</code> IS CORRECT BEFORE RUNNING!</strong></p>
          </li>
          <li>Then create a new partition table (e.g., MBR with <code class="language-plaintext highlighter-rouge">fdisk</code>) and format (e.g., <code class="language-plaintext highlighter-rouge">mkfs.exfat</code>).</li>
        </ul>
      </li>
      <li><strong>If the card continues to exhibit problems:</strong> Discard it to prevent future data loss. It’s likely at the end of its life or has irreparable hardware issues.</li>
    </ul>
  </li>
</ol>

<hr />

<h3 id="phase-7-if-testdisk-fails-to-recover-partitions-or-list-files"><strong>Phase 7: If TestDisk Fails to Recover Partitions or List Files</strong></h3>

<p>If TestDisk’s <code class="language-plaintext highlighter-rouge">[ Analyse ]</code> features (even <code class="language-plaintext highlighter-rouge">Deeper Search</code>) cannot find a usable partition structure, or if the filesystem within a found partition is too damaged to list files:</p>

<ul>
  <li><strong>Consider PhotoRec:</strong>
    <ul>
      <li>PhotoRec is a file data recovery software that comes bundled with TestDisk (you can usually run it with <code class="language-plaintext highlighter-rouge">sudo photorec</code>).</li>
      <li>It works differently: it ignores the filesystem and carves files directly from the raw data based on known file headers and footers.</li>
      <li><strong>Pros:</strong> Can often recover files even when the filesystem is completely destroyed.</li>
      <li><strong>Cons:</strong> Recovers files without their original filenames, directory structure, or timestamps. Files are typically renamed and sorted by type into output directories.</li>
      <li>PhotoRec is also menu-driven. You’ll select the disk/image, choose file types to search for (you can often select all or specific ones like .jpg, .doc, etc.), and specify a destination directory for recovered files.</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">installing rescatux/rescapp on Debian</title><link href="https://ib.bsb.br/installing-rescatuxrescapp-on-debian/" rel="alternate" type="text/html" title="installing rescatux/rescapp on Debian" /><published>2025-05-16T00:00:00+00:00</published><updated>2025-05-16T06:59:31+00:00</updated><id>https://ib.bsb.br/installing-rescatuxrescapp-on-debian</id><content type="html" xml:base="https://ib.bsb.br/installing-rescatuxrescapp-on-debian/"><![CDATA[<h1 id="1-update-your-system">1. Update Your System</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt upgrade <span class="nt">-y</span>
</code></pre></div></div>

<hr />

<h1 id="2-install-build-essentials-and-core-utilities">2. Install Build Essentials and Core Utilities</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> build-essential make coreutils
</code></pre></div></div>

<hr />

<h1 id="3-install-all-required-runtime-dependencies">3. Install All Required Runtime Dependencies</h1>

<p>The following list is derived from the <code class="language-plaintext highlighter-rouge">INSTALL</code> file, plugin scripts, and the codebase. Some packages may already be installed by default, but running these commands is safe and ensures completeness.</p>

<h2 id="31-python-3-and-required-python-modules">3.1. Python 3 and Required Python Modules</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> python3 python3-gi python3-dbus python3-pyqt5 python3-pyqt5.qtwebkit python3-parted
</code></pre></div></div>
<blockquote>
  <p><strong>Note:</strong></p>
  <ul>
    <li><code class="language-plaintext highlighter-rouge">python3-pyqt5.qtwebkit</code> is in the <code class="language-plaintext highlighter-rouge">bullseye</code> repo but may be called <code class="language-plaintext highlighter-rouge">python3-pyqt5.qtwebengine</code> in some newer releases. For Bullseye, the above is correct.</li>
    <li><code class="language-plaintext highlighter-rouge">python3-parted</code> provides the <code class="language-plaintext highlighter-rouge">parted</code> Python bindings.</li>
  </ul>
</blockquote>

<h2 id="32-gui-and-desktop-integration">3.2. GUI and Desktop Integration</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> zenity xdg-utils wmctrl
</code></pre></div></div>

<h2 id="33-dbus-system-integration">3.3. DBus System Integration</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> dbus
</code></pre></div></div>

<h2 id="34-disk-filesystem-and-partition-tools">3.4. Disk, Filesystem, and Partition Tools</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> util-linux reiserfsprogs reiser4progs btrfs-progs xfsprogs xfsdump ntfs-3g dosfstools gawk extundelete os-prober
</code></pre></div></div>

<h2 id="35-raid-lvm-and-encryption">3.5. RAID, LVM, and Encryption</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> dmraid lvm2 cryptsetup libcryptsetup12 cryptsetup-bin
</code></pre></div></div>

<h2 id="36-gpt-and-uefi-tools">3.6. GPT and UEFI Tools</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> gdisk efibootmgr mokutil
</code></pre></div></div>

<h2 id="37-bootloader-and-mbr-tools">3.7. Bootloader and MBR Tools</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> syslinux lilo
</code></pre></div></div>

<h2 id="38-terminal-emulator">3.8. Terminal Emulator</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> xterm
</code></pre></div></div>

<h2 id="39-miscellaneous-utilities">3.9. Miscellaneous Utilities</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> pastebinit hexchat gawk extundelete
</code></pre></div></div>

<h2 id="310-inxi-and-boot-info-script">3.10. Inxi and Boot Info Script</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> inxi boot-info-script
</code></pre></div></div>

<h2 id="311-optional-but-recommended-partition-and-recovery-tools">3.11. Optional but Recommended: Partition and Recovery Tools</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> gparted gpart testdisk
</code></pre></div></div>

<h2 id="312-additional-utilities">3.12. Additional Utilities</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> wget curl
</code></pre></div></div>

<hr />

<h1 id="4-install-the-rescatux-chntpw-package-for-windows-passwordaccount-operations">4. Install the Rescatux chntpw Package (for Windows Password/Account Operations)</h1>

<p><strong>IMPORTANT:</strong><br />
The standard <code class="language-plaintext highlighter-rouge">chntpw</code> package in Debian is not sufficient for all Rescapp features.<br />
You should use the Rescatux-provided version.</p>

<h2 id="41-add-the-rescatux-repository">4.1. Add the Rescatux Repository</h2>

<p>Create the file <code class="language-plaintext highlighter-rouge">/etc/apt/sources.list.d/rescatux.list</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"deb http://rescatux.sourceforge.net/repo/ buster-dev main"</span> | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/rescatux.list
</code></pre></div></div>

<h2 id="42-update-and-install-chntpw">4.2. Update and Install chntpw</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nt">-o</span> Acquire::AllowInsecureRepositories<span class="o">=</span><span class="nb">true</span> <span class="nt">-o</span> Acquire::AllowDowngradeToInsecureRepositories<span class="o">=</span><span class="nb">true </span>update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> chntpw
</code></pre></div></div>
<blockquote>
  <p><strong>Note:</strong></p>
  <ul>
    <li>You may be prompted about unauthenticated packages. Accept them.</li>
    <li>The Rescatux repo is for Buster, but the chntpw package is compatible with Bullseye.</li>
  </ul>
</blockquote>

<hr />

<h1 id="5-optional-selinux-support">5. (Optional) SELinux Support</h1>

<p>If you need SELinux support (rare, mostly for Fedora/RedHat/CentOS rescue):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> python3-selinux python3-semanage policycoreutils-python-utils selinux-basics auditd selinux-policy-default setools
</code></pre></div></div>
<blockquote>
  <p><strong>Note:</strong></p>
  <ul>
    <li>These packages are optional and only needed if you plan to rescue SELinux-enabled systems.</li>
  </ul>
</blockquote>

<hr />

<h1 id="6-clone-the-rescapp-repository">6. Clone the Rescapp Repository</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/rescatux/rescapp.git
<span class="nb">cd </span>rescapp
</code></pre></div></div>

<hr />

<h1 id="7-install-rescapp">7. Install Rescapp</h1>

<p>By default, this will install to <code class="language-plaintext highlighter-rouge">/usr/local</code>.<br />
If you want to install to <code class="language-plaintext highlighter-rouge">/usr</code>, use <code class="language-plaintext highlighter-rouge">prefix=/usr make install</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>make <span class="nb">install</span>
</code></pre></div></div>
<p>or, for system-wide <code class="language-plaintext highlighter-rouge">/usr</code> installation:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>make <span class="nv">prefix</span><span class="o">=</span>/usr <span class="nb">install</span>
</code></pre></div></div>

<hr />

<h1 id="8-optional-verify-installation">8. (Optional) Verify Installation</h1>

<p>Check that the <code class="language-plaintext highlighter-rouge">rescapp</code> binary is in your path:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>which rescapp
</code></pre></div></div>

<p>You should see <code class="language-plaintext highlighter-rouge">/usr/local/bin/rescapp</code> or <code class="language-plaintext highlighter-rouge">/usr/bin/rescapp</code> depending on your install prefix.</p>

<hr />

<h1 id="9-optional-desktop-integration">9. (Optional) Desktop Integration</h1>

<p>If you want Rescapp to appear in your desktop menu, ensure the <code class="language-plaintext highlighter-rouge">.desktop</code> file is installed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls</span> /usr/local/share/applications/rescapp.desktop
</code></pre></div></div>
<p>or</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls</span> /usr/share/applications/rescapp.desktop
</code></pre></div></div>

<hr />

<h1 id="10-run-rescapp">10. Run Rescapp</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rescapp
</code></pre></div></div>

<hr />

<h1 id="11-optional-troubleshooting">11. (Optional) Troubleshooting</h1>

<ul>
  <li>If you encounter missing dependencies, re-check the above lists.</li>
  <li>For issues with chntpw, ensure you are using the Rescatux-provided version.</li>
  <li>For graphical issues, ensure you have a working X session and all Qt5 dependencies.</li>
</ul>

<hr />

<h2 id="summary-table-of-all-key-packages"><strong>Summary Table of All Key Packages</strong></h2>

<table>
  <thead>
    <tr>
      <th>Purpose</th>
      <th>Package Names</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Core build/utils</td>
      <td>build-essential make coreutils</td>
    </tr>
    <tr>
      <td>Python &amp; Qt</td>
      <td>python3 python3-gi python3-dbus python3-pyqt5 python3-pyqt5.qtwebkit python3-parted</td>
    </tr>
    <tr>
      <td>GUI/desktop</td>
      <td>zenity xdg-utils wmctrl</td>
    </tr>
    <tr>
      <td>DBus</td>
      <td>dbus</td>
    </tr>
    <tr>
      <td>Disk/FS tools</td>
      <td>util-linux reiserfsprogs reiser4progs btrfs-progs xfsprogs xfsdump ntfs-3g dosfstools</td>
    </tr>
    <tr>
      <td>RAID/LVM/Crypto</td>
      <td>dmraid lvm2 cryptsetup libcryptsetup12 cryptsetup-bin</td>
    </tr>
    <tr>
      <td>GPT/UEFI</td>
      <td>gdisk efibootmgr mokutil</td>
    </tr>
    <tr>
      <td>Bootloader/MBR</td>
      <td>syslinux lilo</td>
    </tr>
    <tr>
      <td>Terminal emulator</td>
      <td>xterm</td>
    </tr>
    <tr>
      <td>Misc utilities</td>
      <td>pastebinit hexchat gawk extundelete wget curl</td>
    </tr>
    <tr>
      <td>Info scripts</td>
      <td>inxi boot-info-script</td>
    </tr>
    <tr>
      <td>Partition/recovery</td>
      <td>gparted gpart testdisk</td>
    </tr>
    <tr>
      <td>Windows password tool</td>
      <td>chntpw (from Rescatux repo)</td>
    </tr>
    <tr>
      <td>SELinux (optional)</td>
      <td>python3-selinux python3-semanage policycoreutils-python-utils selinux-basics auditd selinux-policy-default setools</td>
    </tr>
  </tbody>
</table>

<hr />

<h1 id="references"><strong>References</strong></h1>

<ul>
  <li><a href="https://github.com/rescatux/rescapp">Rescapp GitHub</a></li>
  <li><a href="https://www.rescatux.org">Rescatux Website</a></li>
  <li><a href="https://github.com/rescatux/chntpw">Rescatux chntpw package</a></li>
  <li><a href="https://packages.debian.org/bullseye/">Debian Bullseye Packages</a></li>
</ul>

<h1 id="bash-script-automation">bash script automation</h1>

<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="#!/bin/bash
# Script to install Rescapp and its dependencies on Debian Bullseye x64
# This script automates the steps from the revised guide.
# It should be run with sudo privileges or by a user who can sudo without a password for apt.
# The script will exit immediately if any command fails.
set -e # Exit immediately if a command exits with a non-zero status.
set -u # Treat unset variables as an error when substituting.
set -o pipefail # The return value of a pipeline is the status of the last command to exit with a non-zero status, or zero if no command exited with a non-zero status.
# --- Configuration ---
# Set DEBIAN_FRONTEND to noninteractive to avoid prompts during package installation
export DEBIAN_FRONTEND=noninteractive
# --- Temporary Directory for Build ---
BUILD_DIR=&quot;&quot; # Initialize BUILD_DIR
# Cleanup function to remove temporary directory
cleanup() {
    if [ -n &quot;$BUILD_DIR&quot; ] &amp;&amp; [ -d &quot;$BUILD_DIR&quot; ]; then
        echo &quot;Cleaning up temporary build directory: $BUILD_DIR&quot;
        sudo rm -rf &quot;$BUILD_DIR&quot;
    fi
}
# Register cleanup function to be called on script exit or interruption
trap cleanup EXIT SIGINT SIGTERM
echo &quot;Starting Rescapp installation process...&quot;
echo &quot;This script will install necessary packages and Rescapp.&quot;
echo &quot;Ensure you have an active internet connection.&quot;
echo &quot;-----------------------------------------------------&quot;
# 1. Update System
echo &quot;[Step 1/7] Updating system packages...&quot;
sudo apt update
sudo apt upgrade -y
# 2. Install Build Essentials and Git
echo &quot;[Step 2/7] Installing build essentials, core utilities, and git...&quot;
sudo apt install -y build-essential make coreutils git
# 3. Install All Required Runtime Dependencies
echo &quot;[Step 3/7] Installing Rescapp runtime dependencies...&quot;
# Note: python3-pyqt5.qtwebkit is for Bullseye. Newer distros might use python3-pyqt5.qtwebengine.
# Lilo might produce a warning during installation; this is generally acceptable for newer systems not relying on Lilo.
sudo apt install -y \
    python3 python3-gi python3-dbus python3-pyqt5 python3-pyqt5.qtwebkit python3-parted \
    zenity xdg-utils wmctrl \
    dbus \
    util-linux reiserfsprogs reiser4progs btrfs-progs xfsprogs xfsdump ntfs-3g dosfstools gawk extundelete os-prober \
    dmraid lvm2 cryptsetup libcryptsetup12 cryptsetup-bin \
    gdisk efibootmgr mokutil \
    syslinux lilo \
    xterm \
    pastebinit hexchat \
    inxi boot-info-script \
    gparted gpart testdisk \
    wget curl
# 4. (Optional) SELinux Support
# If you need to rescue SELinux-enabled systems (e.g., Fedora, RHEL, CentOS),
# uncomment the following section.
# echo &quot;[Step 4/7 - Optional] Installing SELinux support packages...&quot;
# sudo apt install -y \
# python3-selinux python3-semanage policycoreutils-python-utils selinux-basics auditd selinux-policy-default setools
# 5. Install the Rescatux chntpw Package
echo &quot;[Step 4/7] Installing Rescatux chntpw package...&quot;
# This uses the Rescatux repository for a version of chntpw with features needed by Rescapp.
# The repository is for Debian Buster but the chntpw package is generally compatible with Bullseye.
RESCATUX_REPO_FILE=&quot;/etc/apt/sources.list.d/rescatux.list&quot;
RESCATUX_REPO_LINE=&quot;deb http://rescatux.sourceforge.net/repo/ buster-dev main&quot;
if ! grep -qF &quot;$RESCATUX_REPO_LINE&quot; &quot;$RESCATUX_REPO_FILE&quot; 2&gt;/dev/null; then
    echo &quot;$RESCATUX_REPO_LINE&quot; | sudo tee &quot;$RESCATUX_REPO_FILE&quot;
else
    echo &quot;Rescatux repository line already exists in $RESCATUX_REPO_FILE.&quot;
fi
# Allow unauthenticated repositories for this specific source if GPG key is not imported.
sudo apt -o Acquire::AllowInsecureRepositories=true -o Acquire::AllowDowngradeToInsecureRepositories=true update
# You might be prompted to accept unauthenticated packages; this is expected for this repo if not running fully noninteractive.
# The DEBIAN_FRONTEND=noninteractive export should handle this.
sudo apt install -y chntpw
# 6. Clone the Rescapp Repository
BUILD_DIR=$(mktemp -d) # Create a temporary directory
echo &quot;[Step 5/7] Cloning the Rescapp repository into $BUILD_DIR/rescapp...&quot;
git clone https://github.com/rescatux/rescapp.git &quot;$BUILD_DIR/rescapp&quot;
cd &quot;$BUILD_DIR/rescapp&quot;
# 7. Install Rescapp
echo &quot;[Step 6/7] Installing Rescapp (default to /usr/local)...&quot;
# To install to /usr instead, you would use: sudo make prefix=/usr install
sudo make install
# 8. Verify Installation
echo &quot;[Step 7/7] Verifying installation...&quot;
RESCAPP_PATH=$(which rescapp || echo &quot;not_found&quot;) # Avoid error if not found when set -u is active
if [ &quot;$RESCAPP_PATH&quot; != &quot;not_found&quot; ] &amp;&amp; [ -n &quot;$RESCAPP_PATH&quot; ]; then
    echo &quot;Rescapp executable found at: $RESCAPP_PATH&quot;
else
    echo &quot;ERROR: Rescapp executable not found in PATH after installation.&quot;
    # The script will exit here if set -e is active and which fails,
    # but this explicit check is for clarity.
fi
DESKTOP_FILE_USR_LOCAL=&quot;/usr/local/share/applications/rescapp.desktop&quot;
DESKTOP_FILE_USR=&quot;/usr/share/applications/rescapp.desktop&quot;
if [ -f &quot;$DESKTOP_FILE_USR_LOCAL&quot; ]; then
    echo &quot;Rescapp desktop file found at: $DESKTOP_FILE_USR_LOCAL&quot;
elif [ -f &quot;$DESKTOP_FILE_USR&quot; ]; then
    echo &quot;Rescapp desktop file found at: $DESKTOP_FILE_USR&quot;
else
    echo &quot;Warning: Rescapp desktop file not found. Desktop integration might be incomplete.&quot;
fi
echo &quot;-----------------------------------------------------&quot;
echo &quot;Rescapp installation process completed successfully.&quot;
echo &quot;The build files were in the temporary directory $BUILD_DIR and will be cleaned up.&quot;
echo &quot;You can now attempt to run Rescapp by typing &#39;rescapp&#39; in your terminal.&quot;
echo &quot;-----------------------------------------------------&quot;
# Cleanup is handled by the trap EXIT
exit 0" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">#!/bin/bash
# Script to install Rescapp and its dependencies on Debian Bullseye x64
# This script automates the steps from the revised guide.
# It should be run with sudo privileges or by a user who can sudo without a password for apt.
# The script will exit immediately if any command fails.
set -e # Exit immediately if a command exits with a non-zero status.
set -u # Treat unset variables as an error when substituting.
set -o pipefail # The return value of a pipeline is the status of the last command to exit with a non-zero status, or zero if no command exited with a non-zero status.
# --- Configuration ---
# Set DEBIAN_FRONTEND to noninteractive to avoid prompts during package installation
export DEBIAN_FRONTEND=noninteractive
# --- Temporary Directory for Build ---
BUILD_DIR=&quot;&quot; # Initialize BUILD_DIR
# Cleanup function to remove temporary directory
cleanup() {
    if [ -n &quot;$BUILD_DIR&quot; ] &amp;&amp; [ -d &quot;$BUILD_DIR&quot; ]; then
        echo &quot;Cleaning up temporary build directory: $BUILD_DIR&quot;
        sudo rm -rf &quot;$BUILD_DIR&quot;
    fi
}
# Register cleanup function to be called on script exit or interruption
trap cleanup EXIT SIGINT SIGTERM
echo &quot;Starting Rescapp installation process...&quot;
echo &quot;This script will install necessary packages and Rescapp.&quot;
echo &quot;Ensure you have an active internet connection.&quot;
echo &quot;-----------------------------------------------------&quot;
# 1. Update System
echo &quot;[Step 1/7] Updating system packages...&quot;
sudo apt update
sudo apt upgrade -y
# 2. Install Build Essentials and Git
echo &quot;[Step 2/7] Installing build essentials, core utilities, and git...&quot;
sudo apt install -y build-essential make coreutils git
# 3. Install All Required Runtime Dependencies
echo &quot;[Step 3/7] Installing Rescapp runtime dependencies...&quot;
# Note: python3-pyqt5.qtwebkit is for Bullseye. Newer distros might use python3-pyqt5.qtwebengine.
# Lilo might produce a warning during installation; this is generally acceptable for newer systems not relying on Lilo.
sudo apt install -y \
    python3 python3-gi python3-dbus python3-pyqt5 python3-pyqt5.qtwebkit python3-parted \
    zenity xdg-utils wmctrl \
    dbus \
    util-linux reiserfsprogs reiser4progs btrfs-progs xfsprogs xfsdump ntfs-3g dosfstools gawk extundelete os-prober \
    dmraid lvm2 cryptsetup libcryptsetup12 cryptsetup-bin \
    gdisk efibootmgr mokutil \
    syslinux lilo \
    xterm \
    pastebinit hexchat \
    inxi boot-info-script \
    gparted gpart testdisk \
    wget curl
# 4. (Optional) SELinux Support
# If you need to rescue SELinux-enabled systems (e.g., Fedora, RHEL, CentOS),
# uncomment the following section.
# echo &quot;[Step 4/7 - Optional] Installing SELinux support packages...&quot;
# sudo apt install -y \
# python3-selinux python3-semanage policycoreutils-python-utils selinux-basics auditd selinux-policy-default setools
# 5. Install the Rescatux chntpw Package
echo &quot;[Step 4/7] Installing Rescatux chntpw package...&quot;
# This uses the Rescatux repository for a version of chntpw with features needed by Rescapp.
# The repository is for Debian Buster but the chntpw package is generally compatible with Bullseye.
RESCATUX_REPO_FILE=&quot;/etc/apt/sources.list.d/rescatux.list&quot;
RESCATUX_REPO_LINE=&quot;deb http://rescatux.sourceforge.net/repo/ buster-dev main&quot;
if ! grep -qF &quot;$RESCATUX_REPO_LINE&quot; &quot;$RESCATUX_REPO_FILE&quot; 2&gt;/dev/null; then
    echo &quot;$RESCATUX_REPO_LINE&quot; | sudo tee &quot;$RESCATUX_REPO_FILE&quot;
else
    echo &quot;Rescatux repository line already exists in $RESCATUX_REPO_FILE.&quot;
fi
# Allow unauthenticated repositories for this specific source if GPG key is not imported.
sudo apt -o Acquire::AllowInsecureRepositories=true -o Acquire::AllowDowngradeToInsecureRepositories=true update
# You might be prompted to accept unauthenticated packages; this is expected for this repo if not running fully noninteractive.
# The DEBIAN_FRONTEND=noninteractive export should handle this.
sudo apt install -y chntpw
# 6. Clone the Rescapp Repository
BUILD_DIR=$(mktemp -d) # Create a temporary directory
echo &quot;[Step 5/7] Cloning the Rescapp repository into $BUILD_DIR/rescapp...&quot;
git clone https://github.com/rescatux/rescapp.git &quot;$BUILD_DIR/rescapp&quot;
cd &quot;$BUILD_DIR/rescapp&quot;
# 7. Install Rescapp
echo &quot;[Step 6/7] Installing Rescapp (default to /usr/local)...&quot;
# To install to /usr instead, you would use: sudo make prefix=/usr install
sudo make install
# 8. Verify Installation
echo &quot;[Step 7/7] Verifying installation...&quot;
RESCAPP_PATH=$(which rescapp || echo &quot;not_found&quot;) # Avoid error if not found when set -u is active
if [ &quot;$RESCAPP_PATH&quot; != &quot;not_found&quot; ] &amp;&amp; [ -n &quot;$RESCAPP_PATH&quot; ]; then
    echo &quot;Rescapp executable found at: $RESCAPP_PATH&quot;
else
    echo &quot;ERROR: Rescapp executable not found in PATH after installation.&quot;
    # The script will exit here if set -e is active and which fails,
    # but this explicit check is for clarity.
fi
DESKTOP_FILE_USR_LOCAL=&quot;/usr/local/share/applications/rescapp.desktop&quot;
DESKTOP_FILE_USR=&quot;/usr/share/applications/rescapp.desktop&quot;
if [ -f &quot;$DESKTOP_FILE_USR_LOCAL&quot; ]; then
    echo &quot;Rescapp desktop file found at: $DESKTOP_FILE_USR_LOCAL&quot;
elif [ -f &quot;$DESKTOP_FILE_USR&quot; ]; then
    echo &quot;Rescapp desktop file found at: $DESKTOP_FILE_USR&quot;
else
    echo &quot;Warning: Rescapp desktop file not found. Desktop integration might be incomplete.&quot;
fi
echo &quot;-----------------------------------------------------&quot;
echo &quot;Rescapp installation process completed successfully.&quot;
echo &quot;The build files were in the temporary directory $BUILD_DIR and will be cleaned up.&quot;
echo &quot;You can now attempt to run Rescapp by typing &#39;rescapp&#39; in your terminal.&quot;
echo &quot;-----------------------------------------------------&quot;
# Cleanup is handled by the trap EXIT
exit 0</code>
</section>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">ragflow documentation</title><link href="https://ib.bsb.br/ragflow-documentation/" rel="alternate" type="text/html" title="ragflow documentation" /><published>2025-05-16T00:00:00+00:00</published><updated>2025-05-16T06:26:46+00:00</updated><id>https://ib.bsb.br/ragflow-documentation</id><content type="html" xml:base="https://ib.bsb.br/ragflow-documentation/"><![CDATA[<p><em>category</em>.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Get Started"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"RAGFlow Quick Start"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>configurations.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /configurations
---

# Configuration

Configurations for deploying RAGFlow via Docker.

## Guidelines

When it comes to system configurations, you will need to manage the following files:

- [.env](https://github.com/infiniflow/ragflow/blob/main/docker/.env): Contains important environment variables for Docker.
- [service_conf.yaml.template](https://github.com/infiniflow/ragflow/blob/main/docker/service_conf.yaml.template): Configures the back-end services. It specifies the system-level configuration for RAGFlow and is used by its API server and task executor. Upon container startup, the `service_conf.yaml` file will be generated based on this template file. This process replaces any environment variables within the template, allowing for dynamic configuration tailored to the container's environment.
- [docker-compose.yml](https://github.com/infiniflow/ragflow/blob/main/docker/docker-compose.yml): The Docker Compose file for starting up the RAGFlow service.

To update the default HTTP serving port (80), go to [docker-compose.yml](https://github.com/infiniflow/ragflow/blob/main/docker/docker-compose.yml) and change `80:80`
to `&lt;YOUR_SERVING_PORT&gt;:80`.

:::tip NOTE
Updates to the above configurations require a reboot of all containers to take effect:

```bash
docker compose -f docker/docker-compose.yml up -d
```

:::

## Docker Compose

- **docker-compose.yml**  
  Sets up environment for RAGFlow and its dependencies.
- **docker-compose-base.yml**  
  Sets up environment for RAGFlow's dependencies: Elasticsearch/[Infinity](https://github.com/infiniflow/infinity), MySQL, MinIO, and Redis.

:::danger IMPORTANT
We do not actively maintain **docker-compose-CN-oc9.yml**, **docker-compose-gpu-CN-oc9.yml**, or **docker-compose-gpu.yml**, so use them at your own risk. However, you are welcome to file a pull request to improve any of them.
:::

## Docker environment variables

The [.env](https://github.com/infiniflow/ragflow/blob/main/docker/.env) file contains important environment variables for Docker.

### Elasticsearch

- `STACK_VERSION`  
  The version of Elasticsearch. Defaults to `8.11.3`
- `ES_PORT`  
  The port used to expose the Elasticsearch service to the host machine, allowing **external** access to the service running inside the Docker container.  Defaults to `1200`.
- `ELASTIC_PASSWORD`  
  The password for Elasticsearch.

### Kibana

- `KIBANA_PORT`  
  The port used to expose the Kibana service to the host machine, allowing **external** access to the service running inside the Docker container. Defaults to `6601`.
- `KIBANA_USER`  
  The username for Kibana. Defaults to `rag_flow`.
- `KIBANA_PASSWORD`  
  The password for Kibana. Defaults to `infini_rag_flow`.

### Resource management

- `MEM_LIMIT`  
  The maximum amount of the memory, in bytes, that *a specific* Docker container can use while running. Defaults to `8073741824`.

### MySQL

- `MYSQL_PASSWORD`  
  The password for MySQL.
- `MYSQL_PORT`  
  The port used to expose the MySQL service to the host machine, allowing **external** access to the MySQL database running inside the Docker container. Defaults to `5455`.

### MinIO

RAGFlow utilizes MinIO as its object storage solution, leveraging its scalability to store and manage all uploaded files.

- `MINIO_CONSOLE_PORT`  
  The port used to expose the MinIO console interface to the host machine, allowing **external** access to the web-based console running inside the Docker container. Defaults to `9001`
- `MINIO_PORT`  
  The port used to expose the MinIO API service to the host machine, allowing **external** access to the MinIO object storage service running inside the Docker container. Defaults to `9000`.
- `MINIO_USER`  
  The username for MinIO.
- `MINIO_PASSWORD`  
  The password for MinIO.

### Redis

- `REDIS_PORT`  
  The port used to expose the Redis service to the host machine, allowing **external** access to the Redis service running inside the Docker container. Defaults to `6379`.
- `REDIS_PASSWORD`  
  The password for Redis.

### RAGFlow

- `SVR_HTTP_PORT`  
  The port used to expose RAGFlow's HTTP API service to the host machine, allowing **external** access to the service running inside the Docker container. Defaults to `9380`.
- `RAGFLOW-IMAGE`  
  The Docker image edition. Available editions:  
  
  - `infiniflow/ragflow:v0.18.0-slim` (default): The RAGFlow Docker image without embedding models.  
  - `infiniflow/ragflow:v0.18.0`: The RAGFlow Docker image with embedding models including:
    - Built-in embedding models:
      - `BAAI/bge-large-zh-v1.5` 
      - `maidalun1020/bce-embedding-base_v1`


:::tip NOTE  
If you cannot download the RAGFlow Docker image, try the following mirrors.  

- For the `nightly-slim` edition:  
  - `RAGFLOW_IMAGE=swr.cn-north-4.myhuaweicloud.com/infiniflow/ragflow:nightly-slim` or,
  - `RAGFLOW_IMAGE=registry.cn-hangzhou.aliyuncs.com/infiniflow/ragflow:nightly-slim`.
- For the `nightly` edition:  
  - `RAGFLOW_IMAGE=swr.cn-north-4.myhuaweicloud.com/infiniflow/ragflow:nightly` or,
  - `RAGFLOW_IMAGE=registry.cn-hangzhou.aliyuncs.com/infiniflow/ragflow:nightly`.
:::

### Timezone

- `TIMEZONE`  
  The local time zone. Defaults to `'Asia/Shanghai'`.

### Hugging Face mirror site

- `HF_ENDPOINT`  
  The mirror site for huggingface.co. It is disabled by default. You can uncomment this line if you have limited access to the primary Hugging Face domain.

### MacOS

- `MACOS`  
  Optimizations for macOS. It is disabled by default. You can uncomment this line if your OS is macOS.

### User registration

- `REGISTER_ENABLED`
  - `1`: (Default) Enable user registration.
  - `0`: Disable user registration.

## Service configuration

[service_conf.yaml.template](https://github.com/infiniflow/ragflow/blob/main/docker/service_conf.yaml.template) specifies the system-level configuration for RAGFlow and is used by its API server and task executor.

### `ragflow`

- `host`: The API server's IP address inside the Docker container. Defaults to `0.0.0.0`.
- `port`: The API server's serving port inside the Docker container. Defaults to `9380`.

### `mysql`
  
- `name`: The MySQL database name. Defaults to `rag_flow`.
- `user`: The username for MySQL.
- `password`: The password for MySQL.
- `port`: The MySQL serving port inside the Docker container. Defaults to `3306`.
- `max_connections`: The maximum number of concurrent connections to the MySQL database. Defaults to `100`.
- `stale_timeout`: Timeout in seconds.

### `minio`
  
- `user`: The username for MinIO.
- `password`: The password for MinIO.
- `host`: The MinIO serving IP *and* port inside the Docker container. Defaults to `minio:9000`.

### `oauth`  

The OAuth configuration for signing up or signing in to RAGFlow using a third-party account.  It is disabled by default. To enable this feature, uncomment the corresponding lines in **service_conf.yaml.template**.

- `github`: The GitHub authentication settings for your application. Visit the [GitHub Developer Settings](https://github.com/settings/developers) page to obtain your client_id and secret_key.

#### OAuth/OIDC

RAGFlow supports OAuth/OIDC authentication through the following routes:

- `/login/&lt;channel&gt;`: Initiates the OAuth flow for the specified channel
- `/oauth/callback/&lt;channel&gt;`: Handles the OAuth callback after successful authentication

The callback URL should be configured in your OAuth provider as:
```
https://your-app.com/oauth/callback/&lt;channel&gt;
```

For detailed instructions on configuring **service_conf.yaml.template**, please refer to [Usage](https://github.com/infiniflow/ragflow/blob/main/api/apps/auth/README.md#usage).

### `user_default_llm`  

The default LLM to use for a new RAGFlow user. It is disabled by default. To enable this feature, uncomment the corresponding lines in **service_conf.yaml.template**.  

- `factory`: The LLM supplier. Available options:
  - `"OpenAI"`
  - `"DeepSeek"`
  - `"Moonshot"`
  - `"Tongyi-Qianwen"`
  - `"VolcEngine"`
  - `"ZHIPU-AI"`
- `api_key`: The API key for the specified LLM. You will need to apply for your model API key online.

:::tip NOTE  
If you do not set the default LLM here, configure the default LLM on the **Settings** page in the RAGFlow UI.
:::
</code></pre></div></div>
<p>faq.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 10
slug: /faq
---

# FAQs

Answers to questions about general features, troubleshooting, usage, and more.

---

import TOCInline from '@theme/TOCInline';

&lt;TOCInline toc={toc} /&gt;

## General features

---

### What sets RAGFlow apart from other RAG products?

The "garbage in garbage out" status quo remains unchanged despite the fact that LLMs have advanced Natural Language Processing (NLP) significantly. In response, RAGFlow introduces two unique features compared to other Retrieval-Augmented Generation (RAG) products.

- Fine-grained document parsing: Document parsing involves images and tables, with the flexibility for you to intervene as needed.
- Traceable answers with reduced hallucinations: You can trust RAGFlow's responses as you can view the citations and references supporting them.

---

### Differences between RAGFlow full edition and RAGFlow slim edition?

Each RAGFlow release is available in two editions:

- **Slim edition**: excludes built-in embedding models and is identified by a **-slim** suffix added to the version name. Example: `infiniflow/ragflow:v0.18.0-slim`
- **Full edition**: includes built-in embedding models and has no suffix added to the version name. Example: `infiniflow/ragflow:v0.18.0`

---

### Which embedding models can be deployed locally?

RAGFlow offers two Docker image editions, `v0.18.0-slim` and `v0.18.0`:  
  
- `infiniflow/ragflow:v0.18.0-slim` (default): The RAGFlow Docker image without embedding models.  
- `infiniflow/ragflow:v0.18.0`: The RAGFlow Docker image with embedding models including:
  - Built-in embedding models:
    - `BAAI/bge-large-zh-v1.5`
    - `maidalun1020/bce-embedding-base_v1`
  - Embedding models that will be downloaded once you select them in the RAGFlow UI:
    - `BAAI/bge-base-en-v1.5`
    - `BAAI/bge-large-en-v1.5`
    - `BAAI/bge-small-en-v1.5`
    - `BAAI/bge-small-zh-v1.5`
    - `jinaai/jina-embeddings-v2-base-en`
    - `jinaai/jina-embeddings-v2-small-en`
    - `nomic-ai/nomic-embed-text-v1.5`
    - `sentence-transformers/all-MiniLM-L6-v2`

---

### Where to find the version of RAGFlow? How to interpret it?

You can find the RAGFlow version number on the **System** page of the UI:

![Image](https://github.com/user-attachments/assets/20cf7213-2537-4e18-a88c-4dadf6228c6b)

If you build RAGFlow from source, the version number is also in the system log:

```
        ____   ___    ______ ______ __               
       / __ \ /   |  / ____// ____// /____  _      __
      / /_/ // /| | / / __ / /_   / // __ \| | /| / /
     / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ / 
    /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/                             

2025-02-18 10:10:43,835 INFO     1445658 RAGFlow version: v0.15.0-50-g6daae7f2 full
```

Where:

- `v0.15.0`: The officially published release.
- `50`: The number of git commits since the official release.
- `g6daae7f2`: `g` is the prefix, and `6daae7f2` is the first seven characters of the current commit ID.
- `full`/`slim`: The RAGFlow edition.
  - `full`: The full RAGFlow edition.
  - `slim`: The RAGFlow edition without embedding models and Python packages.

---

### Differences between demo.ragflow.io and a locally deployed open-source RAGFlow service?

demo.ragflow.io demonstrates the capabilities of RAGFlow Enterprise. Its DeepDoc models are pre-trained using proprietary data and it offers much more sophisticated team permission controls. Essentially, demo.ragflow.io serves as a preview of RAGFlow's forthcoming SaaS (Software as a Service) offering.

You can deploy an open-source RAGFlow service and call it from a Python client or through RESTful APIs. However, this is not supported on demo.ragflow.io.

---

### Why does it take longer for RAGFlow to parse a document than LangChain?

We put painstaking effort into document pre-processing tasks like layout analysis, table structure recognition, and OCR (Optical Character Recognition) using our vision models. This contributes to the additional time required.

---

### Why does RAGFlow require more resources than other projects?

RAGFlow has a number of built-in models for document structure parsing, which account for the additional computational resources.

---

### Which architectures or devices does RAGFlow support?

We officially support x86 CPU and nvidia GPU. While we also test RAGFlow on ARM64 platforms, we do not maintain RAGFlow Docker images for ARM. If you are on an ARM platform, follow [this guide](./develop/build_docker_image.mdx) to build a RAGFlow Docker image.

---

### Do you offer an API for integration with third-party applications?

The corresponding APIs are now available. See the [RAGFlow HTTP API Reference](./references/http_api_reference.md) or the [RAGFlow Python API Reference](./references/python_api_reference.md) for more information.

---

### Do you support stream output?

Yes, we do.

---

### Do you support sharing dialogue through URL?

No, this feature is not supported.

---

### Do you support multiple rounds of dialogues, referencing previous dialogues as context for the current query?

Yes, we support enhancing user queries based on existing context of an ongoing conversation:

1. On the **Chat** page, hover over the desired assistant and select **Edit**.
2. In the **Chat Configuration** popup, click the **Prompt engine** tab.
3. Switch on **Multi-turn optimization** to enable this feature.

---

### Key differences between AI search and chat?

- **AI search**: This is a single-turn AI conversation using a predefined retrieval strategy (a hybrid search of weighted keyword similarity and weighted vector similarity) and the system's default chat model. It does not involve advanced RAG strategies like knowledge graph, auto-keyword, or auto-question. Retrieved chunks will be listed below the chat model's response.
- **AI chat**: This is a multi-turn AI conversation where you can define your retrieval strategy (a weighted reranking score can be used to replace the weighted vector similarity in a hybrid search) and choose your chat model. In an AI chat, you can configure advanced RAG strategies, such as knowledge graphs, auto-keyword, and auto-question, for your specific case. Retrieved chunks are not displayed along with the answer.

When debugging your chat assistant, you can use AI search as a reference to verify your model settings and retrieval strategy.

---

## Troubleshooting

---

### How to build the RAGFlow image from scratch?

See [Build a RAGFlow Docker image](./develop/build_docker_image.mdx).

### Cannot access https://huggingface.co

A locally deployed RAGflow downloads OCR and embedding modules from [Huggingface website](https://huggingface.co) by default. If your machine is unable to access this site, the following error occurs and PDF parsing fails:

```
FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/huggingface/hub/models--InfiniFlow--deepdoc/snapshots/be0c1e50eef6047b412d1800aa89aba4d275f997/ocr.res'
```

To fix this issue, use https://hf-mirror.com instead:

1. Stop all containers and remove all related resources:

   ```bash
   cd ragflow/docker/
   docker compose down
   ```

2. Uncomment the following line in **ragflow/docker/.env**:

   ```
   # HF_ENDPOINT=https://hf-mirror.com
   ```

3. Start up the server:

   ```bash
   docker compose up -d 
   ```

---

### `MaxRetryError: HTTPSConnectionPool(host='hf-mirror.com', port=443)`

This error suggests that you do not have Internet access or are unable to connect to hf-mirror.com. Try the following:

1. Manually download the resource files from [huggingface.co/InfiniFlow/deepdoc](https://huggingface.co/InfiniFlow/deepdoc) to your local folder **~/deepdoc**.
2. Add a volumes to **docker-compose.yml**, for example:

   ```
   - ~/deepdoc:/ragflow/rag/res/deepdoc
   ```

---

### `WARNING: can't find /raglof/rag/res/borker.tm`

Ignore this warning and continue. All system warnings can be ignored.

---

### `network anomaly There is an abnormality in your network and you cannot connect to the server.`

![anomaly](https://github.com/infiniflow/ragflow/assets/93570324/beb7ad10-92e4-4a58-8886-bfb7cbd09e5d)

You will not log in to RAGFlow unless the server is fully initialized. Run `docker logs -f ragflow-server`.

*The server is successfully initialized, if your system displays the following:*

```
     ____   ___    ______ ______ __               
    / __ \ /   |  / ____// ____// /____  _      __
   / /_/ // /| | / / __ / /_   / // __ \| | /| / /
  / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ / 
 /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/  

 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:9380
 * Running on http://x.x.x.x:9380
 INFO:werkzeug:Press CTRL+C to quit
```

---

### `Realtime synonym is disabled, since no redis connection`

Ignore this warning and continue. All system warnings can be ignored.

![](https://github.com/infiniflow/ragflow/assets/93570324/ef5a6194-084a-4fe3-bdd5-1c025b40865c)

---

### Why does my document parsing stall at under one percent?

![stall](https://github.com/infiniflow/ragflow/assets/93570324/3589cc25-c733-47d5-bbfc-fedb74a3da50)

Click the red cross beside the 'parsing status' bar, then restart the parsing process to see if the issue remains. If the issue persists and your RAGFlow is deployed locally, try the following:

1. Check the log of your RAGFlow server to see if it is running properly:

   ```bash
   docker logs -f ragflow-server
   ```

2. Check if the **task_executor.py** process exists.
3. Check if your RAGFlow server can access hf-mirror.com or huggingface.com.

---

### Why does my pdf parsing stall near completion, while the log does not show any error?

Click the red cross beside the 'parsing status' bar, then restart the parsing process to see if the issue remains. If the issue persists and your RAGFlow is deployed locally, the parsing process is likely killed due to insufficient RAM. Try increasing your memory allocation by increasing the `MEM_LIMIT` value in **docker/.env**.

:::note
Ensure that you restart up your RAGFlow server for your changes to take effect!

```bash
docker compose stop
```

```bash
docker compose up -d
```

:::

![nearcompletion](https://github.com/infiniflow/ragflow/assets/93570324/563974c3-f8bb-4ec8-b241-adcda8929cbb)

---

### `Index failure`

An index failure usually indicates an unavailable Elasticsearch service.

---

### How to check the log of RAGFlow?

```bash
tail -f ragflow/docker/ragflow-logs/*.log
```

---

### How to check the status of each component in RAGFlow?

1. Check the status of the Elasticsearch Docker container:

   ```bash
   $ docker ps
   ```

   *The following is an example result:*

   ```bash
   5bc45806b680   infiniflow/ragflow:latest     "./entrypoint.sh"        11 hours ago   Up 11 hours               0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp, 0.0.0.0:9380-&gt;9380/tcp, :::9380-&gt;9380/tcp   ragflow-server
   91220e3285dd   docker.elastic.co/elasticsearch/elasticsearch:8.11.3   "/bin/tini -- /usr/lâ€¦"   11 hours ago   Up 11 hours (healthy)     9300/tcp, 0.0.0.0:9200-&gt;9200/tcp, :::9200-&gt;9200/tcp           ragflow-es-01
   d8c86f06c56b   mysql:5.7.18        "docker-entrypoint.sâ€¦"   7 days ago     Up 16 seconds (healthy)   0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp     ragflow-mysql
   cd29bcb254bc   quay.io/minio/minio:RELEASE.2023-12-20T01-00-02Z       "/usr/bin/docker-entâ€¦"   2 weeks ago    Up 11 hours      0.0.0.0:9001-&gt;9001/tcp, :::9001-&gt;9001/tcp, 0.0.0.0:9000-&gt;9000/tcp, :::9000-&gt;9000/tcp     ragflow-minio
   ```

2. Follow [this document](./guides/run_health_check.md) to check the health status of the Elasticsearch service.

:::danger IMPORTANT
The status of a Docker container status does not necessarily reflect the status of the service. You may find that your services are unhealthy even when the corresponding Docker containers are up running. Possible reasons for this include network failures, incorrect port numbers, or DNS issues.
:::

---

### `Exception: Can't connect to ES cluster`

1. Check the status of the Elasticsearch Docker container:

   ```bash
   $ docker ps
   ```

   *The status of a healthy Elasticsearch component should look as follows:*  

   ```
   91220e3285dd   docker.elastic.co/elasticsearch/elasticsearch:8.11.3   "/bin/tini -- /usr/lâ€¦"   11 hours ago   Up 11 hours (healthy)     9300/tcp, 0.0.0.0:9200-&gt;9200/tcp, :::9200-&gt;9200/tcp           ragflow-es-01
   ```

2. Follow [this document](./guides/run_health_check.md) to check the health status of the Elasticsearch service.

:::danger IMPORTANT
The status of a Docker container status does not necessarily reflect the status of the service. You may find that your services are unhealthy even when the corresponding Docker containers are up running. Possible reasons for this include network failures, incorrect port numbers, or DNS issues.
:::

3. If your container keeps restarting, ensure `vm.max_map_count` &gt;= 262144 as per [this README](https://github.com/infiniflow/ragflow?tab=readme-ov-file#-start-up-the-server). Updating the `vm.max_map_count` value in **/etc/sysctl.conf** is required, if you wish to keep your change permanent. Note that this configuration works only for Linux.

---

### Can't start ES container and get `Elasticsearch did not exit normally`

This is because you forgot to update the `vm.max_map_count` value in **/etc/sysctl.conf** and your change to this value was reset after a system reboot.

---

### `{"data":null,"code":100,"message":"&lt;NotFound '404: Not Found'&gt;"}`

Your IP address or port number may be incorrect. If you are using the default configurations, enter `http://&lt;IP_OF_YOUR_MACHINE&gt;` (**NOT 9380, AND NO PORT NUMBER REQUIRED!**) in your browser. This should work.

---

### `Ollama - Mistral instance running at 127.0.0.1:11434 but cannot add Ollama as model in RagFlow`

A correct Ollama IP address and port is crucial to adding models to Ollama:

- If you are on demo.ragflow.io, ensure that the server hosting Ollama has a publicly accessible IP address. Note that 127.0.0.1 is not a publicly accessible IP address.
- If you deploy RAGFlow locally, ensure that Ollama and RAGFlow are in the same LAN and can communicate with each other.

See [Deploy a local LLM](./guides/models/deploy_local_llm.mdx) for more information.

---

### Do you offer examples of using DeepDoc to parse PDF or other files?

Yes, we do. See the Python files under the **rag/app** folder.

---

### `FileNotFoundError: [Errno 2] No such file or directory`

1. Check the status of the MinIO Docker container:

   ```bash
   $ docker ps
   ```

   *The status of a healthy Elasticsearch component should look as follows:*  

   ```bash
   cd29bcb254bc   quay.io/minio/minio:RELEASE.2023-12-20T01-00-02Z       "/usr/bin/docker-entâ€¦"   2 weeks ago    Up 11 hours      0.0.0.0:9001-&gt;9001/tcp, :::9001-&gt;9001/tcp, 0.0.0.0:9000-&gt;9000/tcp, :::9000-&gt;9000/tcp     ragflow-minio
   ```

2. Follow [this document](./guides/run_health_check.md) to check the health status of the Elasticsearch service.

:::danger IMPORTANT
The status of a Docker container status does not necessarily reflect the status of the service. You may find that your services are unhealthy even when the corresponding Docker containers are up running. Possible reasons for this include network failures, incorrect port numbers, or DNS issues.
:::

---

## Usage

---

### How to run RAGFlow with a locally deployed LLM?

You can use Ollama or Xinference to deploy local LLM. See [here](./guides/models/deploy_local_llm.mdx) for more information.

---

### How to add an LLM that is not supported?

If your model is not currently supported but has APIs compatible with those of OpenAI, click **OpenAI-API-Compatible** on the **Model providers** page to configure your model:

![openai-api-compatible](https://github.com/user-attachments/assets/b1e964f2-b86e-41af-8528-fd8a96dc5f6f)

---

### How to integrate RAGFlow with Ollama?

- If RAGFlow is locally deployed, ensure that your RAGFlow and Ollama are in the same LAN.
- If you are using our online demo, ensure that the IP address of your Ollama server is public and accessible.

See [here](./guides/models/deploy_local_llm.mdx) for more information.

---

### How to change the file size limit?

For a locally deployed RAGFlow: the total file size limit per upload is 1GB, with a batch upload limit of 32 files. There is no cap on the total number of files per account. To update this 1GB file size limit:

- In **docker/.env**, upcomment `# MAX_CONTENT_LENGTH=1073741824`, adjust the value as needed, and note that `1073741824` represents 1GB in bytes.
- If you update the value of `MAX_CONTENT_LENGTH` in **docker/.env**, ensure that you update `client_max_body_size` in **nginx/nginx.conf** accordingly.

:::tip NOTE
It is not recommended to manually change the 32-file batch upload limit. However, if you use RAGFlow's HTTP API or Python SDK to upload files, the 32-file batch upload limit is automatically removed.
:::

---

### `Error: Range of input length should be [1, 30000]`

This error occurs because there are too many chunks matching your search criteria. Try reducing the **TopN** and increasing **Similarity threshold** to fix this issue:

1. Click **Chat** in the middle top of the page.
2. Right-click the desired conversation &gt; **Edit** &gt; **Prompt engine**
3. Reduce the **TopN** and/or raise **Similarity threshold**.
4. Click **OK** to confirm your changes.

![topn](https://github.com/infiniflow/ragflow/assets/93570324/7ec72ab3-0dd2-4cff-af44-e2663b67b2fc)

---

### How to get an API key for integration with third-party applications?

See [Acquire a RAGFlow API key](./develop/acquire_ragflow_api_key.md).

---

### How to upgrade RAGFlow?

See [Upgrade RAGFlow](./guides/upgrade_ragflow.mdx) for more information.

---

### How to switch the document engine to Infinity?

To switch your document engine from Elasticsearch to [Infinity](https://github.com/infiniflow/infinity):

1. Stop all running containers:  

   ```bash
   $ docker compose -f docker/docker-compose.yml down -v
   ```
:::caution WARNING
`-v` will delete all Docker container volumes, and the existing data will be cleared.
:::

2. In **docker/.env**, set `DOC_ENGINE=${DOC_ENGINE:-infinity}`
3. Restart your Docker image: 

   ```bash
   $ docker compose -f docker-compose.yml up -d
   ```

---

### Where are my uploaded files stored in RAGFlow's image?

All uploaded files are stored in Minio, RAGFlow's object storage solution. For instance, if you upload your file directly to a knowledge base, it is located at `&lt;knowledgebase_id&gt;/filename`.

---
</code></pre></div></div>
<p>release_notes.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /release_notes
---

# Releases

Key features, improvements and bug fixes in the latest releases.

:::info
Each RAGFlow release is available in two editions:
- **Slim edition**: excludes built-in embedding models and is identified by a **-slim** suffix added to the version name. Example: `infiniflow/ragflow:v0.18.0-slim`
- **Full edition**: includes built-in embedding models and has no suffix added to the version name. Example: `infiniflow/ragflow:v0.18.0`
:::

## v0.18.0

Released on April 23, 2025.

### Compatibility changes

From this release onwards, built-in rerank models have been removed because they have minimal impact on retrieval rates but significantly increase retrieval time.

### New features

- MCP server: enables access to RAGFlow's knowledge bases via MCP.
- DeepDoc supports adopting VLM model as a processing pipeline during document layout recognition, enabling in-depth analysis of images in PDF and DOCX files.
- OpenAI-compatible APIs: Agents can be called via OpenAI-compatible APIs.
- User registration control: administrators can enable or disable user registration through an environment variable.
- Team collaboration: Agents can be shared with team members.
- Agent version control: all updates are continuously logged and can be rolled back to a previous version via export.

![export_agent](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/export_agent_as_json.jpg)

### Improvements

- Enhanced answer referencing: Citation accuracy in generated responses is improved.
- Enhanced question-answering experience: users can now manually stop streaming output during a conversation.

### Documentation

#### Added documents

- [Set page rank](./guides/dataset/set_page_rank.md)
- [Enable RAPTOR](./guides/dataset/enable_raptor.md)
- [Set variables for your chat assistant](./guides/chat/set_chat_variables.md)
- [Launch RAGFlow MCP server](./develop/mcp/launch_mcp_server.md)

## v0.17.2

Released on March 13, 2025.

### Compatibility changes

- Removes the **Max_tokens** setting from **Chat configuration**.
- Removes the **Max_tokens** setting from **Generate**, **Rewrite**, **Categorize**, **Keyword** agent components.

From this release onwards, if you still see RAGFlow's responses being cut short or truncated, check the **Max_tokens** setting of your model provider.

### Improvements

- Adds OpenAI-compatible APIs.
- Introduces a German user interface.
- Accelerates knowledge graph extraction.
- Enables Tavily-based web search in the **Retrieval** agent component.
- Adds Tongyi-Qianwen QwQ models (OpenAI-compatible).
- Supports CSV files in the **General** chunking method.

### Fixed issues

- Unable to add models via Ollama/Xinference, an issue introduced in v0.17.1.

### Related APIs

#### HTTP APIs

- [Create chat completion](./references/http_api_reference.md#openai-compatible-api)

#### Python APIs

- [Create chat completion](./references/python_api_reference.md#openai-compatible-api)

## v0.17.1

Released on March 11, 2025.

### Improvements

- Improves English tokenization quality.
- Improves the table extraction logic in Markdown document parsing.
- Updates SiliconFlow's model list.
- Supports parsing XLS files (Excel 97-2003) with improved corresponding error handling.
- Supports Huggingface rerank models.
- Enables relative time expressions ("now", "yesterday", "last week", "next year", and more) in chat assistant and the **Rewrite** agent component.

### Fixed issues

- A repetitive knowledge graph extraction issue.
- Issues with API calling.
- Options in the **PDF parser**, aka **Document parser**, dropdown are missing.
- A Tavily web search issue.
- Unable to preview diagrams or images in an AI chat.

### Documentation

#### Added documents

- [Use tag set](./guides/dataset/use_tag_sets.md)

## v0.17.0

Released on March 3, 2025.

### New features

- AI chat: Implements Deep Research for agentic reasoning. To activate this, enable the **Reasoning** toggle under the **Prompt engine** tab of your chat assistant dialogue.
- AI chat: Leverages Tavily-based web search to enhance contexts in agentic reasoning. To activate this, enter the correct Tavily API key under the **Assistant settings** tab of your chat assistant dialogue.
- AI chat: Supports starting a chat without specifying knowledge bases.
- AI chat: HTML files can also be previewed and referenced, in addition to PDF files.
- Dataset: Adds a **PDF parser**, aka **Document parser**, dropdown menu to dataset configurations. This includes a DeepDoc model option, which is time-consuming, a much faster **naive** option (plain text), which skips DLA (Document Layout Analysis), OCR (Optical Character Recognition), and TSR (Table Structure Recognition) tasks, and several currently *experimental* large model options.
- Agent component: **(x)** or a forward slash `/` can be used to insert available keys (variables) in the system prompt field of the **Generate** or **Template** component.
- Object storage: Supports using Aliyun OSS (Object Storage Service) as a file storage option.
- Models: Updates the supported model list for Tongyi-Qianwen (Qwen), adding DeepSeek-specific models; adds ModelScope as a model provider.
- APIs: Document metadata can be updated through an API.

The following diagram illustrates the workflow of RAGFlow's Deep Research:

![Image](https://github.com/user-attachments/assets/f65d4759-4f09-4d9d-9549-c0e1fe907525)

The following is a screenshot of a conversation that integrates Deep Research:

![Image](https://github.com/user-attachments/assets/165b88ff-1f5d-4fb8-90e2-c836b25e32e9)

### Related APIs

#### HTTP APIs

Adds a body parameter `"meta_fields"` to the [Update document](./references/http_api_reference.md#update-document) method.

#### Python APIs

Adds a key option `"meta_fields"` to the [Update document](./references/python_api_reference.md#update-document) method.

### Documentation

#### Added documents

- [Run retrieval test](./guides/dataset/run_retrieval_test.md)

## v0.16.0

Released on February 6, 2025.

### New features

- Supports DeepSeek R1 and DeepSeek V3.
- GraphRAG refactor: Knowledge graph is dynamically built on an entire knowledge base (dataset) rather than on an individual file, and automatically updated when a newly uploaded file starts parsing. See [here](https://ragflow.io/docs/dev/construct_knowledge_graph).
- Adds an **Iteration** agent component and a **Research report generator** agent template. See [here](./guides/agent/agent_component_reference/iteration.mdx).
- New UI language: Portuguese.
- Allows setting metadata for a specific file in a knowledge base to enhance AI-powered chats. See [here](./guides/dataset/set_metadata.md).
- Upgrades RAGFlow's document engine [Infinity](https://github.com/infiniflow/infinity) to v0.6.0.dev3.
- Supports GPU acceleration for DeepDoc (see [docker-compose-gpu.yml](https://github.com/infiniflow/ragflow/blob/main/docker/docker-compose-gpu.yml)).
- Supports creating and referencing a **Tag** knowledge base as a key milestone towards bridging the semantic gap between query and response.

:::danger IMPORTANT
The **Tag knowledge base** feature is *unavailable* on the [Infinity](https://github.com/infiniflow/infinity) document engine.
:::

### Documentation

#### Added documents

- [Construct knowledge graph](./guides/dataset/construct_knowledge_graph.md)
- [Set metadata](./guides/dataset/set_metadata.md)
- [Begin component](./guides/agent/agent_component_reference/begin.mdx)
- [Generate component](./guides/agent/agent_component_reference/generate.mdx)
- [Interact component](./guides/agent/agent_component_reference/interact.mdx)
- [Retrieval component](./guides/agent/agent_component_reference/retrieval.mdx)
- [Categorize component](./guides/agent/agent_component_reference/categorize.mdx)
- [Keyword component](./guides/agent/agent_component_reference/keyword.mdx)
- [Message component](./guides/agent/agent_component_reference/message.mdx)
- [Rewrite component](./guides/agent/agent_component_reference/rewrite.mdx)
- [Switch component](./guides/agent/agent_component_reference/switch.mdx)
- [Concentrator component](./guides/agent/agent_component_reference/concentrator.mdx)
- [Template component](./guides/agent/agent_component_reference/template.mdx)
- [Iteration component](./guides/agent/agent_component_reference/iteration.mdx)
- [Note component](./guides/agent/agent_component_reference/note.mdx)

## v0.15.1

Released on December 25, 2024.

### Upgrades

- Upgrades RAGFlow's document engine [Infinity](https://github.com/infiniflow/infinity) to v0.5.2.
- Enhances the log display of document parsing status.

### Fixed issues

This release fixes the following issues:

- The `SCORE not found` and `position_int` errors returned by [Infinity](https://github.com/infiniflow/infinity).
- Once an embedding model in a specific knowledge base is changed, embedding models in other knowledge bases can no longer be changed.
- Slow response in question-answering and AI search due to repetitive loading of the embedding model.
- Fails to parse documents with RAPTOR.
- Using the **Table** parsing method results in information loss.
- Miscellaneous API issues.

### Related APIs

#### HTTP APIs

Adds an optional parameter `"user_id"` to the following APIs:

- [Create session with chat assistant](https://ragflow.io/docs/dev/http_api_reference#create-session-with-chat-assistant)
- [Update chat assistant's session](https://ragflow.io/docs/dev/http_api_reference#update-chat-assistants-session)
- [List chat assistant's sessions](https://ragflow.io/docs/dev/http_api_reference#list-chat-assistants-sessions)
- [Create session with agent](https://ragflow.io/docs/dev/http_api_reference#create-session-with-agent)
- [Converse with chat assistant](https://ragflow.io/docs/dev/http_api_reference#converse-with-chat-assistant)
- [Converse with agent](https://ragflow.io/docs/dev/http_api_reference#converse-with-agent)
- [List agent sessions](https://ragflow.io/docs/dev/http_api_reference#list-agent-sessions)

## v0.15.0

Released on December 18, 2024.

### New features

- Introduces additional Agent-specific APIs.
- Supports using page rank score to improve retrieval performance when searching across multiple knowledge bases.
- Offers an iframe in Chat and Agent to facilitate the integration of RAGFlow into your webpage.
- Adds a Helm chart for deploying RAGFlow on Kubernetes.
- Supports importing or exporting an agent in JSON format.
- Supports step run for Agent components/tools.
- Adds a new UI language: Japanese.
- Supports resuming GraphRAG and RAPTOR from a failure, enhancing task management resilience.
- Adds more Mistral models.
- Adds a dark mode to the UI, allowing users to toggle between light and dark themes.

### Improvements

- Upgrades the Document Layout Analysis model in DeepDoc.
- Significantly enhances the retrieval performance when using [Infinity](https://github.com/infiniflow/infinity) as document engine.

### Related APIs

#### HTTP APIs

- [List agent sessions](https://ragflow.io/docs/dev/http_api_reference#list-agent-sessions)
- [List agents](https://ragflow.io/docs/dev/http_api_reference#list-agents)

#### Python APIs

- [List agent sessions](https://ragflow.io/docs/dev/python_api_reference#list-agent-sessions)
- [List agents](https://ragflow.io/docs/dev/python_api_reference#list-agents)

## v0.14.1

Released on November 29, 2024.

### Improvements

Adds [Infinity's configuration file](https://github.com/infiniflow/ragflow/blob/main/docker/infinity_conf.toml) to facilitate integration and customization of [Infinity](https://github.com/infiniflow/infinity) as a document engine. From this release onwards, updates to Infinity's configuration can be made directly within RAGFlow and will take effect immediately after restarting RAGFlow using `docker compose`. [#3715](https://github.com/infiniflow/ragflow/pull/3715)

### Fixed issues

This release fixes the following issues:

- Unable to display or edit content of a chunk after clicking it.
- A `'Not found'` error in Elasticsearch.
- Chinese text becoming garbled during parsing.
- A compatibility issue with Polars.
- A compatibility issue between Infinity and GraphRAG.

## v0.14.0

Released on November 26, 2024.

### New features

- Supports [Infinity](https://github.com/infiniflow/infinity) or Elasticsearch (default) as document engine for vector storage and full-text indexing. [#2894](https://github.com/infiniflow/ragflow/pull/2894)
- Enhances user experience by adding more variables to the Agent and implementing auto-saving.
- Adds a three-step translation agent template, inspired by [Andrew Ng's translation agent](https://github.com/andrewyng/translation-agent).
- Adds an SEO-optimized blog writing agent template.
- Provides HTTP and Python APIs for conversing with an agent.
- Supports the use of English synonyms during retrieval processes.
- Optimizes term weight calculations, reducing the retrieval time by 50%.
- Improves task executor monitoring with additional performance indicators.
- Replaces Redis with Valkey.
- Adds three new UI languages (*contributed by the community*): Indonesian, Spanish, and Vietnamese.

### Compatibility changes

From this release onwards, **service_config.yaml.template** replaces **service_config.yaml** for configuring backend services. Upon Docker container startup, the environment variables defined in this template file are automatically populated and a **service_config.yaml** is auto-generated from it. [#3341](https://github.com/infiniflow/ragflow/pull/3341)

This approach eliminates the need to manually update **service_config.yaml** after making changes to **.env**, facilitating dynamic environment configurations.

:::danger IMPORTANT
Ensure that you [upgrade **both** your code **and** Docker image to this release](https://ragflow.io/docs/dev/upgrade_ragflow#upgrade-ragflow-to-the-most-recent-officially-published-release) before trying this new approach.
:::

### Related APIs

#### HTTP APIs

- [Create session with agent](https://ragflow.io/docs/dev/http_api_reference#create-session-with-agent)
- [Converse with agent](https://ragflow.io/docs/dev/http_api_reference#converse-with-agent)

#### Python APIs

- [Create session with agent](https://ragflow.io/docs/dev/python_api_reference#create-session-with-agent)
- [Converse with agent](https://ragflow.io/docs/dev/python_api_reference#create-session-with-agent)

### Documentation

#### Added documents

- [Configurations](https://ragflow.io/docs/dev/configurations)
- [Manage team members](./guides/team/manage_team_members.md)
- [Run health check on RAGFlow's dependencies](https://ragflow.io/docs/dev/run_health_check)

## v0.13.0

Released on October 31, 2024.

### New features

- Adds the team management functionality for all users.
- Updates the Agent UI to improve usability.
- Adds support for Markdown chunking in the **General** chunking method.
- Introduces an **invoke** tool within the Agent UI.
- Integrates support for Dify's knowledge base API.
- Adds support for GLM4-9B and Yi-Lightning models.
- Introduces HTTP and Python APIs for dataset management, file management within dataset, and chat assistant management.

:::tip NOTE
To download RAGFlow's Python SDK:

```bash
pip install ragflow-sdk==0.13.0
```
:::

### Documentation

#### Added documents

- [Acquire a RAGFlow API key](./develop/acquire_ragflow_api_key.md)
- [HTTP API Reference](./references/http_api_reference.md)
- [Python API Reference](./references/python_api_reference.md)

## v0.12.0

Released on September 30, 2024.

### New features

- Offers slim editions of RAGFlow's Docker images, which do not include built-in BGE/BCE embedding or reranking models.
- Improves the results of multi-round dialogues.
- Enables users to remove added LLM vendors.
- Adds support for **OpenTTS** and **SparkTTS** models.
- Implements an **Excel to HTML** toggle in the **General** chunking method, allowing users to parse a spreadsheet into either HTML tables or key-value pairs by row.
- Adds agent tools **YahooFinance** and **Jin10**.
- Adds an investment advisor agent template.

### Compatibility changes

From this release onwards, RAGFlow offers slim editions of its Docker images to improve the experience for users with limited Internet access. A slim edition of RAGFlow's Docker image does not include built-in BGE/BCE embedding models and has a size of about 1GB; a full edition of RAGFlow is approximately 9GB and includes both built-in embedding models and embedding models that will be downloaded once you select them in the RAGFlow UI.

The default Docker image edition is `nightly-slim`. The following list clarifies the differences between various editions:

- `nightly-slim`: The slim edition of the most recent tested Docker image.
- `v0.12.0-slim`: The slim edition of the most recent **officially released** Docker image.
- `nightly`: The full edition of the most recent tested Docker image.
- `v0.12.0`: The full edition of the most recent **officially released** Docker image.

See [Upgrade RAGFlow](https://ragflow.io/docs/dev/upgrade_ragflow) for instructions on upgrading.

### Documentation

#### Added documents

- [Upgrade RAGFlow](https://ragflow.io/docs/dev/upgrade_ragflow)

## v0.11.0

Released on September 14, 2024.

### New features

-  Introduces an AI search interface within the RAGFlow UI.
-  Supports audio output via **FishAudio** or **Tongyi Qwen TTS**.
-  Allows the use of Postgres for metadata storage, in addition to MySQL.
-  Supports object storage options with S3 or Azure Blob.
-  Supports model vendors: **Anthropic**, **Voyage AI**, and **Google Cloud**.
-  Supports the use of **Tencent Cloud ASR** for audio content recognition.
-  Adds finance-specific agent components: **WenCai**, **AkShare**, **YahooFinance**, and **TuShare**.
-  Adds a medical consultant agent template.
-  Supports running retrieval benchmarking on the following datasets:
    - [ms_marco_v1.1](https://huggingface.co/datasets/microsoft/ms_marco)
    - [trivia_qa](https://huggingface.co/datasets/mandarjoshi/trivia_qa)
    - [miracl](https://huggingface.co/datasets/miracl/miracl)

## v0.10.0

Released on August 26, 2024.

### New features

- Introduces a text-to-SQL template in the Agent UI.
- Implements Agent APIs.
- Incorporates monitoring for the task executor.
- Introduces Agent tools **GitHub**, **DeepL**, **BaiduFanyi**, **QWeather**, and **GoogleScholar**.
- Supports chunking of EML files.
- Supports more LLMs or model services: **GPT-4o-mini**, **PerfXCloud**, **TogetherAI**, **Upstage**, **Novita AI**, **01.AI**, **SiliconFlow**, **PPIO**, **XunFei Spark**, **Baidu Yiyan**, and **Tencent Hunyuan**.

## v0.9.0

Released on August 6, 2024.

### New features

- Supports GraphRAG as a chunking method.
- Introduces Agent component **Keyword** and search tools, including **Baidu**, **DuckDuckGo**, **PubMed**, **Wikipedia**, **Bing**, and **Google**.
- Supports speech-to-text recognition for audio files.
- Supports model vendors **Gemini** and **Groq**.
- Supports inference frameworks, engines, and services including **LM studio**, **OpenRouter**, **LocalAI**, and **Nvidia API**.
- Supports using reranker models in Xinference.

## v0.8.0

Released on July 8, 2024.

### New features

- Supports Agentic RAG, enabling graph-based workflow construction for RAG and agents.
- Supports model vendors **Mistral**, **MiniMax**, **Bedrock**, and **Azure OpenAI**.
- Supports DOCX files in the MANUAL chunking method.
- Supports DOCX, MD, and PDF files in the Q&amp;A chunking method.

## v0.7.0

Released on May 31, 2024.

### New features

- Supports the use of reranker models.
- Integrates reranker and embedding models: [BCE](https://github.com/netease-youdao/BCEmbedding), [BGE](https://github.com/FlagOpen/FlagEmbedding), and [Jina](https://jina.ai/embeddings/).
- Supports LLMs Baichuan and VolcanoArk.
- Implements [RAPTOR](https://arxiv.org/html/2401.18059v1) for improved text retrieval.
- Supports HTML files in the GENERAL chunking method.
- Provides HTTP and Python APIs for deleting documents by ID.
- Supports ARM64 platforms.

:::danger IMPORTANT
While we also test RAGFlow on ARM64 platforms, we do not maintain RAGFlow Docker images for ARM.

If you are on an ARM platform, follow [this guide](./develop/build_docker_image.mdx) to build a RAGFlow Docker image.
:::

### Related APIs

#### HTTP API

- [Delete documents](https://ragflow.io/docs/dev/http_api_reference#delete-documents)

#### Python API

- [Delete documents](https://ragflow.io/docs/dev/python_api_reference#delete-documents)

## v0.6.0

Released on May 21, 2024.

### New features

- Supports streaming output.
- Provides HTTP and Python APIs for retrieving document chunks.
- Supports monitoring of system components, including Elasticsearch, MySQL, Redis, and MinIO.
- Supports disabling **Layout Recognition** in the GENERAL chunking method to reduce file chunking time.

### Related APIs

#### HTTP API

- [Retrieve chunks](https://ragflow.io/docs/dev/http_api_reference#retrieve-chunks)

#### Python API

- [Retrieve chunks](https://ragflow.io/docs/dev/python_api_reference#retrieve-chunks)

## v0.5.0

Released on May 8, 2024.

### New features

- Supports LLM DeepSeek.

</code></pre></div></div>
<p>develop_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Developers"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides for hardcore developers"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>develop\acquire_ragflow_api_key.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /acquire_ragflow_api_key
---

# Acquire RAGFlow API key

An API key is required for the RAGFlow server to authenticate your HTTP/Python or MCP requests. This documents provides instructions on obtaining a RAGFlow API key.

1. Click your avatar in the top right corner of the RAGFlow UI to access the configuration page.
2. Click **API** to switch to the **API** page.
3. Obtain a RAGFlow API key:

![ragflow_api_key](https://github.com/user-attachments/assets/f461ed61-04c6-4faf-b3d8-6b5fa56be4e7)

:::tip NOTE
See the [RAGFlow HTTP API reference](../references/http_api_reference.md) or the [RAGFlow Python API reference](../references/python_api_reference.md) for a complete reference of RAGFlow's HTTP or Python APIs.
:::
</code></pre></div></div>
<p>develop\launch_ragflow_from_source.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /launch_ragflow_from_source
---

# Launch service from source

A guide explaining how to set up a RAGFlow service from its source code. By following this guide, you'll be able to debug using the source code.

## Target audience

Developers who have added new features or modified existing code and wish to debug using the source code, *provided that* their machine has the target deployment environment set up.

## Prerequisites

- CPU &amp;ge; 4 cores
- RAM &amp;ge; 16 GB
- Disk &amp;ge; 50 GB
- Docker &amp;ge; 24.0.0 &amp; Docker Compose &amp;ge; v2.26.1

:::tip NOTE
If you have not installed Docker on your local machine (Windows, Mac, or Linux), see the [Install Docker Engine](https://docs.docker.com/engine/install/) guide.
:::

## Launch a service from source

To launch a RAGFlow service from source code:

### Clone the RAGFlow repository

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
```

### Install Python dependencies

1. Install uv:
   
   ```bash
   pipx install uv
   ```

2. Install Python dependencies:
   - slim:
   ```bash
   uv sync --python 3.10 # install RAGFlow dependent python modules
   ```
   - full:
   ```bash
   uv sync --python 3.10 --all-extras # install RAGFlow dependent python modules
   ```
   *A virtual environment named `.venv` is created, and all Python dependencies are installed into the new environment.*

### Launch third-party services

The following command launches the 'base' services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:

```bash
docker compose -f docker/docker-compose-base.yml up -d
```

### Update `host` and `port` Settings for Third-party Services

1. Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/service_conf.yaml.template** to `127.0.0.1`:

   ```
   127.0.0.1       es01 infinity mysql minio redis
   ```

2. In **docker/service_conf.yaml.template**, update mysql port to `5455` and es port to `1200`, as specified in **docker/.env**.

### Launch the RAGFlow backend service

1. Comment out the `nginx` line in **docker/entrypoint.sh**.

   ```
   # /usr/sbin/nginx
   ```

2. Activate the Python virtual environment:

   ```bash
   source .venv/bin/activate
   export PYTHONPATH=$(pwd)
   ```

3. **Optional:** If you cannot access HuggingFace, set the HF_ENDPOINT environment variable to use a mirror site:
 
   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```

4. Check the configuration in **conf/service_conf.yaml**, ensuring all hosts and ports are correctly set.
   
5. Run the **entrypoint.sh** script to launch the backend service:

   ```shell
   JEMALLOC_PATH=$(pkg-config --variable=libdir jemalloc)/libjemalloc.so;
   LD_PRELOAD=$JEMALLOC_PATH python rag/svr/task_executor.py 1;
   ```
   ```shell
   python api/ragflow_server.py;
   ```

### Launch the RAGFlow frontend service

1. Navigate to the `web` directory and install the frontend dependencies:

   ```bash
   cd web
   npm install
   ```

2. Update `proxy.target` in **.umirc.ts** to `http://127.0.0.1:9380`:

   ```bash
   vim .umirc.ts
   ```

3. Start up the RAGFlow frontend service:

   ```bash
   npm run dev 
   ```

   *The following message appears, showing the IP address and port number of your frontend service:*  

   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)

### Access the RAGFlow service

In your web browser, enter `http://127.0.0.1:&lt;PORT&gt;/`, ensuring the port number matches that shown in the screenshot above.

### Stop the RAGFlow service when the development is done

1. Stop the RAGFlow frontend service:
   ```bash
   pkill npm
   ```

2. Stop the RAGFlow backend service:
   ```bash
   pkill -f "docker/entrypoint.sh"
   ```

</code></pre></div></div>
<p>develop\mcp_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"MCP"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides and references on accessing RAGFlow's knowledge bases via MCP."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>develop\mcp\mcp_client_example.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /mcp_client
---

# RAGFlow MCP client example

We provide a *prototype* MCP client example for testing [here](https://github.com/infiniflow/ragflow/blob/main/mcp/client/client.py).

:::danger IMPORTANT
If your MCP server is running in host mode, include your acquired API key in your client's `headers` as shown below:
```python
async with sse_client("http://localhost:9382/sse", headers={"api_key": "YOUR_KEY_HERE"}) as streams:
    # Rest of your code...
```
:::
</code></pre></div></div>
<p>develop\mcp\mcp_tools.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /mcp_tools
---

# RAGFlow MCP tools

The MCP server currently offers a specialized tool to assist users in searching for relevant information powered by RAGFlow DeepDoc technology:

- **retrieve**: Fetches relevant chunks from specified `dataset_ids` and optional `document_ids` using the RAGFlow retrieve interface, based on a given question. Details of all available datasets, namely, `id` and `description`, are provided within the tool description for each individual dataset.

For more information, see our Python implementation of the [MCP server](https://github.com/infiniflow/ragflow/blob/main/mcp/server/server.py).
</code></pre></div></div>
<p>guides_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides for RAGFlow users and developers."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\manage_files.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 6
slug: /manage_files
---

# Files

Knowledge base, hallucination-free chat, and file management are the three pillars of RAGFlow. RAGFlow's file management allows you to upload files individually or in bulk. You can then link an uploaded file to multiple target knowledge bases. This guide showcases some basic usages of the file management feature.

:::info IMPORTANT
Compared to uploading files directly to various knowledge bases, uploading them to RAGFlow's file management and then linking them to different knowledge bases is *not* an unnecessary step, particularly when you want to delete some parsed files or an entire knowledge base but retain the original files.
:::

## Create folder

RAGFlow's file management allows you to establish your file system with nested folder structures. To create a folder in the root directory of RAGFlow: 

![create new folder](https://github.com/infiniflow/ragflow/assets/93570324/3a37a5f4-43a6-426d-a62a-e5cd2ff7a533)

:::caution NOTE
Each knowledge base in RAGFlow has a corresponding folder under the **root/.knowledgebase** directory. You are not allowed to create a subfolder within it.
:::

## Upload file

RAGFlow's file management supports file uploads from your local machine, allowing both individual and bulk uploads: 

![upload file](https://github.com/infiniflow/ragflow/assets/93570324/5d7ded14-ce2b-4703-8567-9356a978f45c)

![bulk upload](https://github.com/infiniflow/ragflow/assets/93570324/def0db55-824c-4236-b809-a98d8c8674e3)

## Preview file

RAGFlow's file management supports previewing files in the following formats:

- Documents (PDF, DOCS)
- Tables (XLSX)
- Pictures (JPEG, JPG, PNG, TIF, GIF)

![preview](https://github.com/infiniflow/ragflow/assets/93570324/2e931362-8bbf-482c-ac86-b68b09d331bc)

## Link file to knowledge bases

RAGFlow's file management allows you to *link* an uploaded file to multiple knowledge bases, creating a file reference in each target knowledge base. Therefore, deleting a file in your file management will AUTOMATICALLY REMOVE all related file references across the knowledge bases.

![link knowledgebase](https://github.com/infiniflow/ragflow/assets/93570324/6c6b8db4-3269-4e35-9434-6089887e3e3f)

You can link your file to one knowledge base or multiple knowledge bases at one time: 

![link multiple kb](https://github.com/infiniflow/ragflow/assets/93570324/6c508803-fb1f-435d-b688-683066fd7fff)

## Move file to a specific folder

![move files](https://github.com/user-attachments/assets/3a2db469-6811-4ea0-be80-403b61ffe257)

## Search files or folders

**File Management** only supports file name and folder name filtering in the current directory (files or folders in the child directory will not be retrieved).

![search file](https://github.com/infiniflow/ragflow/assets/93570324/77ffc2e5-bd80-4ed1-841f-068e664efffe)

## Rename file or folder

RAGFlow's file management allows you to rename a file or folder:

![rename_file](https://github.com/infiniflow/ragflow/assets/93570324/5abb0704-d9e9-4b43-9ed4-5750ccee011f)


## Delete files or folders

RAGFlow's file management allows you to delete files or folders individually or in bulk. 

To delete a file or folder: 

![delete file](https://github.com/infiniflow/ragflow/assets/93570324/85872728-125d-45e9-a0ee-21e9d4cedb8b)

To bulk delete files or folders:

![bulk delete](https://github.com/infiniflow/ragflow/assets/93570324/519b99ab-ec7f-4c8a-8cea-e0b6dcb3cb46)

&gt; - You are not allowed to delete the **root/.knowledgebase** folder. 
&gt; - Deleting files that have been linked to knowledge bases will **AUTOMATICALLY REMOVE** all associated file references across the knowledge bases.

## Download uploaded file

RAGFlow's file management allows you to download an uploaded file:

![download_file](https://github.com/infiniflow/ragflow/assets/93570324/cf3b297f-7d9b-4522-bf5f-4f45743e4ed5)

&gt; As of RAGFlow v0.18.0, bulk download is not supported, nor can you download an entire folder. 

</code></pre></div></div>
<p>guides\run_health_check.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 8
slug: /run_health_check
---

# Monitoring

Double-check the health status of RAGFlow's dependencies.

---

The operation of RAGFlow depends on four services:

- **Elasticsearch** (default) or [Infinity](https://github.com/infiniflow/infinity) as the document engine
- **MySQL**
- **Redis**
- **MinIO** for object storage

If an exception or error occurs related to any of the above services, such as `Exception: Can't connect to ES cluster`, refer to this document to check their health status.

You can also click you avatar in the top right corner of the page **&gt;** System to view the visualized health status of RAGFlow's core services. The following screenshot shows that all services are 'green' (running healthily). The task executor displays the *cumulative* number of completed and failed document parsing tasks from the past 30 minutes:

![system_status_page](https://github.com/user-attachments/assets/b0c1a11e-93e3-4947-b17a-1bfb4cdab6e4)

Services with a yellow or red light are not running properly. The following is a screenshot of the system page after running `docker stop ragflow-es-10`:

![es_failed](https://github.com/user-attachments/assets/06056540-49f5-48bf-9cc9-a7086bc75790)

You can click on a specific 30-second time interval to view the details of completed and failed tasks:

![done_tasks](https://github.com/user-attachments/assets/49b25ec4-03af-48cf-b2e5-c892f6eaa261)

![done_vs_failed](https://github.com/user-attachments/assets/eaa928d0-a31c-4072-adea-046091e04599)

</code></pre></div></div>
<p>guides\upgrade_ragflow.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 11
slug: /upgrade_ragflow
---

# Upgrading
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Upgrade RAGFlow to `nightly-slim`/`nightly` or the latest, published release.

:::info NOTE
Upgrading RAGFlow in itself will *not* remove your uploaded/historical data. However, be aware that `docker compose -f docker/docker-compose.yml down -v` will remove Docker container volumes, resulting in data loss.
:::

## Upgrade RAGFlow to `nightly-slim`/`nightly`, the most recent, tested Docker image

`nightly-slim` refers to the RAGFlow Docker image *without* embedding models, while `nightly` refers to the RAGFlow Docker image with embedding models. For details on their differences, see [ragflow/docker/.env](https://github.com/infiniflow/ragflow/blob/main/docker/.env).

To upgrade RAGFlow, you must upgrade **both** your code **and** your Docker image:

1. Clone the repo

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   ```

2. Update **ragflow/docker/.env**:

&lt;Tabs
  defaultValue="nightly-slim"
  values={[
    {label: 'nightly-slim', value: 'nightly-slim'},
    {label: 'nightly', value: 'nightly'},
  ]}&gt;
  &lt;TabItem value="nightly-slim"&gt;

```bash
RAGFLOW_IMAGE=infiniflow/ragflow:nightly-slim
```

  &lt;/TabItem&gt;
  &lt;TabItem value="nightly"&gt;

```bash
RAGFLOW_IMAGE=infiniflow/ragflow:nightly
```

  &lt;/TabItem&gt;
&lt;/Tabs&gt;

3. Update RAGFlow image and restart RAGFlow:

   ```bash
   docker compose -f docker/docker-compose.yml pull
   docker compose -f docker/docker-compose.yml up -d
   ```

## Upgrade RAGFlow to the most recent, officially published release

To upgrade RAGFlow, you must upgrade **both** your code **and** your Docker image:

1. Clone the repo

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   ```

2. Switch to the latest, officially published release, e.g., `v0.18.0`:

   ```bash
   git checkout -f v0.18.0
   ```

3. Update **ragflow/docker/.env** as follows:

   ```bash
   RAGFLOW_IMAGE=infiniflow/ragflow:v0.18.0
   ```

4. Update the RAGFlow image and restart RAGFlow:

   ```bash
   docker compose -f docker/docker-compose.yml pull
   docker compose -f docker/docker-compose.yml up -d
   ```

## Frequently asked questions

### Upgrade RAGFlow in an offline environment (without Internet access)

1. From an environment with Internet access, pull the required Docker image.
2. Save the Docker image to a **.tar** file.
   ```bash
   docker save -o ragflow.v0.18.0.tar infiniflow/ragflow:v0.18.0
   ```
3. Copy the **.tar** file to the target server.
4. Load the **.tar** file into Docker:
   ```bash
   docker load -i ragflow.v0.18.0.tar
   ```

</code></pre></div></div>
<p>guides\agent_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Agents"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"RAGFlow v0.8.0 introduces an agent mechanism, featuring a no-code workflow editor on the front end and a comprehensive graph-based task orchestration framework on the backend."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\agent\agent_introduction.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /agent_introduction
---

# Introduction to agents

Key concepts, basic operations, a quick view of the agent editor.

---

## Key concepts

Agents and RAG are complementary techniques, each enhancing the otherâ€™s capabilities in business applications. RAGFlow v0.8.0 introduces an agent mechanism, featuring a no-code workflow editor on the front end and a comprehensive graph-based task orchestration framework on the back end. This mechanism is built on top of RAGFlow's existing RAG solutions and aims to orchestrate search technologies such as query intent classification, conversation leading, and query rewriting to:

- Provide higher retrievals and,
- Accommodate more complex scenarios.

## Create an agent

:::tip NOTE

Before proceeding, ensure that:  

1. You have properly set the LLM to use. See the guides on [Configure your API key](../models/llm_api_key_setup.md) or [Deploy a local LLM](../models/deploy_local_llm.mdx) for more information.
2. You have a knowledge base configured and the corresponding files properly parsed. See the guide on [Configure a knowledge base](../dataset/configure_knowledge_base.md) for more information.

:::

Click the **Agent** tab in the middle top of the page to show the **Agent** page. As shown in the screenshot below, the cards on this page represent the created agents, which you can continue to edit.

![agent_mainpage](https://github.com/user-attachments/assets/5c0bb123-8f4e-42ea-b250-43f640dc6814)

We also provide templates catered to different business scenarios. You can either generate your agent from one of our agent templates or create one from scratch:

1. Click **+ Create agent** to show the **agent template** page:

   ![agent_templates](https://github.com/user-attachments/assets/73bd476c-4bab-4c8c-82f8-6b00fb2cd044)

2. To create an agent from scratch, click the **Blank** card. Alternatively, to create an agent from one of our templates, hover over the desired card, such as **General-purpose chatbot**, click **Use this template**, name your agent in the pop-up dialogue, and click **OK** to confirm.  

   *You are now taken to the **no-code workflow editor** page. The left panel lists the components (operators): Above the dividing line are the RAG-specific components; below the line are tools. We are still working to expand the component list.*

   ![workflow_editor](https://github.com/user-attachments/assets/47b4d5ce-b35a-4d6b-b483-ba495a75a65d)

3. General speaking, now you can do the following:
   - Drag and drop a desired component to your workflow,
   - Select the knowledge base to use,
   - Update settings of specific components,
   - Update LLM settings
   - Sets the input and output for a specific component, and more.
4. Click **Save** to apply changes to your agent and **Run** to test it.

## Components

Please review the flowing description of the RAG-specific components before you proceed:

| Component      | Description                                                                                                                                                                                              |
|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Retrieval**  | A component that retrieves information from specified knowledge bases and returns 'Empty response' if no information is found. Ensure the correct knowledge bases are selected.                          |
| **Generate**   | A component that prompts the LLM to generate responses. You must ensure the prompt is set correctly.                                                                                                     |
| **Interact**   | A component that serves as the interface between human and the bot, receiving user inputs and displaying the agent's responses.                                                                          |
| **Categorize** | A component that uses the LLM to classify user inputs into predefined categories. Ensure you specify the name, description, and examples for each category, along with the corresponding next component. |
| **Message**    | A component that sends out a static message. If multiple messages are supplied, it randomly selects one to send. Ensure its downstream is **Interact**, the interface component.                         |
| **Rewrite**    | A component that rewrites a user query from the **Interact** component, based on the context of previous dialogues.                                                                                      |
| **Keyword**    | A component that extracts keywords from a user query, with TopN specifying the number of keywords to extract.                                                                                            |

:::caution NOTE

- Ensure **Rewrite**'s upstream component is **Relevant** and downstream component is **Retrieval**.
- Ensure the downstream component of **Message** is **Interact**.
- The downstream component of **Begin** is always **Interact**.

:::

## Basic operations

| Operation                 | Description                                                                                                                              |
|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------|
| Add a component           | Drag and drop the desired component from the left panel onto the canvas.                                                                 |
| Delete a component        | On the canvas, hover over the three dots (...) of the component to display the delete option, then select it to remove the component.    |
| Copy a component          | On the canvas, hover over the three dots (...) of the component to display the copy option, then select it to make a copy the component. |
| Update component settings | On the canvas, click the desired component to display the component settings.                                                            |

</code></pre></div></div>
<p>guides\agent\embed_agent_into_webpage.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /embed_agent_into_webpage
---

# Embed agent into webpage

You can use iframe to embed an agent into a third-party webpage.

:::caution WARNING
If your agent's **Begin** component takes a variable, you *cannot* embed it into a webpage.
:::

1. Before proceeding, you must [acquire an API key](../models/llm_api_key_setup.md); otherwise, an error message would appear.
2. On the **Agent** page, click an intended agent **&gt;** **Edit** to access its editing page.
3. Click **Embed into webpage** on the top right corner of the canvas to show the **iframe** window:

   ![agent_embed](https://github.com/user-attachments/assets/f748bb91-1a48-45ca-89ea-5b1c257407cb)

4. Copy the iframe and embed it into a specific location on your webpage.

</code></pre></div></div>
<p>guides\agent\general_purpose_chatbot.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /general_purpose_chatbot
---

# Create chatbot

Create a general-purpose chatbot.

---

Chatbot is one of the most common AI scenarios. However, effectively understanding user queries and responding appropriately remains a challenge. RAGFlow's general-purpose chatbot agent is our attempt to tackle this longstanding issue.  

This chatbot closely resembles the chatbot introduced in [Start an AI chat](../chat/start_chat.md), but with a key difference - it introduces a reflective mechanism that allows it to improve the retrieval from the target knowledge bases by rewriting the user's query.

This document provides guides on creating such a chatbot using our chatbot template.

## Prerequisites

1. Ensure you have properly set the LLM to use. See the guides on [Configure your API key](../models/llm_api_key_setup.md) or [Deploy a local LLM](../models/deploy_local_llm.mdx) for more information.
2. Ensure you have a knowledge base configured and the corresponding files properly parsed. See the guide on [Configure a knowledge base](../dataset/configure_knowledge_base.md) for more information.
3. Make sure you have read the [Introduction to Agentic RAG](./agent_introduction.md).

## Create a chatbot agent from template

To create a general-purpose chatbot agent using our template:

1. Click the **Agent** tab in the middle top of the page to show the **Agent** page.
2. Click **+ Create agent** on the top right of the page to show the **agent template** page.
3. On the **agent template** page, hover over the card on **General-purpose chatbot** and click **Use this template**.  
   *You are now directed to the **no-code workflow editor** page.*

   ![workflow_editor](https://github.com/user-attachments/assets/52e7dc62-4bf5-4fbb-ab73-4a6e252065f0)

:::tip NOTE
RAGFlow's no-code editor spares you the trouble of coding, making agent development effortless.
:::

## Understand each component in the template

Hereâ€™s a breakdown of each component and its role and requirements in the chatbot template:

- **Begin**
  - Function: Sets an opening greeting for users.
  - Purpose: Establishes a welcoming atmosphere and prepares the user for interaction.

- **Interact**
  - Function: Serves as the interface between human and the bot.
  - Role: Acts as the downstream component of **Begin**.  

- **Retrieval**
  - Function: Retrieves information from specified knowledge base(s).
  - Requirement: Must have `knowledgebases` set up to function.

- **Relevant**
  - Function: Assesses the relevance of the retrieved information from the **Retrieval** component to the user query.
  - Process:  
    - If relevant, it directs the data to the **Generate** component for final response generation.
    - Otherwise, it triggers the **Rewrite** component to refine the user query and redo the retrival process.

- **Generate**
  - Function: Prompts the LLM to generate responses based on the retrieved information.  
  - Note: The prompt settings allow you to control the way in which the LLM generates responses. Be sure to review the prompts and make necessary changes.

- **Rewrite**:  
  - Function: Refines a user query when no relevant information from the knowledge base is retrieved.  
  - Usage: Often used in conjunction with **Relevant** and **Retrieval** to create a reflective/feedback loop.  

## Configure your chatbot agent

1. Click **Begin** to set an opening greeting:  
   ![opener](https://github.com/user-attachments/assets/4416bc16-2a84-4f24-a19b-6dc8b1de0908)

2. Click **Retrieval** to select the right knowledge base(s) and make any necessary adjustments:  
   ![setting_knowledge_bases](https://github.com/user-attachments/assets/5f694820-5651-45bc-afd6-cf580ca0228d)

3. Click **Generate** to configure the LLM's summarization behavior:  
   3.1. Confirm the model.  
   3.2. Review the prompt settings. If there are variables, ensure they match the correct component IDs:  
   ![prompt_settings](https://github.com/user-attachments/assets/19e94ea7-7f62-4b73-b526-32fcfa62f1e9)

4. Click **Relevant** to review or change its settings:  
   *You may retain the current settings, but feel free to experiment with changes to understand how the agent operates.*
   ![relevant_settings](https://github.com/user-attachments/assets/9ff7fdd8-7a69-4ee2-bfba-c7fb8029150f)

5. Click **Rewrite** to select a different model for query rewriting or update the maximum loop times for query rewriting:  
   ![choose_model](https://github.com/user-attachments/assets/2bac1d6c-c4f1-42ac-997b-102858c3f550)
   ![loop_time](https://github.com/user-attachments/assets/09a4ce34-7aac-496f-aa59-d8aa33bf0b1f)

:::danger NOTE
Increasing the maximum loop times may significantly extend the time required to receive the final response.
:::

1. Update your workflow where you see necessary.

2. Click to **Save** to apply your changes.  
   *Your agent appears as one of the agent cards on the **Agent** page.*

## Test your chatbot agent

1. Find your chatbot agent on the **Agent** page:  
   ![find_chatbot](https://github.com/user-attachments/assets/6e6382c6-9a86-4190-9fdd-e363b7f64ba9)

2. Experiment with your questions to verify if this chatbot functions as intended:  
   ![test_chatbot](https://github.com/user-attachments/assets/c074d3bd-4c39-4b05-a68b-1fd361f256b3)
</code></pre></div></div>
<p>guides\agent\agent_component_reference_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Agent Components"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"A complete reference for RAGFlow's agent components."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\agent\agent_component_reference\begin.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /begin_component
---

# Begin component

The starting component in a workflow.

---

The **Begin** component sets an opening greeting or accepts inputs from the user. It is automatically populated onto the canvas when you create an agent, whether from a template or from scratch (from a blank template). There should be only one **Begin** component in the workflow.

## Scenarios

A **Begin** component is essential in all cases. Every agent includes a **Begin** component, which cannot be deleted.

## Configurations

Click the component to display its **Configuration** window. Here, you can set an opening greeting and the input parameters (global variables) for the agent.

### ID

The ID is the unique identifier for the component within the workflow. Unlike the IDs of other components, the ID of the **Begin** component *cannot* be changed.

### Opening greeting

An opening greeting is the agent's first message to the user. It can be a welcoming remark or an instruction to guide the user forward.

### Global variables

You can set global variables within the **Begin** component, which can be either required or optional. Once established, users will need to provide values for these variables when interacting or chatting with the agent. Click **+ Add variable** to add a global variable, each with the following attributes:

- **Key**: *Required*  
  The unique variable name.
- **Name**: *Required*  
  A descriptive name providing additional details about the variable.  
  For example, if **Key** is set to `lang`, you can set its **Name** to `Target language`.
- **Type**: *Required*  
  The type of the variable:  
  - **line**: Accepts a single line of text without line breaks.
  - **paragraph**: Accepts multiple lines of text, including line breaks.
  - **options**: Requires the user to select a value for this variable from a dropdown menu. And you are required to set *at least* one option for the dropdown menu.
  - **file**: Requires the user to upload one or multiple files.
  - **integer**: Accepts an integer as input.
  - **boolean**: Requires the user to toggle between on and off.
- **Optional**: A toggle indicating whether the variable is optional. 

:::tip NOTE
To pass in parameters from a client, call:
- HTTP method [Converse with agent](../../../references/http_api_reference.md#converse-with-agent), or
- Python method [Converse with agent](../../../references/python_api_reference.md#converse-with-agent).
:::

:::danger IMPORTANT
- If you set the key type as **file**, ensure the token count of the uploaded file does not exceed your model provider's maximum token limit; otherwise, the plain text in your file will be truncated and incomplete.
- If your agent's **Begin** component takes a variable, you *cannot* embed it into a webpage.
:::

## Examples

As mentioned earlier, the **Begin** component is indispensable for an agent. Still, you can take a look at our three-step interpreter agent template, where the **Begin** component takes two global variables:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Interpreter** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Begin** component to display its **Configuration** window.

## Frequently asked questions

### Is the uploaded file in a knowledge base?

No. Files uploaded to an agent as input are not stored in a knowledge base and hence will not be processed using RAGFlow's built-in OCR, DLR or TSR models, or chunked using RAGFlow's built-in chunking methods. 

### How to upload a webpage or file from a URL?

If you set the type of a variable as **file**, your users will be able to upload a file either from their local device or from an accessible URL. For example:

![upload_file](https://github.com/user-attachments/assets/7ad2a352-0807-4b74-b8d1-d09e5cd37997)

### File size limit for an uploaded file

There is no *specific* file size limit for a file uploaded to an agent. However, note that model providers typically have a default or explicit maximum token setting, which can range from 8196 to 128k: The plain text part of the uploaded file will be passed in as the key value, but if the file's token count exceeds this limit, the string will be truncated and incomplete.

:::tip NOTE
The variables `MAX_CONTENT_LENGTH` in `/docker/.env` and `client_max_body_size` in `/docker/nginx/nginx.conf` set the file size limit for each upload to a knowledge base or **File Management**. These settings DO NOT apply in this scenario.
:::
</code></pre></div></div>
<p>guides\agent\agent_component_reference\categorize.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 5
slug: /categorize_component
---

# Categorize component

A component that classifies user inputs and applies strategies accordingly. 

---

A **Categorize** component is usually the downstream of the **Interact** component.

## Scenarios

A **Categorize** component is essential when you need the LLM to help you identify user intentions and apply appropriate processing strategies.

## Configurations

### Input

The **Categorize** component relies on input variables to specify its data inputs (queries). Click **+ Add variable** in the **Input** section to add the desired input variables. There are two types of input variables: **Reference** and **Text**.

- **Reference**: Uses a component's output or a user input as the data source. You are required to select from the dropdown menu:
  - A component ID under **Component Output**, or 
  - A global variable under **Begin input**, which is defined in the **Begin** component.
- **Text**: Uses fixed text as the query. You are required to enter static text.

### Model

Click the dropdown menu of **Model** to show the model configuration window.

- **Model**: The chat model to use.  
  - Ensure you set the chat model correctly on the **Model providers** page.
  - You can use different models for different components to increase flexibility or improve overall performance.
- **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.  
  This parameter has three options:  
  - **Improvise**: Produces more creative responses.
  - **Precise**: (Default) Produces more conservative responses.
  - **Balance**: A middle ground between **Improvise** and **Precise**.
- **Temperature**: The randomness level of the model's output.  
  Defaults to 0.1.  
  - Lower values lead to more deterministic and predictable outputs.
  - Higher values lead to more creative and varied outputs.
  - A temperature of zero results in the same output for the same prompt.
- **Top P**: Nucleus sampling.  
  - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
  - Defaults to 0.3.
- **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
  - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
  - Defaults to 0.4.
- **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
  - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
  - Defaults to 0.7.

:::tip NOTE
- It is not necessary to stick with the same model for all components. If a specific model is not performing well for a particular task, consider using a different one.
- If you are uncertain about the mechanism behind **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**, simply choose one of the three options of **Preset configurations**.
:::

### Message window size

An integer specifying the number of previous dialogue rounds to input into the LLM. For example, if it is set to 12, the tokens from the last 12 dialogue rounds will be fed to the LLM. This feature consumes additional tokens.

Defaults to 1.

:::tip IMPORTANT
This feature is used for multi-turn dialogue *only*. If your **Categorize** component is not part of a multi-turn dialogue (i.e., it is not in a loop), leave this field as-is.
:::

### Category name

A **Categorize** component must have at least two categories. This field sets the name of the category. Click **+ Add Item** to include the intended categories. 

:::tip NOTE
You will notice that the category name is auto-populated. No worries. Each category is assigned a random name upon creation. Feel free to change it to a name that is understandable to the LLM.
:::

#### Description

Description of this category.  

You can input criteria, situation, or information that may help the LLM determine which inputs belong in this category.

#### Examples

Additional examples that may help the LLM determine which inputs belong in this category.

:::danger IMPORTANT
Examples are more helpful than the description if you want the LLM to classify particular cases into this category.
:::

#### Next step

Specifies the downstream component of this category.

- Once you specify the ID of the downstream component, a link is established between this category and the corresponding component.
- If you manually link this category to a downstream component on the canvas, the ID of that component is auto-populated.

## Examples

You can explore our customer service agent template, where a **Categorize** component (component ID: **Question Categorize**) has four defined categories and takes data inputs from an **Interact** component (component ID: **Interface**):

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Interpreter** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.


</code></pre></div></div>
<p>guides\agent\agent_component_reference\concentrator.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 10
slug: /concentrator_component
---

# Concentrator component

A component that directs execution flow to multiple downstream components.

---

The **Concentrator** component acts as a "repeater" of execution flow, transmitting a flow to multiple downstream components.


## Scenarios

A **Concentrator** component enhances the current UX design. For a component originally designed to support only one downstream component, you can append a **Concentrator**, enabling it to have multiple downstream components.

## Examples

Explore our general-purpose chatbot agent template, featuring a **Concentrator** component (component ID: **medical**) that relays an execution flow from category 2 of the **Categorize** component to two translator components:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **General-purpose chatbot** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\generate.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /generate_component
---

# Generate component

The component that prompts the LLM to respond appropriately.

---

A **Generate** component fine-tunes the LLM and sets its prompt.

## Scenarios

A **Generate** component is essential when you need the LLM to assist with summarizing, translating, or controlling various tasks. 

## Configurations

### Model

Click the dropdown menu of **Model** to show the model configuration window.

- **Model**: The chat model to use.  
  - Ensure you set the chat model correctly on the **Model providers** page.
  - You can use different models for different components to increase flexibility or improve overall performance.
- **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.   
  This parameter has three options:
  - **Improvise**: Produces more creative responses.
  - **Precise**: (Default) Produces more conservative responses.
  - **Balance**: A middle ground between **Improvise** and **Precise**.
- **Temperature**: The randomness level of the model's output.  
  Defaults to 0.1.
  - Lower values lead to more deterministic and predictable outputs.
  - Higher values lead to more creative and varied outputs.
  - A temperature of zero results in the same output for the same prompt.
- **Top P**: Nucleus sampling.  
  - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
  - Defaults to 0.3.
- **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
  - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
  - Defaults to 0.4.
- **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
  - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
  - Defaults to 0.7.

:::tip NOTE
- It is not necessary to stick with the same model for all components. If a specific model is not performing well for a particular task, consider using a different one.
- If you are uncertain about the mechanism behind **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**, simply choose one of the three options of **Preset configurations**.
:::

### System prompt

Typically, you use the system prompt to describe the task for the LLM, specify how it should respond, and outline other miscellaneous requirements. We do not plan to elaborate on this topic, as it can be as extensive as prompt engineering. However, please be aware that the system prompt is often used in conjunction with keys (variables), which serve as various data inputs for the LLM. 

:::danger IMPORTANT
A **Generate** component relies on keys (variables) to specify its data inputs. Its immediate upstream component is *not* necessarily its data input, and the arrows in the workflow indicate *only* the processing sequence. Keys in a **Generate** component are used in conjunction with the system prompt to specify data inputs for the LLM. Use a forward slash `/` or the **(x)** button to show the keys to use.
:::

Below is a prompt excerpt of a **Generate** component from the **Interpreter** template (component ID: **Reflect**):

```text
Your task is to read a source text and a translation to {target_lang}, and give constructive suggestions to improve the translation. The source text and initial translation, delimited by XML tags &lt;SOURCE_TEXT&gt;&lt;/SOURCE_TEXT&gt; and &lt;TRANSLATION&gt;&lt;/TRANSLATION&gt;, are as follows:

&lt;SOURCE_TEXT&gt;
{source_text}
&lt;/SOURCE_TEXT&gt;

&lt;TRANSLATION&gt;
{translation_1}
&lt;/TRANSLATION&gt;

When writing suggestions, pay attention to whether there are ways to improve the translation's fluency, by applying {target_lang} grammar, spelling and punctuation rules, and ensuring there are no unnecessary repetitions.
- Each suggestion should address one specific part of the translation.
- Output the suggestions only.
```

Where `{source_text}` and `{target_lang}` are global variables defined by the **Begin** component, while `{translation_1}` is the output of another **Generate** component with the component ID **Translate directly**.

### Cite 

This toggle sets whether to cite the original text as reference. 


:::tip NOTE
This feature applies *only* after the original documents have been uploaded to the corresponding knowledge base(s) and file parsing is complete.
:::

### Message window size

An integer specifying the number of previous dialogue rounds to input into the LLM. For example, if it is set to 12, the tokens from the last 12 dialogue rounds will be fed to the LLM. This feature consumes additional tokens.

:::tip IMPORTANT
This feature is used for multi-turn dialogue *only*.
:::


## Examples

You can explore our three-step interpreter agent template, where a **Generate** component (component ID: **Reflect**) takes three global variables:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Interpreter** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on component **Reflect**, to display its **Configuration** window, where:
   - `{target_lang}` and `{source_text}` are defined in the **Begin** component and require user input.
   - `{translation_1}` is the output from the upstream component **Translate directly**.

</code></pre></div></div>
<p>guides\agent\agent_component_reference\interact.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /interact_component
---

# Interact component

A component that accepts user inputs and displays responses.

---

An **Interact** component serves as the interface between human and bot, receiving user inputs and displaying the agent's responses.


## Scenarios

An **Interact** component is essential where you need to display the agent's responses or require user-computer interaction.

## Examples

You can explore our three-step interpreter agent template, where the **Interact** component is used to display the final translation, or our customer service agent template, where the **Interact** component is the immediate downstream of **Begin** and is used to display multi-turn dialogue between the user and the agent.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\iteration.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 12
slug: /iteration_component
---

# Iteration component

A component that splits text input into text segments and iterates a predefined workflow for each one.

---

An **Interaction** component can divide text input into text segments and apply its built-in component workflow to each segment. 


## Scenario

An **Iteration** component is essential when a workflow loop is required and the loop count is *not* fixed but depends on number of segments created from the output of specific agent components. 

- If, for instance, you plan to feed several paragraphs into an LLM for content generation, each with its own focus, and feeding them to the LLM all at once could create confusion or contradictions, then you can use an **Iteration** component, which encapsulates a **Generate** component, to repeat the content generation process for each paragraph.
- Another example: If you wish to use the LLM to translate a lengthy paper into a target language without exceeding its token limit, consider using an **Iteration** component, which encapsulates a **Generate** component, to break the paper into smaller pieces and repeat the translation process for each one.

## Internal components

### IterationItem

Each **Iteration** component includes an internal **IterationItem** component. The **IterationItem** component serves as both the starting point and input node of the workflow within the **Iteration** component. It manages the loop of the workflow for all text segments created from the input.

:::tip NOTE
The **IterationItem** component is visible *only* to the components encapsulated by the current **Iteration** components.
:::

![Iterationitem](https://github.com/user-attachments/assets/97117ceb-76c4-432e-aa86-48f253bcb886)

### Build an internal workflow 

You are allowed to pull other components into the **Iteration** component to build an internal workflow, and these "added internal components" are no longer visible to components outside of the current **Iteration** component.

:::danger IMPORTANT
To reference the created text segments from an added internal component, simply add a **Reference** variable that equals **IterationItem** within the **Input** section of that internal component. There is no need to reference the corresponding external component, as the **IterationItem** component manages the loop of the workflow for all created text segments. 
:::

:::tip NOTE
An added internal component can reference an external component when necessary.
:::

## Configurations

### Input

The **Iteration** component uses input variables to specify its data inputs, namely the texts to be segmented. You are allowed to specify multiple input sources for the **Iteration** component. Click **+ Add variable** in the **Input** section to include the desired input variables. There are two types of input variables: **Reference** and **Text**.

- **Reference**: Uses a component's output or a user input as the data source. You are required to select from the dropdown menu:
  - A component ID under **Component Output**, or 
  - A global variable under **Begin input**, which is defined in the **Begin** component.
- **Text**: Uses fixed text as the query. You are required to enter static text.

### Delimiter

The delimiter to use to split the text input into segments:

- Comma (Default)
- Line break
- Tab
- Underline
- Forward slash
- Dash
- Semicolon

## Examples

Explore our research report generator agent template, where the **Iteration** component (component ID: **Sections**) takes subtitles from the **Subtitles** component and generates sections for them:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Iteration** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\keyword.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 6
slug: /keyword_component
---

# Keyword component

A component that extracts keywords from a user query.

---

A **Keyword** component uses the specified LLM to extract keywords from a user query.

## Scenarios

A **Keyword** component is essential where you need to prepare keywords for a potential keyword search.

## Configurations

### Input

The **Keyword** component relies on input variables to specify its data inputs (queries). Click **+ Add variable** in the **Input** section to add the desired input variables. There are two types of input variables: **Reference** and **Text**.

- **Reference**: Uses a component's output or a user input as the data source. You are required to select from the dropdown menu:
  - A component ID under **Component Output**, or 
  - A global variable under **Begin input**, which is defined in the **Begin** component.
- **Text**: Uses fixed text as the query. You are required to enter static text.


### Model

Click the dropdown menu of **Model** to show the model configuration window.

- **Model**: The chat model to use.  
  - Ensure you set the chat model correctly on the **Model providers** page.
  - You can use different models for different components to increase flexibility or improve overall performance.
- **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.   
  This parameter has three options:
  - **Improvise**: Produces more creative responses.
  - **Precise**: (Default) Produces more conservative responses.
  - **Balance**: A middle ground between **Improvise** and **Precise**.
- **Temperature**: The randomness level of the model's output.  
  Defaults to 0.1.
  - Lower values lead to more deterministic and predictable outputs.
  - Higher values lead to more creative and varied outputs.
  - A temperature of zero results in the same output for the same prompt.
- **Top P**: Nucleus sampling.  
  - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
  - Defaults to 0.3.
- **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
  - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
  - Defaults to 0.4.
- **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
  - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
  - Defaults to 0.7.

:::tip NOTE
- It is not necessary to stick with the same model for all components. If a specific model is not performing well for a particular task, consider using a different one.
- If you are uncertain about the mechanism behind **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**, simply choose one of the three options of **Preset**.
:::


### Number of keywords

An integer specifying the number of keywords to extract from the user query. Defaults to 3. Please note that the number of extracted keywords depends on the LLM's capabilities and the token count in the user query, and may *not* match the integer you set.


## Examples

Explore our general-purpose chatbot agent template, where the **Keyword** component (component ID: **keywords**) is used to extract keywords from financial inputs for a potential stock search in the **akshare** component:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **General-purpose chatbot** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Keyword** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\message.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 7
slug: /message_component
---

# Message component

A component that sends out a static message.

---

A **Message** component sends out a static message. If multiple messages are supplied, it randomly selects one to send.

## Configurations

### Messages

The message to send out. 

Click **+ Add message** to add message options. When multiple messages are supplied, the **Message** component randomly selects one to send.

## Examples

Explore our customer service agent template, where the **Message** component (component ID: **What else?**) randomly sends out a message to the user interface if the user inputs is related to personal contact information:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Message** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\note.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 13
slug: /note_component
---

# Note component

The component that keeps design notes.

---

A **note** component allows you to keep design notes, including details about an agent, the output of specific components, the rationale of a particular design, or any information that may assist you, your users, or your fellow developers understand the agent.

## Examples

Explore our customer service agent template, which has five **Note** components:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **note** component to add or update notes.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\retrieval.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 4
slug: /retrieval_component
---

# Retrieval component

A component that retrieves information from specified datasets.

## Scenarios

A **Retrieval** component is essential in most RAG scenarios, where information is extracted from designated knowledge bases before being sent to the LLM for content generation.

## Configurations

Click on a **Retrieval** component to open its configuration window.

### Input

The **Retrieval** component relies on input variables to specify its data inputs (queries). Click **+ Add variable** in the **Input** section to add the desired input variables. There are two types of input variables: **Reference** and **Text**.

- **Reference**: Uses a component's output or a user input as the data source. You are required to select from the dropdown menu:
  - A component ID under **Component Output**, or 
  - A global variable under **Begin input**, which is defined in the **Begin** component.
- **Text**: Uses fixed text as the query. You are required to enter static text.

### Similarity threshold

RAGFlow employs a combination of weighted keyword similarity and weighted vector cosine similarity during retrieval. This parameter sets the threshold for similarities between the user query and chunks stored in the datasets. Any chunk with a similarity score below this threshold will be excluded from the results.

Defaults to 0.2.

### Keyword similarity weight

This parameter sets the weight of keyword similarity in the combined similarity score. The total of the two weights must equal 1.0. Its default value is 0.7, which means the weight of vector similarity in the combined search is 1 - 0.7 = 0.3.

### Top N

This parameter selects the "Top N" chunks from retrieved ones and feed them to the LLM.

Defaults to 8.


### Rerank model

*Optional*

If a rerank model is selected, a combination of weighted keyword similarity and weighted reranking score will be used for retrieval.

:::caution WARNING
Using a rerank model will *significantly* increase the system's response time.
:::

### Tavily API key

*Optional*

Enter your Tavily API key here to enable Tavily web search during retrieval. See [here](https://app.tavily.com/home) for instructions on getting a Tavily API key.

### Use knowledge graph

Whether to use knowledge graph(s) in the specified knowledge base(s) during retrieval for multi-hop question answering. When enabled, this would involve iterative searches across entity, relationship, and community report chunks, greatly increasing retrieval time.

### Knowledge bases 

*Optional*

Select the knowledge base(s) to retrieve data from.

- If no knowledge base is selected, meaning conversations with the agent will not be based on any knowledge base, ensure that the **Empty response** field is left blank to avoid an error.
- If you select multiple knowledge bases, you must ensure that the knowledge bases (datasets) you select use the same embedding model; otherwise, an error message would occur.

### Empty response

- Set this as a response if no results are retrieved from the knowledge base(s) for your query, or 
- Leave this field blank to allow the chat model to improvise when nothing is found.

:::caution WARNING
If you do not specify a knowledge base, you must leave this field blank; otherwise, an error would occur.
:::

## Examples

Explore our customer service agent template, where the **Retrieval** component (component ID: **Search product info**) is used to search the dataset and send the Top N results to the LLM:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Retrieval** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\rewrite.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 8
slug: /rewrite_component
---

# Rewrite component

A component that rewrites a user query.

---

A **Rewrite** component uses a specified LLM to rewrite a user query from the **Interact** component, based on the context of previous dialogues.

## Scenarios

A **Rewrite** component is essential when you need to optimize a user query based on the context of previous conversations. It is usually the upstream component of a **Retrieval** component.

:::tip NOTE
See also the [Keyword](./keyword.mdx) component, a similar component used for multi-turn optimization.
:::

## Configurations

:::tip NOTE
The **Rewrite** component uses the user-agent interaction from the **Interact** component as its data input. Therefore, there is no need to specify its data inputs in the Configurations.
:::

### Model

Click the dropdown menu of **Model** to show the model configuration window.

- **Model**: The chat model to use.  
  - Ensure you set the chat model correctly on the **Model providers** page.
  - You can use different models for different components to increase flexibility or improve overall performance.
- **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.   
  This parameter has three options:
  - **Improvise**: Produces more creative responses.
  - **Precise**: (Default) Produces more conservative responses.
  - **Balance**: A middle ground between **Improvise** and **Precise**.
- **Temperature**: The randomness level of the model's output.  
  Defaults to 0.1.
  - Lower values lead to more deterministic and predictable outputs.
  - Higher values lead to more creative and varied outputs.
  - A temperature of zero results in the same output for the same prompt.
- **Top P**: Nucleus sampling.  
  - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
  - Defaults to 0.3.
- **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
  - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
  - Defaults to 0.4.
- **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
  - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
  - Defaults to 0.7.

:::tip NOTE
- It is not necessary to stick with the same model for all components. If a specific model is not performing well for a particular task, consider using a different one.
- If you are uncertain about the mechanism behind **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**, simply choose one of the three options of **Preset configurations**.
:::


### Message window size

An integer specifying the number of previous dialogue rounds to input into the LLM. For example, if it is set to 12, the tokens from the last 12 dialogue rounds will be fed to the LLM. This feature consumes additional tokens.

Defaults to 1.

:::tip IMPORTANT
This feature is used for multi-turn dialogue *only*. If your **Categorize** component is not part of a multi-turn dialogue (i.e., it is not in a loop), leave this field as-is.
:::

## Examples

Explore our customer service agent template, where the **Rewrite** component (component ID: **Refine Question**) is used to optimize a product-specific user query based on context of previous dialogues before passing it on to the **Retrieval** component.

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Rewrite** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\switch.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 9
slug: /switch_component
---

# Switch component

A component that evaluates whether specified conditions are met and directs the follow of execution accordingly. 

---

A **Switch** component evaluates conditions based on the output of specific components, directing the flow of execution accordingly to enable complex branching logic.

## Scenarios

A **Switch** component is essential for condition-based direction of execution flow. While it shares similarities with the [Categorize](./categorize.mdx) component, which is also used in multi-pronged strategies, the key distinction lies in their approach: the evaluation of the **Switch** component is rule-based, whereas the **Categorize** component involves AI and uses an LLM for decision-making. 

## Configurations

### Case n

A **Switch** component must have at least one case, each with multiple specified conditions and *only one* downstream component. When multiple conditions are specified for a case, you must set the logical relationship between them to either AND or OR.

#### Next step

Specifies the downstream component of this case.

- *Once you specify the ID of the downstream component, a link is established between this case and the corresponding component.*
- *If you manually link this case to a downstream component on the canvas, the ID of that component is auto-populated.*

#### Condition

Evaluates whether the output of specific components meets certain conditions, with **Component ID**, **Operator**, and **Value** together forming a conditional expression.

:::danger IMPORTANT
When you have added multiple conditions for a specific case, a **Logical operator** field appears, requiring you to set the logical relationship between these conditions as either AND or OR.
![Image](https://github.com/user-attachments/assets/102f006e-9906-49c2-af43-de6af03d5074)
:::

- **Component ID**: The ID of the corresponding component.
- **Operator**: The operator required to form a conditional expression.
  - Equals
  - Not equal
  - Greater than
  - Greater equal
  - Less than
  - Less equal
  - Contains 
  - Not contains 
  - Starts with
  - Ends with
  - Is empty
  - Not empty
- **Value**: A single value, which can be an integer, float, or string.  
  - Delimiters, multiple values, or expressions are *not* supported.
  - Strings need not be wrapped in `""` or `''`.

### ELSE 

**Required**. Specifies the downstream component if none of the conditions defined above are met.

*Once you specify the ID of the downstream component, a link is established between ELSE and the corresponding component.*

</code></pre></div></div>
<p>guides\agent\agent_component_reference\template.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 11
slug: /template_component
---

# Template component

A component that formats user inputs or the outputs of other components.

---

A **Template** component acts as a content formatter. It is usually the upstream component of an **Interact** component.


## Scenarios

A **Template** component is useful for organizing various sources of data or information into specific formats.

## Configurations

### Content 

Used together with Keys to organize various data or information sources into desired formats. Example:

```text
&lt;h2&gt;{subtitle}&lt;/h2&gt;
&lt;div&gt;{content}&lt;/div&gt;
```

Where `{subtitle}` and `{content}` are defined keys.

### Key 

A **Template** component relies on keys (variables) to specify its data or information sources. Its immediate upstream component is *not* necessarily its input, and the arrows in the workflow indicate *only* the processing sequence.

Values of keys are categorized into two groups:

- **Component Output**: The value of the key should be a component ID.
- **Begin Input**: The value of the key should be the name of a global variable defined in the **Begin** component.

## Examples

Explore our research report generator agent template, where the **Template** component (component ID: **Article**) organizes user input and the outputs of the **Sections** component into HTML format:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Research report generator** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Template** component to display its **Configuration** window

</code></pre></div></div>
<p>guides\chat_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Chat"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Chat-specific guides."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\chat\implement_deep_research.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /implement_deep_research
---

# Implement deep research

Implements deep research for agentic reasoning.

---

From v0.17.0 onward, RAGFlow supports integrating agentic reasoning in an AI chat. The following diagram illustrates the workflow of RAGFlow's deep research:

![Image](https://github.com/user-attachments/assets/f65d4759-4f09-4d9d-9549-c0e1fe907525)

To activate this feature:

1. Enable the **Reasoning** toggle under the **Prompt engine** tab of your chat assistant dialogue.

![Image](https://github.com/user-attachments/assets/4a1968d0-0128-4371-879f-77f3a70197f5)

2. Enter the correct Tavily API key under the **Assistant settings** tab of your chat assistant dialogue to leverage Tavily-based web search

![Image](https://github.com/user-attachments/assets/e8787532-7e72-49ef-8951-169ae544512f)

*The following is a screenshot of a conversation that integrates Deep Research:*

![Image](https://github.com/user-attachments/assets/165b88ff-1f5d-4fb8-90e2-c836b25e32e9)
</code></pre></div></div>
<p>guides\chat\set_chat_variables.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 4
slug: /set_chat_variables
---

# Set variables

Set variables to be used together with the system prompt for your LLM.

---

When configuring the system prompt for a chat model, variables play an important role in enhancing flexibility and reusability. With variables, you can dynamically adjust the system prompt to be sent to your model. In the context of RAGFlow, if you have defined variables in the **Chat Configuration** dialogue, except for the system's reserved variable `{knowledge}`, you are required to pass in values for them from RAGFlow's [HTTP API](../../references/http_api_reference.md#converse-with-chat-assistant) or through its [Python SDK](../../references/python_api_reference.md#converse-with-chat-assistant).

:::danger IMPORTANT
In RAGFlow, variables are closely linked with the system prompt. When you add a variable in the **Variable** section, include it in the system prompt. Conversely, when deleting a variable, ensure it is removed from the system prompt; otherwise, an error would occur.
:::

## Where to set variables

Hover your mouse over your chat assistant, click **Edit** to open its **Chat Configuration** dialogue, then click the **Prompt engine** tab. Here, you can work on your variables in the **System prompt** field and the **Variable** section:

![set_variables](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/prompt_engine.jpg)

## 1. Manage variables

In the **Variable** section, you add, remove, or update variables.

### `{knowledge}` - a reserved variable

`{knowledge}` is the system's reserved variable, representing the chunks retrieved from the knowledge base(s) specified by **Knowledge bases** under the **Assistant settings** tab. If your chat assistant is associated with certain knowledge bases, you can keep it as is.

:::info NOTE
It does not currently make a difference whether you set `{knowledge}` to optional or mandatory, but note that this design will be updated at a later point.
:::

From v0.17.0 onward, you can start an AI chat without specifying knowledge bases. In this case, we recommend removing the `{knowledge}` variable to prevent unnecessary reference and keeping the **Empty response** field empty to avoid errors.

### Custom variables

Besides `{knowledge}`, you can also define your own variables to pair with the system prompt. To use these custom variables, you must pass in their values through RAGFlow's official APIs. The **Optional** toggle determines whether these variables are required in the corresponding APIs:

- **Disabled** (Default): The variable is mandatory and must be provided.
- **Enabled**: The variable is optional and can be omitted if not needed.



## 2. Update system prompt

After you add or remove variables in the **Variable** section, ensure your changes are reflected in the system prompt to avoid inconsistencies or errors. Here's an example:

```
You are an intelligent assistant. Please answer the question by summarizing chunks from the specified knowledge base(s)...

Your answers should follow a professional and {style} style.

...

Here is the knowledge base:
{knowledge}
The above is the knowledge base.
```

:::tip NOTE
If you have removed `{knowledge}`, ensure that you thoroughly review and update the entire system prompt to achieve optimal results.
:::

## APIs

The *only* way to pass in values for the custom variables defined in the **Chat Configuration** dialogue is to call RAGFlow's [HTTP API](../../references/http_api_reference.md#converse-with-chat-assistant) or through its [Python SDK](../../references/python_api_reference.md#converse-with-chat-assistant).

### HTTP API

See [Converse with chat assistant](../../references/http_api_reference.md#converse-with-chat-assistant). Here's an example:

```json {9}
curl --request POST \
     --url http://{address}/api/v1/chats/{chat_id}/completions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer &lt;YOUR_API_KEY&gt;' \
     --data-binary '
     {
          "question": "xxxxxxxxx",
          "stream": true,
          "style":"hilarious"
     }'
```

### Python API

See [Converse with chat assistant](../../references/python_api_reference.md#converse-with-chat-assistant). Here's an example:

```python {18}
from ragflow_sdk import RAGFlow

rag_object = RAGFlow(api_key="&lt;YOUR_API_KEY&gt;", base_url="http://&lt;YOUR_BASE_URL&gt;:9380")
assistant = rag_object.list_chats(name="Miss R")
assistant = assistant[0]
session = assistant.create_session()    

print("\n==================== Miss R =====================\n")
print("Hello. What can I do for you?")

while True:
    question = input("\n==================== User =====================\n&gt; ")
    style = input("Please enter your preferred style (e.g., formal, informal, hilarious): ")
    
    print("\n==================== Miss R =====================\n")
    
    cont = ""
    for ans in session.ask(question, stream=True, style=style):
        print(ans.content[len(cont):], end='', flush=True)
        cont = ans.content
```


</code></pre></div></div>
<p>guides\chat\start_chat.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /start_chat
---

# Start AI chat

Initiate an AI-powered chat with a configured chat assistant.

---

Knowledge base, hallucination-free chat, and file management are the three pillars of RAGFlow. Chats in RAGFlow are based on a particular knowledge base or multiple knowledge bases. Once you have created your knowledge base, finished file parsing, and [run a retrieval test](../dataset/run_retrieval_test.md), you can go ahead and start an AI conversation.

## Start an AI chat

You start an AI conversation by creating an assistant.

1. Click the **Chat** tab in the middle top of the page **&gt;** **Create an assistant** to show the **Chat Configuration** dialogue *of your next dialogue*.

   &gt; RAGFlow offers you the flexibility of choosing a different chat model for each dialogue, while allowing you to set the default models in **System Model Settings**.

2. Update **Assistant settings**:

   - **Assistant name** is the name of your chat assistant. Each assistant corresponds to a dialogue with a unique combination of knowledge bases, prompts, hybrid search configurations, and large model settings.
   - **Empty response**:
     - If you wish to *confine* RAGFlow's answers to your knowledge bases, leave a response here. Then, when it doesn't retrieve an answer, it *uniformly* responds with what you set here.
     - If you wish RAGFlow to *improvise* when it doesn't retrieve an answer from your knowledge bases, leave it blank, which may give rise to hallucinations.
   - **Show quote**: This is a key feature of RAGFlow and enabled by default. RAGFlow does not work like a black box. Instead, it clearly shows the sources of information that its responses are based on.
   - Select the corresponding knowledge bases. You can select one or multiple knowledge bases, but ensure that they use the same embedding model, otherwise an error would occur.

3. Update **Prompt engine**:

   - In **System**, you fill in the prompts for your LLM, you can also leave the default prompt as-is for the beginning.
   - **Similarity threshold** sets the similarity "bar" for each chunk of text. The default is 0.2. Text chunks with lower similarity scores are filtered out of the final response.
   - **Keyword similarity weight** is set to 0.7 by default. RAGFlow uses a hybrid score system to evaluate the relevance of different text chunks. This value sets the weight assigned to the keyword similarity component in the hybrid score.
     - If **Rerank model** is left empty, the hybrid score system uses keyword similarity and vector similarity, and the default weight assigned to the vector similarity component is 1-0.7=0.3.
     - If **Rerank model** is selected, the hybrid score system uses keyword similarity and reranker score, and the default weight assigned to the reranker score is 1-0.7=0.3.
   - **Top N** determines the *maximum* number of chunks to feed to the LLM. In other words, even if more chunks are retrieved, only the top N chunks are provided as input.
   - **Multi-turn optimization** enhances user queries using existing context in a multi-round conversation. It is enabled by default. When enabled, it will consume additional LLM tokens and significantly increase the time to generate answers.
   - **Use knowledge graph** indicates whether to use knowledge graph(s) in the specified knowledge base(s) during retrieval for multi-hop question answering. When enabled, this would involve iterative searches across entity, relationship, and community report chunks, greatly increasing retrieval time.
   - **Reasoning** indicates whether to generate answers through reasoning processes like Deepseek-R1/OpenAI o1. Once enabled, the chat model autonomously integrates Deep Research during question answering when encountering an unknown topic. This involves the chat model dynamically searching external knowledge and generating final answers through reasoning.
   - **Rerank model** sets the reranker model to use. It is left empty by default.
     - If **Rerank model** is left empty, the hybrid score system uses keyword similarity and vector similarity, and the default weight assigned to the vector similarity component is 1-0.7=0.3.
     - If **Rerank model** is selected, the hybrid score system uses keyword similarity and reranker score, and the default weight assigned to the reranker score is 1-0.7=0.3.
   - **Variable** refers to the variables (keys) to be used in the system prompt. `{knowledge}` is a reserved variable. Click **Add** to add more variables for the system prompt.
      - If you are uncertain about the logic behind **Variable**, leave it *as-is*.
      - As of v0.18.0, if you add custom variables here, the only way you can pass in their values is to call:
         - HTTP method [Converse with chat assistant](../../references/http_api_reference.md#converse-with-chat-assistant), or
         - Python method [Converse with chat assistant](../../references/python_api_reference.md#converse-with-chat-assistant).

4. Update **Model Setting**:

   - In **Model**: you select the chat model. Though you have selected the default chat model in **System Model Settings**, RAGFlow allows you to choose an alternative chat model for your dialogue.
   - **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.   
   This parameter has three options:
      - **Improvise**: Produces more creative responses.
      - **Precise**: (Default) Produces more conservative responses.
      - **Balance**: A middle ground between **Improvise** and **Precise**.
   - **Temperature**: The randomness level of the model's output.  
   Defaults to 0.1.
      - Lower values lead to more deterministic and predictable outputs.
      - Higher values lead to more creative and varied outputs.
      - A temperature of zero results in the same output for the same prompt.
   - **Top P**: Nucleus sampling.  
      - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
      - Defaults to 0.3.
   - **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
      - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
      - Defaults to 0.4.
   - **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
      - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
      - Defaults to 0.7.

5. Now, let's start the show:

   ![question1](https://github.com/user-attachments/assets/c4114a3d-74ff-40a3-9719-6b47c7b11ab1)

:::tip NOTE

1. Click the light bulb icon above the answer to view the expanded system prompt:

![](https://github.com/user-attachments/assets/515ab187-94e8-412a-82f2-aba52cd79e09)

   *The light bulb icon is available only for the current dialogue.*

2. Scroll down the expanded prompt to view the time consumed for each task:

![enlighten](https://github.com/user-attachments/assets/fedfa2ee-21a7-451b-be66-20125619923c)
:::

## Update settings of an existing chat assistant

Hover over an intended chat assistant **&gt;** **Edit** to show the chat configuration dialogue:

![edit_chat](https://github.com/user-attachments/assets/5c514cf0-a959-4cfe-abad-5e42a0e23974)

![chat_config](https://github.com/user-attachments/assets/1a4eaed2-5430-4585-8ab6-930549838c5b)

## Integrate chat capabilities into your application or webpage

RAGFlow offers HTTP and Python APIs for you to integrate RAGFlow's capabilities into your applications. Read the following documents for more information:

- [Acquire a RAGFlow API key](../../develop/acquire_ragflow_api_key.md)
- [HTTP API reference](../../references/http_api_reference.md)
- [Python API reference](../../references/python_api_reference.md)

You can use iframe to embed the created chat assistant into a third-party webpage:

1. Before proceeding, you must [acquire an API key](../models/llm_api_key_setup.md); otherwise, an error message would appear.
2. Hover over an intended chat assistant **&gt;** **Edit** to show the **iframe** window:

   ![chat-embed](https://github.com/user-attachments/assets/13ea3021-31c4-4a14-9b32-328cd3318fb5)

3. Copy the iframe and embed it into a specific location on your webpage.

</code></pre></div></div>
<p>guides\chat\best_practices_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Best practices"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Best practices on chat assistant configuration."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\chat\best_practices\accelerate_question_answering.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /accelerate_question_answering
---

# Accelerate answering
import APITable from '@site/src/components/APITable';

A checklist to speed up question answering.

---

Please note that some of your settings may consume a significant amount of time. If you often find that your question answering is time-consuming, here is a checklist to consider:

- In the **Prompt engine** tab of your **Chat Configuration** dialogue, disabling **Multi-turn optimization** will reduce the time required to get an answer from the LLM.
- In the **Prompt engine** tab of your **Chat Configuration** dialogue, leaving the **Rerank model** field empty will significantly decrease retrieval time.
- When using a rerank model, ensure you have a GPU for acceleration; otherwise, the reranking process will be *prohibitively* slow.

:::tip NOTE 
Please note that rerank models are essential in certain scenarios. There is always a trade-off between speed and performance; you must weigh the pros against cons for your specific case.
:::

- In the **Assistant settings** tab of your **Chat Configuration** dialogue, disabling **Keyword analysis** will reduce the time to receive an answer from the LLM.
- When chatting with your chat assistant, click the light bulb icon above the *current* dialogue and scroll down the popup window to view the time taken for each task:  
   ![enlighten](https://github.com/user-attachments/assets/fedfa2ee-21a7-451b-be66-20125619923c)  


```mdx-code-block
&lt;APITable&gt;
```

| Item name         | Description                                                                                   |
| ----------------- | --------------------------------------------------------------------------------------------- |
| Total             | Total time spent on this conversation round, including chunk retrieval and answer generation. |
| Check LLM         | Time to validate the specified LLM.                                                           |
| Create retriever  | Time to create a chunk retriever.                                                             |
| Bind embedding    | Time to initialize an embedding model instance.                                               |
| Bind LLM          | Time to initialize an LLM instance.                                                           |
| Tune question     | Time to optimize the user query using the context of the mult-turn conversation.              |
| Bind reranker     | Time to initialize an reranker model instance for chunk retrieval.                            |
| Generate keywords | Time to extract keywords from the user query.                                                 |
| Retrieval         | Time to retrieve the chunks.                                                                  |
| Generate answer   | Time to generate the answer.                                                                  |

```mdx-code-block
&lt;/APITable&gt;
```
</code></pre></div></div>
<p>guides\dataset_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Datasets"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides on configuring a knowledge base."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\dataset\configure_knowledge_base.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 0
slug: /configure_knowledge_base
---

# Configure knowledge base

Knowledge base, hallucination-free chat, and file management are the three pillars of RAGFlow. RAGFlow's AI chats are based on knowledge bases. Each of RAGFlow's knowledge bases serves as a knowledge source, *parsing* files uploaded from your local machine and file references generated in **File Management** into the real 'knowledge' for future AI chats. This guide demonstrates some basic usages of the knowledge base feature, covering the following topics:

- Create a knowledge base
- Configure a knowledge base
- Search for a knowledge base
- Delete a knowledge base

## Create knowledge base

With multiple knowledge bases, you can build more flexible, diversified question answering. To create your first knowledge base:

![create knowledge base](https://github.com/infiniflow/ragflow/assets/93570324/110541ed-6cea-4a03-a11c-414a0948ba80)

_Each time a knowledge base is created, a folder with the same name is generated in the **root/.knowledgebase** directory._

## Configure knowledge base

The following screenshot shows the configuration page of a knowledge base. A proper configuration of your knowledge base is crucial for future AI chats. For example, choosing the wrong embedding model or chunking method would cause unexpected semantic loss or mismatched answers in chats. 

![knowledge base configuration](https://github.com/infiniflow/ragflow/assets/93570324/384c671a-8b9c-468c-b1c9-1401128a9b65)

This section covers the following topics:

- Select chunking method
- Select embedding model
- Upload file
- Parse file
- Intervene with file parsing results
- Run retrieval testing

### Select chunking method

RAGFlow offers multiple chunking template to facilitate chunking files of different layouts and ensure semantic integrity. In **Chunking method**, you can choose the default template that suits the layouts and formats of your files. The following table shows the descriptions and the compatible file formats of each supported chunk template:

| **Template** | Description                                                           | File format                                                                                   |
|--------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| General      | Files are consecutively chunked based on a preset chunk token number. | DOCX, XLSX, XLS (Excel 97-2003), PPT, PDF, TXT, JPEG, JPG, PNG, TIF, GIF, CSV, JSON, EML, HTML |
| Q&amp;A          |                                                                       | XLSX, XLS (Excel 97-2003), CSV/TXT                                                             |
| Resume       | Enterprise edition only. You can also try it out on demo.ragflow.io.  | DOCX, PDF, TXT                                                                                |
| Manual       |                                                                       | PDF                                                                                           |
| Table        |                                                                       | XLSX, XLS (Excel 97-2003), CSV/TXT                                                             |
| Paper        |                                                                       | PDF                                                                                           |
| Book         |                                                                       | DOCX, PDF, TXT                                                                                |
| Laws         |                                                                       | DOCX, PDF, TXT                                                                                |
| Presentation |                                                                       | PDF, PPTX                                                                                     |
| Picture      |                                                                       | JPEG, JPG, PNG, TIF, GIF                                                                      |
| One          | Each document is chunked in its entirety (as one).                    | DOCX, XLSX, XLS (Excel 97-2003), PDF, TXT                                                      |
| Tag          | The knowledge base functions as a tag set for the others.             | XLSX, CSV/TXT                                                                                 |

You can also change a file's chunking method on the **Datasets** page.

![change chunking method](https://github.com/infiniflow/ragflow/assets/93570324/ac116353-2793-42b2-b181-65e7082bed42)

### Select embedding model

An embedding model converts chunks into embeddings. It cannot be changed once the knowledge base has chunks. To switch to a different embedding model, you must delete all existing chunks in the knowledge base. The obvious reason is that we *must* ensure that files in a specific knowledge base are converted to embeddings using the *same* embedding model (ensure that they are compared in the same embedding space).

The following embedding models can be deployed locally:

- BAAI/bge-large-zh-v1.5
- maidalun1020/bce-embedding-base_v1

### Upload file

- RAGFlow's **File Management** allows you to link a file to multiple knowledge bases, in which case each target knowledge base holds a reference to the file.
- In **Knowledge Base**, you are also given the option of uploading a single file or a folder of files (bulk upload) from your local machine to a knowledge base, in which case the knowledge base holds file copies. 

While uploading files directly to a knowledge base seems more convenient, we *highly* recommend uploading files to **File Management** and then linking them to the target knowledge bases. This way, you can avoid permanently deleting files uploaded to the knowledge base. 

### Parse file

File parsing is a crucial topic in knowledge base configuration. The meaning of file parsing in RAGFlow is twofold: chunking files based on file layout and building embedding and full-text (keyword) indexes on these chunks. After having selected the chunking method and embedding model, you can start parsing a file:

![parse file](https://github.com/infiniflow/ragflow/assets/93570324/5311f166-6426-447f-aa1f-bd488f1cfc7b)

- Click the play button next to **UNSTART** to start file parsing.
- Click the red-cross icon and then refresh, if your file parsing stalls for a long time. 
- As shown above, RAGFlow allows you to use a different chunking method for a particular file, offering flexibility beyond the default method. 
- As shown above, RAGFlow allows you to enable or disable individual files, offering finer control over knowledge base-based AI chats. 

### Intervene with file parsing results

RAGFlow features visibility and explainability, allowing you to view the chunking results and intervene where necessary. To do so: 

1. Click on the file that completes file parsing to view the chunking results: 

   _You are taken to the **Chunk** page:_

   ![chunks](https://github.com/infiniflow/ragflow/assets/93570324/0547fd0e-e71b-41f8-8e0e-31649c85fd3d)

2. Hover over each snapshot for a quick view of each chunk.

3. Double-click the chunked texts to add keywords or make *manual* changes where necessary:

   ![update chunk](https://github.com/infiniflow/ragflow/assets/93570324/1d84b408-4e9f-46fd-9413-8c1059bf9c76)

:::caution NOTE
You can add keywords to a file chunk to increase its ranking for queries containing those keywords. This action increases its keyword weight and can improve its position in search list.  
:::

4. In Retrieval testing, ask a quick question in **Test text** to double-check if your configurations work:

   _As you can tell from the following, RAGFlow responds with truthful citations._

   ![retrieval test](https://github.com/infiniflow/ragflow/assets/93570324/c03f06f6-f41f-4b20-a97e-ae405d3a950c)

### Run retrieval testing

RAGFlow uses multiple recall of both full-text search and vector search in its chats. Prior to setting up an AI chat, consider adjusting the following parameters to ensure that the intended information always turns up in answers:

- Similarity threshold: Chunks with similarities below the threshold will be filtered. By default, it is set to 0.2.
- Vector similarity weight: The percentage by which vector similarity contributes to the overall score. By default, it is set to 0.3.

See [Run retrieval test](./run_retrieval_test.md) for details.

![retrieval test](https://github.com/infiniflow/ragflow/assets/93570324/c03f06f6-f41f-4b20-a97e-ae405d3a950c)

## Search for knowledge base

As of RAGFlow v0.18.0, the search feature is still in a rudimentary form, supporting only knowledge base search by name.

![search knowledge base](https://github.com/infiniflow/ragflow/assets/93570324/836ae94c-2438-42be-879e-c7ad2a59693e)

## Delete knowledge base

You are allowed to delete a knowledge base. Hover your mouse over the three dot of the intended knowledge base card and the **Delete** option appears. Once you delete a knowledge base, the associated folder under **root/.knowledge** directory is AUTOMATICALLY REMOVED. The consequence is:

- The files uploaded directly to the knowledge base are gone;  
- The file references, which you created from within **File Management**, are gone, but the associated files still exist in **File Management**. 

![delete knowledge base](https://github.com/infiniflow/ragflow/assets/93570324/fec7a508-6cfe-4bca-af90-81d3fdb94098)

</code></pre></div></div>
<p>guides\dataset\construct_knowledge_graph.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 8
slug: /construct_knowledge_graph
---

# Construct knowledge graph

Generate a knowledge graph for your knowledge base.

---

To enhance multi-hop question-answering, RAGFlow adds a knowledge graph construction step between data extraction and indexing, as illustrated below. This step creates additional chunks from existing ones generated by your specified chunking method.

![Image](https://github.com/user-attachments/assets/1ec21d8e-f255-4d65-9918-69b72dfa142b)

From v0.16.0 onward, RAGFlow supports constructing a knowledge graph on a knowledge base, allowing you to construct a *unified* graph across multiple files within your knowledge base. When a newly uploaded file starts parsing, the generated graph will automatically update.

:::danger WARNING
Constructing a knowledge graph requires significant memory, computational resources, and tokens.
:::

## Scenarios

Knowledge graphs are especially useful for multi-hop question-answering involving *nested* logic. They outperform traditional extraction approaches when you are performing question answering on books or works with complex entities and relationships.

:::tip NOTE
RAPTOR (Recursive Abstractive Processing for Tree Organized Retrieval) can also be used for multi-hop question-answering tasks. See [Enable RAPTOR](./enable_raptor.md) for details. You may use either approach or both, but ensure you understand the memory, computational, and token costs involved.
:::

## Prerequisites

The system's default chat model is used to generate knowledge graph. Before proceeding, ensure that you have a chat model properly configured:

![Image](https://github.com/user-attachments/assets/6bc34279-68c3-4d99-8d20-b7bd1dafc1c1)

## Configurations

### Entity types (*Required*)

The types of the entities to extract from your knowledge base. The default types are: **organization**, **person**, **event**, and **category**. Add or remove types to suit your specific knowledge base.

### Method

The method to use to construct knowledge graph:

- **General**: Use prompts provided by [GraphRAG](https://github.com/microsoft/graphrag) to extract entities and relationships.
- **Light**: (Default) Use prompts provided by [LightRAG](https://github.com/HKUDS/LightRAG) to extract entities and relationships. This option consumes fewer tokens, less memory, and fewer computational resources.

### Entity resolution

Whether to enable entity resolution. You can think of this as an entity deduplication switch. When enabled, the LLM will combine similar entities - e.g., '2025' and 'the year of 2025', or 'IT' and 'Information Technology' - to construct a more effective graph.

- (Default) Disable entity resolution.
- Enable entity resolution. This option consumes more tokens.

### Community report generation

In a knowledge graph, a community is a cluster of entities linked by relationships. You can have the LLM generate an abstract for each community, known as a community report. See [here](https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/) for more information. This indicates whether to generate community reports:

- Generate community reports. This option consumes more tokens.
- (Default) Do not generate community reports.

## Procedure

1. On the **Configuration** page of your knowledge base, switch on **Extract knowledge graph** or adjust its settings as needed, and click **Save** to confirm your changes.

   - *The default knowledge graph configurations for your knowledge base are now set and files uploaded from this point onward will automatically use these settings during parsing.*
   - *Files parsed before this update will retain their original knowledge graph settings.*

2. The knowledge graph of your knowledge base does *not* automatically update *until* a newly uploaded file is parsed.

   _A **Knowledge graph** entry appears under **Configuration** once a knowledge graph is created._

3. Click **Knowledge graph** to view the details of the generated graph.
4. To use the created knowledge graph, do either of the following:
   
   - In your **Chat Configuration** dialogue, click the **Assistant settings** tab to add the corresponding knowledge base(s) and click the **Prompt engine** tab to switch on the **Use knowledge graph** toggle.
   - If you are using an agent, click the **Retrieval** agent component to specify the knowledge base(s) and switch on the **Use knowledge graph** toggle.

## Frequently asked questions

### Can I have different knowledge graph settings for different files in my knowledge base?

Yes, you can. Just one graph is generated per knowledge base. The smaller graphs of your files will be *combined* into one big, unified graph at the end of the graph extraction process.

### Does the knowledge graph automatically update when I remove a related file?

Nope. The knowledge graph does *not* automatically update *until* a newly uploaded document is parsed.

### How to remove a generated knowledge graph?

To remove the generated knowledge graph, delete all related files in your knowledge base. Although the **Knowledge graph** entry will still be visible, the graph has actually been deleted.

### Where is the created knowledge graph stored?

All chunks of the created knowledge graph are stored in RAGFlow's document engine: either Elasticsearch or [Infinity](https://github.com/infiniflow/infinity).
</code></pre></div></div>
<p>guides\dataset\enable_excel2html.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 4
slug: /enable_excel2html
---

# Enable Excel2HTML

Convert complex Excel spreadsheets into HTML tables.

---

When using the General chunking method, you can enable the **Excel to HTML** toggle to convert spreadsheet files into HTML tables. If it is disabled, spreadsheet tables will be represented as key-value pairs. For complex tables that cannot be simply represented this way, you must enable this feature.

:::caution WARNING
The feature is disabled by default. If your knowledge base contains spreadsheets with complex tables and you do not enable this feature, RAGFlow will not throw an error but your tables are likely to be garbled.
:::

## Scenarios

Works with complex tables that cannot be represented as key-value pairs. Examples include spreadsheet tables with multiple columns, tables with merged cells, or multiple tables within one sheet. In such cases, consider converting these spreadsheet tables into HTML tables.

## Considerations

- The Excel2HTML feature applies only to spreadsheet files (XLSX or XLS (Excel 97-2003)).
- This feature is associated with the General chunking method. In other words, it is available *only when* you select the General chunking method.
- When this feature is enabled, spreadsheet tables with more than 12 rows will be split into chunks of 12 rows each.

## Procedure

1. On your knowledge base's **Configuration** page, select **General** as the chunking method.

   _The **Excel to HTML** toggle appears._

2. Enable **Excel to HTML** if your knowledge base contains complex spreadsheet tables that cannot be represented as key-value pairs.
3. Leave **Excel to HTML** disabled if your knowledge base has no spreadsheet tables or if its spreadsheet tables can be represented as key-value pairs.
4. If question-answering regarding complex tables is unsatisfactory, check if **Excel to HTML** is enabled.

## Frequently asked questions

### Should I enable this feature for PDFs with complex tables?

Nope. This feature applies to spreadsheet files only. Enabling **Excel to HTML** does not affect your PDFs.
</code></pre></div></div>
<p>guides\dataset\enable_raptor.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 7
slug: /enable_raptor
---

# Enable RAPTOR

A recursive abstractive method used in long-context knowledge retrieval and summarization, balancing broad semantic understanding with fine details.

---

RAPTOR (Recursive Abstractive Processing for Tree Organized Retrieval) is an enhanced document preprocessing technique introduced in a [2024 paper](https://arxiv.org/html/2401.18059v1). Designed to tackle multi-hop question-answering issues, RAPTOR performs recursive clustering and summarization of document chunks to build a hierarchical tree structure. This enables more context-aware retrieval across lengthy documents. RAGFlow v0.6.0 integrates RAPTOR for document clustering as part of its data preprocessing pipeline between data extraction and indexing, as illustrated below.

![document_clustering](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/document_clustering_as_preprocessing.jpg)

Our tests with this new approach demonstrate state-of-the-art (SOTA) results on question-answering tasks requiring complex, multi-step reasoning. By combining RAPTOR retrieval with our built-in chunking methods and/or other retrieval-augmented generation (RAG) approaches, you can further improve your question-answering accuracy.

:::danger WARNING
Enabling RAPTOR requires significant memory, computational resources, and tokens.
:::

## Basic principles

After the original documents are divided into chunks, the chunks are clustered by semantic similarity rather than by their original order in the text. Clusters are then summarized into higher-level chunks by your system's default chat model. This process is applied recursively, forming a tree structure with various levels of summarization from the bottom up. As illustrated in the figure below, the initial chunks form the leaf nodes (shown in blue) and are recursively summarized into a root node (shown in orange).

![raptor](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/clustering_and_summarizing.jpg)

The recursive clustering and summarization capture a broad understanding (by the root node) as well as fine details (by the leaf nodes) necessary for multi-hop question-answering.

## Scenarios

For multi-hop question-answering tasks involving complex, multi-step reasoning, a semantic gap often exists between the question and its answer. As a result, searching with the question often fails to retrieve the relevant chunks that contribute to the correct answer. RAPTOR addresses this challenge by providing the chat model with richer and more context-aware and relevant chunks to summarize, enabling a holistic understanding without losing granular details.

:::tip NOTE
Knowledge graphs can also be used for multi-hop question-answering tasks. See [Construct knowledge graph](./construct_knowledge_graph.md) for details. You may use either approach or both, but ensure you understand the memory, computational, and token costs involved.
:::

## Prerequisites

The system's default chat model is used to summarize clustered content. Before proceeding, ensure that you have a chat model properly configured:

![Image](https://github.com/user-attachments/assets/6bc34279-68c3-4d99-8d20-b7bd1dafc1c1)

## Configurations

The RAPTOR feature is disabled by default. To enable it, manually switch on the **Use RAPTOR to enhance retrieval** toggle on your knowledge base's **Configuration** page.

### Prompt

The following prompt will be applied recursively for cluster summarization, with `{cluster_content}` serving as an internal parameter. We recommend that you keep it as-is for now. The design will be updated at a later point.

```
Please summarize the following paragraphs... Paragraphs as following:
      {cluster_content}
The above is the content you need to summarize.
```

### Max token

The maximum number of tokens per generated summary chunk. Defaults to 256, with a maximum limit of 2048.

### Threshold

In RAPTOR, chunks are clustered by their semantic similarity. The **Threshold** parameter sets the minimum similarity required for chunks to be grouped together.

It defaults to 0.1, with a maximum limit of 1. A higher **Threshold** means fewer chunks in each cluster, while a lower one means more.

### Max cluster

The maximum number of clusters to create. Defaults to 64, with a maximum limit of 1024.

### Random seed

A random seed. Click **+** to change the seed value.

</code></pre></div></div>
<p>guides\dataset\run_retrieval_test.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 10
slug: /run_retrieval_test
---

# Run retrieval test

Conduct a retrieval test on your knowledge base to check whether the intended chunks can be retrieved.

---

After your files are uploaded and parsed, it is recommended that you run a retrieval test before proceeding with the chat assistant configuration. Running a retrieval test is *not* an unnecessary or superfluous step at all! Just like fine-tuning a precision instrument, RAGFlow requires careful tuning to deliver optimal question answering performance. Your knowledge base settings, chat assistant configurations, and the specified large and small models can all significantly impact the final results. Running a retrieval test verifies whether the intended chunks can be recovered, allowing you to quickly identify areas for improvement or pinpoint any issue that needs addressing. For instance, when debugging your question answering system, if you know that the correct chunks can be retrieved, you can focus your efforts elsewhere. For example, in issue [#5627](https://github.com/infiniflow/ragflow/issues/5627), the problem was found to be due to the LLM's limitations.

During a retrieval test, chunks created from your specified chunking method are retrieved using a hybrid search. This search combines weighted keyword similarity with either weighted vector cosine similarity or a weighted reranking score, depending on your settings:

- If no rerank model is selected, weighted keyword similarity will be combined with weighted vector cosine similarity.
- If a rerank model is selected, weighted keyword similarity will be combined with weighted vector reranking score.

In contrast, chunks created from [knowledge graph construction](./construct_knowledge_graph.md) are retrieved solely using vector cosine similarity.

## Prerequisites

- Your files are uploaded and successfully parsed before running a retrieval test.
- A knowledge graph must be successfully built before enabling **Use knowledge graph**.

## Configurations

### Similarity threshold

This sets the bar for retrieving chunks: chunks with similarities below the threshold will be filtered out. By default, the threshold is set to 0.2. This means that only chunks with hybrid similarity score of 20 or higher will be retrieved.

### Keyword similarity weight

This sets the weight of keyword similarity in the combined similarity score, whether used with vector cosine similarity or a reranking score. By default, it is set to 0.7, making the weight of the other component 0.3 (1 - 0.7).

### Rerank model

- If left empty, RAGFlow will use a combination of weighted keyword similarity and weighted vector cosine similarity.
- If a rerank model is selected, weighted keyword similarity will be combined with weighted vector reranking score.

:::danger IMPORTANT
Using a rerank model will significantly increase the time to receive a response.
:::

### Use knowledge graph

In a knowledge graph, an entity description, a relationship description, or a community report each exists as an independent chunk. This switch indicates whether to add these chunks to the retrieval.

The switch is disabled by default. When enabled, RAGFlow performs the following during a retrieval test:

1. Extract entities and entity types from your query using the LLM.
2. Retrieve top N entities from the graph based on their PageRank values, using the extracted entity types.
3. Find similar entities and their N-hop relationships from the graph using the embeddings of the extracted query entities.
4. Retrieve similar relationships from the graph using the query embedding.
5. Rank these retrieved entities and relationships by multiplying each one's PageRank value with its similarity score to the query, returning the top n as the final retrieval.
6. Retrieve the report for the community involving the most entities in the final retrieval.  
   *The retrieved entity descriptions, relationship descriptions, and the top 1 community report are sent to the LLM for content generation.*

:::danger IMPORTANT
Using a knowledge graph in a retrieval test will significantly increase the time to receive a response.
:::

### Test text

This field is where you put in your testing query.

## Procedure

1. Navigate to the **Retrieval testing** page of your knowledge base, enter your query in **Test text**, and click **Testing** to run the test.
2. If the results are unsatisfactory, tune the options listed in the Configuration section and rerun the test.

   *The following is a screenshot of a retrieval test conducted without using knowledge graph. It demonstrates a hybrid search combining weighted keyword similarity and weighted vector cosine similarity. The overall hybrid similarity score is 28.56, calculated as 25.17 (term similarity score) x 0.7 + 36.49 (vector similarity score) x 0.3:*  
   ![Image](https://github.com/user-attachments/assets/541554d4-3f3e-44e1-954b-0ae77d7372c6)

   *The following is a screenshot of a retrieval test conducted using a knowledge graph. It shows that only vector similarity is used for knowledge graph-generated chunks:*  
   ![Image](https://github.com/user-attachments/assets/30a03091-0f7b-4058-901a-f4dc5ca5aa6b)

:::caution WARNING
If you have adjusted the default settings, such as keyword similarity weight or similarity threshold, to achieve the optimal results, be aware that these changes will not be automatically saved. You must apply them to your chat assistant settings or the **Retrieval** agent component settings.
:::

## Frequently asked questions

### Is an LLM used when the Use Knowledge Graph switch is enabled?

Yes, your LLM will be involved to analyze your query and extract the related entities and relationship from the knowledge graph. This also explains why additional tokens and time will be consumed.
</code></pre></div></div>
<p>guides\dataset\set_metadata.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /set_metada
---

# Set metadata

Add metadata to an uploaded file

---

On the **Dataset** page of your knowledge base, you can add metadata to any uploaded file. This approach enables you to 'tag' additional information like URL, author, date, and more to an existing file or dataset. In an AI-powered chat, such information will be sent to the LLM with the retrieved chunks for content generation.

For example, if you have a dataset of HTML files and want the LLM to cite the source URL when responding to your query, add a `"url"` parameter to each file's metadata.

![Image](https://github.com/user-attachments/assets/78cb5035-e96c-43f9-82d7-8fef1b68c843)

:::tip NOTE
Ensure that your metadata is in JSON format; otherwise, your updates will not be applied.
:::

![Image](https://github.com/user-attachments/assets/379cf2c5-4e37-4b79-8aeb-53bf8e01d326)
</code></pre></div></div>
<p>guides\dataset\set_page_rank.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /set_page_rank
---

# Set page rank

Create a step-retrieval strategy using page rank.

---

## Scenario

In an AI-powered chat, you can configure a chat assistant or an agent to respond using knowledge retrieved from multiple specified knowledge bases (datasets), provided that they employ the same embedding model. In situations where you prefer information from certain knowledge base(s) to take precedence or to be retrieved first, you can use RAGFlow's page rank feature to increase the ranking of chunks from these knowledge bases. For example, if you have configured a chat assistant to draw from two knowledge bases, knowledge base A for 2024 news and knowledge base B for 2023 news, but wish to prioritize news from year 2024, this feature is particularly useful.

:::info NOTE
It is important to note that this 'page rank' feature operates at the level of the entire knowledge base rather than on individual files or documents.
:::

## Configuration

On the **Configuration** page of your knowledge base, drag the slider under **Page rank** to set the page rank value for your knowledge base. You are also allowed to input the intended page rank value in the field next to the slider.

:::info NOTE
The page rank value must be an integer. Range: [0,100]

- 0: Disabled (Default)
- A specific value: enabled
:::

:::tip NOTE
If you set the page rank value to a non-integer, say 1.7, it will be rounded down to the nearest integer, which in this case is 1.
:::

## Scoring mechanism

If you configure a chat assistant's **similarity threshold** to 0.2, only chunks with a hybrid score greater than 0.2 x 100 = 20 will be retrieved and sent to the chat model for content generation. This initial filtering step is crucial for narrowing down relevant information.

If you have assigned a page rank of 1 to knowledge base A (2024 news) and 0 to knowledge base B (2023 news), the final hybrid scores of the retrieved chunks will be adjusted accordingly. A chunk retrieved from knowledge base A with an initial score of 50 will receive a boost of 1 x 100 = 100 points, resulting in a final score of 50 + 1 x 100 = 150. In this way, chunks retrieved from knowledge base A will always precede chunks from knowledge base B.
</code></pre></div></div>
<p>guides\dataset\use_tag_sets.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 6
slug: /use_tag_sets
---

# Use tag set

Use a tag set to tag chunks in your datasets.

---

Retrieval accuracy is the touchstone for a production-ready RAG framework. In addition to retrieval-enhancing approaches like auto-keyword, auto-question, and knowledge graph, RAGFlow introduces an auto-tagging feature to address semantic gaps. The auto-tagging feature automatically maps tags in the user-defined tag sets to relevant chunks within your knowledge base based on similarity with each chunk. This automation mechanism allows you to apply an additional "layer" of domain-specific knowledge to existing datasets, which is particularly useful when dealing with a large number of chunks.

To use this feature, ensure you have at least one properly configured tag set, specify the tag set(s) on the **Configuration** page of your knowledge base (dataset), and then re-parse your documents to initiate the auto-tagging process. During this process, each chunk in your dataset is compared with every entry in the specified tag set(s), and tags are automatically applied based on similarity.

:::caution NOTE
The auto-tagging feature is *unavailable* on the [Infinity](https://github.com/infiniflow/infinity) document engine.
:::

## Scenarios

Auto-tagging applies in situations where chunks are so similar to each other that the intended chunks cannot be distinguished from the rest. For example, when you have a few chunks about iPhone and a majority about iPhone case or iPhone accessaries, it becomes difficult to retrieve those chunks about iPhone without additional information.

## Create tag set

You can consider a tag set as a closed set, and the tags to attach to the chunks in your dataset (knowledge base) are *exclusively* from the specified tag set. You use a tag set to "inform" RAGFlow which chunks to tag and which tags to apply.

### Prepare a tag table file

A tag set can comprise one or multiple table files in XLSX, CSV, or TXT formats. Each table file in the tag set contains two columns, **Description** and **Tag**:

- The first column provides descriptions of the tags listed in the second column. These descriptions can be example chunks or example queries. Similarity will be calculated between each entry in this column and every chunk in your dataset.
- The **Tag** column includes tags to pair with the description entries. Multiple tags should be separated by a comma (,).

:::tip NOTE
As a rule of thumb, consider including the following entries in your tag table:

- Descriptions of intended chunks, along with their corresponding tags.
- User queries that fail to retrieve the correct responses using other methods, ensuring their tags match the intended chunks in your dataset.
:::

### Create a tag set

1. Click **+ Create knowledge base** to create a knowledge base.
2. Navigate to the **Configuration** page of the created knowledge base and choose **Tag** as the default chunking method.
3. Navigate to the **Dataset** page and upload and parse your table file in XLSX, CSV, or TXT formats.  
   _A tag cloud appears under the **Tag view** section, indicating the tag set is created:_  
   ![Image](https://github.com/user-attachments/assets/abefbcbf-c130-4abe-95e1-267b0d2a0505)
4. Click the **Table** tab to view the tag frequency table:  
   ![Image](https://github.com/user-attachments/assets/af91d10c-5ea5-491f-ab21-3803d5ebf59f)

:::danger IMPORTANT
A tag set is *not* involved in document indexing or retrieval. Do not specify a tag set when configuring your chat assistant or agent.
:::

## Tag chunks

Once a tag set is created, you can apply it to your dataset:

1. Navigate to the **Configuration** page of your knowledge base (dataset).
2. Select the tag set from the **Tag sets** dropdown and click **Save** to confirm.

:::tip NOTE
If the tag set is missing from the dropdown, check that it has been created or configured correctly.
:::

3. Re-parse your documents to start the auto-tagging process.  
   _In an AI chat scenario using auto-tagged datasets, each query will be tagged using the corresponding tag set(s) and chunks with these tags will have a higher chance to be retrieved._

## Update tag set

Creating a tag set is *not* for once and for all. Oftentimes, you may find it necessary to update or delete existing tags or add new entries. 

- You can update the existing tag set in the tag frequency table.
- To add new entries, you can add and parse new table files in XLSX, CSV, or TXT formats.

### Update tag set in tag frequency table

1. Navigate to the **Configuration** page in your tag set.
2. Click the **Table** tab under **Tag view** to view the tag frequncy table, where you can update tag names or delete tags.

:::danger IMPORTANT
When a tag set is updated, you must re-parse the documents in your dataset so that their tags can be updated accordingly.
:::

### Add new table files

1. Navigate to the **Configuration** page in your tag set.
2. Navigate to the **Dataset** page and upload and parse your table file in XLSX, CSV, or TXT formats.

:::danger IMPORTANT
If you add new table files to your tag set, it is at your own discretion whether to re-parse your documents in your datasets.
:::

## Frequently asked questions

### Can I reference more than one tag set?

Yes, you can. Usually one tag set suffices. When using multiple tag sets, ensure they are independent of each other; otherwise, consider merging your tag sets.

### Difference between a tag set and a standard knowledge base?

A standard knowledge base is a dataset. It will be searched by RAGFlow's document engine and the retrieved chunks will be fed to the LLM. In contrast, a tag set is used solely to attach tags to chunks within your dataset. It does not directly participate in the retrieval process, and you should not choose a tag set when selecting datasets for your chat assistant or agent.

### Difference between auto-tag and auto-keyword?

Both features enhance retrieval in RAGFlow. The auto-keyword feature relies on the LLM and consumes a significant number of tokens, whereas the auto-tag feature is based on vector similarity and predefined tag set(s). You can view the keywords applied in the auto-keyword feature as an open set, as they are generated by the LLM. In contrast, a tag set can be considered a user-defined close set, requiring upload tag set(s) in specified formats before use.

</code></pre></div></div>
<p>guides\dataset\best_practices_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Best practices"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Best practices on configuring a knowledge base."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\dataset\best_practices\accelerate_doc_indexing.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /accelerate_doc_indexing
---

# Accelerate indexing
import APITable from '@site/src/components/APITable';

A checklist to speed up document parsing and indexing.

---

Please note that some of your settings may consume a significant amount of time. If you often find that document parsing is time-consuming, here is a checklist to consider:

- Use GPU to reduce embedding time.
- On the configuration page of your knowledge base, switch off **Use RAPTOR to enhance retrieval**.
- Extracting knowledge graph (GraphRAG) is time-consuming.
- Disable **Auto-keyword** and **Auto-question** on the configuration page of your knowledge base, as both depend on the LLM.
- **v0.17.0+:** If your document is plain text PDF and does not require GPU-intensive processes like OCR (Optical Character Recognition), TSR (Table Structure Recognition), or DLA (Document Layout Analysis), you can choose **Naive** over **DeepDoc** or other time-consuming large model options in the **Document parser** dropdown. This will substantially reduce document parsing time.

</code></pre></div></div>
<p>guides\models_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Models"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">-1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides on model settings."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\models\llm_api_key_setup.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /llm_api_key_setup
---

# Configure model API key

An API key is required for RAGFlow to interact with an online AI model. This guide provides information about setting your model API key in RAGFlow.

## Get model API key

RAGFlow supports most mainstream LLMs. Please refer to [Supported Models](../../references/supported_models.mdx) for a complete list of supported models. You will need to apply for your model API key online. Note that most LLM providers grant newly-created accounts trial credit, which will expire in a couple of months, or a promotional amount of free quota.

:::note
If you find your online LLM is not on the list, don't feel disheartened. The list is expanding, and you can [file a feature request](https://github.com/infiniflow/ragflow/issues/new?assignees=&amp;labels=feature+request&amp;projects=&amp;template=feature_request.yml&amp;title=%5BFeature+Request%5D%3A+) with us! Alternatively, if you have customized or locally-deployed models, you can [bind them to RAGFlow using Ollama, Xinference, or LocalAI](./deploy_local_llm.mdx).
:::

## Configure model API key

You have two options for configuring your model API key:

- Configure it in **service_conf.yaml.template** before starting RAGFlow.
- Configure it on the **Model providers** page after logging into RAGFlow.

### Configure model API key before starting up RAGFlow

1. Navigate to **./docker/ragflow**.
2. Find entry **user_default_llm**:
   - Update `factory` with your chosen LLM.
   - Update `api_key` with yours.
   - Update `base_url` if you use a proxy to connect to the remote service.
3. Reboot your system for your changes to take effect.
4. Log into RAGFlow.  
   _After logging into RAGFlow, you will find your chosen model appears under **Added models** on the **Model providers** page._

### Configure model API key after logging into RAGFlow

:::caution WARNING
After logging into RAGFlow, configuring your model API key through the **service_conf.yaml.template** file will no longer take effect.
:::

After logging into RAGFlow, you can *only* configure API Key on the **Model providers** page:

1. Click on your logo on the top right of the page **&gt;** **Model providers**.
2. Find your model card under **Models to be added** and click **Add the model**:
   ![add model](https://github.com/infiniflow/ragflow/assets/93570324/07e43f63-367c-4c9c-8ed3-8a3a24703f4e)
3. Paste your model API key.
4. Fill in your base URL if you use a proxy to connect to the remote service.
5. Click **OK** to confirm your changes.

:::note
To update an existing model API key at a later point:
![update api key](https://github.com/infiniflow/ragflow/assets/93570324/0bfba679-33f7-4f6b-9ed6-f0e6e4b228ad)
:::
</code></pre></div></div>
<p>guides\team_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Team"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Team-specific guides."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\team\join_or_leave_team.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /join_or_leave_team
---

# Join or leave a team

Accept an invite to join a team, decline an invite, or leave a team.

---

Once you join a team, you can do the following:

- Upload documents to the team owner's shared datasets (knowledge bases).
- Parse documents in the team owner's shared datasets.
- Use the team owner's shared Agents.

:::tip NOTE
You cannot invite users to a team unless you are its owner.
:::

## Prerequisites

1. Ensure that your Email address that received the team invitation is associated with a RAGFlow user account.
2. The team owner should share his knowledge bases by setting their **Permission** to **Team**.

## Accept or decline team invite

1. You will be notified when you receive an invitation to join a team:

![team_notification](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/team_notification.jpg)

2. Click on your avatar in the top right corner of the page, then select **Team** in the left-hand panel to access the **Team** page.

![team](https://github.com/user-attachments/assets/0eac2503-26bc-4568-b3f2-bcd84069a07a)

_On the **Team** page, you can view the information about members of your team and the teams you have joined._

![accept_or_decline_team_invite](https://github.com/user-attachments/assets/6a2cb61f-03d5-4423-9ed1-71df97ff4114)

_After accepting the team invite, you should be able to view and update the team owner's knowledge bases whose **Permissions** is set to **Team**._

## Leave a joined team

![leave_team](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/quit.jpg)
</code></pre></div></div>
<p>guides\team\manage_team_members.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /manage_team_members
---

# Manage team members

Invite or remove team members.

---

By default, each RAGFlow user is assigned a single team named after their name. RAGFlow allows you to invite RAGFlow users to your team. Your team members can help you:

- Upload documents to your shared datasets (knowledge bases).
- Parse documents in your shared datasets.
- Use your shared Agents.

:::tip NOTE
- Your team members are currently *not* allowed to invite users to your team, and only you, the team owner, is permitted to do so.
- Sharing added models with team members is only available in RAGFlow's Enterprise edition.
:::

## Prerequisites

1. Ensure that the invited team member is a RAGFlow user and that the Email address used is associated with a RAGFlow user account.
2. To allow your team members to view and update your knowledge base, ensure that you set **Permissions** on its **Configuration** page from **Only me** to **Team**.

## Invite team members

Click on your avatar in the top right corner of the page, then select **Team** in the left-hand panel to access the **Team** page.

![team](https://github.com/user-attachments/assets/0eac2503-26bc-4568-b3f2-bcd84069a07a)

_On the **Team** page, you can view the information about members of your team and the teams you have joined._

You are, by default, the owner of your own team and the only person permitted to invite users to join your team or remove team members.

![invite_team_member](https://github.com/user-attachments/assets/d85b55c3-7e86-4f04-a414-ca18a9ee8963)

## Remove team members

![remove_members](https://github.com/user-attachments/assets/5c1a6ab5-8862-47a0-ad09-77fe88866508)
</code></pre></div></div>
<p>guides\team\share_agents.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 5
slug: /share_agent
---

# Share Agent

Share an Agent with your team members.

---

When ready, you may share your Agents with your team members so that they can use them. Please note that your Agents are not shared automatically; you must manually enable sharing by selecting the corresponding **Permissions** radio button:

1. Click the intended Agent to open its editing canvas. 
2. Click **Settings** to show the **Agent settings** dialogue.
3. Change **Permissions** from **Only me** to **Team**.
4. Click **Save** to apply your changes.

![share_agent](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/share_agent.jpg)

*When completed, your team members will see your shared Agents like this:*

![shared_agent](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/shared_agent.jpg)
</code></pre></div></div>
<p>guides\team\share_chat_assistant.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 4
slug: /share_chat_assistant
---

# Share chat assistant

Sharing chat assistant is currently exclusive to RAGFlow Enterprise, but will be made available in due course.
</code></pre></div></div>
<p>guides\team\share_knowledge_bases.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /share_datasets
---

# Share knowledge base

Share a knowledge base with team members.

---

When ready, you may share your knowledge bases with your team members so that they can upload and parse files in them. Please note that your knowledge bases are not shared automatically; you must manually enable sharing by selecting the appropriate **Permissions** radio button:

1. Navigate to the knowledge base's **Configuration** page.
2. Change **Permissions** from **Only me** to **Team**.
3. Click **Save** to apply your changes.

![share_knowledge_base](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/share_knowledge_base.jpg)

*Once completed, your team members will see your shared knowledge bases like this:*

![shared_knowledge_base](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/shared_knowledge_base.jpg)
</code></pre></div></div>
<p>guides\team\share_model.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 6
slug: /share_model
---

# Share models

Sharing models is currently exclusive to RAGFlow Enterprise.
</code></pre></div></div>
<p>references_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"References"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Miscellaneous References"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>references\supported_models.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 0
slug: /supported_models
---

# Supported models
import APITable from '@site/src/components/APITable';

A complete list of models supported by RAGFlow, which will continue to expand.

```mdx-code-block
&lt;APITable&gt;
```

| Provider              | Chat               | Embedding          | Rerank             | Img2txt            | Speech2txt         | TTS                |
| --------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |
| Anthropic             | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Azure-OpenAI          | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: |                    |
| BAAI                  |                    | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |
| BaiChuan              | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| BaiduYiyan            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| Bedrock               | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| Cohere                | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| DeepSeek              | :heavy_check_mark: |                    |                    |                    |                    |                    |
| FastEmbed             |                    | :heavy_check_mark: |                    |                    |                    |                    |
| Fish Audio            |                    |                    |                    |                    |                    | :heavy_check_mark: |
| Gemini                | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |                    |
| Google Cloud          | :heavy_check_mark: |                    |                    |                    |                    |                    |
| GPUStack              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: |
| Groq                  | :heavy_check_mark: |                    |                    |                    |                    |                    |
| HuggingFace           | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| Jina                  |                    | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |
| LeptonAI              | :heavy_check_mark: |                    |                    |                    |                    |                    |
| LocalAI               | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |                    |
| LM-Studio             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| MiniMax               | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Mistral               | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| ModelScope            | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Moonshot              | :heavy_check_mark: |                    |                    | :heavy_check_mark: |                    |                    |
| Novita AI             | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| NVIDIA                | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| Ollama                | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |                    |
| OpenAI                | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |
| OpenAI-API-Compatible | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| OpenRouter            | :heavy_check_mark: |                    |                    | :heavy_check_mark: |                    |                    |
| PerfXCloud            | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| Replicate             | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| PPIO                  | :heavy_check_mark: |                    |                    |                    |                    |                    |
| SILICONFLOW           | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| StepFun               | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Tencent Hunyuan       | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Tencent Cloud         |                    |                    |                    |                    | :heavy_check_mark: |                    |
| TogetherAI            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| Tongyi-Qianwen        | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |
| Upstage               | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| VLLM                  | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| VolcEngine            | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Voyage AI             |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| Xinference            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |
| XunFei Spark          | :heavy_check_mark: |                    |                    |                    |                    | :heavy_check_mark: |
| Youdao                |                    | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |
| ZHIPU-AI              | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |                    |
| 01.AI                 | :heavy_check_mark: |                    |                    |                    |                    |                    |

```mdx-code-block
&lt;/APITable&gt;
```

:::danger IMPORTANT
If your model is not listed here but has APIs compatible with those of OpenAI, click **OpenAI-API-Compatible** on the **Model providers** page to configure your model.
::: 

:::note
The list of supported models is extracted from [this source](https://github.com/infiniflow/ragflow/blob/main/rag/llm/__init__.py) and may not be the most current. For the latest supported model list, please refer to the Python file.
:::

</code></pre></div></div>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">Rust CLI parallel processing for copy/move files</title><link href="https://ib.bsb.br/rust-cli-parallel-processing-for-copymove-files/" rel="alternate" type="text/html" title="Rust CLI parallel processing for copy/move files" /><published>2025-05-16T00:00:00+00:00</published><updated>2025-05-16T19:02:11+00:00</updated><id>https://ib.bsb.br/rust-cli-parallel-processing-for-copymove-files</id><content type="html" xml:base="https://ib.bsb.br/rust-cli-parallel-processing-for-copymove-files/"><![CDATA[<p>This tool provides basic file and directory <code class="language-plaintext highlighter-rouge">copy</code>, <code class="language-plaintext highlighter-rouge">move</code> (cut-and-paste), and <code class="language-plaintext highlighter-rouge">delete</code> operations, with an option for parallel processing on multi-core systems.</p>

<p>This guide is tailored for an <code class="language-plaintext highlighter-rouge">arm64 Debian Bullseye</code> environment, such as those found on RK3588-based single-board computers or servers.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>Before you begin, ensure your <code class="language-plaintext highlighter-rouge">arm64 Debian Bullseye</code> system is set up with the following:</p>

<ol>
  <li><strong>Rust Language Toolchain:</strong>
    <ul>
      <li>If you don’t have Rust installed, visit <a href="https://rustup.rs/">https://rustup.rs/</a> and follow the instructions. This will install <code class="language-plaintext highlighter-rouge">rustc</code> (the compiler) and <code class="language-plaintext highlighter-rouge">cargo</code> (the build tool and package manager).
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--proto</span> <span class="s1">'=https'</span> <span class="nt">--tlsv1</span>.2 <span class="nt">-sSf</span> https://sh.rustup.rs | sh
<span class="nb">source</span> <span class="nv">$HOME</span>/.cargo/env 
<span class="c"># You might need to open a new terminal or re-login for changes to take effect</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>Essential Build Tools:</strong>
    <ul>
      <li>Install common build utilities often required by Rust crates that might link against C libraries or need system configuration.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>build-essential pkg-config libssl-dev
</code></pre></div>        </div>
        <p><em>(Note: <code class="language-plaintext highlighter-rouge">libssl-dev</code> is a common dependency for many Rust crates, though not strictly required by this specific MVP’s direct dependencies, it’s good practice to have it for broader Rust development).</em></p>
      </li>
    </ul>
  </li>
</ol>

<h2 id="1-creating-the-project-and-adding-code">1. Creating the Project and Adding Code</h2>

<p>First, create a new Rust project using Cargo and navigate into its directory:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cargo new rapidcopy_rust_cli_mvp
<span class="nb">cd </span>rapidcopy_rust_cli_mvp
</code></pre></div></div>

<p>Next, you’ll replace the default <code class="language-plaintext highlighter-rouge">src/main.rs</code> and <code class="language-plaintext highlighter-rouge">Cargo.toml</code> files with the code for our RapidCopy-rs MVP.</p>

<p><strong>Replace the contents of <code class="language-plaintext highlighter-rouge">Cargo.toml</code> with the following:</strong></p>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[package]</span>
<span class="py">name</span> <span class="p">=</span> <span class="s">"rapidcopy_rust_cli_mvp"</span>
<span class="py">version</span> <span class="p">=</span> <span class="s">"0.1.0"</span>
<span class="py">edition</span> <span class="p">=</span> <span class="s">"2021"</span>

<span class="nn">[dependencies]</span>
<span class="py">clap</span> <span class="o">=</span> <span class="p">{</span> <span class="py">version</span> <span class="p">=</span> <span class="s">"4.5.38"</span><span class="p">,</span> <span class="py">features</span> <span class="p">=</span> <span class="p">[</span><span class="s">"derive"</span><span class="p">]</span> <span class="p">}</span>
<span class="py">indicatif</span> <span class="p">=</span> <span class="s">"0.17.11"</span>
<span class="py">rayon</span> <span class="p">=</span> <span class="s">"1.10.0"</span>
<span class="py">thiserror</span> <span class="p">=</span> <span class="s">"1.0.69"</span>
<span class="py">walkdir</span> <span class="p">=</span> <span class="s">"2.5.0"</span>
<span class="py">libc</span> <span class="p">=</span> <span class="s">"0.2.172"</span>
<span class="py">path-absolutize</span> <span class="p">=</span> <span class="s">"3.1.1"</span>
</code></pre></div></div>

<p><strong>Replace the contents of <code class="language-plaintext highlighter-rouge">src/main.rs</code> with the following Rust code:</strong></p>

<section class="code-block-container" role="group" aria-label="Rust Code Block" data-filename="rust_code_block.rs" data-code="// src/main.rs

use clap::{Parser, Subcommand};
use indicatif::{HumanBytes, MultiProgress, ProgressBar, ProgressStyle};
use path_absolutize::*; // For Path::absolutize()
use rayon::prelude::*;
use std::{
    ffi::OsString,
    fs,
    io::{self, ErrorKind, Read, Write},
    path::{Path, PathBuf},
    sync::{
        atomic::{AtomicU64, Ordering as AtomicOrdering},
        Arc, Mutex,
    },
    thread,
    time::Duration,
};
use thiserror::Error;
use walkdir::{DirEntry, WalkDir};

// --- Argument Parsing ---
#[derive(Parser, Debug)]
#[clap(author, version, about = &quot;RapidCopy-rs (MVP): Fast file operations CLI&quot;, long_about = None)]
struct Cli {
    #[clap(subcommand)]
    command: Commands,

    #[clap(short, long, global = true, help = &quot;Enable verbose output&quot;)]
    verbose: bool,
}

#[derive(Subcommand, Debug)]
enum Commands {
    /// Copies a file or directory recursively
    Copy {
        /// Source path
        source: PathBuf,
        /// Destination path
        destination: PathBuf,
        #[clap(short, long, help = &quot;Overwrite existing files at the destination&quot;)]
        overwrite: bool,
        #[clap(long, help = &quot;Enable parallel processing for directory contents&quot;)]
        parallel: bool,
    },
    /// Moves a file or directory recursively (cut and paste)
    Move {
        /// Source path
        source: PathBuf,
        /// Destination path
        destination: PathBuf,
        #[clap(short, long, help = &quot;Overwrite existing files at the destination if copying&quot;)]
        overwrite: bool,
        #[clap(short = &#39;f&#39;, long, help = &quot;Force deletion of source without interactive confirmation (used in move)&quot;)]
        force_delete_source: bool,
        #[clap(long, help = &quot;Enable parallel processing for directory contents during copy phase&quot;)]
        parallel: bool,
    },
    /// Deletes a file or directory recursively
    Delete {
        /// Path to delete
        path: PathBuf,
        #[clap(short, long, help = &quot;Force deletion without interactive confirmation&quot;)]
        force: bool,
    },
}

// --- Error Handling ---
#[derive(Error, Debug)]
pub enum AppError {
    #[error(&quot;I/O error accessing &#39;{path:?}&#39;: {source}&quot;)]
    Io {
        source: io::Error,
        path: PathBuf,
    },
    #[error(&quot;WalkDir error for path &#39;{path:?}&#39;: {source}&quot;)]
    WalkDir {
        source: walkdir::Error,
        path: PathBuf,
    },
    #[error(&quot;Source path does not exist: {0}&quot;)]
    SourceDoesNotExist(PathBuf),
    #[error(&quot;Destination path &#39;{0}&#39; already exists and overwrite is false&quot;)]
    DestinationExists(PathBuf),
    #[error(&quot;Cannot copy/move directory &#39;{source_dir}&#39; to existing file &#39;{dest_file}&#39;&quot;)]
    DirToExistingFile {
        source_dir: PathBuf,
        dest_file: PathBuf,
    },
    #[error(&quot;Cannot copy/move file &#39;{source_file}&#39; to existing directory &#39;{dest_dir}&#39; (ambiguous destination name or destination is a file and overwrite is false)&quot;)]
    FileToExistingDirAmbiguous {
        source_file: PathBuf,
        dest_dir: PathBuf,
    },
    #[error(&quot;User did not confirm deletion of &#39;{0}&#39;&quot;)]
    DeletionNotConfirmed(PathBuf),
    #[error(&quot;Failed to get metadata for path: {0}&quot;)]
    MetadataError(PathBuf),
    #[error(&quot;Source path &#39;{0}&#39; has no filename component&quot;)]
    NoFileName(PathBuf),
    #[error(&quot;Failed to strip prefix for path processing: &#39;{path}&#39; relative to &#39;{base}&#39;&quot;)]
    StripPrefixError { path: PathBuf, base: PathBuf },
    #[error(&quot;Operation failed with multiple errors during parallel processing:\n{0:#?}&quot;)]
    ParallelOperationFailed(Vec&lt;String&gt;),
    #[error(&quot;Cannot move/copy &#39;{0}&#39; to a subdirectory of itself &#39;{1}&#39;&quot;)]
    RecursiveMoveOrCopy(PathBuf, PathBuf),
    #[error(&quot;Indicatif style template error: {0}&quot;)]
    TemplateError(String),
}

impl From&lt;walkdir::Error&gt; for AppError {
    fn from(err: walkdir::Error) -&gt; Self {
        let path = err
            .path()
            .unwrap_or_else(|| Path::new(&quot;&lt;unknown path from walkdir error&gt;&quot;))
            .to_path_buf();
        AppError::WalkDir { source: err, path }
    }
}

fn io_err_with_path(err: io::Error, path: &amp;Path) -&gt; AppError {
    AppError::Io {
        source: err,
        path: path.to_path_buf(),
    }
}

impl From&lt;indicatif::style::TemplateError&gt; for AppError {
    fn from(err: indicatif::style::TemplateError) -&gt; Self {
        AppError::TemplateError(err.to_string())
    }
}

// --- Core Logic Module ---
mod core_logic {
    use super::*;

    const BUFFER_SIZE: usize = 128 * 1024;

    #[derive(Debug)]
    struct ProgressInfo {
        items_processed: AtomicU64,
        bytes_processed: AtomicU64,
        total_items_to_process: u64,
        total_bytes_to_process: u64,
    }

    impl ProgressInfo {
        fn new(total_items: u64, total_bytes: u64) -&gt; Self {
            Self {
                items_processed: AtomicU64::new(0),
                bytes_processed: AtomicU64::new(0),
                total_items_to_process: total_items,
                total_bytes_to_process: total_bytes,
            }
        }
    }

    fn pre_scan_directory(dir: &amp;Path, verbose: bool) -&gt; Result&lt;(u64, u64), AppError&gt; {
        if verbose {
            println!(&quot;Pre-scanning directory &#39;{}&#39;...&quot;, dir.display());
        }
        let mut total_items = 0u64;
        let mut total_bytes = 0u64;
        total_items += 1; // Count the root directory itself
        for entry_result in WalkDir::new(dir).min_depth(1).follow_links(false) {
            let entry = entry_result?; 
            total_items += 1;
            if entry.file_type().is_file() {
                total_bytes += entry.metadata()?.len(); 
            }
        }
        if verbose {
            println!(
                &quot;Pre-scan complete: {} items, {}&quot;,
                total_items,
                HumanBytes(total_bytes)
            );
        }
        Ok((total_items, total_bytes))
    }

    fn copy_single_file(
        source: &amp;Path,
        destination: &amp;Path,
        overwrite: bool,
        mp_opt: Option&lt;&amp;MultiProgress&gt;,
        overall_progress_info_opt: Option&lt;&amp;Arc&lt;ProgressInfo&gt;&gt;,
        verbose: bool,
    ) -&gt; Result&lt;u64, AppError&gt; {
        if destination.exists() {
            let dest_meta =
                fs::metadata(destination).map_err(|e| io_err_with_path(e, destination))?;
            if dest_meta.is_dir() {
                return Err(AppError::FileToExistingDirAmbiguous {
                    source_file: source.to_path_buf(),
                    dest_dir: destination.to_path_buf(),
                });
            }
            if !overwrite {
                if verbose {
                    println!(
                        &quot;Skipping &#39;{}&#39;: destination file exists and overwrite is false.&quot;,
                        destination.display()
                    );
                }
                if let Some(stats) = overall_progress_info_opt {
                    stats.items_processed.fetch_add(1, AtomicOrdering::Relaxed);
                }
                return Ok(0);
            }
        }

        if let Some(parent) = destination.parent() {
            if !parent.exists() {
                fs::create_dir_all(parent).map_err(|e| io_err_with_path(e, parent))?;
            }
        }

        let file_size = fs::metadata(source)
            .map_err(|e| io_err_with_path(e, source))?
            .len();

        let pb = if let Some(mp) = mp_opt {
            let p = mp.add(ProgressBar::new(file_size));
            p.enable_steady_tick(Duration::from_millis(250));
            p
        } else {
            let p = ProgressBar::new(file_size);
            p.enable_steady_tick(Duration::from_millis(100));
            p
        };

        pb.set_style(
            ProgressStyle::default_bar()
                .template(
                    &quot;{msg:&lt;30.bold.dim} [{bar:40.cyan/blue}] {bytes}/{total_bytes} ({bytes_per_sec}, {eta})&quot;,
                )?
                .progress_chars(&quot;=&gt; &quot;),
        );
        let filename = source.file_name().unwrap_or_default().to_string_lossy();
        pb.set_message(filename.chars().take(30).collect::&lt;String&gt;());

        let mut source_file = fs::File::open(source).map_err(|e| io_err_with_path(e, source))?;
        let mut dest_file =
            fs::File::create(destination).map_err(|e| io_err_with_path(e, destination))?;

        let mut buffer = vec![0; BUFFER_SIZE];
        let mut copied_bytes_for_this_file = 0u64;

        loop {
            let bytes_read = source_file
                .read(&amp;mut buffer)
                .map_err(|e| io_err_with_path(e, source))?;
            if bytes_read == 0 {
                break;
            }
            dest_file
                .write_all(&amp;buffer[..bytes_read])
                .map_err(|e| io_err_with_path(e, destination))?;
            copied_bytes_for_this_file += bytes_read as u64;
            pb.set_position(copied_bytes_for_this_file);
            if let Some(stats) = overall_progress_info_opt {
                stats
                    .bytes_processed
                    .fetch_add(bytes_read as u64, AtomicOrdering::Relaxed);
            }
        }
        pb.finish_with_message(format!(&quot;Copied {} ({})&quot;, filename, HumanBytes(file_size)));
        if let Some(stats) = overall_progress_info_opt {
            stats.items_processed.fetch_add(1, AtomicOrdering::Relaxed);
        }
        Ok(file_size)
    }

    fn copy_dir_recursive(
        source_dir: &amp;Path,
        target_dest_dir: &amp;Path, 
        overwrite: bool,
        parallel: bool,
        _overall_item_pb: &amp;ProgressBar, 
        progress_info: &amp;Arc&lt;ProgressInfo&gt;,
        verbose: bool, // verbose is available here
    ) -&gt; Result&lt;(), Vec&lt;String&gt;&gt; {
        if verbose {
            println!(
                &quot;Recursively copying contents of &#39;{}&#39; into &#39;{}&#39;&quot;,
                source_dir.display(),
                target_dest_dir.display()
            );
        }

        if !target_dest_dir.exists() {
            fs::create_dir_all(target_dest_dir)
                .map_err(|e| vec![io_err_with_path(e, target_dest_dir).to_string()])?;
            if verbose {
                println!(&quot;Created directory &#39;{}&#39;&quot;, target_dest_dir.display());
            }
        }
      
        let entries: Vec&lt;DirEntry&gt; = WalkDir::new(source_dir)
            .min_depth(1) 
            .follow_links(false)
            .into_iter()
            .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()
            .map_err(|e| {
                vec![AppError::WalkDir {
                    source: e,
                    path: source_dir.to_path_buf(),
                }
                .to_string()]
            })?;

        let multi_progress_manager_opt = if parallel &amp;&amp; entries.iter().filter(|e| e.path().is_file()).count() &gt; 1 {
            Some(MultiProgress::new())
        } else {
            None
        };

        let errors_arc = Arc::new(Mutex::new(Vec::&lt;String&gt;::new()));

        let process_entry_closure = |entry: DirEntry| {
            let entry_path = entry.path();
            let relative_path = match entry_path.strip_prefix(source_dir) {
                Ok(p) =&gt; p,
                Err(_) =&gt; {
                    errors_arc.lock().unwrap().push(
                        AppError::StripPrefixError {
                            path: entry_path.to_path_buf(),
                            base: source_dir.to_path_buf(),
                        }
                        .to_string(),
                    );
                    return;
                }
            };
            let dest_path = target_dest_dir.join(relative_path);

            if entry.file_type().is_dir() {
                if verbose {
                    println!(&quot;Creating directory &#39;{}&#39;&quot;, dest_path.display());
                }
                if let Err(e) = fs::create_dir_all(&amp;dest_path) {
                    errors_arc
                        .lock()
                        .unwrap()
                        .push(io_err_with_path(e, &amp;dest_path).to_string());
                    return;
                }
                progress_info
                    .items_processed
                    .fetch_add(1, AtomicOrdering::Relaxed);
            } else if entry.file_type().is_file() {
                match copy_single_file(
                    entry_path,
                    &amp;dest_path,
                    overwrite,
                    multi_progress_manager_opt.as_ref(),
                    Some(progress_info), 
                    verbose,
                ) {
                    Ok(_) =&gt; {}
                    Err(e) =&gt; {
                        errors_arc.lock().unwrap().push(format!(
                            &quot;Error copying file &#39;{}&#39;: {}&quot;,
                            entry_path.display(),
                            e
                        ));
                    }
                }
            } else {
                progress_info
                    .items_processed
                    .fetch_add(1, AtomicOrdering::Relaxed);
                if verbose {
                    println!(&quot;Skipping non-file/non-directory entry: {}&quot;, entry_path.display());
                }
            }
        };

        if parallel &amp;&amp; entries.len() &gt; 1 {
            entries.into_par_iter().for_each(process_entry_closure);
        } else {
            for entry in entries {
                process_entry_closure(entry);
            }
        }

        if let Some(mp) = multi_progress_manager_opt {
            if let Err(e) = mp.clear() {
                if verbose {
                    // Error during UI cleanup is not critical to the copy operation itself.
                    eprintln!(&quot;Warning: Failed to clear multi-progress display: {}&quot;, e);
                }
            }
        }

        let final_errors = Arc::try_unwrap(errors_arc)
            .expect(&quot;Mutex still has multiple owners for errors_arc&quot;)
            .into_inner()
            .expect(&quot;Mutex was poisoned for errors_arc&quot;);

        if !final_errors.is_empty() {
            return Err(final_errors);
        }
        Ok(())
    }

    pub fn handle_copy_operation_main(
        source: &amp;Path,
        destination: &amp;Path,
        overwrite: bool,
        parallel: bool,
        verbose: bool,
    ) -&gt; Result&lt;u64, AppError&gt; {
        if verbose {
            println!(
                &quot;Copy operation: &#39;{}&#39; -&gt; &#39;{}&#39;&quot;,
                source.display(),
                destination.display()
            );
        }
        if !source.exists() {
            return Err(AppError::SourceDoesNotExist(source.to_path_buf()));
        }

        let source_meta = fs::metadata(source).map_err(|e| io_err_with_path(e, source))?;
        let source_abs = source.absolutize().map_err(|e| io_err_with_path(e, source))?.into_owned();


        let target_path_for_item: PathBuf; 
        let effective_dest_dir_for_contents: PathBuf; 

        if source_meta.is_dir() {
            let source_name = source
                .file_name()
                .ok_or_else(|| AppError::NoFileName(source.to_path_buf()))?;
            if destination.exists()
                &amp;&amp; fs::metadata(destination)
                    .map_err(|e| io_err_with_path(e, destination))?
                    .is_dir()
            {
                target_path_for_item = destination.join(source_name);
            } else if destination.exists()
                &amp;&amp; fs::metadata(destination)
                    .map_err(|e| io_err_with_path(e, destination))?
                    .is_file()
            {
                return Err(AppError::DirToExistingFile {
                    source_dir: source.to_path_buf(),
                    dest_file: destination.to_path_buf(),
                });
            } else {
                target_path_for_item = destination.to_path_buf();
            }
            effective_dest_dir_for_contents = target_path_for_item.clone(); 
            
            let target_abs_check = target_path_for_item.absolutize().map_err(|e| io_err_with_path(e, &amp;target_path_for_item))?.into_owned();
            if target_abs_check.starts_with(&amp;source_abs) {
                 return Err(AppError::RecursiveMoveOrCopy(source.to_path_buf(), target_path_for_item));
            }

        } else { // Source is a file
            if destination.exists()
                &amp;&amp; fs::metadata(destination)
                    .map_err(|e| io_err_with_path(e, destination))?
                    .is_dir()
            {
                target_path_for_item = destination.join(
                    source
                        .file_name()
                        .ok_or_else(|| AppError::NoFileName(source.to_path_buf()))?,
                );
            } else if !destination.exists()
                &amp;&amp; destination.to_string_lossy().ends_with(std::path::MAIN_SEPARATOR)
            {
                fs::create_dir_all(destination).map_err(|e| io_err_with_path(e, destination))?;
                target_path_for_item = destination.join(
                    source
                        .file_name()
                        .ok_or_else(|| AppError::NoFileName(source.to_path_buf()))?,
                );
            } else {
                target_path_for_item = destination.to_path_buf();
            }
            effective_dest_dir_for_contents = target_path_for_item
                .parent()
                .unwrap_or_else(|| Path::new(&quot;.&quot;))
                .to_path_buf();
        }

        let (total_items_scanned, total_bytes_scanned) = if source_meta.is_dir() {
            pre_scan_directory(source, verbose)?
        } else {
            (1, source_meta.len()) 
        };
        let progress_info = Arc::new(ProgressInfo::new(
            total_items_scanned,
            total_bytes_scanned,
        ));

        let overall_pb = ProgressBar::new(total_items_scanned.max(1)); 
        overall_pb.set_style(
            ProgressStyle::default_bar()
                .template(&quot;{msg} [{elapsed_precise}] {wide_bar:.cyan/blue} Overall Items: {pos}/{len} ({items_per_sec}, ETA {eta})&quot;)?
                .progress_chars(&quot;=&gt; &quot;),
        );
        let source_filename_owned: OsString =
            source.file_name().unwrap_or_default().to_os_string();

        overall_pb.set_message(format!(
            &quot;Preparing to copy {}&quot;,
            source_filename_owned.to_string_lossy()
        ));
        overall_pb.enable_steady_tick(Duration::from_millis(200));

        let overall_pb_clone_for_updater = overall_pb.clone();
        let progress_info_clone_for_updater = progress_info.clone();
        let source_filename_clone_for_updater = source_filename_owned.clone();

        let bytes_updater_thread = thread::spawn(move || {
            while !overall_pb_clone_for_updater.is_finished() {
                let items_done = progress_info_clone_for_updater
                    .items_processed
                    .load(AtomicOrdering::Relaxed);
                let bytes_done = progress_info_clone_for_updater
                    .bytes_processed
                    .load(AtomicOrdering::Relaxed);

                overall_pb_clone_for_updater.set_position(items_done); 
                overall_pb_clone_for_updater.set_message(format!(
                    &quot;Copying {} (Bytes: {} / {}) | Items: {}/{}&quot;,
                    source_filename_clone_for_updater.to_string_lossy(),
                    HumanBytes(bytes_done),
                    HumanBytes(progress_info_clone_for_updater.total_bytes_to_process),
                    items_done,
                    progress_info_clone_for_updater.total_items_to_process
                ));
                thread::sleep(Duration::from_millis(200));
            }
            let items_done = progress_info_clone_for_updater
                .items_processed
                .load(AtomicOrdering::Relaxed);
            let bytes_done = progress_info_clone_for_updater
                .bytes_processed
                .load(AtomicOrdering::Relaxed);
            overall_pb_clone_for_updater.set_position(items_done);
            overall_pb_clone_for_updater.set_message(format!(
                &quot;Finished {} (Bytes: {} / {}) | Items: {}/{}&quot;,
                source_filename_clone_for_updater.to_string_lossy(),
                HumanBytes(bytes_done),
                HumanBytes(progress_info_clone_for_updater.total_bytes_to_process),
                items_done,
                progress_info_clone_for_updater.total_items_to_process
            ));
        });

        let result = if source_meta.is_dir() {
            if !target_path_for_item.exists() {
                 fs::create_dir_all(&amp;target_path_for_item).map_err(|e| io_err_with_path(e, &amp;target_path_for_item))?;
                 if verbose { println!(&quot;Created target base directory &#39;{}&#39;&quot;, target_path_for_item.display()); }
            }
            progress_info.items_processed.fetch_add(1, AtomicOrdering::Relaxed);

            copy_dir_recursive(
                source, 
                &amp;effective_dest_dir_for_contents, 
                overwrite,
                parallel,
                &amp;overall_pb,
                &amp;progress_info,
                verbose,
            )
            .map_err(AppError::ParallelOperationFailed)?;
            Ok(progress_info.items_processed.load(AtomicOrdering::Relaxed))
        } else if source_meta.is_file() {
            copy_single_file(
                source,
                &amp;target_path_for_item,
                overwrite,
                None, 
                Some(&amp;progress_info),
                verbose,
            )?;
            Ok(1) 
        } else {
            if verbose {
                println!(
                    &quot;Warning: Source &#39;{}&#39; is not a regular file or directory. Skipping.&quot;,
                    source.display()
                );
            }
            Ok(0)
        };

        overall_pb.set_position(progress_info.items_processed.load(AtomicOrdering::Relaxed)); 
        overall_pb.finish_with_message(format!(&quot;Copy of &#39;{}&#39; complete.&quot;, source.display()));
        bytes_updater_thread
            .join()
            .expect(&quot;Bytes updater thread panicked&quot;);
        result
    }

    pub fn handle_delete_operation(
        path: &amp;Path,
        force: bool,
        verbose: bool,
    ) -&gt; Result&lt;u64, AppError&gt; {
        if verbose {
            println!(
                &quot;Delete operation: &#39;{}&#39; (Force: {})&quot;,
                path.display(),
                force
            );
        }

        if !path.exists() {
            if force {
                if verbose {
                    println!(
                        &quot;Path &#39;{}&#39; does not exist. Nothing to delete (forced).&quot;,
                        path.display()
                    );
                }
                return Ok(0);
            }
            return Err(AppError::SourceDoesNotExist(path.to_path_buf()));
        }

        if !force {
            print!(
                &quot;Are you sure you want to delete &#39;{}&#39; and all its contents? (yes/no): &quot;,
                path.display()
            );
            io::stdout().flush().map_err(|e| io_err_with_path(e, path))?;
            let mut confirmation = String::new();
            io::stdin()
                .read_line(&amp;mut confirmation)
                .map_err(|e| io_err_with_path(e, path))?;
            if confirmation.trim().to_lowercase() != &quot;yes&quot; {
                return Err(AppError::DeletionNotConfirmed(path.to_path_buf()));
            }
        }

        let items_deleted_count: u64;

        if path.is_dir() {
            let (total_items, _) = pre_scan_directory(path, verbose)?; 

            let pb = ProgressBar::new(total_items.max(1));
            pb.set_style(
                ProgressStyle::default_bar()
                    .template(
                        &quot;{msg:.red.bold} [{bar:40.red/yellow}] Items: {pos}/{len} ({elapsed_precise})&quot;,
                    )?
                    .progress_chars(&quot;=&gt; &quot;),
            );
            pb.set_message(format!(
                &quot;Deleting dir {}&quot;,
                path.file_name().unwrap_or_default().to_string_lossy()
            ));
            pb.enable_steady_tick(Duration::from_millis(200));
            
            fs::remove_dir_all(path).map_err(|e| io_err_with_path(e, path))?;
            items_deleted_count = total_items; 
            pb.set_position(items_deleted_count);
            pb.finish_with_message(format!(&quot;Deleted directory &#39;{}&#39;&quot;, path.display()));
        } else {
            if verbose {
                println!(&quot;Deleting file &#39;{}&#39;...&quot;, path.display());
            }
            fs::remove_file(path).map_err(|e| io_err_with_path(e, path))?;
            items_deleted_count = 1;
        }

        if verbose {
            println!(
                &quot;Deletion of &#39;{}&#39; complete. {} items affected.&quot;,
                path.display(),
                items_deleted_count
            );
        }
        Ok(items_deleted_count)
    }

    pub fn handle_move_operation(
        source: PathBuf,
        destination: PathBuf,
        overwrite: bool,
        force_delete_source: bool, 
        parallel: bool,
        verbose: bool,
    ) -&gt; Result&lt;u64, AppError&gt; {
        if verbose {
            println!(
                &quot;Move operation: &#39;{}&#39; -&gt; &#39;{}&#39;&quot;,
                source.display(),
                destination.display()
            );
        }
        if !source.exists() {
            return Err(AppError::SourceDoesNotExist(source.clone()));
        }

        let source_meta = fs::metadata(&amp;source).map_err(|io_e| io_err_with_path(io_e, &amp;source))?;
        let source_abs = source.absolutize().map_err(|e| io_err_with_path(e, &amp;source))?.into_owned();

        let final_dest_target = if destination.exists()
            &amp;&amp; fs::metadata(&amp;destination)
                .map_err(|e| io_err_with_path(e, &amp;destination))?
                .is_dir()
        {
            destination.join(
                source
                    .file_name()
                    .ok_or_else(|| AppError::NoFileName(source.clone()))?,
            )
        } else {
            destination.clone()
        };

        if source_meta.is_dir() {
            let target_abs_check = final_dest_target.absolutize().map_err(|e| io_err_with_path(e, &amp;final_dest_target))?.into_owned();
            if target_abs_check.starts_with(&amp;source_abs) {
                 return Err(AppError::RecursiveMoveOrCopy(source, final_dest_target));
            }
        }

        if final_dest_target.exists() &amp;&amp; !overwrite {
            if source_meta.is_file() {
                 let dest_meta = fs::metadata(&amp;final_dest_target).map_err(|e| io_err_with_path(e, &amp;final_dest_target))?;
                 if dest_meta.is_file() {
                    return Err(AppError::DestinationExists(final_dest_target));
                 }
            } else if source_meta.is_dir() {
                let dest_meta = fs::metadata(&amp;final_dest_target).map_err(|e| io_err_with_path(e, &amp;final_dest_target))?;
                if dest_meta.is_file() {
                    return Err(AppError::DirToExistingFile { source_dir: source.clone(), dest_file: final_dest_target });
                }
                 return Err(AppError::DestinationExists(final_dest_target));
            }
        }

        match fs::rename(&amp;source, &amp;final_dest_target) {
            Ok(_) =&gt; {
                if verbose {
                    println!(
                        &quot;Successfully moved (renamed) &#39;{}&#39; to &#39;{}&#39;&quot;,
                        source.display(),
                        final_dest_target.display()
                    );
                }
                Ok(1) 
            }
            Err(e) =&gt; {
                let should_fallback_to_copy_delete = e.kind() == ErrorKind::CrossesDevices
                    || e.raw_os_error() == Some(libc::EXDEV)
                    || (source_meta.is_file()
                        &amp;&amp; destination.exists() 
                        &amp;&amp; fs::metadata(&amp;destination)
                            .map_err(|io_e| io_err_with_path(io_e, &amp;destination))?
                            .is_dir());

                if should_fallback_to_copy_delete {
                    if verbose {
                        println!(
                            &quot;Rename failed (reason: {}, OS error: {:?}), attempting copy then delete...&quot;,
                            e.kind(),
                            e.raw_os_error()
                        );
                    }
                    
                    let items_copied = handle_copy_operation_main(
                        &amp;source,
                        &amp;destination, 
                        overwrite,
                        parallel,
                        verbose,
                    )?;
                    if verbose {
                        println!(
                            &quot;Copy phase of move complete. Now deleting source &#39;{}&#39;.&quot;,
                            source.display()
                        );
                    }

                    match handle_delete_operation(&amp;source, force_delete_source, verbose) { 
                        Ok(_) =&gt; {
                            if verbose {
                                println!(
                                    &quot;Source &#39;{}&#39; deleted successfully after copy.&quot;,
                                    source.display()
                                );
                            }
                            Ok(items_copied)
                        }
                        Err(del_err) =&gt; {
                            eprintln!(&quot;CRITICAL: Error deleting source &#39;{}&#39; after copy: {}. Copied files remain at &#39;{}&#39;. Manual cleanup of source may be required.&quot;, source.display(), del_err, destination.display());
                            Err(AppError::Io {
                                source: io::Error::new(
                                    ErrorKind::Other,
                                    &quot;Failed to delete source after successful copy during move operation&quot;,
                                ),
                                path: source,
                            })
                        }
                    }
                } else {
                    Err(io_err_with_path(e, &amp;source))
                }
            }
        }
    }
}

// --- Main Application Logic ---
fn main() -&gt; Result&lt;(), ()&gt; {
    let cli = Cli::parse();

    let result: Result&lt;u64, AppError&gt; = match cli.command {
        Commands::Copy {
            source,
            destination,
            overwrite,
            parallel,
        } =&gt; core_logic::handle_copy_operation_main(
            &amp;source,
            &amp;destination,
            overwrite,
            parallel,
            cli.verbose,
        ),
        Commands::Move {
            source,
            destination,
            overwrite,
            force_delete_source,
            parallel,
        } =&gt; core_logic::handle_move_operation(
            source,
            destination,
            overwrite,
            force_delete_source,
            parallel,
            cli.verbose,
        ),
        Commands::Delete { path, force } =&gt; {
            core_logic::handle_delete_operation(&amp;path, force, cli.verbose)
        }
    };

    match result {
        Ok(items_affected) =&gt; {
            if items_affected &gt; 0 || cli.verbose {
                println!(
                    &quot;Operation completed successfully. {} items affected.&quot;,
                    items_affected
                );
            } else if !cli.verbose {
                println!(
                    &quot;Operation completed (no items affected or action skipped). Use --verbose for details.&quot;
                );
            }
            Ok(())
        }
        Err(e) =&gt; {
            eprintln!(&quot;Error: {}&quot;, e);
            if let AppError::ParallelOperationFailed(errors) = e {
                for (i, err_msg) in errors.iter().enumerate() {
                    eprintln!(&quot;  Parallel error {}: {}&quot;, i + 1, err_msg);
                }
            }
            Err(())
        }
    }
}" data-download-link="" data-download-label="Download Rust">
  <code class="language-rust">// src/main.rs

use clap::{Parser, Subcommand};
use indicatif::{HumanBytes, MultiProgress, ProgressBar, ProgressStyle};
use path_absolutize::*; // For Path::absolutize()
use rayon::prelude::*;
use std::{
    ffi::OsString,
    fs,
    io::{self, ErrorKind, Read, Write},
    path::{Path, PathBuf},
    sync::{
        atomic::{AtomicU64, Ordering as AtomicOrdering},
        Arc, Mutex,
    },
    thread,
    time::Duration,
};
use thiserror::Error;
use walkdir::{DirEntry, WalkDir};

// --- Argument Parsing ---
#[derive(Parser, Debug)]
#[clap(author, version, about = &quot;RapidCopy-rs (MVP): Fast file operations CLI&quot;, long_about = None)]
struct Cli {
    #[clap(subcommand)]
    command: Commands,

    #[clap(short, long, global = true, help = &quot;Enable verbose output&quot;)]
    verbose: bool,
}

#[derive(Subcommand, Debug)]
enum Commands {
    /// Copies a file or directory recursively
    Copy {
        /// Source path
        source: PathBuf,
        /// Destination path
        destination: PathBuf,
        #[clap(short, long, help = &quot;Overwrite existing files at the destination&quot;)]
        overwrite: bool,
        #[clap(long, help = &quot;Enable parallel processing for directory contents&quot;)]
        parallel: bool,
    },
    /// Moves a file or directory recursively (cut and paste)
    Move {
        /// Source path
        source: PathBuf,
        /// Destination path
        destination: PathBuf,
        #[clap(short, long, help = &quot;Overwrite existing files at the destination if copying&quot;)]
        overwrite: bool,
        #[clap(short = &#39;f&#39;, long, help = &quot;Force deletion of source without interactive confirmation (used in move)&quot;)]
        force_delete_source: bool,
        #[clap(long, help = &quot;Enable parallel processing for directory contents during copy phase&quot;)]
        parallel: bool,
    },
    /// Deletes a file or directory recursively
    Delete {
        /// Path to delete
        path: PathBuf,
        #[clap(short, long, help = &quot;Force deletion without interactive confirmation&quot;)]
        force: bool,
    },
}

// --- Error Handling ---
#[derive(Error, Debug)]
pub enum AppError {
    #[error(&quot;I/O error accessing &#39;{path:?}&#39;: {source}&quot;)]
    Io {
        source: io::Error,
        path: PathBuf,
    },
    #[error(&quot;WalkDir error for path &#39;{path:?}&#39;: {source}&quot;)]
    WalkDir {
        source: walkdir::Error,
        path: PathBuf,
    },
    #[error(&quot;Source path does not exist: {0}&quot;)]
    SourceDoesNotExist(PathBuf),
    #[error(&quot;Destination path &#39;{0}&#39; already exists and overwrite is false&quot;)]
    DestinationExists(PathBuf),
    #[error(&quot;Cannot copy/move directory &#39;{source_dir}&#39; to existing file &#39;{dest_file}&#39;&quot;)]
    DirToExistingFile {
        source_dir: PathBuf,
        dest_file: PathBuf,
    },
    #[error(&quot;Cannot copy/move file &#39;{source_file}&#39; to existing directory &#39;{dest_dir}&#39; (ambiguous destination name or destination is a file and overwrite is false)&quot;)]
    FileToExistingDirAmbiguous {
        source_file: PathBuf,
        dest_dir: PathBuf,
    },
    #[error(&quot;User did not confirm deletion of &#39;{0}&#39;&quot;)]
    DeletionNotConfirmed(PathBuf),
    #[error(&quot;Failed to get metadata for path: {0}&quot;)]
    MetadataError(PathBuf),
    #[error(&quot;Source path &#39;{0}&#39; has no filename component&quot;)]
    NoFileName(PathBuf),
    #[error(&quot;Failed to strip prefix for path processing: &#39;{path}&#39; relative to &#39;{base}&#39;&quot;)]
    StripPrefixError { path: PathBuf, base: PathBuf },
    #[error(&quot;Operation failed with multiple errors during parallel processing:\n{0:#?}&quot;)]
    ParallelOperationFailed(Vec&lt;String&gt;),
    #[error(&quot;Cannot move/copy &#39;{0}&#39; to a subdirectory of itself &#39;{1}&#39;&quot;)]
    RecursiveMoveOrCopy(PathBuf, PathBuf),
    #[error(&quot;Indicatif style template error: {0}&quot;)]
    TemplateError(String),
}

impl From&lt;walkdir::Error&gt; for AppError {
    fn from(err: walkdir::Error) -&gt; Self {
        let path = err
            .path()
            .unwrap_or_else(|| Path::new(&quot;&lt;unknown path from walkdir error&gt;&quot;))
            .to_path_buf();
        AppError::WalkDir { source: err, path }
    }
}

fn io_err_with_path(err: io::Error, path: &amp;Path) -&gt; AppError {
    AppError::Io {
        source: err,
        path: path.to_path_buf(),
    }
}

impl From&lt;indicatif::style::TemplateError&gt; for AppError {
    fn from(err: indicatif::style::TemplateError) -&gt; Self {
        AppError::TemplateError(err.to_string())
    }
}

// --- Core Logic Module ---
mod core_logic {
    use super::*;

    const BUFFER_SIZE: usize = 128 * 1024;

    #[derive(Debug)]
    struct ProgressInfo {
        items_processed: AtomicU64,
        bytes_processed: AtomicU64,
        total_items_to_process: u64,
        total_bytes_to_process: u64,
    }

    impl ProgressInfo {
        fn new(total_items: u64, total_bytes: u64) -&gt; Self {
            Self {
                items_processed: AtomicU64::new(0),
                bytes_processed: AtomicU64::new(0),
                total_items_to_process: total_items,
                total_bytes_to_process: total_bytes,
            }
        }
    }

    fn pre_scan_directory(dir: &amp;Path, verbose: bool) -&gt; Result&lt;(u64, u64), AppError&gt; {
        if verbose {
            println!(&quot;Pre-scanning directory &#39;{}&#39;...&quot;, dir.display());
        }
        let mut total_items = 0u64;
        let mut total_bytes = 0u64;
        total_items += 1; // Count the root directory itself
        for entry_result in WalkDir::new(dir).min_depth(1).follow_links(false) {
            let entry = entry_result?; 
            total_items += 1;
            if entry.file_type().is_file() {
                total_bytes += entry.metadata()?.len(); 
            }
        }
        if verbose {
            println!(
                &quot;Pre-scan complete: {} items, {}&quot;,
                total_items,
                HumanBytes(total_bytes)
            );
        }
        Ok((total_items, total_bytes))
    }

    fn copy_single_file(
        source: &amp;Path,
        destination: &amp;Path,
        overwrite: bool,
        mp_opt: Option&lt;&amp;MultiProgress&gt;,
        overall_progress_info_opt: Option&lt;&amp;Arc&lt;ProgressInfo&gt;&gt;,
        verbose: bool,
    ) -&gt; Result&lt;u64, AppError&gt; {
        if destination.exists() {
            let dest_meta =
                fs::metadata(destination).map_err(|e| io_err_with_path(e, destination))?;
            if dest_meta.is_dir() {
                return Err(AppError::FileToExistingDirAmbiguous {
                    source_file: source.to_path_buf(),
                    dest_dir: destination.to_path_buf(),
                });
            }
            if !overwrite {
                if verbose {
                    println!(
                        &quot;Skipping &#39;{}&#39;: destination file exists and overwrite is false.&quot;,
                        destination.display()
                    );
                }
                if let Some(stats) = overall_progress_info_opt {
                    stats.items_processed.fetch_add(1, AtomicOrdering::Relaxed);
                }
                return Ok(0);
            }
        }

        if let Some(parent) = destination.parent() {
            if !parent.exists() {
                fs::create_dir_all(parent).map_err(|e| io_err_with_path(e, parent))?;
            }
        }

        let file_size = fs::metadata(source)
            .map_err(|e| io_err_with_path(e, source))?
            .len();

        let pb = if let Some(mp) = mp_opt {
            let p = mp.add(ProgressBar::new(file_size));
            p.enable_steady_tick(Duration::from_millis(250));
            p
        } else {
            let p = ProgressBar::new(file_size);
            p.enable_steady_tick(Duration::from_millis(100));
            p
        };

        pb.set_style(
            ProgressStyle::default_bar()
                .template(
                    &quot;{msg:&lt;30.bold.dim} [{bar:40.cyan/blue}] {bytes}/{total_bytes} ({bytes_per_sec}, {eta})&quot;,
                )?
                .progress_chars(&quot;=&gt; &quot;),
        );
        let filename = source.file_name().unwrap_or_default().to_string_lossy();
        pb.set_message(filename.chars().take(30).collect::&lt;String&gt;());

        let mut source_file = fs::File::open(source).map_err(|e| io_err_with_path(e, source))?;
        let mut dest_file =
            fs::File::create(destination).map_err(|e| io_err_with_path(e, destination))?;

        let mut buffer = vec![0; BUFFER_SIZE];
        let mut copied_bytes_for_this_file = 0u64;

        loop {
            let bytes_read = source_file
                .read(&amp;mut buffer)
                .map_err(|e| io_err_with_path(e, source))?;
            if bytes_read == 0 {
                break;
            }
            dest_file
                .write_all(&amp;buffer[..bytes_read])
                .map_err(|e| io_err_with_path(e, destination))?;
            copied_bytes_for_this_file += bytes_read as u64;
            pb.set_position(copied_bytes_for_this_file);
            if let Some(stats) = overall_progress_info_opt {
                stats
                    .bytes_processed
                    .fetch_add(bytes_read as u64, AtomicOrdering::Relaxed);
            }
        }
        pb.finish_with_message(format!(&quot;Copied {} ({})&quot;, filename, HumanBytes(file_size)));
        if let Some(stats) = overall_progress_info_opt {
            stats.items_processed.fetch_add(1, AtomicOrdering::Relaxed);
        }
        Ok(file_size)
    }

    fn copy_dir_recursive(
        source_dir: &amp;Path,
        target_dest_dir: &amp;Path, 
        overwrite: bool,
        parallel: bool,
        _overall_item_pb: &amp;ProgressBar, 
        progress_info: &amp;Arc&lt;ProgressInfo&gt;,
        verbose: bool, // verbose is available here
    ) -&gt; Result&lt;(), Vec&lt;String&gt;&gt; {
        if verbose {
            println!(
                &quot;Recursively copying contents of &#39;{}&#39; into &#39;{}&#39;&quot;,
                source_dir.display(),
                target_dest_dir.display()
            );
        }

        if !target_dest_dir.exists() {
            fs::create_dir_all(target_dest_dir)
                .map_err(|e| vec![io_err_with_path(e, target_dest_dir).to_string()])?;
            if verbose {
                println!(&quot;Created directory &#39;{}&#39;&quot;, target_dest_dir.display());
            }
        }
      
        let entries: Vec&lt;DirEntry&gt; = WalkDir::new(source_dir)
            .min_depth(1) 
            .follow_links(false)
            .into_iter()
            .collect::&lt;Result&lt;Vec&lt;_&gt;, _&gt;&gt;()
            .map_err(|e| {
                vec![AppError::WalkDir {
                    source: e,
                    path: source_dir.to_path_buf(),
                }
                .to_string()]
            })?;

        let multi_progress_manager_opt = if parallel &amp;&amp; entries.iter().filter(|e| e.path().is_file()).count() &gt; 1 {
            Some(MultiProgress::new())
        } else {
            None
        };

        let errors_arc = Arc::new(Mutex::new(Vec::&lt;String&gt;::new()));

        let process_entry_closure = |entry: DirEntry| {
            let entry_path = entry.path();
            let relative_path = match entry_path.strip_prefix(source_dir) {
                Ok(p) =&gt; p,
                Err(_) =&gt; {
                    errors_arc.lock().unwrap().push(
                        AppError::StripPrefixError {
                            path: entry_path.to_path_buf(),
                            base: source_dir.to_path_buf(),
                        }
                        .to_string(),
                    );
                    return;
                }
            };
            let dest_path = target_dest_dir.join(relative_path);

            if entry.file_type().is_dir() {
                if verbose {
                    println!(&quot;Creating directory &#39;{}&#39;&quot;, dest_path.display());
                }
                if let Err(e) = fs::create_dir_all(&amp;dest_path) {
                    errors_arc
                        .lock()
                        .unwrap()
                        .push(io_err_with_path(e, &amp;dest_path).to_string());
                    return;
                }
                progress_info
                    .items_processed
                    .fetch_add(1, AtomicOrdering::Relaxed);
            } else if entry.file_type().is_file() {
                match copy_single_file(
                    entry_path,
                    &amp;dest_path,
                    overwrite,
                    multi_progress_manager_opt.as_ref(),
                    Some(progress_info), 
                    verbose,
                ) {
                    Ok(_) =&gt; {}
                    Err(e) =&gt; {
                        errors_arc.lock().unwrap().push(format!(
                            &quot;Error copying file &#39;{}&#39;: {}&quot;,
                            entry_path.display(),
                            e
                        ));
                    }
                }
            } else {
                progress_info
                    .items_processed
                    .fetch_add(1, AtomicOrdering::Relaxed);
                if verbose {
                    println!(&quot;Skipping non-file/non-directory entry: {}&quot;, entry_path.display());
                }
            }
        };

        if parallel &amp;&amp; entries.len() &gt; 1 {
            entries.into_par_iter().for_each(process_entry_closure);
        } else {
            for entry in entries {
                process_entry_closure(entry);
            }
        }

        if let Some(mp) = multi_progress_manager_opt {
            if let Err(e) = mp.clear() {
                if verbose {
                    // Error during UI cleanup is not critical to the copy operation itself.
                    eprintln!(&quot;Warning: Failed to clear multi-progress display: {}&quot;, e);
                }
            }
        }

        let final_errors = Arc::try_unwrap(errors_arc)
            .expect(&quot;Mutex still has multiple owners for errors_arc&quot;)
            .into_inner()
            .expect(&quot;Mutex was poisoned for errors_arc&quot;);

        if !final_errors.is_empty() {
            return Err(final_errors);
        }
        Ok(())
    }

    pub fn handle_copy_operation_main(
        source: &amp;Path,
        destination: &amp;Path,
        overwrite: bool,
        parallel: bool,
        verbose: bool,
    ) -&gt; Result&lt;u64, AppError&gt; {
        if verbose {
            println!(
                &quot;Copy operation: &#39;{}&#39; -&gt; &#39;{}&#39;&quot;,
                source.display(),
                destination.display()
            );
        }
        if !source.exists() {
            return Err(AppError::SourceDoesNotExist(source.to_path_buf()));
        }

        let source_meta = fs::metadata(source).map_err(|e| io_err_with_path(e, source))?;
        let source_abs = source.absolutize().map_err(|e| io_err_with_path(e, source))?.into_owned();


        let target_path_for_item: PathBuf; 
        let effective_dest_dir_for_contents: PathBuf; 

        if source_meta.is_dir() {
            let source_name = source
                .file_name()
                .ok_or_else(|| AppError::NoFileName(source.to_path_buf()))?;
            if destination.exists()
                &amp;&amp; fs::metadata(destination)
                    .map_err(|e| io_err_with_path(e, destination))?
                    .is_dir()
            {
                target_path_for_item = destination.join(source_name);
            } else if destination.exists()
                &amp;&amp; fs::metadata(destination)
                    .map_err(|e| io_err_with_path(e, destination))?
                    .is_file()
            {
                return Err(AppError::DirToExistingFile {
                    source_dir: source.to_path_buf(),
                    dest_file: destination.to_path_buf(),
                });
            } else {
                target_path_for_item = destination.to_path_buf();
            }
            effective_dest_dir_for_contents = target_path_for_item.clone(); 
            
            let target_abs_check = target_path_for_item.absolutize().map_err(|e| io_err_with_path(e, &amp;target_path_for_item))?.into_owned();
            if target_abs_check.starts_with(&amp;source_abs) {
                 return Err(AppError::RecursiveMoveOrCopy(source.to_path_buf(), target_path_for_item));
            }

        } else { // Source is a file
            if destination.exists()
                &amp;&amp; fs::metadata(destination)
                    .map_err(|e| io_err_with_path(e, destination))?
                    .is_dir()
            {
                target_path_for_item = destination.join(
                    source
                        .file_name()
                        .ok_or_else(|| AppError::NoFileName(source.to_path_buf()))?,
                );
            } else if !destination.exists()
                &amp;&amp; destination.to_string_lossy().ends_with(std::path::MAIN_SEPARATOR)
            {
                fs::create_dir_all(destination).map_err(|e| io_err_with_path(e, destination))?;
                target_path_for_item = destination.join(
                    source
                        .file_name()
                        .ok_or_else(|| AppError::NoFileName(source.to_path_buf()))?,
                );
            } else {
                target_path_for_item = destination.to_path_buf();
            }
            effective_dest_dir_for_contents = target_path_for_item
                .parent()
                .unwrap_or_else(|| Path::new(&quot;.&quot;))
                .to_path_buf();
        }

        let (total_items_scanned, total_bytes_scanned) = if source_meta.is_dir() {
            pre_scan_directory(source, verbose)?
        } else {
            (1, source_meta.len()) 
        };
        let progress_info = Arc::new(ProgressInfo::new(
            total_items_scanned,
            total_bytes_scanned,
        ));

        let overall_pb = ProgressBar::new(total_items_scanned.max(1)); 
        overall_pb.set_style(
            ProgressStyle::default_bar()
                .template(&quot;{msg} [{elapsed_precise}] {wide_bar:.cyan/blue} Overall Items: {pos}/{len} ({items_per_sec}, ETA {eta})&quot;)?
                .progress_chars(&quot;=&gt; &quot;),
        );
        let source_filename_owned: OsString =
            source.file_name().unwrap_or_default().to_os_string();

        overall_pb.set_message(format!(
            &quot;Preparing to copy {}&quot;,
            source_filename_owned.to_string_lossy()
        ));
        overall_pb.enable_steady_tick(Duration::from_millis(200));

        let overall_pb_clone_for_updater = overall_pb.clone();
        let progress_info_clone_for_updater = progress_info.clone();
        let source_filename_clone_for_updater = source_filename_owned.clone();

        let bytes_updater_thread = thread::spawn(move || {
            while !overall_pb_clone_for_updater.is_finished() {
                let items_done = progress_info_clone_for_updater
                    .items_processed
                    .load(AtomicOrdering::Relaxed);
                let bytes_done = progress_info_clone_for_updater
                    .bytes_processed
                    .load(AtomicOrdering::Relaxed);

                overall_pb_clone_for_updater.set_position(items_done); 
                overall_pb_clone_for_updater.set_message(format!(
                    &quot;Copying {} (Bytes: {} / {}) | Items: {}/{}&quot;,
                    source_filename_clone_for_updater.to_string_lossy(),
                    HumanBytes(bytes_done),
                    HumanBytes(progress_info_clone_for_updater.total_bytes_to_process),
                    items_done,
                    progress_info_clone_for_updater.total_items_to_process
                ));
                thread::sleep(Duration::from_millis(200));
            }
            let items_done = progress_info_clone_for_updater
                .items_processed
                .load(AtomicOrdering::Relaxed);
            let bytes_done = progress_info_clone_for_updater
                .bytes_processed
                .load(AtomicOrdering::Relaxed);
            overall_pb_clone_for_updater.set_position(items_done);
            overall_pb_clone_for_updater.set_message(format!(
                &quot;Finished {} (Bytes: {} / {}) | Items: {}/{}&quot;,
                source_filename_clone_for_updater.to_string_lossy(),
                HumanBytes(bytes_done),
                HumanBytes(progress_info_clone_for_updater.total_bytes_to_process),
                items_done,
                progress_info_clone_for_updater.total_items_to_process
            ));
        });

        let result = if source_meta.is_dir() {
            if !target_path_for_item.exists() {
                 fs::create_dir_all(&amp;target_path_for_item).map_err(|e| io_err_with_path(e, &amp;target_path_for_item))?;
                 if verbose { println!(&quot;Created target base directory &#39;{}&#39;&quot;, target_path_for_item.display()); }
            }
            progress_info.items_processed.fetch_add(1, AtomicOrdering::Relaxed);

            copy_dir_recursive(
                source, 
                &amp;effective_dest_dir_for_contents, 
                overwrite,
                parallel,
                &amp;overall_pb,
                &amp;progress_info,
                verbose,
            )
            .map_err(AppError::ParallelOperationFailed)?;
            Ok(progress_info.items_processed.load(AtomicOrdering::Relaxed))
        } else if source_meta.is_file() {
            copy_single_file(
                source,
                &amp;target_path_for_item,
                overwrite,
                None, 
                Some(&amp;progress_info),
                verbose,
            )?;
            Ok(1) 
        } else {
            if verbose {
                println!(
                    &quot;Warning: Source &#39;{}&#39; is not a regular file or directory. Skipping.&quot;,
                    source.display()
                );
            }
            Ok(0)
        };

        overall_pb.set_position(progress_info.items_processed.load(AtomicOrdering::Relaxed)); 
        overall_pb.finish_with_message(format!(&quot;Copy of &#39;{}&#39; complete.&quot;, source.display()));
        bytes_updater_thread
            .join()
            .expect(&quot;Bytes updater thread panicked&quot;);
        result
    }

    pub fn handle_delete_operation(
        path: &amp;Path,
        force: bool,
        verbose: bool,
    ) -&gt; Result&lt;u64, AppError&gt; {
        if verbose {
            println!(
                &quot;Delete operation: &#39;{}&#39; (Force: {})&quot;,
                path.display(),
                force
            );
        }

        if !path.exists() {
            if force {
                if verbose {
                    println!(
                        &quot;Path &#39;{}&#39; does not exist. Nothing to delete (forced).&quot;,
                        path.display()
                    );
                }
                return Ok(0);
            }
            return Err(AppError::SourceDoesNotExist(path.to_path_buf()));
        }

        if !force {
            print!(
                &quot;Are you sure you want to delete &#39;{}&#39; and all its contents? (yes/no): &quot;,
                path.display()
            );
            io::stdout().flush().map_err(|e| io_err_with_path(e, path))?;
            let mut confirmation = String::new();
            io::stdin()
                .read_line(&amp;mut confirmation)
                .map_err(|e| io_err_with_path(e, path))?;
            if confirmation.trim().to_lowercase() != &quot;yes&quot; {
                return Err(AppError::DeletionNotConfirmed(path.to_path_buf()));
            }
        }

        let items_deleted_count: u64;

        if path.is_dir() {
            let (total_items, _) = pre_scan_directory(path, verbose)?; 

            let pb = ProgressBar::new(total_items.max(1));
            pb.set_style(
                ProgressStyle::default_bar()
                    .template(
                        &quot;{msg:.red.bold} [{bar:40.red/yellow}] Items: {pos}/{len} ({elapsed_precise})&quot;,
                    )?
                    .progress_chars(&quot;=&gt; &quot;),
            );
            pb.set_message(format!(
                &quot;Deleting dir {}&quot;,
                path.file_name().unwrap_or_default().to_string_lossy()
            ));
            pb.enable_steady_tick(Duration::from_millis(200));
            
            fs::remove_dir_all(path).map_err(|e| io_err_with_path(e, path))?;
            items_deleted_count = total_items; 
            pb.set_position(items_deleted_count);
            pb.finish_with_message(format!(&quot;Deleted directory &#39;{}&#39;&quot;, path.display()));
        } else {
            if verbose {
                println!(&quot;Deleting file &#39;{}&#39;...&quot;, path.display());
            }
            fs::remove_file(path).map_err(|e| io_err_with_path(e, path))?;
            items_deleted_count = 1;
        }

        if verbose {
            println!(
                &quot;Deletion of &#39;{}&#39; complete. {} items affected.&quot;,
                path.display(),
                items_deleted_count
            );
        }
        Ok(items_deleted_count)
    }

    pub fn handle_move_operation(
        source: PathBuf,
        destination: PathBuf,
        overwrite: bool,
        force_delete_source: bool, 
        parallel: bool,
        verbose: bool,
    ) -&gt; Result&lt;u64, AppError&gt; {
        if verbose {
            println!(
                &quot;Move operation: &#39;{}&#39; -&gt; &#39;{}&#39;&quot;,
                source.display(),
                destination.display()
            );
        }
        if !source.exists() {
            return Err(AppError::SourceDoesNotExist(source.clone()));
        }

        let source_meta = fs::metadata(&amp;source).map_err(|io_e| io_err_with_path(io_e, &amp;source))?;
        let source_abs = source.absolutize().map_err(|e| io_err_with_path(e, &amp;source))?.into_owned();

        let final_dest_target = if destination.exists()
            &amp;&amp; fs::metadata(&amp;destination)
                .map_err(|e| io_err_with_path(e, &amp;destination))?
                .is_dir()
        {
            destination.join(
                source
                    .file_name()
                    .ok_or_else(|| AppError::NoFileName(source.clone()))?,
            )
        } else {
            destination.clone()
        };

        if source_meta.is_dir() {
            let target_abs_check = final_dest_target.absolutize().map_err(|e| io_err_with_path(e, &amp;final_dest_target))?.into_owned();
            if target_abs_check.starts_with(&amp;source_abs) {
                 return Err(AppError::RecursiveMoveOrCopy(source, final_dest_target));
            }
        }

        if final_dest_target.exists() &amp;&amp; !overwrite {
            if source_meta.is_file() {
                 let dest_meta = fs::metadata(&amp;final_dest_target).map_err(|e| io_err_with_path(e, &amp;final_dest_target))?;
                 if dest_meta.is_file() {
                    return Err(AppError::DestinationExists(final_dest_target));
                 }
            } else if source_meta.is_dir() {
                let dest_meta = fs::metadata(&amp;final_dest_target).map_err(|e| io_err_with_path(e, &amp;final_dest_target))?;
                if dest_meta.is_file() {
                    return Err(AppError::DirToExistingFile { source_dir: source.clone(), dest_file: final_dest_target });
                }
                 return Err(AppError::DestinationExists(final_dest_target));
            }
        }

        match fs::rename(&amp;source, &amp;final_dest_target) {
            Ok(_) =&gt; {
                if verbose {
                    println!(
                        &quot;Successfully moved (renamed) &#39;{}&#39; to &#39;{}&#39;&quot;,
                        source.display(),
                        final_dest_target.display()
                    );
                }
                Ok(1) 
            }
            Err(e) =&gt; {
                let should_fallback_to_copy_delete = e.kind() == ErrorKind::CrossesDevices
                    || e.raw_os_error() == Some(libc::EXDEV)
                    || (source_meta.is_file()
                        &amp;&amp; destination.exists() 
                        &amp;&amp; fs::metadata(&amp;destination)
                            .map_err(|io_e| io_err_with_path(io_e, &amp;destination))?
                            .is_dir());

                if should_fallback_to_copy_delete {
                    if verbose {
                        println!(
                            &quot;Rename failed (reason: {}, OS error: {:?}), attempting copy then delete...&quot;,
                            e.kind(),
                            e.raw_os_error()
                        );
                    }
                    
                    let items_copied = handle_copy_operation_main(
                        &amp;source,
                        &amp;destination, 
                        overwrite,
                        parallel,
                        verbose,
                    )?;
                    if verbose {
                        println!(
                            &quot;Copy phase of move complete. Now deleting source &#39;{}&#39;.&quot;,
                            source.display()
                        );
                    }

                    match handle_delete_operation(&amp;source, force_delete_source, verbose) { 
                        Ok(_) =&gt; {
                            if verbose {
                                println!(
                                    &quot;Source &#39;{}&#39; deleted successfully after copy.&quot;,
                                    source.display()
                                );
                            }
                            Ok(items_copied)
                        }
                        Err(del_err) =&gt; {
                            eprintln!(&quot;CRITICAL: Error deleting source &#39;{}&#39; after copy: {}. Copied files remain at &#39;{}&#39;. Manual cleanup of source may be required.&quot;, source.display(), del_err, destination.display());
                            Err(AppError::Io {
                                source: io::Error::new(
                                    ErrorKind::Other,
                                    &quot;Failed to delete source after successful copy during move operation&quot;,
                                ),
                                path: source,
                            })
                        }
                    }
                } else {
                    Err(io_err_with_path(e, &amp;source))
                }
            }
        }
    }
}

// --- Main Application Logic ---
fn main() -&gt; Result&lt;(), ()&gt; {
    let cli = Cli::parse();

    let result: Result&lt;u64, AppError&gt; = match cli.command {
        Commands::Copy {
            source,
            destination,
            overwrite,
            parallel,
        } =&gt; core_logic::handle_copy_operation_main(
            &amp;source,
            &amp;destination,
            overwrite,
            parallel,
            cli.verbose,
        ),
        Commands::Move {
            source,
            destination,
            overwrite,
            force_delete_source,
            parallel,
        } =&gt; core_logic::handle_move_operation(
            source,
            destination,
            overwrite,
            force_delete_source,
            parallel,
            cli.verbose,
        ),
        Commands::Delete { path, force } =&gt; {
            core_logic::handle_delete_operation(&amp;path, force, cli.verbose)
        }
    };

    match result {
        Ok(items_affected) =&gt; {
            if items_affected &gt; 0 || cli.verbose {
                println!(
                    &quot;Operation completed successfully. {} items affected.&quot;,
                    items_affected
                );
            } else if !cli.verbose {
                println!(
                    &quot;Operation completed (no items affected or action skipped). Use --verbose for details.&quot;
                );
            }
            Ok(())
        }
        Err(e) =&gt; {
            eprintln!(&quot;Error: {}&quot;, e);
            if let AppError::ParallelOperationFailed(errors) = e {
                for (i, err_msg) in errors.iter().enumerate() {
                    eprintln!(&quot;  Parallel error {}: {}&quot;, i + 1, err_msg);
                }
            }
            Err(())
        }
    }
}</code>
</section>

<h2 id="to-build-and-run">To Build and Run</h2>

<ol>
  <li>Save <code class="language-plaintext highlighter-rouge">src/main.rs</code> and <code class="language-plaintext highlighter-rouge">Cargo.toml</code>.</li>
  <li>Ensure Rust and build essentials are installed on your <code class="language-plaintext highlighter-rouge">arm64 Debian Bullseye</code> machine.</li>
  <li>Build: <code class="language-plaintext highlighter-rouge">cargo build --release</code></li>
</ol>]]></content><author><name></name></author><category term="software&gt;rust" /></entry><entry><title type="html">Copying data in Linux</title><link href="https://ib.bsb.br/copying-data-in-linux/" rel="alternate" type="text/html" title="Copying data in Linux" /><published>2025-05-15T00:00:00+00:00</published><updated>2025-05-15T21:45:24+00:00</updated><id>https://ib.bsb.br/copying-data-in-linux</id><content type="html" xml:base="https://ib.bsb.br/copying-data-in-linux/"><![CDATA[<p>Copying a large volume of data like 500GiB, especially when it consists of thousands of individual files, from an SD card to an external hard disk in Linux requires strategies that minimize overhead and maximize throughput. The key is to leverage parallel processing to utilize multiple CPU cores and choose tools that handle file operations efficiently.</p>

<p><strong>Understanding the Bottlenecks</strong></p>

<p>Before diving into tools, it’s helpful to understand potential bottlenecks:</p>
<ol>
  <li><strong>Per-File Overhead (CPU Bound):</strong> When dealing with thousands of small files, the operating system incurs overhead for each file operation (opening, reading metadata, writing metadata, closing). This can make the CPU a bottleneck even if the drives aren’t saturated. Parallel processing helps here.</li>
  <li><strong>I/O Throughput (Drive Bound):</strong> The read speed of your SD card and the write speed of your external HDD (especially if it’s a mechanical drive vs. an SSD) will ultimately limit transfer rates for large files or when per-file overhead is minimized.</li>
  <li><strong>Single-Threaded Operations:</strong> Standard <code class="language-plaintext highlighter-rouge">cp</code> or <code class="language-plaintext highlighter-rouge">mv</code> commands are typically single-threaded, processing one file at a time, making them inefficient for this scale.</li>
</ol>

<p>Here are several effective Linux tools and techniques to accomplish this task, focusing on speed and resource utilization:</p>

<p><strong>1. <code class="language-plaintext highlighter-rouge">rsync</code> with GNU <code class="language-plaintext highlighter-rouge">parallel</code> (Recommended for Robustness &amp; Parallelism)</strong></p>

<p><code class="language-plaintext highlighter-rouge">rsync</code> is a powerful and versatile tool for copying and synchronizing files. While <code class="language-plaintext highlighter-rouge">rsync</code> itself processes files sequentially within a single instance, you can use it with GNU <code class="language-plaintext highlighter-rouge">parallel</code> to run multiple <code class="language-plaintext highlighter-rouge">rsync</code> jobs concurrently, significantly speeding up the transfer of many files.</p>

<ul>
  <li><strong>How it Works:</strong> <code class="language-plaintext highlighter-rouge">find</code> lists all files and directories. GNU <code class="language-plaintext highlighter-rouge">parallel</code> takes this list and launches multiple <code class="language-plaintext highlighter-rouge">rsync</code> processes, each handling a subset of the files/directories simultaneously. This leverages multiple CPU cores to manage the per-file operations and can better saturate your drive’s I/O capabilities.</li>
  <li><strong>Key Advantages:</strong> Robust error handling, ability to resume interrupted transfers (with <code class="language-plaintext highlighter-rouge">rsync</code>), preserves permissions and metadata, detailed progress.</li>
  <li><strong>Installation:</strong> If <code class="language-plaintext highlighter-rouge">parallel</code> isn’t installed: <code class="language-plaintext highlighter-rouge">sudo apt update &amp;&amp; sudo apt install parallel</code> (Debian/Ubuntu) or <code class="language-plaintext highlighter-rouge">sudo dnf install parallel</code> (Fedora/RHEL).</li>
</ul>

<p><strong>Example Command (Copying contents of source into destination):</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure destination directory exists: mkdir -p /media/user/externalhdd/backup_destination</span>
find /media/user/sdcard/source_folder/ <span class="nt">-mindepth</span> 1 <span class="nt">-print0</span> | <span class="se">\</span>
  parallel <span class="nt">-0</span> <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="nt">--eta</span> <span class="nt">--joblog</span> /tmp/rsync_parallel.log <span class="se">\</span>
  rsync <span class="nt">-aP</span> <span class="o">{}</span> /media/user/externalhdd/backup_destination/
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">/media/user/sdcard/source_folder/</code>: Your source directory on the SD card. The trailing slash means “contents of.”</li>
  <li><code class="language-plaintext highlighter-rouge">-mindepth 1</code>: Excludes the top-level source directory itself from the list, processing its contents.</li>
  <li><code class="language-plaintext highlighter-rouge">-print0</code>: Handles filenames with spaces or special characters safely.</li>
  <li><code class="language-plaintext highlighter-rouge">parallel -0 -j$(nproc) --eta --joblog /tmp/rsync_parallel.log</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">-0</code>: Expects null-terminated input from <code class="language-plaintext highlighter-rouge">find</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">-j$(nproc)</code>: Runs a number of jobs equal to your CPU cores. You can set a specific number, e.g., <code class="language-plaintext highlighter-rouge">-j4</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">--eta</code>: Shows estimated time of arrival.</li>
      <li><code class="language-plaintext highlighter-rouge">--joblog /tmp/rsync_parallel.log</code>: Logs the progress and success/failure of each parallel job.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">rsync -aP {} /media/user/externalhdd/backup_destination/</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">-a</code>: Archive mode (preserves permissions, timestamps, symbolic links, etc.).</li>
      <li><code class="language-plaintext highlighter-rouge">-P</code>: Combines <code class="language-plaintext highlighter-rouge">--progress</code> and <code class="language-plaintext highlighter-rouge">--partial</code> (for resumability).</li>
      <li><code class="language-plaintext highlighter-rouge">{}</code>: Placeholder for the file/directory passed by <code class="language-plaintext highlighter-rouge">parallel</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">/media/user/externalhdd/backup_destination/</code>: The destination. The trailing slash is important for <code class="language-plaintext highlighter-rouge">rsync</code> to copy items <em>into</em> this directory.</li>
    </ul>
  </li>
</ul>

<p><strong>Dry Run (Highly Recommended):</strong> Before running the actual copy, perform a dry run:
Add <code class="language-plaintext highlighter-rouge">rsync -anP</code> (note the <code class="language-plaintext highlighter-rouge">n</code> for dry-run) in the command above, or add <code class="language-plaintext highlighter-rouge">--dry-run</code> to the <code class="language-plaintext highlighter-rouge">parallel</code> command.</p>

<p><strong>2. <code class="language-plaintext highlighter-rouge">tar</code> Pipelined (Efficient for Many Small Files)</strong></p>

<p>This classic method archives the source files into a single stream (<code class="language-plaintext highlighter-rouge">stdout</code>) and pipes this stream directly to another <code class="language-plaintext highlighter-rouge">tar</code> process that extracts it at the destination (<code class="language-plaintext highlighter-rouge">stdin</code>). This significantly reduces the overhead of individual file system operations, especially beneficial for mechanical drives and vast numbers of tiny files.</p>

<ul>
  <li><strong>How it Works:</strong> <code class="language-plaintext highlighter-rouge">tar</code> reads all source files sequentially and writes them as a continuous data stream. The receiving <code class="language-plaintext highlighter-rouge">tar</code> process reads this stream and recreates the files and directory structure.</li>
  <li><strong>Key Advantages:</strong> Can be very fast for scenarios with extreme numbers of small files by minimizing disk head seeking.</li>
  <li><strong>Considerations:</strong> Less easily resumable if interrupted compared to <code class="language-plaintext highlighter-rouge">rsync</code>. Progress indication is often through tools like <code class="language-plaintext highlighter-rouge">pv</code> (Pipe Viewer).</li>
</ul>

<p><strong>Example Command:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure destination directory exists: mkdir -p /media/user/externalhdd/backup_destination</span>
<span class="o">(</span><span class="nb">cd</span> /media/user/sdcard/source_folder/ <span class="o">&amp;&amp;</span> <span class="nb">tar</span> <span class="nt">-cf</span> - .<span class="o">)</span> | pv | <span class="o">(</span><span class="nb">cd</span> /media/user/externalhdd/backup_destination/ <span class="o">&amp;&amp;</span> <span class="nb">tar</span> <span class="nt">-xf</span> -<span class="o">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">(cd /media/user/sdcard/source_folder/ &amp;&amp; tar -cf - .)</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">cd ...</code>: Changes to the source directory. The subshell <code class="language-plaintext highlighter-rouge">(...)</code> ensures this <code class="language-plaintext highlighter-rouge">cd</code> doesn’t affect your main shell’s working directory.</li>
      <li><code class="language-plaintext highlighter-rouge">tar -cf - .</code>: Creates (<code class="language-plaintext highlighter-rouge">c</code>) an archive of the current directory (<code class="language-plaintext highlighter-rouge">.</code>) and writes it to standard output (<code class="language-plaintext highlighter-rouge">f -</code>).</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">pv</code>: (Optional, install with <code class="language-plaintext highlighter-rouge">sudo apt install pv</code>) Pipe Viewer shows progress of data through the pipe.</li>
  <li><code class="language-plaintext highlighter-rouge">(cd /media/user/externalhdd/backup_destination/ &amp;&amp; tar -xf -)</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">cd ...</code>: Changes to the destination directory in a subshell.</li>
      <li><code class="language-plaintext highlighter-rouge">tar -xf -</code>: Extracts (<code class="language-plaintext highlighter-rouge">x</code>) the archive from standard input (<code class="language-plaintext highlighter-rouge">f -</code>).</li>
    </ul>
  </li>
</ul>

<p><strong>3. <code class="language-plaintext highlighter-rouge">find</code> with <code class="language-plaintext highlighter-rouge">xargs</code> and <code class="language-plaintext highlighter-rouge">cp --parents</code> (Parallel Basic Copy)</strong></p>

<p>This method uses <code class="language-plaintext highlighter-rouge">find</code> to locate files, and <code class="language-plaintext highlighter-rouge">xargs</code> to execute <code class="language-plaintext highlighter-rouge">cp</code> commands in parallel. The crucial <code class="language-plaintext highlighter-rouge">--parents</code> option for <code class="language-plaintext highlighter-rouge">cp</code> ensures the source directory structure is replicated at the destination.</p>

<ul>
  <li><strong>How it Works:</strong> <code class="language-plaintext highlighter-rouge">find</code> generates a list of files. <code class="language-plaintext highlighter-rouge">xargs</code> takes this list and runs multiple <code class="language-plaintext highlighter-rouge">cp</code> commands simultaneously. <code class="language-plaintext highlighter-rouge">cp --parents</code> recreates the necessary parent directories at the destination.</li>
  <li><strong>Key Advantages:</strong> Uses standard <code class="language-plaintext highlighter-rouge">cp</code>, can be effective if <code class="language-plaintext highlighter-rouge">rsync</code>’s overhead is a concern for a simple copy.</li>
  <li><strong>Considerations:</strong> <code class="language-plaintext highlighter-rouge">cp</code> doesn’t have <code class="language-plaintext highlighter-rouge">rsync</code>’s advanced resumability or delta-transfer capabilities (though not relevant for an initial full copy).</li>
</ul>

<p><strong>Example Command:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure base destination directory exists: mkdir -p /media/user/externalhdd/backup_destination</span>
<span class="nb">cd</span> /media/user/sdcard/source_folder/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
  find <span class="nb">.</span> <span class="nt">-type</span> f <span class="nt">-print0</span> | <span class="se">\</span>
  xargs <span class="nt">-0</span> <span class="nt">-P</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="nt">-I</span> <span class="o">{}</span> <span class="nb">cp</span> <span class="nt">--parents</span> <span class="nt">-a</span> <span class="o">{}</span> /media/user/externalhdd/backup_destination/
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">cd /media/user/sdcard/source_folder/</code>: Change to the source directory to make relative paths work with <code class="language-plaintext highlighter-rouge">cp --parents</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">find . -type f -print0</code>: Finds only files (<code class="language-plaintext highlighter-rouge">-type f</code>) in the current directory (<code class="language-plaintext highlighter-rouge">.</code>) and its subdirectories.</li>
  <li><code class="language-plaintext highlighter-rouge">xargs -0 -P$(nproc) -I {}</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">-0</code>: Null-terminated input.</li>
      <li><code class="language-plaintext highlighter-rouge">-P$(nproc)</code>: Parallel processes up to the number of CPU cores.</li>
      <li><code class="language-plaintext highlighter-rouge">-I {}</code>: Replaces <code class="language-plaintext highlighter-rouge">{}</code> with each input item. This makes <code class="language-plaintext highlighter-rouge">cp --parents</code> work correctly with paths containing spaces.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">cp --parents -a {} /media/user/externalhdd/backup_destination/</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">--parents</code>: Recreates the source directory structure under the destination.</li>
      <li><code class="language-plaintext highlighter-rouge">-a</code>: Archive mode (like <code class="language-plaintext highlighter-rouge">rsync -a</code>, equivalent to <code class="language-plaintext highlighter-rouge">-dR --preserve=all</code>).</li>
      <li><code class="language-plaintext highlighter-rouge">{}</code>: The file to copy.</li>
      <li><code class="language-plaintext highlighter-rouge">/media/user/externalhdd/backup_destination/</code>: The target directory where the structure from <code class="language-plaintext highlighter-rouge">source_folder</code> will be created.</li>
    </ul>
  </li>
</ul>

<p><strong>4. <code class="language-plaintext highlighter-rouge">fpsync</code> (Specialized Parallel <code class="language-plaintext highlighter-rouge">rsync</code> Wrapper)</strong></p>

<p><code class="language-plaintext highlighter-rouge">fpsync</code> is a tool designed to parallelize <code class="language-plaintext highlighter-rouge">rsync</code>. It uses <code class="language-plaintext highlighter-rouge">fpart</code> to partition the file list and then runs multiple <code class="language-plaintext highlighter-rouge">rsync</code> workers.</p>

<ul>
  <li><strong>How it Works:</strong> Automates the process of splitting the workload and managing parallel <code class="language-plaintext highlighter-rouge">rsync</code> instances.</li>
  <li><strong>Key Advantages:</strong> Tailored for this exact scenario; can be very efficient.</li>
  <li><strong>Installation:</strong> May need to be installed via your package manager (e.g., <code class="language-plaintext highlighter-rouge">sudo apt install fpart</code>, as <code class="language-plaintext highlighter-rouge">fpsync</code> is often bundled with it).</li>
</ul>

<p><strong>Example Command:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure destination directory exists: mkdir -p /media/user/externalhdd/backup_destination</span>
fpsync <span class="nt">-n</span> <span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="nt">-v</span> <span class="se">\</span>
  /media/user/sdcard/source_folder/ /media/user/externalhdd/backup_destination/
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">-n $(nproc)</code>: Number of parallel <code class="language-plaintext highlighter-rouge">rsync</code> workers (e.g., number of CPU cores).</li>
  <li><code class="language-plaintext highlighter-rouge">-v</code>: Verbose mode.</li>
  <li><code class="language-plaintext highlighter-rouge">/media/user/sdcard/source_folder/</code>: Source directory.</li>
  <li><code class="language-plaintext highlighter-rouge">/media/user/externalhdd/backup_destination/</code>: Destination directory.</li>
  <li><strong>Note on batching:</strong> <code class="language-plaintext highlighter-rouge">fpsync</code> uses <code class="language-plaintext highlighter-rouge">fpart</code> underneath. If you need to control batching by number of files per job (rather than just total workers), you might pass <code class="language-plaintext highlighter-rouge">fpart</code> options using <code class="language-plaintext highlighter-rouge">fpsync -o "-f &lt;num_files&gt;"</code>. Check <code class="language-plaintext highlighter-rouge">man fpart</code> for details.</li>
</ul>

<p><strong>5. <code class="language-plaintext highlighter-rouge">mc</code> (Midnight Commander - TUI Alternative)</strong></p>

<p>For users who prefer a Text-based User Interface, Midnight Commander is a powerful console file manager. Its built-in copy operations (<code class="language-plaintext highlighter-rouge">F5</code>) are generally well-optimized and can handle large numbers of files more gracefully than a simple desktop file manager.</p>

<ul>
  <li><strong>How it Works:</strong> Provides an interactive way to select source files/directories and copy them. While it might not offer the same granular parallel control as CLI combinations, it’s often faster than basic <code class="language-plaintext highlighter-rouge">cp</code> for large jobs.</li>
  <li><strong>Installation:</strong> <code class="language-plaintext highlighter-rouge">sudo apt install mc</code> or <code class="language-plaintext highlighter-rouge">sudo dnf install mc</code>.</li>
  <li><strong>Usage:</strong> Run <code class="language-plaintext highlighter-rouge">mc</code>, navigate panels to source and destination, select files (e.g., <code class="language-plaintext highlighter-rouge">Insert</code> key or <code class="language-plaintext highlighter-rouge">*</code>), press <code class="language-plaintext highlighter-rouge">F5</code> to copy.</li>
</ul>

<p><strong>Additional Considerations for Maximizing Speed:</strong></p>

<ul>
  <li><strong>Hardware:</strong> Ensure both SD card reader and external HDD are connected to the fastest available USB ports (USB 3.0+). An SSD external drive will be significantly faster than a mechanical HDD.</li>
  <li><strong>Filesystem Mount Options:</strong> Mounting filesystems with <code class="language-plaintext highlighter-rouge">noatime</code> or <code class="language-plaintext highlighter-rouge">relatime</code> can reduce some disk I/O by not updating file access times on every read.
<code class="language-plaintext highlighter-rouge">sudo mount -o remount,noatime /media/user/sdcard</code> (if applicable and safe for your use case).</li>
  <li><strong>I/O Scheduler:</strong> For mechanical drives, the I/O scheduler can matter. Modern kernels often default to <code class="language-plaintext highlighter-rouge">bfq</code> or <code class="language-plaintext highlighter-rouge">mq-deadline</code>, which are generally good.</li>
  <li><strong>System Load:</strong> Minimize other disk-intensive or CPU-intensive processes during the copy.</li>
  <li><strong>Resource Monitoring:</strong> Use tools like <code class="language-plaintext highlighter-rouge">iotop</code> (to see disk I/O per process), <code class="language-plaintext highlighter-rouge">htop</code> (CPU/memory), <code class="language-plaintext highlighter-rouge">vmstat</code>, or <code class="language-plaintext highlighter-rouge">dstat</code> to identify bottlenecks during the transfer.</li>
  <li><strong>GUI <code class="language-plaintext highlighter-rouge">rsync</code> Front-ends:</strong> If you prefer a GUI but want <code class="language-plaintext highlighter-rouge">rsync</code>’s power, tools like <code class="language-plaintext highlighter-rouge">grsync</code> provide a graphical interface to <code class="language-plaintext highlighter-rouge">rsync</code>.</li>
</ul>

<p><strong>Which Method to Choose?</strong></p>

<ul>
  <li><strong>For general robustness, features, and good parallel performance:</strong> <code class="language-plaintext highlighter-rouge">rsync</code> with GNU <code class="language-plaintext highlighter-rouge">parallel</code> (Method 1) or <code class="language-plaintext highlighter-rouge">fpsync</code> (Method 4) are excellent choices.</li>
  <li><strong>For potentially the highest speed with extreme numbers of very small files (especially to/from mechanical drives):</strong> The <code class="language-plaintext highlighter-rouge">tar</code> pipe (Method 2) can be very effective.</li>
  <li><strong>For a simpler parallel <code class="language-plaintext highlighter-rouge">cp</code> approach:</strong> <code class="language-plaintext highlighter-rouge">find</code> with <code class="language-plaintext highlighter-rouge">xargs</code> and <code class="language-plaintext highlighter-rouge">cp --parents</code> (Method 3) is a solid option.</li>
  <li><strong>For an interactive TUI approach:</strong> <code class="language-plaintext highlighter-rouge">mc</code> (Method 5) is user-friendly.</li>
</ul>

<p>Always test with a smaller subset of your data and use dry-run options where available before committing to the full 500GiB transfer. This allows you to verify commands and estimate performance. Remember to replace placeholder paths with your actual SD card and external HDD mount points.</p>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">GitHub Actions Build Workflow Locally on a Debian</title><link href="https://ib.bsb.br/github-actions-build-workflow-locally-on-a-debian/" rel="alternate" type="text/html" title="GitHub Actions Build Workflow Locally on a Debian" /><published>2025-05-15T00:00:00+00:00</published><updated>2025-05-15T09:21:47+00:00</updated><id>https://ib.bsb.br/github-actions-build-workflow-locally-on-a-debian</id><content type="html" xml:base="https://ib.bsb.br/github-actions-build-workflow-locally-on-a-debian/"><![CDATA[<p><strong>Goal:</strong> To build the OS images and related artifacts as specified in the workflow, using a Debian-based environment like Finnix.</p>

<p><strong>Assumed Environment:</strong></p>
<ul>
  <li>A running Debian-based Linux distribution (e.g., Finnix booted, or a standard Debian/Ubuntu desktop/server). This guide assumes your distribution is reasonably compatible with Debian 12 (Bullseye), as the workflow specifies <code class="language-plaintext highlighter-rouge">image-debian-12</code>.</li>
  <li>Internet connectivity (for downloading packages and tools).</li>
  <li><code class="language-plaintext highlighter-rouge">sudo</code> privileges.</li>
</ul>

<hr />

<h3 id="1-prerequisites--initial-setup">1. Prerequisites &amp; Initial Setup</h3>

<p>Before starting the build, ensure your system and environment are ready.</p>

<p><strong>a. Resource Requirements:</strong>
The original GitHub Actions workflow specifies <code class="language-plaintext highlighter-rouge">runs-on</code> parameters that suggest the following minimum resources. Ensure your local machine or VM has:</p>
<ul>
  <li><strong>CPU:</strong> At least 4 cores.</li>
  <li><strong>Memory (RAM):</strong> At least 4 GB.</li>
  <li><strong>Disk Space:</strong> At least 100 GB free, especially in your build directory and system partitions like <code class="language-plaintext highlighter-rouge">/tmp</code> and <code class="language-plaintext highlighter-rouge">/var</code>.</li>
</ul>

<p><strong>b. System Update (Recommended):</strong>
Open a terminal and update your package lists:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get update
</code></pre></div></div>
<p><em>(On a fresh Finnix boot, this might be less critical but is good practice on persistent systems.)</em></p>

<p><strong>c. Install Essential Tools:</strong>
Some tools might already be present, especially on Finnix, but ensure they are installed:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> git curl ca-certificates gnupg
</code></pre></div></div>

<p><strong>d. Clone Your Repository:</strong>
If you haven’t already, clone the repository containing the workflow and the source code.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone &lt;your-repository-url&gt;
<span class="nb">cd</span> &lt;your-repository-name&gt;
</code></pre></div></div>
<p>Replace <code class="language-plaintext highlighter-rouge">&lt;your-repository-url&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;your-repository-name&gt;</code> with your actual repository details.</p>

<p><strong>e. Checkout the Target Tag:</strong>
The workflow triggers on any tag push (<code class="language-plaintext highlighter-rouge">on: push: tags: - '*'</code>). To reproduce a build for a specific tag (e.g., <code class="language-plaintext highlighter-rouge">v1.2.3</code>), check it out:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Replace v1.2.3 with your actual tag</span>
<span class="nb">export </span><span class="nv">TARGET_TAG</span><span class="o">=</span><span class="s2">"v1.2.3"</span>
git checkout tags/<span class="k">${</span><span class="nv">TARGET_TAG</span><span class="k">}</span> <span class="nt">-b</span> build-<span class="k">${</span><span class="nv">TARGET_TAG</span><span class="k">}</span>
</code></pre></div></div>
<p>This creates a local branch <code class="language-plaintext highlighter-rouge">build-${TARGET_TAG}</code> based on the tag.</p>

<p><strong>f. Define the Tag Name Environment Variable:</strong>
The workflow uses <code class="language-plaintext highlighter-rouge">github.ref_name</code> for the tag. We’ll simulate this:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">GITHUB_REF_NAME</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">TARGET_TAG</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Building for tag: </span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<hr />

<h3 id="2-installing-build-tools-and-dependencies">2. Installing Build Tools and Dependencies</h3>

<p>This section mirrors the setup steps from your GitHub Actions workflow.</p>

<p><strong>a. Install Go:</strong>
The workflow uses <code class="language-plaintext highlighter-rouge">actions/setup-go@v5</code> with <code class="language-plaintext highlighter-rouge">go-version: stable</code>.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> golang-go
<span class="c"># Verify installation (optional)</span>
go version
</code></pre></div></div>
<p>For most “stable” use cases, the version of Go provided by Debian’s repositories should suffice.</p>

<p><strong>b. Repository Permissions (Conditional):</strong>
The workflow runs <code class="language-plaintext highlighter-rouge">sudo chown -R $(id -u):$(id -g) .</code>. This is often for GitHub Actions runner environments. Locally, if you cloned as your user and manage <code class="language-plaintext highlighter-rouge">sudo</code> appropriately, you might not need this. If you encounter permission errors during <code class="language-plaintext highlighter-rouge">make</code> or <code class="language-plaintext highlighter-rouge">mkosi</code> operations related to file ownership <em>within your working directory</em>, you might revisit this. Generally, proceed without it first.</p>

<p><strong>c. Install Core Build Dependencies:</strong>
These are the packages listed in the workflow.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">--yes</span> <span class="se">\</span>
    binutils <span class="se">\</span>
    debian-archive-keyring <span class="se">\</span>
    devscripts <span class="se">\</span>
    make <span class="se">\</span>
    parted <span class="se">\</span>
    pipx <span class="se">\</span>
    qemu-utils
</code></pre></div></div>

<p><strong>d. Setup Incus (Daily Build):</strong>
The workflow uses a script to get daily builds of Incus.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"Setting up Incus (daily build)..."</span>
curl https://pkgs.zabbly.com/get/incus-daily | <span class="nb">sudo </span>sh

<span class="c"># The script above should handle repository setup and installation.</span>
<span class="c"># The workflow then initializes Incus and sets socket permissions.</span>
<span class="nb">sudo </span>incus admin init <span class="nt">--auto</span>

<span class="c"># The workflow uses 'sudo chmod 666 /var/lib/incus/unix.socket'.</span>
<span class="c"># This makes the socket world-writable, which is generally not recommended for production.</span>
<span class="c"># A better long-term approach is to add your user to the 'incus' group:</span>
<span class="c">#   sudo usermod -a -G incus $USER</span>
<span class="c">#   # Then log out and back in, or start a new shell: newgrp incus</span>
<span class="c"># This allows running 'incus' commands without sudo.</span>
<span class="c"># For immediate effect in the current script, or if not adding user to group,</span>
<span class="c"># the chmod command from the workflow can be used, but be aware of its implications.</span>
<span class="nb">sudo chmod </span>666 /var/lib/incus/unix.socket <span class="c"># As per workflow; consider security implications</span>

<span class="c"># Verify Incus (you might need sudo if group membership isn't active yet or chmod wasn't run)</span>
incus list
<span class="c"># If 'incus list' fails due to permissions and you haven't run chmod 666, try: sudo incus list</span>
</code></pre></div></div>

<p><strong>e. Setup mkosi:</strong>
<code class="language-plaintext highlighter-rouge">mkosi</code> is installed using <code class="language-plaintext highlighter-rouge">pipx</code> from a specific git commit.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipx <span class="nb">install </span>git+https://github.com/systemd/mkosi.git@v25.3

<span class="c"># IMPORTANT: Understanding PATH for mkosi and sudo</span>
<span class="c"># pipx installs applications to $HOME/.local/bin by default for the current user.</span>
<span class="c"># Add this to your PATH for the current session if it's not already configured in your .bashrc/.zshrc:</span>
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">HOME</span><span class="k">}</span><span class="s2">/.local/bin:</span><span class="k">${</span><span class="nv">PATH</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Make sure </span><span class="nv">$HOME</span><span class="s2">/.local/bin is in your PATH. Current PATH: </span><span class="nv">$PATH</span><span class="s2">"</span>

<span class="c"># Verify mkosi installation</span>
mkosi <span class="nt">--version</span>
</code></pre></div></div>
<p>If <code class="language-plaintext highlighter-rouge">pipx</code> prompts you to run <code class="language-plaintext highlighter-rouge">pipx ensurepath</code>, do so, and it might require opening a new terminal or sourcing your shell’s profile file (<code class="language-plaintext highlighter-rouge">.bashrc</code>, <code class="language-plaintext highlighter-rouge">.zshrc</code>, etc.).</p>

<hr />

<h3 id="3-configuring-for-the-build">3. Configuring for the Build</h3>

<p>Prepare files and environment variables specific to this build.</p>

<p><strong>a. Create <code class="language-plaintext highlighter-rouge">mkosi.version</code> File:</strong>
This file stores the version (tag name) for the build.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="o">&gt;</span> mkosi.version
<span class="nb">cat </span>mkosi.version <span class="c"># Verify content</span>
</code></pre></div></div>

<p><strong>b. Handle Secrets (<code class="language-plaintext highlighter-rouge">mkosi.crt</code>, <code class="language-plaintext highlighter-rouge">mkosi.key</code>):</strong>
The workflow uses GitHub secrets <code class="language-plaintext highlighter-rouge">secrets.SB_CERT</code> and <code class="language-plaintext highlighter-rouge">secrets.SB_KEY</code>. You must provide these files locally. <strong>These are sensitive files; handle them securely.</strong>
Create <code class="language-plaintext highlighter-rouge">mkosi.crt</code> and <code class="language-plaintext highlighter-rouge">mkosi.key</code> in your repository root with the <em>actual certificate and private key content</em>.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Example: Replace placeholder content with your actual secrets.</span>
<span class="nb">echo</span> <span class="s2">"---BEGIN CERTIFICATE---
This is a placeholder for your SB_CERT.
Replace this with the actual certificate content.
---END CERTIFICATE---"</span> <span class="o">&gt;</span> mkosi.crt
<span class="nb">chmod </span>644 mkosi.crt

<span class="nb">echo</span> <span class="s2">"---BEGIN PRIVATE KEY---
This is a placeholder for your SB_KEY.
Replace this with the actual private key content.
---END PRIVATE KEY---"</span> <span class="o">&gt;</span> mkosi.key
<span class="nb">chmod </span>600 mkosi.key <span class="c"># Restrict permissions for the private key</span>

<span class="nb">echo</span> <span class="s2">"IMPORTANT: Placeholder mkosi.crt and mkosi.key created. REPLACE with actual content."</span>
</code></pre></div></div>

<hr />

<h3 id="4-executing-the-build">4. Executing the Build</h3>

<p>This is where <code class="language-plaintext highlighter-rouge">mkosi</code> and your <code class="language-plaintext highlighter-rouge">Makefile</code> perform the image creation.</p>

<p><strong>a. Run the Main Build Command:</strong>
The workflow uses <code class="language-plaintext highlighter-rouge">make build-iso</code>. This assumes your <code class="language-plaintext highlighter-rouge">Makefile</code> has this target.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The original workflow exports PATH="${PATH}:/root/.local/bin".</span>
<span class="c"># This implies 'pipx install' might have been run as root, or the runner's $HOME is /root.</span>
<span class="c"># Since we installed mkosi as the current user (via pipx to $HOME/.local/bin),</span>
<span class="c"># and 'make build-iso' might invoke 'mkosi' which often requires root privileges</span>
<span class="c"># for operations like mounting filesystems, you'll likely need to run 'make' with sudo.</span>
<span class="c">#</span>
<span class="c"># Using 'sudo -E' is crucial here:</span>
<span class="c"># -E preserves the existing environment variables, including:</span>
<span class="c">#   1. GITHUB_REF_NAME: Needed by your build scripts.</span>
<span class="c">#   2. PATH: Ensures sudo can find 'mkosi' from $HOME/.local/bin (of the user who ran export).</span>

<span class="nb">echo</span> <span class="s2">"Running the build via 'sudo -E make build-iso'..."</span>
<span class="nb">sudo</span> <span class="nt">-E</span> make build-iso
</code></pre></div></div>

<p><strong>b. Troubleshooting Build Failures:</strong>
If the build fails:</p>
<ul>
  <li><strong>Examine <code class="language-plaintext highlighter-rouge">make</code> output:</strong> Look for the first error message.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">mkosi</code> logs:</strong> <code class="language-plaintext highlighter-rouge">mkosi</code> often creates detailed logs. Check for logs inside the <code class="language-plaintext highlighter-rouge">mkosi.output/</code> directory or its subdirectories (e.g., <code class="language-plaintext highlighter-rouge">mkosi.output/*.log</code>, <code class="language-plaintext highlighter-rouge">mkosi.output/build-*/</code>).</li>
  <li><strong>Verbose <code class="language-plaintext highlighter-rouge">make</code>:</strong> If your <code class="language-plaintext highlighter-rouge">Makefile</code> supports it, try: <code class="language-plaintext highlighter-rouge">sudo -E make build-iso VERBOSE=1</code> (or similar flags like <code class="language-plaintext highlighter-rouge">V=1</code>) for more detailed command output.</li>
  <li><strong>Incus issues:</strong> If <code class="language-plaintext highlighter-rouge">mkosi</code> uses Incus containers for the build:
    <ul>
      <li>Check container status: <code class="language-plaintext highlighter-rouge">sudo incus list</code></li>
      <li>View container logs: <code class="language-plaintext highlighter-rouge">sudo incus logs &lt;container_name_shown_in_list_or_mkosi_output&gt;</code></li>
    </ul>
  </li>
  <li><strong>Permissions:</strong> Ensure <code class="language-plaintext highlighter-rouge">sudo -E</code> was used. Double-check file permissions for scripts or configuration files used by <code class="language-plaintext highlighter-rouge">make</code> or <code class="language-plaintext highlighter-rouge">mkosi</code>.</li>
  <li><strong>Dependencies:</strong> Verify all packages from step 2.c were installed successfully.</li>
</ul>

<p><strong>c. Organize Output Files:</strong>
After a successful build, <code class="language-plaintext highlighter-rouge">mkosi</code> places output files (typically in <code class="language-plaintext highlighter-rouge">mkosi.output/</code>). The workflow then moves these.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> upload
<span class="nb">echo</span> <span class="s2">"Moving build artifacts to 'upload/' directory..."</span>

<span class="c"># Adjust these mv commands if your output filenames differ based on mkosi config or GITHUB_REF_NAME.</span>
<span class="c"># The glob patterns like *.usr-x86-64.* should handle variations.</span>
<span class="nb">mv </span>mkosi.output/debug.raw upload/
<span class="nb">mv </span>mkosi.output/incus.raw upload/

<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.raw upload/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.img
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.iso upload/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.iso
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.efi upload/
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.usr-x86-64.<span class="k">*</span> upload/
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.usr-x86-64-verity.<span class="k">*</span> upload/
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.usr-x86-64-verity-sig.<span class="k">*</span> upload/

<span class="nb">echo</span> <span class="s2">"Files moved to 'upload/' directory:"</span>
<span class="nb">ls</span> <span class="nt">-lh</span> upload/
</code></pre></div></div>
<p>If files are missing, review the build logs from <code class="language-plaintext highlighter-rouge">make build-iso</code> to see what <code class="language-plaintext highlighter-rouge">mkosi</code> actually produced in <code class="language-plaintext highlighter-rouge">mkosi.output/</code>.</p>

<hr />

<h3 id="5-compressing-the-artifacts">5. Compressing the Artifacts</h3>

<p>The workflow compresses the built files using <code class="language-plaintext highlighter-rouge">gzip</code>.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install gzip if not already present</span>
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nb">gzip

echo</span> <span class="s2">"Compressing files in 'upload/' directory..."</span>
<span class="nb">cd </span>upload
<span class="k">for </span>i <span class="k">in</span> <span class="k">*</span><span class="p">;</span> <span class="k">do
  if</span> <span class="o">[</span> <span class="nt">-f</span> <span class="s2">"</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span> <span class="c"># Check if it's a regular file</span>
    <span class="nb">echo</span> <span class="s2">"Compressing </span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">..."</span>
    <span class="nb">gzip</span> <span class="nt">-9</span> <span class="s2">"</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">"</span>
  <span class="k">fi
done
</span><span class="nb">cd</span> .. <span class="c"># Return to repository root</span>

<span class="nb">echo</span> <span class="s2">"Compressed files in 'upload/' directory:"</span>
<span class="nb">ls</span> <span class="nt">-lh</span> upload/
</code></pre></div></div>

<hr />

<h3 id="6-managing-build-artifacts-local-release">6. Managing Build Artifacts (Local “Release”)</h3>

<p>The GitHub Actions workflow uploads these compressed files to a GitHub Release. Locally, your “release” artifacts are in the <code class="language-plaintext highlighter-rouge">upload/</code> directory.</p>

<p>You can:</p>
<ul>
  <li>Copy them to another location for testing or distribution.</li>
  <li>Optionally, create an actual GitHub Release from your local machine using the GitHub CLI (<code class="language-plaintext highlighter-rouge">gh</code>):
    <ol>
      <li><strong>Install GitHub CLI:</strong> (If not already installed. Instructions at <a href="https://cli.github.com/">cli.github.com</a>)
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Example for Debian/Ubuntu:</span>
<span class="nb">type</span> <span class="nt">-p</span> curl <span class="o">&gt;</span>/dev/null <span class="o">||</span> <span class="nb">sudo </span>apt <span class="nb">install </span>curl <span class="nt">-y</span>
curl <span class="nt">-fsSL</span> https://cli.github.com/packages/githubcli-archive-keyring.gpg | <span class="nb">sudo dd </span><span class="nv">of</span><span class="o">=</span>/usr/share/keyrings/githubcli-archive-keyring.gpg <span class="se">\</span>
<span class="o">&amp;&amp;</span> <span class="nb">sudo chmod </span>go+r /usr/share/keyrings/githubcli-archive-keyring.gpg <span class="se">\</span>
<span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"deb [arch=</span><span class="si">$(</span>dpkg <span class="nt">--print-architecture</span><span class="si">)</span><span class="s2"> signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main"</span> | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/github-cli.list <span class="o">&gt;</span> /dev/null <span class="se">\</span>
<span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt update <span class="se">\</span>
<span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt <span class="nb">install </span>gh <span class="nt">-y</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Authenticate <code class="language-plaintext highlighter-rouge">gh</code> CLI:</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gh auth login
</code></pre></div>        </div>
      </li>
      <li><strong>Create Release and Upload Files:</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure you are in the root of your repository directory</span>
gh release create <span class="s2">"</span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span> ./upload/<span class="k">*</span> <span class="nt">--title</span> <span class="s2">"Release </span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--notes</span> <span class="s2">"Locally built release for </span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ul>

<hr />

<h3 id="7-important-considerations">7. Important Considerations</h3>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Makefile</code> and <code class="language-plaintext highlighter-rouge">mkosi</code> Configurations:</strong> This tutorial heavily relies on your project’s <code class="language-plaintext highlighter-rouge">Makefile</code> (with the <code class="language-plaintext highlighter-rouge">build-iso</code> target) and <code class="language-plaintext highlighter-rouge">mkosi</code> configuration files (<code class="language-plaintext highlighter-rouge">mkosi.conf</code>, <code class="language-plaintext highlighter-rouge">mkosi.local.conf</code>, <code class="language-plaintext highlighter-rouge">mkosi.conf.d/*</code>, <code class="language-plaintext highlighter-rouge">mkosi.build</code> scripts) being correct and present in your repository.</li>
  <li><strong>Root Privileges (<code class="language-plaintext highlighter-rouge">sudo -E</code>):</strong> Be mindful of commands requiring <code class="language-plaintext highlighter-rouge">sudo</code>. Using <code class="language-plaintext highlighter-rouge">sudo -E</code> is critical for <code class="language-plaintext highlighter-rouge">make build-iso</code> to ensure it inherits necessary environment variables like your modified <code class="language-plaintext highlighter-rouge">PATH</code> (for <code class="language-plaintext highlighter-rouge">mkosi</code>) and <code class="language-plaintext highlighter-rouge">GITHUB_REF_NAME</code>.</li>
  <li><strong>Environment Differences:</strong> A local environment will always have subtle differences from a GitHub Actions runner.</li>
  <li><strong>Finnix Specifics:</strong> If using Finnix, remember its live nature means installed packages or system changes are typically not persistent across reboots unless you’ve configured persistence. For a single build session, this is usually fine.</li>
  <li><strong>Clean Builds:</strong> For truly reproducible builds, consider cleaning previous build outputs (e.g., <code class="language-plaintext highlighter-rouge">mkosi.output/</code>, <code class="language-plaintext highlighter-rouge">upload/</code>) before starting a new build. Your <code class="language-plaintext highlighter-rouge">Makefile</code> might have a <code class="language-plaintext highlighter-rouge">clean</code> target: <code class="language-plaintext highlighter-rouge">sudo -E make clean</code>.</li>
  <li><strong>Resource Intensive:</strong> OS image building can be very demanding on CPU, RAM, and disk I/O.</li>
  <li><strong>Alternative: Containerized Builds:</strong> For higher fidelity reproduction and isolation, consider running the entire build process within a Docker or Podman container based on a <code class="language-plaintext highlighter-rouge">debian:12</code> image. This is more complex to set up but offers a cleaner environment.</li>
</ul>

<hr />]]></content><author><name></name></author><category term="scratchpad" /></entry></feed>