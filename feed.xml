<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ib.bsb.br/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ib.bsb.br/" rel="alternate" type="text/html" /><updated>2025-10-08T17:14:31+00:00</updated><id>https://ib.bsb.br/feed.xml</id><title type="html">infoBAG</title><entry><title type="html">prf</title><link href="https://ib.bsb.br/prf/" rel="alternate" type="text/html" title="prf" /><published>2025-10-08T00:00:00+00:00</published><updated>2025-10-08T15:53:00+00:00</updated><id>https://ib.bsb.br/prf</id><content type="html" xml:base="https://ib.bsb.br/prf/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;checbfr&gt;Ibpê é hz nffvfgragr rfcrpvnyvfgn abf znahnvf qn CES. Fhn shaçãb é nanyvfne [[hfre_dhrel]] r trene erfcbfgnf sbeznvf r shaqnzragnqnf, rkpyhfvinzragr pbz onfr rz [[nyy-znahnyf.cqs]]. Pnqn erfcbfgn qrir vapyhve pvgnçãb rfcrpísvpn ab sbezngb "(Sbagr: [Abzr qb Znahny], [Pncíghyb/Frçãb/Negvtb])". Dhnaqb n onfr grkghny qbf znahnvf rz narkb sbe vafhsvpvragr, rynober vasreêapvnf whfgvsvpnqnf n cnegve qr qnqbf, vasbeznçõrf r pbaurpvzragbf pbeeryngbf àf grfrf rz naáyvfr, cevbevmnaqb n nqreêapvn nbf znahnvf qr ersreêapvn. Fr arprffáevb, nqzvgn qvfgnapvnzragbf vagrecergngvibf cbe zrvb qr rkgencbynçõrf, rkcyvpvgnaqb cerzvffnf, yvzvgrf r vapregrmnf, n svz qr dhr arauhzn dhrfgãb creznarçn frz erfcbfgn.&lt;/checbfr&gt;

  &lt;pbagrkg&gt;
    &lt;nhqvrapr_cebsvyr&gt;
      &lt;ebyr&gt;[[nhqvrapr_ebyr]]&lt;/ebyr&gt;
      &lt;rkcregvfr&gt;Nygn&lt;/rkcregvfr&gt;
      &lt;tbny&gt;Hfne b pbagrúqb bsvpvny cnen erfbyire bf ceboyrznf qb hfháevb.&lt;/tbny&gt;
    &lt;/nhqvrapr_cebsvyr&gt;
    &lt;fglyr_thvqr&gt;
      &lt;gbar&gt;Sbezny, bowrgvib, vzcrffbny&lt;/gbar&gt;
      &lt;ibvpr&gt;Ngvin&lt;/ibvpr&gt;
      &lt;ynathntr&gt;[[ynathntr]]&lt;/ynathntr&gt;
      &lt;grezvabybtl&gt;Rzcertne n grezvabybtvn wheíqvpn/bcrenpvbany rkngnzragr pbzb abf znahnvf.&lt;/grezvabybtl&gt;
    &lt;/fglyr_thvqr&gt;
  &lt;/pbagrkg&gt;

  &lt;pbafgenvagf&gt;
    &lt;pbafgenvag&gt;PEÍGVPB: Onfrne-fr ncranf rz [[nyy-znahnyf.cqs]].&lt;/pbafgenvag&gt;
    &lt;pbafgenvag&gt;PEÍGVPB: Gbqn erfcbfgn qrir pbagre n pvgnçãb ab sbezngb rkvtvqb bh n senfr qr vaqvfcbavovyvqnqr.&lt;/pbafgenvag&gt;
    &lt;pbafgenvag&gt;An nhfêapvn qr onfr grkghny fhsvpvragr abf znahnvf rz narkb ([[nyy-znahnyf.cqs]]), cebqhmn vasreêapvnf shaqnzragnqnf rz rivqêapvnf pbeeryngnf, cerfreinaqb n pbasbezvqnqr pbz bf znahnvf; fbzragr dhnaqb vaqvfcrafáiry, rkgencbyr pbz cerzvffnf r yvzvgrf qrpynenqbf, cnen rivgne ynphanf qr erfcbfgn.&lt;/pbafgenvag&gt;
    &lt;pbafgenvag&gt;Erfcbaqre pnqn vgrz qr [[hfre_dhrel]] frcnenqnzragr.&lt;/pbafgenvag&gt;
    &lt;pbafgenvag&gt;Rz pnfb qr pbasyvgb rager gerpubf, ncerfragne nzonf nf pvgnçõrf r rkcyvpvgne n qviretêapvn.&lt;/pbafgenvag&gt;
  &lt;/pbafgenvagf&gt;

  &lt;bhgchg_sbezng_fcrpvsvpngvba&gt;
    &lt;vafgehpgvba&gt;Ncerfragr n fníqn pbzb hzn féevr qr erfcbfgnf.&lt;/vafgehpgvba&gt;
    &lt;vafgehpgvba&gt;Pnqn erfcbfgn qrir vavpvne pbz hz pnorçnyub rz artevgb dhr pbeerfcbaqn nb vgrz qb hfháevb.&lt;/vafgehpgvba&gt;
    &lt;vafgehpgvba&gt;Sbezngr gbqnf nf pvgnçõrf pbzb: (Sbagr: [Abzr qb Znahny], [Pncíghyb/Frçãb/Negvtb]).&lt;/vafgehpgvba&gt;
  &lt;/bhgchg_sbezng_fcrpvsvpngvba&gt;

  &lt;vafgehpgvbaf&gt;
    &lt;vafgehpgvba&gt;1. Yrvn [[hfre_dhrel]] r qrpbzchaun rz vgraf bowrgvibf.&lt;/vafgehpgvba&gt;
    &lt;vafgehpgvba&gt;2. Cnen pnqn vgrz, ohfdhr rz [[nyy-znahnyf.cqs]] b(f) gerpub(f) ncyvpáiry(vf).&lt;/vafgehpgvba&gt;
    &lt;vafgehpgvba&gt;3. Fr ubhire gerpubf/irefõrf pbasyvgnagrf, ncerfragr pnqn onfr pbz fhn pvgnçãb r rkcyvpvgr n qviretêapvn.&lt;/vafgehpgvba&gt;
    &lt;vafgehpgvba&gt;4. Erqvwn erfcbfgn sbezny r bowrgvin rkpyhfvinzragr pbz onfr abf gerpubf ybpnyvmnqbf.&lt;/vafgehpgvba&gt;
    &lt;vafgehpgvba&gt;5. Narkr pvgnçãb rkngn ab sbezngb rkvtvqb bh qrpyner n vaqvfcbavovyvqnqr.&lt;/vafgehpgvba&gt;
    &lt;vafgehpgvba&gt;6. Cerfreir n grezvabybtvn qb znahny; aãb nqvpvbar bcvavãb bh sbagrf rkgreanf.&lt;/vafgehpgvba&gt;
    &lt;vafgehpgvba&gt;7. Sbezngr n fníqn cbe vgraf, pbz pnorçnyub rz artevgb r pvgnçãb nb svany qr pnqn vgrz.&lt;/vafgehpgvba&gt;
  &lt;/vafgehpgvbaf&gt;

  &lt;vachg_qngn&gt;
    &lt;hfre_dhrel&gt;[[hfre_dhrel]]&lt;/hfre_dhrel&gt;
    &lt;znahnyf_cqs&gt;[[nyy-znahnyf.cqs nyernql cebivqrq ivn nggnpuzragf]]&lt;/znahnyf_cqs&gt;
    &lt;ynathntr&gt;cg-OE&lt;/ynathntr&gt;
    &lt;nhqvrapr_ebyr&gt;CES Znantre&lt;/nhqvrapr_ebyr&gt;
  &lt;/vachg_qngn&gt;

  &lt;rknzcyrf&gt;
    &lt;rknzcyr&gt;
      &lt;vachg_qngn&gt;
        &lt;hfre_dhrel&gt;Dhny é b cebprqvzragb cnen &amp;yg;grzn_rfcrpvsvpb&amp;tg; rz oyvgm?&lt;/hfre_dhrel&gt;
      &lt;/vachg_qngn&gt;
      &lt;bhgchg&gt;&amp;yg;fgebat&amp;tg;Cebprqvzragb cnen &amp;yg;grzn_rfcrpvsvpb&amp;tg;&amp;yg;/fgebat&amp;tg;\aGrkgb fvagrgvmnqb qb znahny cregvaragr. (Sbagr: [Znahny K], [Pncíghyb/Frçãb L]).&lt;/bhgchg&gt;
    &lt;/rknzcyr&gt;
    &lt;rknzcyr&gt;
      &lt;vachg_qngn&gt;
        &lt;hfre_dhrel&gt;Dhnaqb ncyvpne zrqvqn N if. O?&lt;/hfre_dhrel&gt;
      &lt;/vachg_qngn&gt;
      &lt;bhgchg&gt;&amp;yg;fgebat&amp;tg;Ncyvpnçãb qr zrqvqn N if. O&amp;yg;/fgebat&amp;tg;\aB nedhvib ncerfragn bevragnçõrf qvfgvagnf: N — erfhzb pbeerfcbaqragr (Sbagr: [Znahny K], [Frçãb 3.2]); O — erfhzb pbeerfcbaqragr (Sbagr: [Znahny L], [Frçãb 5.1]).&lt;/bhgchg&gt;
    &lt;/rknzcyr&gt;
  &lt;/rknzcyrf&gt;
</code></pre></div></div>]]></content><author><name></name></author><category term="AI&gt;prompt" /></entry><entry><title type="html">auto xrandr for two monitors bash script</title><link href="https://ib.bsb.br/auto-xrandr-for-two-monitors-bash-script/" rel="alternate" type="text/html" title="auto xrandr for two monitors bash script" /><published>2025-10-07T00:00:00+00:00</published><updated>2025-10-07T20:19:32+00:00</updated><id>https://ib.bsb.br/auto-xrandr-for-two-monitors-bash-script</id><content type="html" xml:base="https://ib.bsb.br/auto-xrandr-for-two-monitors-bash-script/"><![CDATA[<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="#!/usr/bin/env bash
#
# monitor-setup-refactored-fixed.sh
#
# Purpose: Robust interactive multi-monitor configuration with visual verification overlay
#          Designed for Debian 11 (ARM64), Alacritty terminal, ratpoison WM, X11 session.
#
# Key reliability fixes to prevent post-overlay &quot;freeze&quot;:
#   - Tkinter overlay closes from GUI thread using root.after(); no threading.Timer.
#   - Hard timeout guard around overlay (coreutils `timeout`) so we always proceed.
#   - Prompts and input read/write via /dev/tty to bypass pipe buffering and focus quirks.
#   - No layout churn during testing: only target output&#39;s mode changes until final apply.
#   - Line-buffered logging to file via tee; prompts go directly to TTY for immediate display.
#
# Requirements: bash, xrandr, python3, python3-tk, coreutils (timeout), awk, grep, sed.
# Usage: Run in an X11 terminal. Answer y/n/q per prompt after each 5s overlay.
# Autostart: Optional (~/.config/autostart). Set TARGET_USER and INSTALL_AUTOSTART as needed.

set -Eeuo pipefail

LOGF=${LOGF:-/tmp/monitor-setup.log}
# Log to file; prompts will go to /dev/tty, not to this pipe
exec &gt; &gt;(tee -a &quot;$LOGF&quot;) 2&gt;&amp;1

# ---------------- Utilities ----------------
log() { printf &#39;[%(%F %T)T] %s\n&#39; -1 &quot;$*&quot;; }
fail() { log &quot;ERROR: $*&quot;; exit 1; }
need() { command -v &quot;$1&quot; &gt;/dev/null 2&gt;&amp;1 || fail &quot;$1 not found&quot;; }

# ---------------- Requirements ----------------
need xrandr
need python3
python3 - &lt;&lt;&#39;PY&#39; 2&gt;/dev/null || fail &quot;python3-tk not available. Install python3-tk.&quot;
import tkinter  # noqa
PY
need timeout

# ---------------- Config ----------------
TARGET_USER=${TARGET_USER:-linaro}
INSTALL_AUTOSTART=${INSTALL_AUTOSTART:-1}
DISPLAY=${DISPLAY:-:0}
export DISPLAY

# XAUTHORITY so we can talk to the right X server (works under sudo as well)
if [[ $(id -u) -eq 0 ]]; then
  TARGET_HOME=$(getent passwd &quot;$TARGET_USER&quot; | cut -d: -f6 || true)
  [[ -z &quot;${TARGET_HOME:-}&quot; ]] &amp;&amp; TARGET_HOME=&quot;/home/$TARGET_USER&quot;
  if [[ -f &quot;$TARGET_HOME/.Xauthority&quot; ]]; then
    export XAUTHORITY=&quot;$TARGET_HOME/.Xauthority&quot;
  elif [[ -f &quot;/root/.Xauthority&quot; ]]; then
    export XAUTHORITY=&quot;/root/.Xauthority&quot;
  fi
else
  export XAUTHORITY=&quot;${XAUTHORITY:-$HOME/.Xauthority}&quot;
fi

# Ensure we can read prompts from a controlling terminal
if [[ ! -t 0 ]] &amp;&amp; [[ -r /dev/tty ]]; then
  exec &lt;/dev/tty
fi

# ---------------- Wait for X ----------------
for _ in {1..15}; do xrandr &gt;/dev/null 2&gt;&amp;1 &amp;&amp; break || sleep 1; done
xrandr &gt;/dev/null 2&gt;&amp;1 || fail &quot;X server not ready on $DISPLAY&quot;

# ---------------- Globals ----------------
SAFEMODES=(
  1920x1080 1680x1050 1600x900 1440x900 1366x768 1280x1024 1280x960
  1280x800 1280x720 1024x768 800x600
)

declare -A FINAL_MODE WIDTH HEIGHT

# ---------------- Helpers ----------------
get_outputs() { xrandr --query | awk &#39;/ connected/{print $1}&#39;; }

sort_outputs() {
  local -a outs; mapfile -t outs &lt; &lt;(get_outputs)
  ((${#outs[@]})) || return 1
  local prefs=(HDMI DP DVI eDP VGA)
  for p in &quot;${prefs[@]}&quot;; do for o in &quot;${outs[@]}&quot;; do [[ $o == ${p}* ]] &amp;&amp; echo &quot;$o&quot;; done; done
  for o in &quot;${outs[@]}&quot;; do
    local hit=0; for p in &quot;${prefs[@]}&quot;; do [[ $o == ${p}* ]] &amp;&amp; { hit=1; break; }; done
    ((hit==0)) &amp;&amp; echo &quot;$o&quot;
  done
}

get_modes_for_output() {
  local out=&quot;$1&quot;
  xrandr --query | awk -v o=&quot;$out&quot; &#39;
    $1==o {in=1; next}
    /^[A-Z]/ {in=0}
    in &amp;&amp; $1 ~ /^[0-9]+x[0-9]+/ {print $1}
  &#39; | awk &#39;!seen[$0]++&#39;
}

pick_testlist() {
  # stdin: available modes; stdout: up to 3 modes
  mapfile -t avail
  ((${#avail[@]})) || return 1
  local -a picked=()
  for safe in &quot;${SAFEMODES[@]}&quot;; do
    for m in &quot;${avail[@]}&quot;; do [[ $m == &quot;$safe&quot; ]] &amp;&amp; { picked+=(&quot;$m&quot;); break; }; done
    ((${#picked[@]}&gt;=3)) &amp;&amp; break
  done
  if ((${#picked[@]})); then printf &#39;%s\n&#39; &quot;${picked[@]}&quot;; return 0; fi
  # else: top-3 by area
  printf &#39;%s\n&#39; &quot;${avail[@]}&quot; | awk -Fx &#39;{print $1*$2, $0}&#39; | sort -nr | awk &#39;{print $2}&#39; | head -n3
}

current_geometry() {
  local out=&quot;$1&quot;
  xrandr --query | awk -v o=&quot;$out&quot; &#39;
    $1==o { if (match($0, /[0-9]+x[0-9]+\+[0-9]+\+[0-9]+/)) { print substr($0, RSTART, RLENGTH) } }
  &#39;
}

show_rectangle() {
  local W=&quot;$1&quot; H=&quot;$2&quot; X=&quot;$3&quot; Y=&quot;$4&quot;
  log &quot;   displaying overlay ${W}x${H}+${X}+${Y} for 5s&quot;
  timeout 7s python3 - &quot;$W&quot; &quot;$H&quot; &quot;$X&quot; &quot;$Y&quot; &lt;&lt;&#39;PY&#39;
import sys, tkinter as t
W,H,X,Y = map(int, sys.argv[1:])
root = t.Tk()
root.overrideredirect(True)
root.geometry(f&quot;{W}x{H}+{X}+{Y}&quot;)
frame = t.Frame(root, width=W, height=H, highlightbackground=&#39;red&#39;, highlightthickness=8)
frame.pack()
root.attributes(&#39;-topmost&#39;, True)
root.after(5000, root.quit)
root.mainloop()
try:
    root.destroy()
except Exception:
    pass
PY
  local code=$?
  if (( code != 0 )); then log &quot;   [WARN] overlay exited non-zero ($code); continuing&quot;; fi
}

ask_ynq() {
  # Prompt to /dev/tty; read one char from /dev/tty; returns 0=y,1=n,2=q
  local prompt=&quot;$1&quot; ch
  while true; do
    printf &quot;%s&quot; &quot;$prompt&quot; &gt; /dev/tty
    IFS= read -r -n1 ch &lt; /dev/tty || ch=&quot;&quot;
    printf &quot;\n&quot; &gt; /dev/tty
    case &quot;$ch&quot; in
      y|Y) return 0;;
      n|N) return 1;;
      q|Q) return 2;;
      *) prompt=&quot;Please type y (accept), n (next), or q (quit): &quot;;;
    esac
  done
}

# ---------------- Main ----------------
log &quot;Starting interactive monitor configuration...&quot;

mapfile -t outputs &lt; &lt;(get_outputs)
((${#outputs[@]})) || fail &quot;No connected monitors detected&quot;

mapfile -t sorted &lt; &lt;(sort_outputs)

for out in &quot;${sorted[@]}&quot;; do
  log &quot;======== Configuring $out ========&quot;
  mapfile -t avail &lt; &lt;(get_modes_for_output &quot;$out&quot;)
  if ((${#avail[@]}==0)); then log &quot;No modes found for $out, skipping&quot;; continue; fi

  mapfile -t testlist &lt; &lt;(printf &#39;%s\n&#39; &quot;${avail[@]}&quot; | pick_testlist)
  if ((${#testlist[@]}==0)); then log &quot;No usable candidates for $out, skipping&quot;; continue; fi

  sel=&quot;&quot;; sel_w=0; sel_h=0

  for mode in &quot;${testlist[@]}&quot;; do
    [[ -z &quot;$mode&quot; ]] &amp;&amp; continue
    log &quot;---&gt; Trying $mode for $out&quot;
    if ! xrandr --output &quot;$out&quot; --mode &quot;$mode&quot;; then log &quot;    Could not apply $mode&quot;; continue; fi
    sleep 0.2

    geom=$(current_geometry &quot;$out&quot; || true)
    if [[ ! $geom =~ ^([0-9]+)x([0-9]+)\+([0-9]+)\+([0-9]+)$ ]]; then
      log &quot;    Could not parse geometry; skipping&quot;
      continue
    fi
    W=${BASH_REMATCH[1]}; H=${BASH_REMATCH[2]}; X=${BASH_REMATCH[3]}; Y=${BASH_REMATCH[4]}

    show_rectangle &quot;$W&quot; &quot;$H&quot; &quot;$X&quot; &quot;$Y&quot;

    ask_ynq &quot;Was the red rectangle fully enclosed on $out at $mode? [y=accept / n=next / q=quit] &quot;; rc=$?
    case &quot;$rc&quot; in
      0) sel=&quot;$mode&quot;; sel_w=$W; sel_h=$H; break ;;
      1) ;;  # try next
      2) log &quot;User aborted&quot;; exit 1 ;;
    esac
  done

  if [[ -z &quot;$sel&quot; ]]; then
    log &quot;!! No accepted mode for $out. Falling back to ${testlist[0]}&quot;
    mode=&quot;${testlist[0]}&quot;
    if xrandr --output &quot;$out&quot; --mode &quot;$mode&quot;; then
      sleep 0.2
      geom=$(current_geometry &quot;$out&quot; || true)
      if [[ $geom =~ ^([0-9]+)x([0-9]+)\+([0-9]+)\+([0-9]+)$ ]]; then
        sel_w=${BASH_REMATCH[1]}; sel_h=${BASH_REMATCH[2]}; sel=&quot;$mode&quot;
      else
        log &quot;!! Could not determine geometry for fallback; skipping monitor&quot;
        continue
      fi
    else
      log &quot;!! Fallback apply failed; skipping monitor&quot;
      continue
    fi
  fi

  FINAL_MODE[&quot;$out&quot;]=&quot;$sel&quot;
  WIDTH[&quot;$out&quot;]=$sel_w
  HEIGHT[&quot;$out&quot;]=$sel_h

done

# ---------------- Final apply ----------------
log &quot;Applying selected monitor configuration...&quot;

pos=0
primary_out=&quot;${sorted[0]}&quot;; maxarea=0

for out in &quot;${sorted[@]}&quot;; do
  mode=&quot;${FINAL_MODE[$out]:-}&quot;
  if [[ -z &quot;$mode&quot; ]]; then log &quot;Skipping $out (no selected mode)&quot;; continue; fi
  xrandr --output &quot;$out&quot; --mode &quot;$mode&quot; --pos &quot;${pos}x0&quot;
  area=$(( WIDTH[$out] * HEIGHT[$out] ))
  if (( area &gt; maxarea )); then maxarea=$area; primary_out=&quot;$out&quot;; fi
  pos=$(( pos + WIDTH[$out] ))
  sleep 0.1
done

xrandr --output &quot;$primary_out&quot; --primary || true
log &quot;[SUCCESS] Configuration complete. Primary: $primary_out&quot;

# ---------------- Autostart (optional) ----------------
if (( INSTALL_AUTOSTART )); then
  TARGET_HOME=${TARGET_HOME:-$(getent passwd &quot;$TARGET_USER&quot; | cut -d: -f6 || echo &quot;/home/$TARGET_USER&quot;)}
  AUTOSTART_DIR=&quot;$TARGET_HOME/.config/autostart&quot;
  SCRIPT_PATH=&quot;$TARGET_HOME/monitor-setup-refactored-fixed.sh&quot;
  AUTOSTART_FILE=&quot;$AUTOSTART_DIR/monitor-setup-refactored-fixed.desktop&quot;

  mkdir -p &quot;$AUTOSTART_DIR&quot;
  cp -- &quot;$0&quot; &quot;$SCRIPT_PATH&quot; || true
  chmod +x &quot;$SCRIPT_PATH&quot;
  if [[ $(id -u) -eq 0 ]]; then chown -R &quot;$TARGET_USER:$TARGET_USER&quot; &quot;$TARGET_HOME/.config&quot; &quot;$SCRIPT_PATH&quot; || true; fi

  cat &gt; &quot;$AUTOSTART_FILE&quot; &lt;&lt;EOF
[Desktop Entry]
Type=Application
Exec=bash &quot;$SCRIPT_PATH&quot;
Hidden=false
NoDisplay=false
X-GNOME-Autostart-enabled=true
Name=Auto Monitor Setup
Comment=Autoconfigure monitors layout interactively at login
EOF
  chmod 644 &quot;$AUTOSTART_FILE&quot;
  if [[ $(id -u) -eq 0 ]]; then chown &quot;$TARGET_USER:$TARGET_USER&quot; &quot;$AUTOSTART_FILE&quot; || true; fi
  log &quot;[INFO] Autostart installed at $AUTOSTART_FILE for user $TARGET_USER&quot;
fi

exit 0" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">#!/usr/bin/env bash
#
# monitor-setup-refactored-fixed.sh
#
# Purpose: Robust interactive multi-monitor configuration with visual verification overlay
#          Designed for Debian 11 (ARM64), Alacritty terminal, ratpoison WM, X11 session.
#
# Key reliability fixes to prevent post-overlay &quot;freeze&quot;:
#   - Tkinter overlay closes from GUI thread using root.after(); no threading.Timer.
#   - Hard timeout guard around overlay (coreutils `timeout`) so we always proceed.
#   - Prompts and input read/write via /dev/tty to bypass pipe buffering and focus quirks.
#   - No layout churn during testing: only target output&#39;s mode changes until final apply.
#   - Line-buffered logging to file via tee; prompts go directly to TTY for immediate display.
#
# Requirements: bash, xrandr, python3, python3-tk, coreutils (timeout), awk, grep, sed.
# Usage: Run in an X11 terminal. Answer y/n/q per prompt after each 5s overlay.
# Autostart: Optional (~/.config/autostart). Set TARGET_USER and INSTALL_AUTOSTART as needed.

set -Eeuo pipefail

LOGF=${LOGF:-/tmp/monitor-setup.log}
# Log to file; prompts will go to /dev/tty, not to this pipe
exec &gt; &gt;(tee -a &quot;$LOGF&quot;) 2&gt;&amp;1

# ---------------- Utilities ----------------
log() { printf &#39;[%(%F %T)T] %s\n&#39; -1 &quot;$*&quot;; }
fail() { log &quot;ERROR: $*&quot;; exit 1; }
need() { command -v &quot;$1&quot; &gt;/dev/null 2&gt;&amp;1 || fail &quot;$1 not found&quot;; }

# ---------------- Requirements ----------------
need xrandr
need python3
python3 - &lt;&lt;&#39;PY&#39; 2&gt;/dev/null || fail &quot;python3-tk not available. Install python3-tk.&quot;
import tkinter  # noqa
PY
need timeout

# ---------------- Config ----------------
TARGET_USER=${TARGET_USER:-linaro}
INSTALL_AUTOSTART=${INSTALL_AUTOSTART:-1}
DISPLAY=${DISPLAY:-:0}
export DISPLAY

# XAUTHORITY so we can talk to the right X server (works under sudo as well)
if [[ $(id -u) -eq 0 ]]; then
  TARGET_HOME=$(getent passwd &quot;$TARGET_USER&quot; | cut -d: -f6 || true)
  [[ -z &quot;${TARGET_HOME:-}&quot; ]] &amp;&amp; TARGET_HOME=&quot;/home/$TARGET_USER&quot;
  if [[ -f &quot;$TARGET_HOME/.Xauthority&quot; ]]; then
    export XAUTHORITY=&quot;$TARGET_HOME/.Xauthority&quot;
  elif [[ -f &quot;/root/.Xauthority&quot; ]]; then
    export XAUTHORITY=&quot;/root/.Xauthority&quot;
  fi
else
  export XAUTHORITY=&quot;${XAUTHORITY:-$HOME/.Xauthority}&quot;
fi

# Ensure we can read prompts from a controlling terminal
if [[ ! -t 0 ]] &amp;&amp; [[ -r /dev/tty ]]; then
  exec &lt;/dev/tty
fi

# ---------------- Wait for X ----------------
for _ in {1..15}; do xrandr &gt;/dev/null 2&gt;&amp;1 &amp;&amp; break || sleep 1; done
xrandr &gt;/dev/null 2&gt;&amp;1 || fail &quot;X server not ready on $DISPLAY&quot;

# ---------------- Globals ----------------
SAFEMODES=(
  1920x1080 1680x1050 1600x900 1440x900 1366x768 1280x1024 1280x960
  1280x800 1280x720 1024x768 800x600
)

declare -A FINAL_MODE WIDTH HEIGHT

# ---------------- Helpers ----------------
get_outputs() { xrandr --query | awk &#39;/ connected/{print $1}&#39;; }

sort_outputs() {
  local -a outs; mapfile -t outs &lt; &lt;(get_outputs)
  ((${#outs[@]})) || return 1
  local prefs=(HDMI DP DVI eDP VGA)
  for p in &quot;${prefs[@]}&quot;; do for o in &quot;${outs[@]}&quot;; do [[ $o == ${p}* ]] &amp;&amp; echo &quot;$o&quot;; done; done
  for o in &quot;${outs[@]}&quot;; do
    local hit=0; for p in &quot;${prefs[@]}&quot;; do [[ $o == ${p}* ]] &amp;&amp; { hit=1; break; }; done
    ((hit==0)) &amp;&amp; echo &quot;$o&quot;
  done
}

get_modes_for_output() {
  local out=&quot;$1&quot;
  xrandr --query | awk -v o=&quot;$out&quot; &#39;
    $1==o {in=1; next}
    /^[A-Z]/ {in=0}
    in &amp;&amp; $1 ~ /^[0-9]+x[0-9]+/ {print $1}
  &#39; | awk &#39;!seen[$0]++&#39;
}

pick_testlist() {
  # stdin: available modes; stdout: up to 3 modes
  mapfile -t avail
  ((${#avail[@]})) || return 1
  local -a picked=()
  for safe in &quot;${SAFEMODES[@]}&quot;; do
    for m in &quot;${avail[@]}&quot;; do [[ $m == &quot;$safe&quot; ]] &amp;&amp; { picked+=(&quot;$m&quot;); break; }; done
    ((${#picked[@]}&gt;=3)) &amp;&amp; break
  done
  if ((${#picked[@]})); then printf &#39;%s\n&#39; &quot;${picked[@]}&quot;; return 0; fi
  # else: top-3 by area
  printf &#39;%s\n&#39; &quot;${avail[@]}&quot; | awk -Fx &#39;{print $1*$2, $0}&#39; | sort -nr | awk &#39;{print $2}&#39; | head -n3
}

current_geometry() {
  local out=&quot;$1&quot;
  xrandr --query | awk -v o=&quot;$out&quot; &#39;
    $1==o { if (match($0, /[0-9]+x[0-9]+\+[0-9]+\+[0-9]+/)) { print substr($0, RSTART, RLENGTH) } }
  &#39;
}

show_rectangle() {
  local W=&quot;$1&quot; H=&quot;$2&quot; X=&quot;$3&quot; Y=&quot;$4&quot;
  log &quot;   displaying overlay ${W}x${H}+${X}+${Y} for 5s&quot;
  timeout 7s python3 - &quot;$W&quot; &quot;$H&quot; &quot;$X&quot; &quot;$Y&quot; &lt;&lt;&#39;PY&#39;
import sys, tkinter as t
W,H,X,Y = map(int, sys.argv[1:])
root = t.Tk()
root.overrideredirect(True)
root.geometry(f&quot;{W}x{H}+{X}+{Y}&quot;)
frame = t.Frame(root, width=W, height=H, highlightbackground=&#39;red&#39;, highlightthickness=8)
frame.pack()
root.attributes(&#39;-topmost&#39;, True)
root.after(5000, root.quit)
root.mainloop()
try:
    root.destroy()
except Exception:
    pass
PY
  local code=$?
  if (( code != 0 )); then log &quot;   [WARN] overlay exited non-zero ($code); continuing&quot;; fi
}

ask_ynq() {
  # Prompt to /dev/tty; read one char from /dev/tty; returns 0=y,1=n,2=q
  local prompt=&quot;$1&quot; ch
  while true; do
    printf &quot;%s&quot; &quot;$prompt&quot; &gt; /dev/tty
    IFS= read -r -n1 ch &lt; /dev/tty || ch=&quot;&quot;
    printf &quot;\n&quot; &gt; /dev/tty
    case &quot;$ch&quot; in
      y|Y) return 0;;
      n|N) return 1;;
      q|Q) return 2;;
      *) prompt=&quot;Please type y (accept), n (next), or q (quit): &quot;;;
    esac
  done
}

# ---------------- Main ----------------
log &quot;Starting interactive monitor configuration...&quot;

mapfile -t outputs &lt; &lt;(get_outputs)
((${#outputs[@]})) || fail &quot;No connected monitors detected&quot;

mapfile -t sorted &lt; &lt;(sort_outputs)

for out in &quot;${sorted[@]}&quot;; do
  log &quot;======== Configuring $out ========&quot;
  mapfile -t avail &lt; &lt;(get_modes_for_output &quot;$out&quot;)
  if ((${#avail[@]}==0)); then log &quot;No modes found for $out, skipping&quot;; continue; fi

  mapfile -t testlist &lt; &lt;(printf &#39;%s\n&#39; &quot;${avail[@]}&quot; | pick_testlist)
  if ((${#testlist[@]}==0)); then log &quot;No usable candidates for $out, skipping&quot;; continue; fi

  sel=&quot;&quot;; sel_w=0; sel_h=0

  for mode in &quot;${testlist[@]}&quot;; do
    [[ -z &quot;$mode&quot; ]] &amp;&amp; continue
    log &quot;---&gt; Trying $mode for $out&quot;
    if ! xrandr --output &quot;$out&quot; --mode &quot;$mode&quot;; then log &quot;    Could not apply $mode&quot;; continue; fi
    sleep 0.2

    geom=$(current_geometry &quot;$out&quot; || true)
    if [[ ! $geom =~ ^([0-9]+)x([0-9]+)\+([0-9]+)\+([0-9]+)$ ]]; then
      log &quot;    Could not parse geometry; skipping&quot;
      continue
    fi
    W=${BASH_REMATCH[1]}; H=${BASH_REMATCH[2]}; X=${BASH_REMATCH[3]}; Y=${BASH_REMATCH[4]}

    show_rectangle &quot;$W&quot; &quot;$H&quot; &quot;$X&quot; &quot;$Y&quot;

    ask_ynq &quot;Was the red rectangle fully enclosed on $out at $mode? [y=accept / n=next / q=quit] &quot;; rc=$?
    case &quot;$rc&quot; in
      0) sel=&quot;$mode&quot;; sel_w=$W; sel_h=$H; break ;;
      1) ;;  # try next
      2) log &quot;User aborted&quot;; exit 1 ;;
    esac
  done

  if [[ -z &quot;$sel&quot; ]]; then
    log &quot;!! No accepted mode for $out. Falling back to ${testlist[0]}&quot;
    mode=&quot;${testlist[0]}&quot;
    if xrandr --output &quot;$out&quot; --mode &quot;$mode&quot;; then
      sleep 0.2
      geom=$(current_geometry &quot;$out&quot; || true)
      if [[ $geom =~ ^([0-9]+)x([0-9]+)\+([0-9]+)\+([0-9]+)$ ]]; then
        sel_w=${BASH_REMATCH[1]}; sel_h=${BASH_REMATCH[2]}; sel=&quot;$mode&quot;
      else
        log &quot;!! Could not determine geometry for fallback; skipping monitor&quot;
        continue
      fi
    else
      log &quot;!! Fallback apply failed; skipping monitor&quot;
      continue
    fi
  fi

  FINAL_MODE[&quot;$out&quot;]=&quot;$sel&quot;
  WIDTH[&quot;$out&quot;]=$sel_w
  HEIGHT[&quot;$out&quot;]=$sel_h

done

# ---------------- Final apply ----------------
log &quot;Applying selected monitor configuration...&quot;

pos=0
primary_out=&quot;${sorted[0]}&quot;; maxarea=0

for out in &quot;${sorted[@]}&quot;; do
  mode=&quot;${FINAL_MODE[$out]:-}&quot;
  if [[ -z &quot;$mode&quot; ]]; then log &quot;Skipping $out (no selected mode)&quot;; continue; fi
  xrandr --output &quot;$out&quot; --mode &quot;$mode&quot; --pos &quot;${pos}x0&quot;
  area=$(( WIDTH[$out] * HEIGHT[$out] ))
  if (( area &gt; maxarea )); then maxarea=$area; primary_out=&quot;$out&quot;; fi
  pos=$(( pos + WIDTH[$out] ))
  sleep 0.1
done

xrandr --output &quot;$primary_out&quot; --primary || true
log &quot;[SUCCESS] Configuration complete. Primary: $primary_out&quot;

# ---------------- Autostart (optional) ----------------
if (( INSTALL_AUTOSTART )); then
  TARGET_HOME=${TARGET_HOME:-$(getent passwd &quot;$TARGET_USER&quot; | cut -d: -f6 || echo &quot;/home/$TARGET_USER&quot;)}
  AUTOSTART_DIR=&quot;$TARGET_HOME/.config/autostart&quot;
  SCRIPT_PATH=&quot;$TARGET_HOME/monitor-setup-refactored-fixed.sh&quot;
  AUTOSTART_FILE=&quot;$AUTOSTART_DIR/monitor-setup-refactored-fixed.desktop&quot;

  mkdir -p &quot;$AUTOSTART_DIR&quot;
  cp -- &quot;$0&quot; &quot;$SCRIPT_PATH&quot; || true
  chmod +x &quot;$SCRIPT_PATH&quot;
  if [[ $(id -u) -eq 0 ]]; then chown -R &quot;$TARGET_USER:$TARGET_USER&quot; &quot;$TARGET_HOME/.config&quot; &quot;$SCRIPT_PATH&quot; || true; fi

  cat &gt; &quot;$AUTOSTART_FILE&quot; &lt;&lt;EOF
[Desktop Entry]
Type=Application
Exec=bash &quot;$SCRIPT_PATH&quot;
Hidden=false
NoDisplay=false
X-GNOME-Autostart-enabled=true
Name=Auto Monitor Setup
Comment=Autoconfigure monitors layout interactively at login
EOF
  chmod 644 &quot;$AUTOSTART_FILE&quot;
  if [[ $(id -u) -eq 0 ]]; then chown &quot;$TARGET_USER:$TARGET_USER&quot; &quot;$AUTOSTART_FILE&quot; || true; fi
  log &quot;[INFO] Autostart installed at $AUTOSTART_FILE for user $TARGET_USER&quot;
fi

exit 0</code>
</section>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">b3 - SELL</title><link href="https://ib.bsb.br/b3-sell/" rel="alternate" type="text/html" title="b3 - SELL" /><published>2025-10-06T00:00:00+00:00</published><updated>2025-10-06T20:42:20+00:00</updated><id>https://ib.bsb.br/b3-sell</id><content type="html" xml:base="https://ib.bsb.br/b3-sell/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You are an AI portfolio sell-scenario planner for B3/BOVESPA. Operate deterministically, never fabricate data, and always respond in Portuguese (pt-BR). Timezone: America/Sao_Paulo.

GOAL
- Receber entradas mínimas do usuário e gerar DOIS cenários de venda que, cada um, somem ≥ target_brl em receita bruta:
  A) “Realizar lucros”: prioriza ações que foram simultaneamente fortes em 30d e 12m.
  B) “Cortar perdas”: prioriza ações que foram simultaneamente fracas em 30d e 12m.
- Vender apenas quantidades INTEIRAS, sem venda a descoberto, ignorando taxas/IR por padrão.

INTERAÇÃO (UMA ÚNICA SOLICITAÇÃO DE ENTRADA, SE NECESSÁRIO)
- Se as entradas ainda não foram fornecidas, solicitar exatamente uma vez, aceitando UM dos formatos:
  JSON:
  {
    "positions": [
      {"ticker": "PETR4", "quantity": 200},
      {"ticker": "VALE3", "quantity": 50}
    ],
    "target_brl": 5000.00
  }
  CSV (e target_brl separado):
  ticker,quantity
  PETR4,200
  VALE3,50
  target_brl=5000.00

ENTRADAS VÁLIDAS
- positions: lista de objetos com ticker (B3) e quantity (inteiro ≥ 0).
- target_brl: número &gt; 0 em BRL.

COLETA DE DADOS (ONLINE, SEM INVENTAR NÚMEROS)
- Preferência: https://www.fundamentus.com.br
- Alternativas confiáveis se a preferência estiver indisponível: site da B3 e portais financeiros reconhecidos.
- Para CADA ticker:
  - Obter: preço atual (BRL), variação 30 dias (%), variação 12 meses (%).
  - Registrar em metadata por ticker:
    - source_url: URL exata consultada
    - fetched_at: data/hora de coleta (America/Sao_Paulo)
    - metrics_available: quais métricas foram obtidas (preço, 30d, 12m)
- Se um ticker não for encontrado, estiver suspenso ou sem preço (&gt; 0), reportar e excluí-lo dos rankings; ainda assim tentar atender o alvo com os demais.

PRÉ-CÁLCULO
- Para cada ticker i:
  - ret_30d[i], ret_12m[i], price[i] (BRL), qty[i] (int)
  - position_value[i] = price[i] * qty[i]
  - Validar price[i] &gt; 0 e qty[i] ≥ 0.

FALTA DE MÉTRICAS
- Se faltar apenas uma métrica (30d OU 12m): usar a métrica disponível; desempate por position_value (desc).
- Se faltarem ambas (30d E 12m) para um ticker: excluí-lo do ranking combinado; poderá entrar ao final, apenas como recurso, se necessário para atingir o alvo (com nota de limitação).
- Se todas as ações estiverem sem métricas de desempenho, reportar impossibilidade de aplicar o critério “duplo” e oferecer alternativa baseada apenas em preço/valor de posição (sujeito a confirmação do usuário).

CENÁRIO A — REALIZAR LUCROS (FORTES EM 30D E 12M)
- Ranking combinado (50/50):
  - rank30 = rank(desc, ret_30d)
  - rank12 = rank(desc, ret_12m)
  - score_A = rank30 + rank12
  - Ordenação: score_A asc; desempates (nesta ordem):
    1) maior position_value
    2) maior price_brl
    3) ticker em ordem alfabética asc
- Seleção (gananciosa com inteiros):
  - remaining = target_brl
  - Para cada ticker na ordem:
    - need = ceil(remaining / price)
    - sell = min(need, qty)
    - Se sell &lt;= 0, pular
    - revenue += sell * price; remaining -= sell * price
    - Parar assim que remaining &lt;= 0 (não vender além do necessário)

CENÁRIO B — CORTAR PERDAS (FRACOS EM 30D E 12M)
- Ranking combinado (50/50):
  - rank30w = rank(asc, ret_30d)
  - rank12w = rank(asc, ret_12m)
  - score_B = rank30w + rank12w
  - Ordenação e desempates idênticos ao Cenário A
- Seleção: mesma regra gananciosa com inteiros até remaining &lt;= 0

CASOS-LIMITE E ERROS
- Se a soma máxima possível de venda (∑ price * qty) &lt; target_brl:
  - Reportar claramente o máximo alcançável, listar as vendas correspondentes e o gap remanescente.
- Se dados online não puderem ser obtidos para um ticker, reportar e continuar com os demais.
- Se rounding criar leve “overshoot”, manter a menor venda que fez remaining &lt;= 0; não adicionar vendas extras.
- Nunca vender quantidade superior à disponível; nunca usar frações de ação.

SAÍDA (SEMPRE EM PORTUGUÊS)
1) JSON estruturado:
{
  "scenario_A": {
    "policy": "Realizar lucros — fortes em 30d e 12m (peso igual)",
    "items": [
      {"ticker": "...", "qty_sold": 0, "price_brl": 0.00, "revenue_brl": 0.00, "pct_of_position": 0.0, "note": "justificativa breve (ex.: top em 30d e 12m)"}
    ],
    "total_revenue_brl": 0.00
  },
  "scenario_B": {
    "policy": "Cortar perdas — fracos em 30d e 12m (peso igual)",
    "items": [
      {"ticker": "...", "qty_sold": 0, "price_brl": 0.00, "revenue_brl": 0.00, "pct_of_position": 0.0, "note": "justificativa breve (ex.: bottom em 30d e 12m)"}
    ],
    "total_revenue_brl": 0.00
  },
  "metadata": {
    "target_brl": 0.00,
    "currency": "BRL",
    "timestamp": "YYYY-MM-DDTHH:MM:SS-03:00",
    "sources": [
      {"ticker": "...", "source_url": "...", "fetched_at": "YYYY-MM-DDTHH:MM:SS-03:00", "metrics_available": ["price","30d","12m"]}
    ],
    "limitations": [
      "listar limitações encontradas, se houver (ex.: métrica ausente, ticker excluído, etc.)"
    ]
  }
}
2) Tabelas Markdown para cada cenário com colunas:
   Ticker | Qtde vendida | Preço (BRL) | Receita (BRL) | % da posição | Observação
3) Notas/avisos:
   - Análise operacional baseada em dados públicos; não constitui recomendação financeira.
   - Sem taxas/IR por padrão; sem venda a descoberto; liquidez presumida.

COMPORTAMENTO
- Pedir as entradas mínimas uma única vez se ausentes; caso já fornecidas, prosseguir diretamente.
- Não adiar resultados nem prometer execução futura: realizar a busca e o cálculo na própria resposta.
- Não inventar dados; se indisponíveis após tentativa de múltiplas fontes confiáveis, declarar explicitamente.
</code></pre></div></div>]]></content><author><name></name></author><category term="AI&gt;prompt" /></entry><entry><title type="html">FIFA Two‑Button Controls</title><link href="https://ib.bsb.br/fifa-twobutton-controls/" rel="alternate" type="text/html" title="FIFA Two‑Button Controls" /><published>2025-10-05T00:00:00+00:00</published><updated>2025-10-05T00:39:38+00:00</updated><id>https://ib.bsb.br/fifa-twobutton-controls</id><content type="html" xml:base="https://ib.bsb.br/fifa-twobutton-controls/"><![CDATA[<h3 id="ea-sports-fc-25--twobutton-controls-conceptual-map">EA Sports FC 25 • Two‑Button Controls (Conceptual Map)</h3>

<blockquote>
  <p>Use <strong>generic names</strong> below; hardware labels differ by platform. Typical equivalence:
<strong>PASS</strong> ≈ (PlayStation: ✕) / (Xbox: A) | <strong>SHOOT</strong> ≈ (PlayStation: ○) / (Xbox: B)</p>
</blockquote>

<p><strong>Attack</strong></p>

<ul>
  <li><strong>Move/Aim:</strong> Left Stick.</li>
  <li><strong>PASS (tap):</strong> Short ground pass.</li>
  <li><strong>PASS (hold):</strong> Longer/lofted pass or lead into space (angle with Left Stick).</li>
  <li><strong>SHOOT (tap/hold):</strong> Shoot on goal. When wide/in crossing zones, <strong>SHOOT</strong> acts as <strong>cross</strong>.</li>
  <li><strong>Clearance:</strong> Long <strong>SHOOT</strong> from deep.</li>
</ul>

<p><strong>Defense</strong></p>

<ul>
  <li><strong>PASS (tap/hold):</strong> Close down/stand tackle (contain then poke).</li>
  <li><strong>SHOOT (tap):</strong> Slide tackle (last resort).</li>
  <li><strong>SHOOT (hold):</strong> Big clearance under pressure.</li>
</ul>

<blockquote>
  <p>Why this mapping? It preserves the <strong>“stand‑tackle first, slide rarely”</strong> and <strong>“cross/finish with same strike input by context”</strong> ideas, aligning with the source’s conservative, percentage‑play philosophy.</p>
</blockquote>

<hr />

<h3 id="10second-match-plan-twobutton-edition">10‑Second Match Plan (Two‑Button edition)</h3>

<p>(Adapted from the source’s 10‑second plan while removing advanced inputs. )</p>

<ol>
  <li><strong>Tempo discipline:</strong> Glide with the Left Stick; add tiny bursts by pushing further, then settle.</li>
  <li><strong>Pattern out of defense:</strong> <strong>PASS → PASS → (angle) PASS long</strong> up the channel or <strong>SHOOT (cross)</strong> if wide.</li>
  <li><strong>Around the box:</strong> Aim with Left Stick; <strong>SHOOT</strong> with <strong>2–3 bars</strong>—still the right power discipline idea from the source.</li>
  <li><strong>Defend first by shape:</strong> Stay goal‑side with Left Stick; <strong>PASS</strong> to contain, <strong>tap PASS</strong> to poke; <strong>SHOOT</strong> slide only if the angle is clean.</li>
  <li><strong>Wing outlet:</strong> If trapped, funnel the ball wide with <strong>PASS</strong>, then <strong>SHOOT</strong> to cross.</li>
</ol>

<hr />

<h3 id="onball-microrules-twobutton-friendly">On‑Ball Micro‑Rules (Two‑Button friendly)</h3>

<ul>
  <li><strong>Change pace:</strong> walk → burst → settle (Left‑Stick pacing). Don’t “turbo everywhere”—a direct carry‑over.</li>
  <li><strong>One easy fake:</strong> <strong>Left‑Stick feint</strong>—a small lateral touch, then <strong>PASS</strong> or <strong>SHOOT</strong>.</li>
  <li>
    <p><strong>Power discipline:</strong></p>

    <ul>
      <li><strong>Shots:</strong> 2–3 bars.</li>
      <li><strong>Crosses:</strong> moderate hold on <strong>SHOOT</strong>.</li>
      <li><strong>Passes:</strong> tap/hold by distance (aim ahead of runners).
(This restates the source’s power heuristics with two buttons. )</li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="defending-that-works-twobutton-edition">Defending That Works (Two‑Button edition)</h3>

<ul>
  <li><strong>Shape first:</strong> Mirror attacker with Left Stick; <strong>hold PASS</strong> to contain; add a quick <strong>tap PASS</strong> when the ball leaves their foot. Same sequencing principle as the source.</li>
  <li><strong>Block the lane:</strong> Stand between passer and runner; take a <strong>diagonal</strong> angle to cut the obvious ball (adapts “kill the through‑ball lane”).</li>
  <li><strong>Tackle timing:</strong> <strong>Tap PASS</strong> when the touch is loose. <strong>SHOOT</strong> slide only at good angles or as last resort—explicitly lifted from the source’s caution.</li>
</ul>

<hr />

<h3 id="transitions">Transitions</h3>

<ul>
  <li><strong>Won it?</strong> First idea: <strong>one‑touch PASS</strong> out of pressure, then a longer <strong>PASS (hold)</strong> into space or a <strong>SHOOT cross</strong> if wide. Mirrors “one touch out → vertical” from the PDF.</li>
  <li><strong>Lost it?</strong> Protect central space in front of your CBs; contain with <strong>PASS</strong>, don’t drag centre‑backs. (Same “select CDM, protect zone” principle, minus manual switch requirements.)</li>
</ul>

<hr />

<h3 id="setpiece-quick-wins-twobutton">Set‑Piece Quick Wins (Two‑Button)</h3>

<ul>
  <li><strong>Corners (right‑footer on left):</strong> <strong>Short PASS</strong>, immediate <strong>return PASS</strong>, then <strong>SHOOT</strong> (cross) to the near‑post run. This mirrors the source’s “short → return → low/near” pattern in a simplified way.</li>
  <li><strong>Direct Free Kicks (20–25m):</strong> Aim a touch outside far post, <strong>moderate SHOOT</strong>, or <strong>PASS short</strong> if the wall/angle is poor (adapts the source’s guidance minus finesse input).</li>
  <li><strong>Goal‑kicks/under heavy press:</strong> <strong>PASS</strong> to a full‑back or <strong>SHOOT</strong> long to reset.</li>
</ul>

<hr />

<h2 id="two-alwayson-tactics-for-muscle-memory">Two “Always‑On” Tactics for Muscle Memory</h2>

<blockquote>
  <p>The goal is <strong>repeatability</strong>: two simple blueprints you can run all match until they become automatic.</p>
</blockquote>

<h3 id="a-2pass--strike-2ps--your-breadandbutter-attack-loop">A. <strong>“2‑Pass &amp; Strike” (2PS)</strong> — your bread‑and‑butter attack loop</h3>

<p><strong>When:</strong> Any settled possession in midfield.
<strong>Loop (four beats):</strong></p>

<ol>
  <li><strong>Square up</strong>—face forward with Left Stick; don’t over‑dribble.</li>
  <li><strong>PASS</strong> to the nearest support (turn body to the target first).</li>
  <li><strong>PASS</strong> again immediately to the next forward option (or back to the first if pressure bites).</li>
  <li>
    <p>Now <strong>commit</strong>:</p>

    <ul>
      <li>If central and 20–25m: <strong>SHOOT</strong> (2–3 bars).</li>
      <li>If the ball gets wide: <strong>SHOOT</strong> for a cross.
<strong>Why it works:</strong> It compresses the source’s “look for triangles, power discipline” into a two‑button rhythm—<strong>Pass → Pass → Decide (Shoot/Cross)</strong>.</li>
    </ul>
  </li>
</ol>

<p><strong>Checklist to cue the move</strong></p>

<ul>
  <li>See a <strong>triangle</strong> (you + two teammates)? Run 2PS.</li>
  <li>If no lane after 2 passes: recycle and <em>restart</em> 2PS.</li>
  <li>Don’t take a third dribble—<strong>make the third action a finish or cross</strong>.</li>
</ul>

<hr />

<h3 id="b-funnel--break-fb--defend-smart-counter-simple">B. <strong>“Funnel &amp; Break” (F&amp;B)</strong> — defend smart, counter simple</h3>

<p><strong>When:</strong> Opponent builds through the middle.
<strong>Loop (four beats):</strong></p>

<ol>
  <li><strong>Funnel:</strong> Use Left Stick to sit <strong>goal‑side and slightly to their strong foot</strong>; <strong>hold PASS</strong> to contain—guide them wide.</li>
  <li><strong>Win it:</strong> Time a <strong>tap PASS</strong> when the touch is away from the foot.</li>
  <li><strong>First exit:</strong> <strong>Immediate PASS</strong> to a free midfielder or full‑back.</li>
  <li><strong>Break:</strong> <strong>PASS (hold)</strong> up the channel or <strong>SHOOT cross</strong> early if your wide player is free.
<strong>Why it works:</strong> It re‑uses the source’s “shape first, kill lanes, one‑touch out then vertical” without advanced defensive inputs.</li>
</ol>

<p><strong>Checklist to cue the move</strong></p>

<ul>
  <li>If they face you centrally: <strong>Funnel</strong> (shoulders angled to show outside).</li>
  <li>If you win it near touchline: <strong>early SHOOT cross</strong>—don’t dribble back inside.</li>
</ul>

<hr />

<h2 id="short-repeatable-practice-drills">Short, Repeatable Practice Drills</h2>

<blockquote>
  <p>Total: ~10 minutes to ingrain 2PS and F&amp;B.</p>
</blockquote>

<ol>
  <li>
    <p><strong>2‑Minute Triangle Rondo (solo vs CPU, Amateur):</strong></p>

    <ul>
      <li>Only <strong>PASS</strong>; every third pass must aim <strong>forward</strong>.</li>
      <li>Add one <strong>SHOOT</strong> finish every minute from the D with 2–3 bars.
(Reinforces “triangles + power discipline”.)</li>
    </ul>
  </li>
  <li>
    <p><strong>3‑Minute Wing Cross Ladder:</strong></p>

    <ul>
      <li>Receive wide → one touch inside → <strong>SHOOT cross</strong>; finish with near‑post run.</li>
      <li>Focus on <strong>moderate cross power</strong>; don’t over‑hit.</li>
    </ul>
  </li>
  <li>
    <p><strong>3‑Minute Contain Drill (Two‑Button version of the source’s “3‑minute jockey drill”):</strong></p>

    <ul>
      <li><strong>0–1 min:</strong> Left‑Stick mirror only; stay half‑step away, goal‑side.</li>
      <li><strong>1–2 min:</strong> Add <strong>PASS (hold)</strong> contain, release as you close.</li>
      <li><strong>2–3 min:</strong> <strong>Tap PASS</strong> at the loose touch; if you miss, reset shape first.
(Adapts the original drill’s progression.)</li>
    </ul>
  </li>
  <li>
    <p><strong>2‑Minute Funnel &amp; Break Reps:</strong></p>

    <ul>
      <li>Start central → guide wide (contain) → win it → <strong>PASS</strong> out → <strong>PASS (hold)</strong> long.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="troubleshooting--common-mistakes">Troubleshooting &amp; Common Mistakes</h2>

<ul>
  <li><strong>Over‑dribbling:</strong> The playbook is <strong>two passes then a decision</strong>. If you hit a third dribble, you’re likely late.</li>
  <li><strong>Shot power too high:</strong> Cap yourself to <strong>2–3 bars</strong> unless it’s a cut‑back sitter. Source emphasis retained.</li>
  <li><strong>Sliding early:</strong> If you press <strong>SHOOT</strong> before angling the run, you’ll foul or miss. Slide only on clear angles—same last‑resort warning as the source.</li>
  <li><strong>Forcing the middle:</strong> If the lane is blocked, <strong>funnel your own attack wide</strong>: PASS to the flank, <strong>SHOOT cross</strong>.</li>
</ul>

<hr />

<h2 id="optional-helpful-prematch-tweaks">Optional, Helpful Pre‑Match Tweaks</h2>

<ul>
  <li><strong>Camera:</strong> Tele Broadcast (around Height ≈ 12, Zoom ≈ 5), matching the source’s suggestion for clarity.</li>
  <li>
    <p><strong>Simple instructions to favor the patterns:</strong></p>

    <ul>
      <li>Strikers: <strong>Get In Behind</strong> (creates the forward lane for 2PS).</li>
      <li>CDM(s): <strong>Stay Back</strong> (keeps the shield for F&amp;B).
(These are the spirit of the source’s optional notes.)</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="onepage-summary-printable-text-block">One‑Page Summary (printable text block)</h2>

<p><strong>EA Sports FC 25 — Two‑Button Cheat Card (PASS + SHOOT)</strong></p>

<ul>
  <li><strong>Plan:</strong> 2PS = <strong>PASS → PASS → SHOOT/CROSS</strong>. F&amp;B = <strong>Contain → Win → PASS Out → Long PASS/Cross</strong>.</li>
  <li><strong>On ball:</strong> left‑stick pace; triangles; shots <strong>2–3 bars</strong>.</li>
  <li><strong>Defend:</strong> contain with <strong>PASS</strong>; <strong>tap PASS</strong> to poke; <strong>SHOOT</strong> slide only late/clean.</li>
  <li><strong>Transitions:</strong> one‑touch <strong>PASS</strong> out; go vertical; wide? <strong>SHOOT cross</strong>.</li>
  <li><strong>Set pieces:</strong> short <strong>PASS</strong>; return; <strong>SHOOT</strong> (cross) near post.</li>
  <li><strong>If stuck:</strong> recycle and <strong>restart 2PS</strong>; or switch wide and cross.</li>
</ul>

<hr />

<h2 id="og4--weaknesses--tradeoffs-explicit">OG4 — Weaknesses / trade‑offs (explicit)</h2>

<ul>
  <li>Two‑Button removes granular tools (e.g., manual jockey, second‑man press, finesse shots). You trade precision for <strong>simplicity + repetition</strong>. The frameworks above compensate by emphasizing <strong>angles, timing, and power discipline</strong>, all preserved from the source.</li>
</ul>

<hr />

<h2 id="og5--next-iterations">OG5 — Next iterations</h2>

<p>If you want, I can condense this into a true <strong>single‑page printable</strong> or tailor the drills to your preferred formation. For now, run <strong>2PS</strong> in possession and <strong>F&amp;B</strong> out of possession until they become automatic.</p>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">Fireworks AI - Whisper API</title><link href="https://ib.bsb.br/fireworks-ai-whisper-api/" rel="alternate" type="text/html" title="Fireworks AI - Whisper API" /><published>2025-10-05T00:00:00+00:00</published><updated>2025-10-05T19:23:54+00:00</updated><id>https://ib.bsb.br/fireworks-ai-whisper-api</id><content type="html" xml:base="https://ib.bsb.br/fireworks-ai-whisper-api/"><![CDATA[<table>
  <tbody>
    <tr>
      <td>Whisper V3 Turbo API &amp; Playground</td>
      <td>Fireworks AI</td>
    </tr>
  </tbody>
</table>

<p>===============</p>

<p><a href="https://fireworks.ai/startup-program">Join the <strong>Fireworks Startups Program</strong> and unlock credits, expert support, and community to scale fast. Join here</a></p>

<p><a href="https://fireworks.ai/"><img src="https://cdn.sanity.io/images/pv37i0yn/production/2095949f01e1cf9cf0841096b67d547d9dbfba2e-215x24.svg" alt="Image 3: Fireworks Logo" /></a></p>

<ul>
  <li>Platform</li>
  <li>Models</li>
  <li>Developers</li>
  <li><a href="https://fireworks.ai/pricing">Pricing</a></li>
  <li>Partners</li>
  <li>Resources</li>
  <li>Company</li>
</ul>

<p><a href="https://fireworks.ai/login">Log In</a><a href="https://fireworks.ai/signup">Get Started</a></p>

<p><img src="https://cdn.sanity.io/images/pv37i0yn/production/f549e0e24c286535bc06df0b0310d270183469e8-40x40.svg" alt="Image 4: OpenAi Logo MArk" /></p>

<h1 id="whisper-v3-turbo">Whisper V3 Turbo</h1>

<p>Whisper large-v3-turbo is a finetuned version of a pruned Whisper large-v3. In other words, it’s the exact same model, except that the number of decoding layers have reduced from 32 to 4. As a result, the model is way faster, at the expense of a minor quality degradation.</p>

<p><a href="https://app.fireworks.ai/playground?model=accounts/fireworks/models/whisper-v3-turbo">Try Model</a></p>

<h2 id="fireworks-features">Fireworks Features</h2>

<h3 id="serverless">Serverless</h3>

<p>Immediately run model on pre-configured GPUs and pay-per-token</p>

<p><a href="https://docs.fireworks.ai/getting-started/quickstart">Learn More</a></p>

<h3 id="on-demand-deployment">On-demand Deployment</h3>

<p>On-demand deployments give you dedicated GPUs for Whisper V3 Turbo using Fireworks’ reliable, high-performance system with no rate limits.</p>

<p><a href="https://fireworks.ai/docs/guides/ondemand-deployments">Learn More</a></p>

<h2 id="whisper-v3-turbo-faqs">Whisper V3 Turbo FAQs</h2>

<h3 id="what-is-whisper-v3-turbo-and-who-developed-it">What is Whisper V3 Turbo and who developed it?</h3>

<h3 id="what-applications-and-use-cases-does-whisper-v3-turbo-excel-at">What applications and use-cases does Whisper V3 Turbo excel at?</h3>

<h3 id="what-is-the-maximum-context-length-for-whisper-v3-turbo">What is the maximum context length for Whisper V3 Turbo?</h3>

<h3 id="what-is-the-usable-context-window">What is the usable context window?</h3>

<h3 id="what-are-known-failure-modes-of-whisper-v3-turbo">What are known failure modes of Whisper V3 Turbo?</h3>

<h3 id="how-many-parameters-does-whisper-v3-turbo-have">How many parameters does Whisper V3 Turbo have?</h3>

<h3 id="what-license-governs-commercial-use-of-whisper-v3-turbo">What license governs commercial use of Whisper V3 Turbo?</h3>

<h2 id="info--pricing">Info &amp; Pricing</h2>

<h3 id="provider">Provider</h3>

<p>OpenAI</p>

<h3 id="model-type">Model Type</h3>

<p>Audio</p>

<h3 id="serverless-1">Serverless</h3>

<p>Available</p>

<h3 id="pricing-per-minute">Pricing Per Minute</h3>

<p>$0.0009</p>

<hr />

<p><img src="https://cdn.sanity.io/images/pv37i0yn/production/2095949f01e1cf9cf0841096b67d547d9dbfba2e-215x24.svg" alt="Image 5: Fireworks Logo" /></p>

<h3 id="platform">Platform</h3>

<p><a href="https://fireworks.ai/ai-native">AI Native</a><a href="https://fireworks.ai/enterprise">Enterprise</a><a href="https://fireworks.ai/customers">Customers</a></p>

<h3 id="use-cases">Use Cases</h3>

<p><a href="https://fireworks.ai/usecases/code-assistance">Code Assistance</a><a href="https://fireworks.ai/conversational-ai">Conversational AI</a><a href="https://fireworks.ai/usecases/agentic-systems">Agentic Systems</a><a href="https://fireworks.ai/usecases/search">Search</a><a href="https://fireworks.ai/usecases/multimedia">Multimedia</a><a href="https://fireworks.ai/usecases/enterprise-rag">Enterprise RAG</a></p>

<h3 id="developers">Developers</h3>

<p><a href="https://fireworks.ai/models">Model Library</a><a href="https://docs.fireworks.ai/getting-started/introduction">Docs</a><a href="https://docs.fireworks.ai/tools-sdks/firectl/firectl">CLI</a><a href="https://docs.fireworks.ai/api-reference/introduction">API</a><a href="https://docs.fireworks.ai/updates/changelog">Changelog</a></p>

<h3 id="pricing">Pricing</h3>

<p><a href="https://fireworks.ai/pricing#serverless-pricing">Serverless</a><a href="https://fireworks.ai/pricing#on-demand-pricing">On-Demand</a><a href="https://fireworks.ai/pricing#fine-tuning-pricing">Fine Tuning</a><a href="https://fireworks.ai/contact-reserved">Enterprise</a></p>

<h3 id="partners">Partners</h3>

<p><a href="https://fireworks.ai/partners#cloud-infra">Cloud and Infrastructure</a><a href="https://fireworks.ai/partners#consulting">Consulting and Services</a><a href="https://fireworks.ai/partners#technology">Technology</a><a href="https://fireworks.ai/startup-program">Fireworks for Startups</a></p>

<h3 id="resources">Resources</h3>

<p><a href="https://fireworks.ai/blog">Blog</a><a href="https://demos.fireworks.ai/">Demos</a><a href="https://docs.fireworks.ai/examples/introduction">Cookbooks</a></p>

<h3 id="company">Company</h3>

<p><a href="https://fireworks.ai/team">Leadership</a><a href="https://fireworks.ai/team#investors">Investors</a><a href="https://job-boards.greenhouse.io/fireworksai">Careers</a><a href="https://trust.fireworks.ai/">Trust Center</a></p>

<p>© 2025 Fireworks AI, Inc. All rights reserved.</p>

<p><a href="https://x.com/FireworksAI_HQ"></a><a href="https://www.youtube.com/@fireworksai"></a><a href="https://www.linkedin.com/company/fireworks-ai"></a><a href="https://discord.gg/fireworks"></a></p>

<p>–</p>

<h1 id="transcribe-audio">Transcribe audio</h1>

<p>&lt;CardGroup cols={1}&gt;</p>
<Card title="Try notebook" icon="rocket" href="https://colab.research.google.com/github/fw-ai/cookbook/blob/main/learn/audio/audio_prerecorded_speech_to_text/audio_prerecorded_speech_to_text.ipynb">
    Send a sample audio to get a transcription.
  </Card>
<p>&lt;/CardGroup&gt;</p>

<h3 id="request">Request</h3>

<h5 id="multi-part-form">(multi-part form)</h5>

<ParamField query="file" type="file | string" required="">
  The input audio file to transcribe or an URL to the public audio file.

  Max audio file size is 1 GB, there is no limit for audio duration. Common file formats such as mp3, flac, and wav are supported. Note that the audio will be resampled to 16kHz, downmixed to mono, and reformatted to 16-bit signed little-endian format before transcription. Pre-converting the file before sending it to the API can improve runtime performance.
</ParamField>

<ParamField query="model" type="string" default="whisper-v3" optional="">
  String name of the ASR model to use. Can be one of `whisper-v3` or `whisper-v3-turbo`. Please use the following serverless endpoints:

  * [https://audio-prod.us-virginia-1.direct.fireworks.ai](https://audio-prod.us-virginia-1.direct.fireworks.ai) (for `whisper-v3`);
  * [https://audio-turbo.us-virginia-1.direct.fireworks.ai](https://audio-turbo.us-virginia-1.direct.fireworks.ai) (for `whisper-v3-turbo`);
</ParamField>

<ParamField query="vad_model" type="string" default="silero" optional="">
  String name of the voice activity detection (VAD) model to use. Can be one of `silero`, or `whisperx-pyannet`.
</ParamField>

<ParamField query="alignment_model" type="string" default="mms_fa" optional="">
  String name of the alignment model to use. Currently supported:

  * `mms_fa` optimal accuracy for multilingual speech.
  * `tdnn_ffn` optimal accuracy for English-only speech.
  * `gentle` best accuracy for English-only speech (requires a dedicated endpoint, contact us at <a href="mailto:inquiries@fireworks.ai">[inquiries@fireworks.ai](mailto:inquiries@fireworks.ai)</a>).
</ParamField>

<ParamField query="language" type="string | null" optional="">
  The target language for transcription. See the [Supported Languages](#supported-languages) section below for a complete list of available languages.
</ParamField>

<ParamField query="prompt" type="string | null" optional="">
  The input prompt that the model will use when generating the transcription. Can be used to specify custom words or specify the style of the transcription. E.g. `Um, here's, uh, what was recorded.` will make the model to include the filler words into the transcription.
</ParamField>

<ParamField query="temperature" type="float | list[float]" default="0">
  Sampling temperature to use when decoding text tokens during transcription. Alternatively, fallback decoding can be enabled by passing a list of temperatures like `0.0,0.2,0.4,0.6,0.8,1.0`. This can help to improve performance.
</ParamField>

<ParamField query="response_format" type="string" default="json">
  The format in which to return the response. Can be one of `json`, `text`, `srt`, `verbose_json`, or `vtt`.
</ParamField>

<ParamField query="timestamp_granularities" type="string | list[string]" optional="">
  The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported. Can be one of `word`, `segment`, or `word,segment`. If not present, defaults to `segment`.
</ParamField>

<ParamField query="diarize" type="string" optional="">
  Whether to get speaker diarization for the transcription. Can be one of `true`, or `false`. If not present, defaults to `false`.

  Enabling diarization also requires other fields to hold specific values:

  1. `response_format` must be set `verbose_json`.
  2. `timestamp_granularities` must include `word` to use diarization.
</ParamField>

<ParamField query="min_speakers" type="int" optional="">
  The minimum number of speakers to detect for diarization. `diarize` must be set `true` to use `min_speakers`. If not present, defaults to `1`.
</ParamField>

<ParamField query="max_speakers" type="int" optional="">
  The maximum number of speakers to detect for diarization. `diarize` must be set `true` to use `max_speakers`. If not present, defaults to `inf`.
</ParamField>

<ParamField query="preprocessing" type="string" optional="">
  Audio preprocessing mode. Currently supported:

  * `none` to skip audio preprocessing.
  * `dynamic` for arbitrary audio content with variable loudness.
  * `soft_dynamic` for speech intense recording such as podcasts and voice-overs.
  * `bass_dynamic` for boosting lower frequencies;
</ParamField>

<h3 id="response">Response</h3>

<Tabs>
  <Tab title="json/text/srt/vtt">
    <ResponseField name="text" type="string" required="" />
  </Tab>

  <Tab title="verbose_json">
    <ResponseField name="task" type="string" default="transcribe" required="">
      The task which was performed. Either `transcribe` or `translate`.
    </ResponseField>

    <ResponseField name="language" type="string" required="">
      The language of the transcribed/translated text.
    </ResponseField>

    <ResponseField name="duration" type="number" required="">
      The duration of the transcribed/translated audio, in seconds.
    </ResponseField>

    <ResponseField name="text" type="string" required="">
      The transcribed/translated text.
    </ResponseField>

    <ResponseField name="words" type="object[] | null" optional="">
      Extracted words and their corresponding timestamps.

      <Expandable title="Word properties">
        <ResponseField name="word" type="string" required="">
          The text content of the word.
        </ResponseField>

        <ResponseField name="language" type="string" required="">
          The language of the word.
        </ResponseField>

        <ResponseField name="probability" type="number" required="">
          The probability of the word.
        </ResponseField>

        <ResponseField name="hallucination_score" type="number" required="">
          The hallucination score of the word.
        </ResponseField>

        <ResponseField name="start" type="number" required="">
          Start time of the word in seconds.
        </ResponseField>

        <ResponseField name="end" type="number" required="">
          End time of the word in seconds.
        </ResponseField>

        <ResponseField name="speaker_id" type="string" optional="">
          Speaker label for the word.
        </ResponseField>
      </Expandable>
    </ResponseField>

    <ResponseField name="segments" type="object[] | null" optional="">
      Segments of the transcribed/translated text and their corresponding details.

      <Expandable title="Segment properties (partial)">
        <ResponseField name="id" type="number" required="">
          The id of the segment.
        </ResponseField>

        <ResponseField name="text" type="string" required="">
          The text content of the segment.
        </ResponseField>

        <ResponseField name="start" type="number" required="">
          Start time of the segment in seconds.
        </ResponseField>

        <ResponseField name="end" type="number" required="">
          End time of the segment in seconds.
        </ResponseField>

        <ResponseField name="speaker_id" type="string" optional="">
          Speaker label for the segment.
        </ResponseField>

        <ResponseField name="words" type="object[] | null" optional="">
          Extracted words in the segment.
        </ResponseField>
      </Expandable>
    </ResponseField>
  </Tab>
</Tabs>

<RequestExample>
  ```curl curl

  # Download audio file
  curl -L -o "audio.flac" "https://tinyurl.com/4997djsh"

  # Make request
  curl -X POST "https://audio-prod.us-virginia-1.direct.fireworks.ai/v1/audio/transcriptions" \
  -H "Authorization: <FIREWORKS_API_KEY>" \
  -F "file=@audio.flac"
  ```

  ```python fireworks sdk
  !pip install fireworks-ai requests python-dotenv

  from fireworks.client.audio import AudioInference
  import requests
  import os
  from dotenv import load_dotenv
  import time

  # Create a .env file with your API key
  load_dotenv()


  # Download audio sample
  audio = requests.get("https://tinyurl.com/4cb74vas").content

  # Prepare client
  client = AudioInference(
      model="whisper-v3",
      base_url="https://audio-prod.us-virginia-1.direct.fireworks.ai",
      # Or for the turbo version
      # model="whisper-v3-turbo",
      # base_url="https://audio-turbo.us-virginia-1.direct.fireworks.ai",
      api_key=os.getenv("FIREWORKS_API_KEY"),
  )

  # Make request
  start = time.time()
  r = await client.transcribe_async(audio=audio)
  print(f"Took: {(time.time() - start):.3f}s. Text: '{r.text}'")
  ```

  ```python Python (openai sdk)
  !pip install openai requests python-dotenv

  from openai import OpenAI
  import os
  import requests
  from dotenv import load_dotenv

  load_dotenv()

  client = OpenAI(
      base_url="https://audio-prod.us-virginia-1.direct.fireworks.ai/v1",
      api_key=os.getenv("FIREWORKS_API_KEY")
      )
  audio_file= requests.get("https://tinyurl.com/4cb74vas").content
  transcription = client.audio.transcriptions.create(
      model="whisper-v3",
      file=audio_file
  )
  print(transcription.text)
  ```
&lt;/RequestExample&gt;

### Supported Languages

The following languages are supported for transcription:

<Accordion title="Language Code &amp; Name">
  | Language Code | Language Name       |
  | ------------- | ------------------- |
  | en            | English             |
  | zh            | Chinese             |
  | de            | German              |
  | es            | Spanish             |
  | ru            | Russian             |
  | ko            | Korean              |
  | fr            | French              |
  | ja            | Japanese            |
  | pt            | Portuguese          |
  | tr            | Turkish             |
  | pl            | Polish              |
  | ca            | Catalan             |
  | nl            | Dutch               |
  | ar            | Arabic              |
  | sv            | Swedish             |
  | it            | Italian             |
  | id            | Indonesian          |
  | hi            | Hindi               |
  | fi            | Finnish             |
  | vi            | Vietnamese          |
  | he            | Hebrew              |
  | uk            | Ukrainian           |
  | el            | Greek               |
  | ms            | Malay               |
  | cs            | Czech               |
  | ro            | Romanian            |
  | da            | Danish              |
  | hu            | Hungarian           |
  | ta            | Tamil               |
  | no            | Norwegian           |
  | th            | Thai                |
  | ur            | Urdu                |
  | hr            | Croatian            |
  | bg            | Bulgarian           |
  | lt            | Lithuanian          |
  | la            | Latin               |
  | mi            | Maori               |
  | ml            | Malayalam           |
  | cy            | Welsh               |
  | sk            | Slovak              |
  | te            | Telugu              |
  | fa            | Persian             |
  | lv            | Latvian             |
  | bn            | Bengali             |
  | sr            | Serbian             |
  | az            | Azerbaijani         |
  | sl            | Slovenian           |
  | kn            | Kannada             |
  | et            | Estonian            |
  | mk            | Macedonian          |
  | br            | Breton              |
  | eu            | Basque              |
  | is            | Icelandic           |
  | hy            | Armenian            |
  | ne            | Nepali              |
  | mn            | Mongolian           |
  | bs            | Bosnian             |
  | kk            | Kazakh              |
  | sq            | Albanian            |
  | sw            | Swahili             |
  | gl            | Galician            |
  | mr            | Marathi             |
  | pa            | Punjabi             |
  | si            | Sinhala             |
  | km            | Khmer               |
  | sn            | Shona               |
  | yo            | Yoruba              |
  | so            | Somali              |
  | af            | Afrikaans           |
  | oc            | Occitan             |
  | ka            | Georgian            |
  | be            | Belarusian          |
  | tg            | Tajik               |
  | sd            | Sindhi              |
  | gu            | Gujarati            |
  | am            | Amharic             |
  | yi            | Yiddish             |
  | lo            | Lao                 |
  | uz            | Uzbek               |
  | fo            | Faroese             |
  | ht            | Haitian Creole      |
  | ps            | Pashto              |
  | tk            | Turkmen             |
  | nn            | Nynorsk             |
  | mt            | Maltese             |
  | sa            | Sanskrit            |
  | lb            | Luxembourgish       |
  | my            | Myanmar             |
  | bo            | Tibetan             |
  | tl            | Tagalog             |
  | mg            | Malagasy            |
  | as            | Assamese            |
  | tt            | Tatar               |
  | haw           | Hawaiian            |
  | ln            | Lingala             |
  | ha            | Hausa               |
  | ba            | Bashkir             |
  | jw            | Javanese            |
  | su            | Sundanese           |
  | yue           | Cantonese           |
  | zh-hant       | Traditional Chinese |
  | zh-hans       | Simplified Chinese  |
</Accordion>


--

# Create Batch Request

&lt;CardGroup cols={1}&gt;
  <Card title="Try notebook" icon="rocket" href="https://colab.research.google.com/github/fw-ai/cookbook/blob/main/learn/batch-api/batch_api.ipynb">
    Create a batch request for our audio transcription service
  </Card>
&lt;/CardGroup&gt;

### Headers

<ParamField header="Authorization" type="string" required="">
  Your Fireworks API key, e.g. `Authorization=FIREWORKS_API_KEY`. Alternatively, can be provided as a query param.
</ParamField>

### Path Parameters

<ParamField query="path" type="string" required="">
  The relative route of the target API operation (e.g. `"v1/audio/transcriptions"`, `"v1/audio/translations"`). This should correspond to a valid route supported by the backend service.
</ParamField>

### Query Parameters

<ParamField query="endpoint_id" type="string" required="">
  Identifies the target backend service or model to handle the request. Currently supported:

  * `audio-prod`: [https://audio-prod.us-virginia-1.direct.fireworks.ai](https://audio-prod.us-virginia-1.direct.fireworks.ai)
  * `audio-turbo`: [https://audio-turbo.us-virginia-1.direct.fireworks.ai](https://audio-turbo.us-virginia-1.direct.fireworks.ai)
</ParamField>

### Body

Request body fields vary depending on the selected `endpoint_id` and `path`.

The request body must conform to the schema defined by the corresponding synchronous API.\
For example, transcription requests typically accept fields such as `model`, `diarize`, and `response_format`.\
Refer to the relevant synchronous API for required fields:

* [Transcribe audio](https://docs.fireworks.ai/api-reference/audio-transcriptions)
* [Translate audio](https://docs.fireworks.ai/api-reference/audio-translations)

### Response

<Tabs>
  <Tab title="json">
    <ResponseField name="status" type="string" required="">
      The status of the batch request submission.\
      A value of `"submitted"` indicates the batch request was accepted and queued for processing.
    </ResponseField>

    <ResponseField name="batch_id" type="string" required="">
      A unique identifier assigned to the batch job.
      This ID can be used to check job status or retrieve results later.
    </ResponseField>

    <ResponseField name="account_id" type="string" required="">
      The unique identifier of the account associated with the batch job.
    </ResponseField>

    <ResponseField name="endpoint_id" type="string" required="">
      The backend service selected to process the request.\
      This typically matches the `endpoint_id` used during submission.
    </ResponseField>

    <ResponseField name="message" type="string" optional="">
      A human-readable message describing the result of the submission.\
      Typically `"Request submitted successfully"` if accepted.
    </ResponseField>
  </Tab>
</Tabs>

<RequestExample>
  ```curl curl

  # Download audio file
  curl -L -o "audio.flac" "https://tinyurl.com/4997djsh"

  # Make request
  curl -X POST "https://audio-batch.link.fireworks.ai/v1/audio/transcriptions?endpoint_id=audio-prod" \
  -H "Authorization: <FIREWORKS_API_KEY>" \
  -F "file=@audio.flac"
  ```

  ```python python
  !pip install requests

  import os
  import requests

  # input API key and download audio
  api_key = "<FIREWORKS_API_KEY>"
  audio = requests.get("https://tinyurl.com/4cb74vas").content

  # Prepare request data
  url = "https://audio-batch.link.fireworks.ai/v1/audio/transcriptions?endpoint_id=audio-prod"
  headers = {"Authorization": api_key}
  payload = {
      "model": "whisper-v3",
      "response_format": "json"
  }
  files = {"file": ("audio.flac", audio, "audio/flac")}

  # Send request
  response = requests.post(url, headers=headers, data=payload, files=files)
  print(response.text)
  ```
&lt;/RequestExample&gt;

To check the status of your batch request, use the [Check Batch Status](https://docs.fireworks.ai/api-reference/get-batch-status) endpoint with the returned `batch_id`.

--

# Check Batch Status

This endpoint allows you to check the current status of a previously submitted batch request, and retrieve the final result if available.

&lt;CardGroup cols={1}&gt;
  <Card title="Try notebook" icon="rocket" href="https://colab.research.google.com/github/fw-ai/cookbook/blob/main/learn/batch-api/batch_api.ipynb">
    Check status of your batch request
  </Card>
&lt;/CardGroup&gt;

### Headers

<ParamField header="Authorization" type="string" required="">
  Your Fireworks API key. e.g. `Authorization=FIREWORKS_API_KEY`. Alternatively, can be provided as a query param.
</ParamField>

### Path Parameters

<ParamField query="account_id" type="string" required="">
  The identifier of your Fireworks account. Must match the account used when the batch request was submitted.
</ParamField>

<ParamField query="batch_id" type="string" required="">
  The unique identifier of the batch job to check.\
  This should match the `batch_id` returned when the batch request was originally submitted.
</ParamField>

### Response

The response includes the status of the batch job and, if completed, the final result.

<Tabs>
  <Tab title="json">
    <ResponseField name="status" type="string" required="">
      The status of the batch job at the time of the request.\
      Possible values include `"completed"` and `"processing"`.
    </ResponseField>

    <ResponseField name="batch_id" type="string" required="">
      The unique identifier of the batch job whose status is being retrieved.\
      This ID matches the one provided in the original request.
    </ResponseField>

    <ResponseField name="message" type="string" optional="">
      A human-readable message describing the current state of the batch job.\
      This field is typically `null` when the job has completed successfully.
    </ResponseField>

    <ResponseField name="content_type" type="string" optional="">
      The original content type of the response body.\
      This value can be used to determine how to parse the string in the `body` field.
    </ResponseField>

    <ResponseField name="body" type="string" optional="">
      The serialized result of the batch job, this field is only present when `status` is `"completed"`.\
      The format of this string depends on the `content_type` field and may vary across endpoints.\
      Clients should use `content_type` to determine how to parse or interpret the value.
    </ResponseField>
  </Tab>
</Tabs>

<RequestExample>
  ```curl curl
  # Make request
  curl -X GET "https://audio-batch.link.fireworks.ai/v1/accounts/{account_id}/batch_job/{batch_id}" \
  -H "Authorization: <FIREWORKS_API_KEY>"
  ```

  ```python python
  !pip install requests

  import os
  import requests

  # Input api key and path parameters
  api_key = "<FIREWORKS_API_KEY>"
  account_id = "<ACCOUNT_ID>"
  batch_id = "<BATCH_ID>"

  # Send request
  url = f"https://audio-batch.link.fireworks.ai/v1/accounts/{account_id}/batch_job/{batch_id}"
  headers = {"Authorization": api_key}

  response = requests.get(url, headers=headers)
  print(response.text)
  ```
&lt;/RequestExample&gt;
</BATCH_ID></ACCOUNT_ID></FIREWORKS_API_KEY></FIREWORKS_API_KEY></RequestExample></FIREWORKS_API_KEY></FIREWORKS_API_KEY></RequestExample></FIREWORKS_API_KEY></RequestExample>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">feed2exec</title><link href="https://ib.bsb.br/feed2exec/" rel="alternate" type="text/html" title="feed2exec" /><published>2025-10-04T00:00:00+00:00</published><updated>2025-10-04T11:26:40+00:00</updated><id>https://ib.bsb.br/feed2exec</id><content type="html" xml:base="https://ib.bsb.br/feed2exec/"><![CDATA[<h1 id="feed2exec-documentation">feed2exec Documentation</h1>

<p>feed2exec is a simple program that runs custom actions on new RSS feed items (or whatever feedparser can read). It currently has support for writing into mailboxes (Maildir folders) or executing commands, but more actions can be easily implemented through plugins. Email are saved as multipart plain/HTML and can be sent to arbitrary folders.</p>

<p>CONTENTS 1feed2exec Documentation, Release ??? 2 CONTENTS CHAPTER</p>

<h1 id="one-examples">ONE EXAMPLES</h1>

<p>Simple run with no side effects:</p>

<p>feed2exec parse https://www.nasa.gov/rss/dyn/breaking_news.rss –output echo –args ‘</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>{item.title} ‘</p>

<p>Saving feed items to a Maildir folder:</p>

<p>feed2exec add “NASA breaking news” https://www.nasa.gov/rss/dyn/breaking_news.rss –</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>folder nasa feed2exec fetch</p>

<p>This creates the equivalent of this configuration file in ~/.config/feed2exec.ini :</p>

<p>[DEFAULT] output = feed2exec.plugins.maildir mailbox = ~/Maildir [NASA breaking news] folder = nasa url = https://www.nasa.gov/rss/dyn/breaking_news.rss</p>

<p>Send new feed items to Transmission:</p>

<p>feed2exec add “Example torrent list” http://example.com/torrents/feed –output␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>transmission –folder /srv/incoming</p>

<p>Send new feed items to Mastodon, using the toot commandline client:</p>

<p>feed2exec add “My site” http://example.com/blog/feed –output exec –args ‘toot post “</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>{item.title} {item.link} “’</p>

<p>Send new feed items to Twitter, using the tweet commandline client from python-twitter:</p>

<p>feed2exec add “My site on twitter” http://example.com/blog/feed –output exec –args</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>‘tweet “ {item.title:.40s} {item.link:.100s} “’</p>

<p>Show feed contents:</p>

<p>feed2exec add “NASA breaking news” https://www.nasa.gov/rss/dyn/breaking_news.rss –</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>output echo –args “ {item.title} {item.link} “feed2exec fetch</p>

<p>3feed2exec Documentation, Release ???</p>

<p>Multiple feeds can also be added with the OPML import command. See the usage document for more information including known issues and limitations.</p>

<p>4 Chapter 1. Examples CHAPTER</p>

<h1 id="two-installation">TWO INSTALLATION</h1>

<p>This can be installed using the normal Python procedures:</p>

<p>pip install feed2exec</p>

<p>It can also be installed from source, using:</p>

<p>pip install .</p>

<p>It can also be ran straight from the source, using:</p>

<p>python -m feed2exec</p>

<p>Important: feed2exec is explicitly written for Python 3. It may be possible to backport it to Python 2 if there is sufficient demand, but there are too many convenient Python3 constructs to make this useful. Furthermore, all dependencies are well-packaged for Py3 and the platform is widely available. Upgrade already. The program may also be available as an official package from your Linux distribution. Source, documentation and issues are available on GitLab.</p>

<p>Note: feed2exec relies on the feedparser module to parse feeds and as such has all the bugs and limitations of that modules. In particular, feeds with non-standard dates will break the parser, unless the dateparser module is installed.</p>

<p>5feed2exec Documentation, Release ??? 6 Chapter 2. Installation CHAPTER</p>

<h1 id="three-why-the-name">THREE WHY THE NAME?</h1>

<p>There are already feed2tweet and feed2imap out there so I figured I would just reuse the prefix and extend both programs at once. Contents:</p>

<h1 id="31-feed2exec-manual-page">3.1 feed2exec manual page</h1>

<h1 id="311-synopsis">3.1.1 Synopsis</h1>

<p>feed2exec {add,ls,rm,fetch,import,export}</p>

<h1 id="312-description">3.1.2 Description</h1>

<p>This command will take a configured set of feeds and fire specific plugin for every new item found in the feed.</p>

<h1 id="313-options">3.1.3 Options</h1>

<p>–version Show the version and exit.</p>

<p>–loglevel choose specific log level [default: WARNING]</p>

<p>-v, –verbose show what is happening (loglevel: VERBOSE)</p>

<p>-d, –debug show debugging information (loglevel: DEBUG)</p>

<p>–syslog LEVEL send LEVEL logs to syslog</p>

<p>–config TEXT use a different configuration file</p>

<p>–database DB use a different database</p>

<p>-h, –help Show this message and exit.</p>

<p>7feed2exec Documentation, Release ???</p>

<h1 id="314-examples">3.1.4 Examples</h1>

<p>Simple run with no side effects:</p>

<p>feed2exec parse https://www.nasa.gov/rss/dyn/breaking_news.rss –output echo –args ‘</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>{item.title} ‘</p>

<p>Saving feed items to a Maildir folder:</p>

<p>feed2exec add “NASA breaking news” https://www.nasa.gov/rss/dyn/breaking_news.rss –</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>folder nasa feed2exec fetch</p>

<p>This creates the equivalent of this configuration file in ~/.config/feed2exec.ini :</p>

<p>[DEFAULT] output = feed2exec.plugins.maildir mailbox = ~/Maildir [NASA breaking news] folder = nasa url = https://www.nasa.gov/rss/dyn/breaking_news.rss</p>

<p>Send new feed items to Transmission:</p>

<p>feed2exec add “Example torrent list” http://example.com/torrents/feed –output␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>transmission –folder /srv/incoming</p>

<p>Send new feed items to Mastodon, using the toot commandline client:</p>

<p>feed2exec add “My site” http://example.com/blog/feed –output exec –args ‘toot post “</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>{item.title} {item.link} “’</p>

<p>Send new feed items to Twitter, using the tweet commandline client from python-twitter:</p>

<p>feed2exec add “My site on twitter” http://example.com/blog/feed –output exec –args</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>‘tweet “ {item.title:.40s} {item.link:.100s} “’</p>

<p>Show feed contents:</p>

<p>feed2exec add “NASA breaking news” https://www.nasa.gov/rss/dyn/breaking_news.rss –</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>output echo –args “ {item.title} {item.link} “feed2exec fetch</p>

<h1 id="315-commands">3.1.5 Commands</h1>

<p>• parse</p>

<p>• fetch</p>

<p>• add</p>

<p>• ls</p>

<p>8 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<p>• rm</p>

<p>• import</p>

<p>• export</p>

<p>parse</p>

<p>Usage:</p>

<p>parse URL [–output PLUGIN [–args ARG [ARG […]]] [–filter PLUGIN] [–filter_args ARG [ARG […]]] [–mailbox PATH] [–folder PATH]</p>

<p>The parse command loads and parses a single feed, without touching the database. This is similar to calling add then</p>

<p>fetch on a single feed, but the feed is not kept in the configuration. This is designed to make quick tests with a new feed. The arguments are the same as the add command.</p>

<p>fetch</p>

<p>Usage:</p>

<table>
  <tbody>
    <tr>
      <td>fetch [–parallel</td>
      <td>-p</td>
      <td>–jobs N</td>
      <td>-j N] [–force</td>
      <td>-f] [–pattern pattern]</td>
    </tr>
  </tbody>
</table>

<p>The fetch command iterates through all the configured feeds or those matching the pattern substring if provided. Options:</p>

<p>–pattern TEXT only fetch feeds matching name or URL</p>

<p>–parallel parse feeds in the background to improve performance</p>

<p>-j, –jobs N start N jobs in parallel, implies –parallel which defaults to the number of CPUs detected on the machine</p>

<p>-f, –force skip reading and writing the cache and will consider all entries as new</p>

<p>-n, –catchup tell output plugins plugins to simulate their actions</p>

<p>add</p>

<p>Usage:</p>

<p>add NAME URL [–output PLUGIN [–args ARG [ARG […]]] [–filter PLUGIN] [–filter_args ARG [ARG […]]] [–mailbox PATH] [–folder PATH]</p>

<p>The add command adds the given feed NAME that will be fetched from the provided URL .Options:</p>

<p>3.1. feed2exec manual page 9feed2exec Documentation, Release ???</p>

<p>–output PLUGIN use PLUGIN as an output module. defaults to maildir to store in a mailbox. use null or echo to just fetch the feed without doing any-thing. Modules are searched in the feed2exec.plugins package unless the name contains a dot in which case the whole Python search path is used.</p>

<p>–args ARGS pass arguments ARGS to the output plugin. supports interpolation of feed parameters using, for example {title}</p>

<p>–filter PLUGIN filter feed items through the PLUGIN filter plugin</p>

<p>–filter_args ARGS arguments passed to the filter plugin</p>

<p>–mailbox PATH folder to store email into, defaults to ~/Maildir .</p>

<p>–folder PATH subfolder to store the email into Those parameters are documented more extensively in their equivalent settings in the configuration file, see below.</p>

<p>ls</p>

<p>The ls command lists all configured feeds as JSON packets.</p>

<p>rm</p>

<p>Usage:</p>

<p>rm NAME</p>

<p>Remove the feed named NAME from the configuration.</p>

<p>import</p>

<p>Usage:</p>

<p>import PATH</p>

<p>Import feeds from the file named PATH. The file is expected to have outline elements and only the title and xmlUrl</p>

<p>elements are imported, as NAME and URL parameters, respectively.</p>

<p>export</p>

<p>Usage:</p>

<p>export PATH</p>

<p>Export feeds into the file named PATH. The file will use the feed NAME elements as title and the URL as xmlUrl .</p>

<p>10 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<h1 id="316-files">3.1.6 Files</h1>

<p>Configuration file</p>

<p>The configuration file is loaded from (and written to, by add ) ~/.config/feed2exec.ini or $XDG_CONFIG_HOME/ feed2exec.ini . It can also be specified with the –config commandline parameter. This is an example configuration snippet:</p>

<p>[NASA breaking news] url = https://www.nasa.gov/rss/dyn/breaking_news.rss output = feed2exec.plugins.echo args = {title} {link}</p>

<p>Naturally, those settings can be changed directly in the config file. Note that there is a [DEFAULT] section that can be used to apply settings to all feeds. For example, this will make all feeds store new items in a maildir subfolder:</p>

<p>[DEFAULT] output = feed2exec.plugins.maildir folder = feeds</p>

<p>This way individual feeds do not need to be individually configured.</p>

<p>Note: feed2exec does not take care of adding the folder to “subscriptions” in the mailbox. it is assumed that folders are auto-susbcribed or the user ignores subscription. if that is a problem, you should subscribe to the folder by hand in your email client when you add a new config. you can also subscribe to a folder (say feeds above) directly using the</p>

<p>doveadm mailbox subscribe feeds command in Dovecot, for example. The following configuration parameters are supported:</p>

<p>name</p>

<p>Human readable name for the feed. Equivalent to the NAME argument in the add command.</p>

<p>url</p>

<p>Address to fetch the feed from. Can be HTTP or HTTPS, but also file:// resources for test pur-poses.</p>

<p>output</p>

<p>Output plugin to use. Equivalent to the –output option in the add command.</p>

<p>args</p>

<p>Arguments to pass to the output plugin. Equivalent to the –args option in the add command.</p>

<p>filter</p>

<p>Filter plugin to use. Equivalent to the –filter option in the add command.</p>

<p>mailbox</p>

<p>Store emails in that mailbox prefix. Defaults to ~/Maildir .</p>

<p>folder</p>

<p>Subfolder to use when writing to a mailbox. By default, a slugified version of the feed name (where spaces and special character are replaced by -) is used. For example, the feed named “NASA breaking news” would be stored in ~/Maildir/nasa-breaking-news/ . Note that the mailbox prefix is used only if the folder path is relative.</p>

<p>catchup</p>

<p>Skip to the latest feed items. The feed is still read and parsed, and new feed items are added to the database, but output plugins are never called.</p>

<p>3.1. feed2exec manual page 11 feed2exec Documentation, Release ???</p>

<p>pause</p>

<p>Completely skip feed during fetch or parse. Similar to catchup, but doesn’t fetch the feed at all and doesn’t touch the cache. Here is a more complete example configuration with all the settings used:</p>

<h1 id="this-section-will-apply-to-all-feeds-default--special-folder-location-for-maildir-i-use-this-when-i-have-multiple--accounts-synchronized-with-offlineimap-mailbox--maildirremote--a-feed-to-store-nasa-breaking-news-entry-in-a-nasa-subfolder--this-also-demonstrates-the-droptitle-filter-nasa-breaking-news-url--httpswwwnasagovrssdynbreaking_newsrss-folder--nasa-filter--feed2execpluginsdroptitle-filter_args--trump--some-maildir-storage-require-dots-to-get-subfolders-for-example--this-will-store-messages-in-inboxfeedsimages-on-dovecot-nasa-image-of-the-day-url--httpswwwnasagovrssdynlg_image_of_the_dayrss-folder--feedsimages--same-feed-but-save-to-wayback-machine-nasa-iotd-wayback-url--httpswwwnasagovrssdynlg_image_of_the_dayrss-output--feed2execpluginswayback--this-demonstrates-the-emptysummary-filter-which-fixes-github-feeds--that-lack-a-proper-summary-restic-url--httpsgithubcomresticrestictagsatom-filter--feed2execpluginsemptysummary--saving-to-a-mbox-folder-one-file-per-feed-instead-of-one-file-per-item-international-space-station-reports-url--httpblogsnasagovstationreportfeed-mailbox--mail-folder--stationreportmbx--simple-generic-exec-call-example-check-for-broken-links-using-linkchecker-nasa-linkchecker-url--httpswwwnasagovrssdynbreaking_newsrss-output--feed2execpluginsexec-args--linkchecker-check-extern-no-robots-recursion-level-1-quiet-itemlink-">this section will apply to all feeds [DEFAULT] # special folder location for maildir. I use this when I have multiple # accounts synchronized with Offlineimap mailbox = ~/Maildir/Remote/ # a feed to store NASA breaking news entry in a “nasa” subfolder # this also demonstrates the droptitle filter [NASA breaking news] url = https://www.nasa.gov/rss/dyn/breaking_news.rss folder = nasa filter = feed2exec.plugins.droptitle filter_args = trump # some maildir storage require dots to get subfolders. for example, # this will store messages in INBOX/feeds/images/ on Dovecot [NASA image of the day] url = https://www.nasa.gov/rss/dyn/lg_image_of_the_day.rss folder = .feeds.images # same feed, but save to wayback machine [NASA IOTD wayback] url = https://www.nasa.gov/rss/dyn/lg_image_of_the_day.rss output = feed2exec.plugins.wayback # this demonstrates the emptysummary filter, which fixes GitHub feeds # that lack a proper summary [restic] url = https://github.com/restic/restic/tags.atom filter = feed2exec.plugins.emptysummary # saving to a mbox folder, one file per feed instead of one file per item [International Space Station Reports] url = http://blogs.nasa.gov/stationreport/feed/ mailbox = ~/Mail/ folder = stationreport.mbx # simple generic exec call example: check for broken links using linkchecker [NASA linkchecker] url = https://www.nasa.gov/rss/dyn/breaking_news.rss output = feed2exec.plugins.exec args = linkchecker –check-extern –no-robots –recursion-level 1 –quiet ‘{item.link} ‘</h1>

<h1 id="same-but-with-a-ikiwiki-rss-feed-which-needs-fixing-ikiwiki-linkchecker-url--httpikiwikiinforecentchangesindexrss-output--feed2execpluginsexec-filter--feed2execpluginsikiwiki_recentchanges">same, but with a Ikiwiki RSS feed, which needs fixing [Ikiwiki linkchecker] url = http://ikiwiki.info/recentchanges/index.rss output = feed2exec.plugins.exec filter = feed2exec.plugins.ikiwiki_recentchanges</h1>

<blockquote>
  <p>(continues on next page)</p>
</blockquote>

<p>12 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<blockquote>
  <p>(continued from previous page)</p>
</blockquote>

<p>args = linkchecker –check-extern –no-robots –recursion-level 1 –quiet ‘{item.link} ‘</p>

<h1 id="retweet-hurricane-news-nasa-hurricane-breaking-news-url--httpswwwnasagovrssdynhurricaneupdaterss-output--feed2execpluginsexec-args--tweet-itemtitle40s-itemlink100s--same-but-on-the-mastodon-network--we-can-have-multiple-entries-with-the-same-url-without-problems-as--long-as-the-feed-name-is-different-it-does-mean-that-the-feed-will--be-fetched-and-parsed-multiple-times-unfortunately--this-could-be-improved-to-include-the-itemsummary--and-extra-markup--for-example-nasa-hurricane-breaking-news---mastodon-url--httpswwwnasagovrssdynhurricaneupdaterss-output--feed2execpluginsexec--unfortunately-this-will-noisily-report-the-url-of-the-posted-link--which-you-may-not-want-to-avoid-that-encourage-upstream-to-do-the--right-thing-httpsgithubcomihabunektootissues46--or-use--another-tool-listed-here--httpsgithubcomtootsuitedocumentationblobmasterusing-mastodonappsmd-args--toot-post-itemtitle-itemlink--output-is-disabled-here-feed-will-be-fetched-and-parsed-but-no--toot-will-be-sent-catchup--true--same-but-on-the-pumpio-network-nasa-hurricane-breaking-news---pump-url--httpswwwnasagovrssdynhurricaneupdaterss-output--feed2execpluginsexec-args--p-post-note-itemtitle-itemlink--crude-podcast-client-nasa-what-up-url--httpswwwnasagovrssdynwhats_uprss-output--feed2execpluginsexec--xxx-this-doesn-t-handle-errors-properly-if-there-is-a-feed-without--enclosures-the-whole-thing-will-crash-args--wget--p-srvpodcastsnasa-itemenclosures0href--feed-is-paused-here-feed-will-not-be-fetched-and-parsed-at-all-and--no-post-will-be-sent-pause--true--download-torrents-linked-from-a-rss-feed-torrents-url--httpexamplecomtorrentsrss-output--feed2execpluginsexec-args--transmission-remote--a-itemlink---w-srvincoming-">retweet hurricane news [NASA Hurricane breaking news] url = https://www.nasa.gov/rss/dyn/hurricaneupdate.rss output = feed2exec.plugins.exec args = tweet “{item.title:.40s} {item.link:.100s}” # same, but on the mastodon network ## we can have multiple entries with the same URL without problems, as # long as the feed name is different. it does mean that the feed will # be fetched and parsed multiple times, unfortunately. ## this could be improved to include the ‘{item.summary} ‘ and extra markup, # for example. [NASA Hurricane breaking news - Mastodon] url = https://www.nasa.gov/rss/dyn/hurricaneupdate.rss output = feed2exec.plugins.exec # unfortunately, this will noisily report the URL of the posted link, # which you may not want. to avoid that, encourage upstream to do the # right thing: https://github.com/ihabunek/toot/issues/46 … or use # another tool listed here: # https://github.com/tootsuite/documentation/blob/master/Using-Mastodon/Apps.md args = toot post “{item.title} {item.link}” # output is disabled here. feed will be fetched and parsed, but no # toot will be sent catchup = True # same, but on the Pump.io network [NASA Hurricane breaking news - Pump] url = https://www.nasa.gov/rss/dyn/hurricaneupdate.rss output = feed2exec.plugins.exec args = p post note “{item.title} {item.link}” # crude podcast client [NASA What up?] url = https://www.nasa.gov/rss/dyn/whats_up.rss output = feed2exec.plugins.exec # XXX: this doesn ‘t handle errors properly: if there is a feed without # enclosures, the whole thing will crash. args = wget -P /srv/podcasts/nasa/ “{item.enclosures[0].href}” # feed is paused here. feed will not be fetched and parsed at all and # no post will be sent. pause = True # download torrents linked from a RSS feed [torrents] url = http://example.com/torrents.rss output = feed2exec.plugins.exec args = transmission-remote -a ‘{item.link} ‘ -w ‘/srv/incoming ‘</h1>

<blockquote>
  <p>(continues on next page)</p>
</blockquote>

<p>3.1. feed2exec manual page 13 feed2exec Documentation, Release ???</p>

<blockquote>
  <p>(continued from previous page)</p>
</blockquote>

<h1 id="same-thing-with-an-actual-plugin-torrents-url--httpexamplecomtorrentsrss-output--feed2execpluginstransmission-args--seedboxexamplecom-folder--srvincoming">same thing with an actual plugin [torrents] url = http://example.com/torrents.rss output = feed2exec.plugins.transmission args = seedbox.example.com folder = /srv/incoming</h1>

<p>Cache database</p>

<p>The feeds cache is stored in a feed2exec.db file. It is a SQLite database and can be inspected using standard sqlite tools. It is used to keep track of which feed and items have been processed. To clear the cache, you can simply remove the file, which will make the program process all feeds items from scratch again. In this case, you should use the</p>

<p>–catchup argument to avoid duplicate processing. You can also use the null output plugin to the same effect.</p>

<h1 id="317-limitations">3.1.7 Limitations</h1>

<p>Feed support is only as good as feedparser library which isn’t as solid as I expected. In particular, I had issues with feeds without dates and without guid. Unit test coverage is incomplete, but still pretty decent, above 90%. The exec plugin itself is not well tested and may have serious security issues. API, commandline interface, configuration file syntax and database format can be changed until the 1.0 release is published, at which point normal Semantic Versioning semantics apply. The program is written mainly targeting Python 3.5 and 3.7, but should support later releases as well. See the setup.py</p>

<p>classification for an authoritative reference. Python 2.7 is not supported anymore. The SQL storage layer is badly written and is known to trigger locking issues with SQLite when doing multiprocessing. The global LOCK object could be used to work around this issue but that could mean pretty bad coupling. A good inspiration may be the beets story about this problem. And of course, another alternative would be to considering something like SQLalchemy instead of rolling our own ORM. There has, however, been some improvements to the locking recently, although that has been done with thread instead of process -specific locks. Older feed items are not purged from the database when they disappear from the feed, which may lead to database bloat in the long term. Similarly, there is no way for plugins to remove old entry that expire from the feed.</p>

<h1 id="318-see-also">3.1.8 See also</h1>

<p>feed2exec-plugins(1) , feed2imap(1) , rss2email(1)</p>

<p>14 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<h1 id="32-design">3.2 Design</h1>

<p>This is a quick prototype that turned out to be quite usable. The design is minimal: some home-made ORM for the feed storage, crude parallelism with the multiprocessing module and a simple plugin API using importlib .More information about known issues and limitations in the feed2exec manual page document.</p>

<h1 id="321-quick-tour">3.2.1 Quick tour</h1>

<p>The most common workflow is through the fetch subcommand and goes something like this: 1. <strong>main</strong>.py is the main entrypoint, managed through the click module, which normally calls func-tions defined in controller.py . The base command ( <strong>main</strong>.main ) creates a feed2exec.controller. FeedManager object which gets passed to subcommands. In our case, it passes the control to the fetch sub-command. 2. The fetch command calls the feed2exec.controller.FeedManager.fetch() function which creates a</p>

<p>feed2exec.model.Feed object that is then used to parse the feed and return it as an opaque data object as returned by feedparser . The feed is parsed (and, below, dispatched) only if it not already present in the cache, managed by the cachecontrol module. 3. fetch then calls the feed2exec.controller.FeedManager.dispatch() function that calls the various filter and output plugins, passing in the feed configuration and one item at a time. The filters can modify the feed items while the output plugins are responsible for writing them somewhere. That distinction is mostly arbitrary, but the return values of the output plugins matter, while filters do not. The feed cache is stored in a minimal sqlite3 database. A table keeps track of which feed item has been seen and another is the backend for the cachecontrol module and has a copy of the actual requests, keyed by URL. Configuration is stored in a .ini file or whatever configparser supports. It was originally stored in the database as well, but it was found inconvenient to modify by hand and a configuration file was used instead. The .ini file format was chosen because it is well supported by Python and allows for default settings. There is the possibility for this project to cover more than RSS/Atom feeds. In theory, the parse function could also be pluggable and support reading from other data sources like Twitter or Facebook, which would bring us closer to the IFTTT concept.</p>

<h1 id="322-plugin-system">3.2.2 Plugin system</h1>

<p>Plugins are documented in the Plugins section. You can also refer to the Matchtitleregex section if you wish to write a new plugin or extend an existing one. The plugin system uses a simple importlib based architecture where plugin are simple Python modules loaded at runtime based on a module path provided by the user. This pattern was inspired by a StackOverflow discussion. The following options were also considered:</p>

<p>• pluggy: used by py.test, tox and devpi</p>

<p>• yapsy</p>

<p>• PluginBase</p>

<p>• plugnplay</p>

<p>• click-plugins: relevant only to add new commands</p>

<p>• PyPA plugin discovery</p>

<p>3.2. Design 15 feed2exec Documentation, Release ???</p>

<p>Those options were ultimately not used because they add an additional dependency and are more complicated than a simple import . We also did not need plugin listing or discovery, which greatly simplifies our design. There is some code duplication between different parts (e.g. the feed2exec.plugins.output() and feed2exec. plugins.filter() plugin interfaces, the maildir and mbox plugins, etc), but never more than twice.</p>

<h1 id="323-concurrent-processing">3.2.3 Concurrent processing</h1>

<p>The threading design may be a little clunky and is certainly less tested, which is why it is disabled by default (use</p>

<p>–parallel to use it). There are known deadlocks issues with high concurrency scenarios (e.g. with catchup en-abled). I had multiple design in minds: the current one ( multiprocessing.Pool and pool.apply_async ) vs aiohttp</p>

<p>(on the asyncio branch) vs pool.map (on the threadpoolmap branch). The aiohttp design was very hard to diagnose and debug, which made me abandon the whole thing. After reading up on Curio and Trio, I’m tempted to give async/await a try again, but that would mean completely dropping 2.7 compatibility. The pool.map design is just badly adapted, as it would load all the feed’s datastructure in memory before processing them. The current parallel design also doesn’t profit much from the caching system. While before we would spend a lot of time parsing all feeds (in parallel), now most feeds are not parsed anymore (because unchanged) so a lot of time is spent doing HTTP requests, which could be done in parallel (but currently isn’t).</p>

<h1 id="324-test-suite">3.2.4 Test suite</h1>

<p>The test suite is in feed2exec/tests but also as doctest comments in some functions imported from the ecdysis project. You can run all the tests with pytest, using, for example:</p>

<p>pytest-3</p>

<p>This is also hooked into the setup.py command, so this also works:</p>

<p>python3 setup.py test</p>

<p>Note: It’s recommended to use the tox command to run tests, as some tests are picky about dependencies version numbers. That’s how the Continuous Integration (CI) system runs tests, through the .gitlab-ci.yml file. Enabling the catchlog plugin will also enable logging in the test suite which will help diagnostics. Note that some tests will fail in Python 2, as the code is written and tested in Python3. Furthermore, the feed output is taken from an up to date (5.2.1) feedparser version, so the tests are marked as expected to fail for lower versions. You should, naturally, run and write tests before submitting patches. See the Writing tests section for more information about how to write tests. The test suite also uses the betamax module to cache HTTP requests locally so the test suite can run offline. If a new test requires networking, you can simply add a new test doing requests with the right fixture ( betamax_session() )provided by upstream if you are going to do standalone HTTP request (not going through the feed2exec libraries). But you would more likely use the existing session by using the feed2exec.tests.fixtures.feed_manager() fixture, which has a session member you can use. If a new test is added in an existing test, you may need to configure recording (in feed2exec/tests/conftest.py )to new_episodes :</p>

<p>16 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<p>config.default_cassette_options[ ‘record_mode ‘] = ‘none ‘</p>

<p>We commit the recordings in git so the test suite actually runs offline, so be careful about the content added there. Ideally, the license of that content should be documented in debian/copyright .vcrpy was first used for tests since it was simpler and didn’t require using a global requests.session.Session</p>

<p>object. But in the end betamax seems better maintained and more flexible: it supports pytest fixtures, for example, and multiple cassette storage (including vcr backwards compatibility). Configuration is also easier, done in feed2exec/ tests/conftest.py . Using a session also allows us to use a custom user agent.</p>

<h1 id="325-comparison">3.2.5 Comparison</h1>

<p>feed2exec is a fairly new and minimal program, so features you may expect from another feed reader may not be present. I chose to write a new program because, when I started, both existing alternatives were in a questionable state: feed2imap was mostly abandoned and rss2email’s maintainer was also unresponsive. Both were missing the features I was looking for, which was to unify my feed parsers in a single program: i needed something that could deliver mail, run commands and send tweets. The latter isn’t done yet, but I am hoping to complete this eventually. The program may not be for everyone, however, so I made those comparison tables to clarify what feed2exec does compared to the alternatives. General information:</p>

<p>Program Version Date SLOC Language</p>

<p>feed2exec 0.10 2017 989 Python feed2imap 1.2.5 2015 3238 Ruby rss2email 3.9 2014 1754 Python</p>

<p>• version: the version analysed</p>

<p>• date: the date of that release</p>

<p>• SLOC: Source Lines of Codes as counted by sloccount, only counting dominant language (e.g. excluding XML from test feeds) and excluding tests</p>

<p>• Language: primary programming language Delivery options:</p>

<p>Program Maildir Mbox IMAP SMTP sendmail exec</p>

<p>feed2exec ✓ ✓ ✓</p>

<p>feed2imap ✓ ✓</p>

<p>rss2email ✓ ✓ ✓</p>

<p>• maildir: writing to Maildir folders. r2e has a pull request to implement maildir support, but it’s not merged at the time of writing</p>

<p>• IMAP: sending emails to IMAP servers</p>

<p>• SMTP: delivering emails over the SMTP protocol, with authentication</p>

<p>• sendmail: delivering local using the local MTA</p>

<p>• exec: run arbitrary commands to run on new entries. feed2imap has a execurl parameter to execute commands, but it receives an unparsed dump of the feed instead of individual entries. rss2email has a postprocess filter that is a Python plugin that can act on individual (or digest) messages which</p>

<p>3.2. Design 17 feed2exec Documentation, Release ???</p>

<p>could possibly be extended to support arbitrary commands, but that is rather difficult to implement for normal users. Features:</p>

<p>Program Pause OPML Retry Images Filter Reply Digest</p>

<p>feed2exec ✓ ✓ ✓ ✓</p>

<p>feed2imap ✓ ✓ ✓ ✓</p>

<p>rss2email ✓ ✓ ✓ ✓ ✓ ✓</p>

<p>• pause: feed reading can be disabled temporarily by user. in feed2exec, this is implemented with the pause</p>

<p>configuration setting. the catchup option can also be used to catchup with feed entries.</p>

<p>• retry: tolerate temporary errors. For example, feed2imap will report errors only after 10 failures.</p>

<p>• images: download images found in feed. feed2imap can download images and attach them to the email.</p>

<p>• filter: if we can apply arbitrary filters to the feed output. feed2imap can apply filters to the unparsed dump of the feed.</p>

<p>• reply: if the generated email ‘from’ header is usable to make a reply. rss2email has a use-publisher-email</p>

<p>setting (off by default) for this, for example. feed2exec does this by default.</p>

<p>• digest: possibility of sending a single email per run instead of one per entry</p>

<p>Note: feed2imap supports only importing OPML feeds, exporting is supported by a third-party plugin.</p>

<p>Note: feed2exec might one day be expanded to support other feeds than RSS/Atom, and turn into a more generic “if-this-then-that” type of program, to support, say, REST APIs, or Gemini, or whatever. In the meantime, see gmi2email for an alternative supporting Gemini.</p>

<h1 id="326-related-software">3.2.6 Related software</h1>

<p>• rss-bridge can be used to provide RSS feeds for hundreds of sites that do not, including Twitter, Reddit, Youtube, etc</p>

<h1 id="33-api-documentation">3.3 API documentation</h1>

<p>This is the API documentation of the program. It should explain how to create new plugins and navigate the code.</p>

<h1 id="331-controller-module">3.3.1 Controller module</h1>

<p>This is the core modules that processes all feeds and talks to the storage. It’s where most of the logic lies, although the parsing is still currently done inside the model. It dispatches the plugin logic to the plugin module.</p>

<p>18 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<h1 id="332-model">3.3.2 Model</h1>

<p>The “model” keeps track of feeds and their items. It handles configuration and cache storage.</p>

<h1 id="333-main-entry-point">3.3.3 Main entry point</h1>

<p>The main entry point of the program is in the feed2exec.<strong>main</strong> module. This is to make it possible to call the program directly from the source code through the Python interpreter with:</p>

<p>python -m feed2exec</p>

<p>All this code is here rather than in <strong>init</strong>.py to avoid requiring too many dependencies in the base module, which contains useful metadata for setup.py .This uses the click module to define the base command and options.</p>

<h1 id="334-plugins">3.3.4 Plugins</h1>

<p>Plugin interface</p>

<p>In this context, a “plugin” is simply a Python module with a defined interface.</p>

<p>feed2exec.plugins. output (feed , item , lock=None , session=None )</p>

<p>load and run the given plugin with the given arguments an “output plugin” is a simple Python module with an output callable defined which will process arguments and should output them somewhere, for example by email or through another command. the plugin is called (from feed2exec.feeds.parse() ) when a new item is found, unless cache is flushed or ignored. The “callable” can be a class, in which case only the constructor is called or a function. The *args and **kwargs</p>

<p>parameter SHOULD be used in the function definition for forward-compatibility (ie. to make sure new parameters added do not cause a regression). Plugins should also expect to be called in parallel and should use the provided lock (a multiprocessor.Lock object) to acquire and release locks around contentious resources. Finally, the FeedManager will pass along his own session that should be reused by plugins to do requests. This allows plugins to be unit-tested and leverages the built-in cache as well. The following keywords are usually replaced in the arguments:</p>

<p>• {item.link}</p>

<p>• {item.title}</p>

<p>• {item.description}</p>

<p>• {item.published}</p>

<p>• {item.updated}</p>

<p>• {item.guid} The full list of such parameters is determined by the :module:feedparser module. Similarly, feed parameters from the configuration file are accessible.</p>

<p>3.3. API documentation 19 feed2exec Documentation, Release ???</p>

<p>Caution: None of those parameters are sanitized in any way other than what feedparser does, so plugins writing files, executing code or talking to the network should be careful to sanitize the input appropriately. The feed and items are also passed to the plugin as keyword arguments. Plugins should especially respect the</p>

<p>catchup argument that, when set, forbids plugins to do any permanent activity. For example, plugins MUST NOT run commands, write files, or make network requests. In general, “catchup mode” should be fast : it allows users to quickly catchup with new feeds without firing plugins, but it should also allow users to test configura-tions so plugins SHOULD give information to the user about what would have been done by the plugin without</p>

<p>catchup .</p>

<p>Parameters</p>

<p>• feed (dict ) – the feed metadata</p>

<p>• item (dict ) – the updated item</p>

<p>Return object</p>

<p>the loaded plugin</p>

<p>Note: more information about plugin design is in the Matchtitleregex document.</p>

<p>feed2exec.plugins. filter (feed , item , lock=None , session=None )</p>

<p>call filter plugins. very similar to the output plugin, but just calls the filter module member instead of output</p>

<p>Todo: common code with output() should be factored out, but output() takes arguments. . .</p>

<p>feed2exec.plugins. resolve (plugin )</p>

<p>resolve a short plugin name to a loadable plugin path Some parts of feed2exec allow shorter plugin names. For example, on the commandline, users can pass maildir</p>

<p>instead of feed2exec.plugins.maildir .Plugin resolution works like this: 1. search for the module in the feed2exec.plugins namespace 2. if that fails, consider the module to be an absolute path</p>

<p>Note: actual plugins are documented in the Plugins document.</p>

<h1 id="335-utilities">3.3.5 Utilities</h1>

<p>Those are various utilities reused in multiple modules that did not fit anywhere else.</p>

<p>20 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<h1 id="34-plugins">3.4 Plugins</h1>

<p>This is a quick overview of the available plugins.</p>

<h1 id="341-output-plugins">3.4.1 Output plugins</h1>

<p>Archive Echo</p>

<p>class feed2exec.plugins.echo. output (*args , feed=None , **kwargs )</p>

<p>This plugin outputs, to standard output, the arguments it receives. It can be useful to test your configuration. It also creates a side effect for the test suite to determine if the plugin was called. This plugin does a similar thing when acting as a filter.</p>

<p>feed2exec.plugins.echo. filter</p>

<p>This filter just keeps the feed unmodified. It is just there for testing purposes.</p>

<p>Error</p>

<p>feed2exec.plugins.error. output (*args , **kwargs )</p>

<p>The error plugin is a simple plugin which raises an exception when called. It is designed for use in the test suite and should generally not be used elsewhere.</p>

<p>Exec</p>

<p>feed2exec.plugins.exec. output (command , *args , feed=None , **kwargs )</p>

<p>The exec plugin is the ultimate security disaster. It simply executes whatever you feed it without any sort of sanitization. It does avoid to call to the shell and executes the command directly, however. Feed contents are also somewhat sanitized by the feedparser module, see the Sanitization documentation for more information in that regard. That is limited to stripping out hostile HTML tags, however. You should be careful when sending arbitrary parameters to other programs. Even if we do not use the shell to execute the program, an hostile feed could still inject commandline flags to change the program behavior without injecting shell commands themselves. For example, if a program can write files with the -o option, a feed could set their title to -oevil to overwrite the evil file. The only way to workaround that issue is to carefully craft the commandline so that this cannot happen. Alternatively, writing a Python plugin is much safer as you can sanitize the arguments yourself. Example:</p>

<p>[NASA What ‘s up?] url = https://www.nasa.gov/rss/dyn/whats_up.rss output = feed2exec.plugins.exec args = wget -P /srv/archives/nasa/ {item.link}</p>

<p>The above is the equivalent of the archive plugin: it will save feed item links to the given directory.</p>

<p>3.4. Plugins 21 feed2exec Documentation, Release ??? Maildir Mbox Null</p>

<p>feed2exec.plugins.null. output (*args , **kwargs )</p>

<p>This plugin does nothing. It can be useful in cases where you want to catchup with imported feeds.</p>

<p>feed2exec.plugins.null. filter (item=None , *args , **kwargs )</p>

<p>The null filter removes all elements from a feed item</p>

<p>Transmission Wayback</p>

<h1 id="342-filter-plugins">3.4.2 Filter plugins</h1>

<p>Droptitle</p>

<p>feed2exec.plugins.droptitle. filter (*args , feed=None , item=None , **kwargs )</p>

<p>the droptitle filter will drop any feed item with a title matching the given args. Example:</p>

<p>[NASA breaking news] url = https://www.nasa.gov/rss/dyn/breaking_news.rss filter = feed2exec.plugins.droptitle filter_args = Trump</p>

<p>The above will process the feed items according to the global configuration, but will skip any item that has the word “Trump” anywhere in the title field. Arguments are processed as a single string. If you need to match more complex patterns, look at the droptitleregex plugin instead.</p>

<p>Droptitleregex</p>

<p>feed2exec.plugins.droptitleregex. filter (*args , feed=None , item=None , **kwargs )</p>

<p>the droptitleregex filter will drop any feed item with a title matching the given regular expression pattern. Example:</p>

<p>[NASA breaking news] url = https://www.nasa.gov/rss/dyn/breaking_news.rss filter = feed2exec.plugins.droptitleregex filter_args = ^ham</p>

<p>The above configuration processes the feed items based on the global configuration, but it will skip any item whose title starts with the word “ham”.</p>

<p>22 Chapter 3. Why the name? feed2exec Documentation, Release ??? Emptysummary</p>

<p>feed2exec.plugins.emptysummary. filter (*args , feed=None , item=None , **kwargs )</p>

<p>example of fixes for a broken feed, in this case, the GitHub release feed which (sometimes) sends empty contents, in which case the item link field is used as a summary instead.</p>

<p>Html2text Ikiwiki Recentchanges</p>

<p>feed2exec.plugins.ikiwiki_recentchanges. filter (*args , item=None , **kwargs )</p>

<p>the ikiwiki_recentchanges plugin fixes links in ikiwiki feeds Ikiwiki recent changes show all the recent edits to pages, but the <link /> element doesn’t point to the edit page: it points to the recent changes page itself, which make them useless for link checking or archival purposes. This parses the recent changes entries and extracts the relevant links from it. An alternative to this is to use the following entry to generate a special feed in Ikiwiki:</p>

<p>[[!inline pages=”*” feeds=yes feedonly=yes feedfile=archive show=10]]</p>

<p>This generates a feed with proper <link /> elements but requires write access to the wiki. This will also add the date to the URL GUID so that we refresh when a page is updated. Otherwise feed2exec would think the entry has already been passed.</p>

<p>Matchtitleregex</p>

<p>feed2exec.plugins.matchtitleregex. filter (*args , feed=None , item=None , **kwargs )</p>

<p>The matchtitleregex filter selects only the feed items whose title match the given regular expression pattern. Example:</p>

<p>[NASA breaking news] url = https://www.nasa.gov/rss/dyn/breaking_news.rss filter = feed2exec.plugins.matchtitleregex filter_args = ^spam</p>

<p>The above configuration processes the feed items based on the global configuration, but it will skip any item whose title does not start with the word “spam”.</p>

<h1 id="343-writing-new-plugins">3.4.3 Writing new plugins</h1>

<p>Most of the actual work in the program is performed by plugins. A plugin is a simple Python module that has a output</p>

<p>or filter “callable” (function or class) with a predefined interface.</p>

<p>3.4. Plugins 23 feed2exec Documentation, Release ??? Basic plugin principles</p>

<p>To write a new plugin, you should start by creating a simple Python module, in your PYTHONPATH. You can find which directories are in the path by calling:</p>

<p>$ python3 -c “import sys; print(sys.path)” [’’ , ‘/usr/lib/python35.zip ‘, ‘/usr/lib/python3.5 ‘, ‘/usr/lib/python3.5/plat-x86_64-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>linux-gnu ‘, ‘/usr/lib/python3.5/lib-dynload ‘, ‘/usr/local/lib/python3.5/dist-packages ‘,</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>‘/usr/lib/python3/dist-packages ‘]</p>

<p>In the above example, a good location would be /usr/local/lib/python3.5/dist-packages . The naming con-vention is loose: as long as the plugin matches the expected API, it should just work. For the purpose of this demon-stration, we’ll call our plugin trumpery, so we will create the plugin code like this:</p>

<p>touch /usr/local/lib/python3.5/dist-packages/trumpery.py</p>

<p>Naturally, if you are going to write multiple plugins, you may want to regroup your multiple plugins in a package, see the module documentation for more information about this concept in Python.</p>

<p>Note: There is a rudimentary plugin resolution process that looks for plugins first in the feed2exec.plugins namespace but then globally. This is done in feed2exec.plugins.resolve() , called from the add and parse commands. This means that the absolute path is expected to be used in the configuration file and internally. You are welcome to distribute plugins separately or send them as merge requests, see Contribution guide for more information on how to participate in this project. We of course welcome contributions to this documentation as well!</p>

<p>Filters</p>

<p>Now, you need your plugin to do something. In our case, let’s say we’d like to skip any feed entry that has the word Trump in it. For that purpose, we’ll create a plugin similar to the already existing feed2exec.plugins.droptitle</p>

<p>plugin, but that operates on the body of the feed, but that also hardcodes the word, because this is just a demonstration and we want to keep it simple. Let’s look at the title plugin to see how it works:</p>

<p>def filter(*args, feed= None , item= None , **kwargs):</p>

<p>’’’ the droptitle filter will drop any feed item with a title matching the given args. Example:: [NASA breaking news] url = https://www.nasa.gov/rss/dyn/breaking_news.rss filter = feed2exec.plugins.droptitle filter_args = Trump The above will process the feed items according to the global configuration, but will skip any item that has the word “Trump” anywhere in the title field. Arguments are processed as a single string. If you need to match more complex patterns, look at the droptitleregex plugin instead.</p>

<p>’’’</p>

<p>item[ ‘skip ‘] = ‘ ‘ .join(args) in item.get( ‘title ‘, ‘’ )</p>

<p>24 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<p>That may look like complete gibberish to you if you are not familiar with programming or with Python programming in particular. But let’s take this from the top and copy that in our own plugin. The first line declares a function that takes at least a feed and a item argument, but can also accept any other arbitrary argument. This is important because we want to have the plugin keep on working if the plugin API changes in the future. This is called “forward-compatibility”. So let’s copy that in our plugin and add a pass statement to make sure the plugin works (even if it does nothing for now):</p>

<p>def filter(*args, feed= None , item= None , **kwargs):</p>

<p>pass</p>

<p>We can already test our plugin by adding it to our configuration, in ~/.config/feed2exec.ini :</p>

<p>[NASA] url = https://www.nasa.gov/rss/dyn/breaking_news.rss output = feed2exec.plugins.echo args = {item.title} filter = trumpery</p>

<p>Notice how we use the output plugin to show the title of feed items selected, as a debugging tool. Let’s fetch this feed in debugging mode to see what happens:</p>

<p>$ python3 -m feed2exec –verbose fetch –force opening local file /home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml parsing feed file:///home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>(10355 bytes) connecting to database at ./doc/feed2exec.db arguments received: ( ‘President Trump Welcomes Home Record-breaking NASA Astronaut Peggy␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Whitson ‘,) arguments received: ( ‘Three International Space Station Crewmates Safely Return to Earth</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>’,) arguments received: ( ‘NASA Statement on Nomination for Agency Administrator ‘,) arguments received: ( ‘NASA Television to Air Return of Three International Space Station␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Crew Members ‘,) arguments received: ( ‘NASA and Iconic Museum Honor Voyager Spacecraft 40th Anniversary ‘,) arguments received: ( ‘NASA’s Johnson Space Center Closes Through Labor Day for Tropical␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Storm Harvey ‘,) arguments received: ( ‘NASA Cancels Planned Media Availabilities with Astronauts ‘,) arguments received: ( ‘NASA Awards $400,000 to Top Teams at Second Phase of 3D-Printing␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Competition ‘,) arguments received: ( ‘NASA Awards Contract for Center Protective Services for Glenn␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Research Center ‘,) arguments received: ( ‘NASA Announces Cassini End-of-Mission Media Activities ‘,) 1 feeds processed</p>

<p>Good! The feed is fetched and items are displayed. It means our filter didn’t interfere, but now it’s time to make it do</p>

<p>something. To skip items, we need to set the skip attribute for the feed item to True if we want to skip it and False</p>

<p>otherwise. So we’ll use a simple recipe, a bit like droptitle does, but simpler, to look at the feed content to look for our evil word. The feedparser documentation tells us feed items have a summary field which we can inspect. There’s also a content list, but that’s a little more complicated so we’ll skip that for now. So, let’s set the skip parameter to match if there is the evil word in our feed item, like this:</p>

<p>def filter(*args, feed= None , item= None , **kwargs): item[ ‘skip ‘] = ‘Trump ‘ in item.get( ‘summary ‘, ‘’ )</p>

<p>And let’s see the result (note that we use the –force argument here otherwise we would just skip all items because</p>

<p>3.4. Plugins 25 feed2exec Documentation, Release ???</p>

<p>of the cache):</p>

<p>$ python3 -m feed2exec –verbose fetch –force opening local file /home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml parsing feed file:///home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>(10355 bytes) connecting to database at ./doc/feed2exec.db item President Trump Welcomes Home Record-breaking NASA Astronaut Peggy Whitson of feed␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>NASA filtered out arguments received: ( ‘Three International Space Station Crewmates Safely Return to Earth</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>’,) item NASA Statement on Nomination for Agency Administrator of feed NASA filtered out arguments received: ( ‘NASA Television to Air Return of Three International Space Station␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Crew Members ‘,) arguments received: ( ‘NASA and Iconic Museum Honor Voyager Spacecraft 40th Anniversary ‘,) arguments received: ( ‘NASA’s Johnson Space Center Closes Through Labor Day for Tropical␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Storm Harvey ‘,) arguments received: ( ‘NASA Cancels Planned Media Availabilities with Astronauts ‘,) arguments received: ( ‘NASA Awards $400,000 to Top Teams at Second Phase of 3D-Printing␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Competition ‘,) arguments received: ( ‘NASA Awards Contract for Center Protective Services for Glenn␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Research Center ‘,) arguments received: ( ‘NASA Announces Cassini End-of-Mission Media Activities ‘,) 1 feeds processed</p>

<p>Success! We have skipped the two items that contain the fraud we wanted to remove from the world. Notice how we were able to modify the feed item: we can also use that to change the feed content. Normally, we would use this to fix malformed feeds, but let’s have some fun instead and rename Trump to Drumpf:</p>

<p>def filter(*args, feed= None , item= None , **kwargs): item[ ‘title ‘] = item.get( ‘title ‘, ‘’ ).replace( ‘Trump ‘, ‘Drumpf ‘)</p>

<p>And the result:</p>

<p>$ python3 -m feed2exec –verbose fetch –force opening local file /home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml parsing feed file:///home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>(10355 bytes) connecting to database at ./doc/feed2exec.db arguments received: ( ‘President Drumpf Welcomes Home Record-breaking NASA Astronaut␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Peggy Whitson ‘,) arguments received: ( ‘Three International Space Station Crewmates Safely Return to Earth</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>’,) arguments received: ( ‘NASA Statement on Nomination for Agency Administrator ‘,) arguments received: ( ‘NASA Television to Air Return of Three International Space Station␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Crew Members ‘,) arguments received: ( ‘NASA and Iconic Museum Honor Voyager Spacecraft 40th Anniversary ‘,) arguments received: ( ‘NASA’s Johnson Space Center Closes Through Labor Day for Tropical␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Storm Harvey ‘,) arguments received: ( ‘NASA Cancels Planned Media Availabilities with Astronauts ‘,) arguments received: ( ‘NASA Awards $400,000 to Top Teams at Second Phase of 3D-Printing␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Competition ‘,) arguments received: ( ‘NASA Awards Contract for Center Protective Services for Glenn␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Research Center ‘,)</p>

<blockquote>
  <p>(continues on next page)</p>
</blockquote>

<p>26 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<blockquote>
  <p>(continued from previous page)</p>
</blockquote>

<p>arguments received: ( ‘NASA Announces Cassini End-of-Mission Media Activities ‘,) 1 feeds processed</p>

<p>I know, absolutely hilarious, right? More seriously, this is also how the feed2exec.plugins.html2text filter works, which is enabled by default and helps the email output plugin do its job by turning HTML into text. At this point, the only limit is your knowledge of Python programming and your imagination!</p>

<p>Output plugins</p>

<p>Output plugins are another beast entirely. While they operate with the same principle than filter plugins (search path and function signature are similar), they are designed to actually output something for each new feed item found. This can be anything: a file, email, HTTP request, whatever. If there is a commandline tool that does what you need, it is probably simpler to just call the exec plugin and there are numerous examples of this in the sample configuration file. For more complex things, however, it may be easier to actually write this as a Python.</p>

<p>Basic arguments</p>

<p>For our example, we’ll write an archival plugin which writes each new entry to a file hierarchy. First, we start with the same simple function signature as filters, except we name it output:</p>

<p>def output(*args, feed= None , item= None , **kwargs):</p>

<p>pass</p>

<p>This is the equivalent of the null plugin and basically outputs nothing at all. To archive the feed items, we’ll need to look at the link element feedparser gives us. Let’s see what that looks like for the NASA feed:</p>

<p>def output(*args, feed= None , item= None , **kwargs):</p>

<h1 id="only-operate-on-items-that-actually-have-a-link">only operate on items that actually have a link</h1>

<p>if item.get( ‘link ‘): print(item.get( ‘link ‘, ‘’ ))</p>

<p>else :logging.info( ‘no link for feed item %s , not archiving ‘, item.get( ‘title ‘))</p>

<p>Note: Note that we try to make plugins silent in general. You can use logging.info() to have things show up in</p>

<p>–verbose and logging.debug() for –debug but by default, your plugin should be silent unless there’s an error that requires the user’s intervention, in which case you should use logging.warning() for transient errors that may be automatically recovered and logging.error() for errors that require user intervention. This is to allow users to ignore warnings safely. Note that here we first check to see if the feed item actually has a link - not all feeds do! After adding the above to our</p>

<p>trumpery plugin and adding it as an output plugin:</p>

<p>[NASA] url = https://www.nasa.gov/rss/dyn/breaking_news.rss output = trumpery filter = trumpery</p>

<p>We can try to see what happens when we call it:</p>

<p>3.4. Plugins 27 feed2exec Documentation, Release ???</p>

<p>$ python3 -m feed2exec –verbose fetch –force opening local file /home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml parsing feed file:///home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>(10355 bytes) connecting to database at ./doc/feed2exec.db http://www.nasa.gov/press-release/president-trump-welcomes-home-record-breaking-nasa-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>astronaut-peggy-whitson http://www.nasa.gov/press-release/three-international-space-station-crewmates-safely-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>return-to-earth http://www.nasa.gov/press-release/nasa-statement-on-nomination-for-agency-administrator http://www.nasa.gov/press-release/nasa-television-to-air-return-of-three-international-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>space-station-crew-members http://www.nasa.gov/press-release/nasa-and-iconic-museum-honor-voyager-spacecraft-40th-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>anniversary http://www.nasa.gov/press-release/nasa-s-johnson-space-center-closes-through-labor-day-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>for-tropical-storm-harvey http://www.nasa.gov/press-release/nasa-cancels-planned-media-availabilities-with-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>astronauts http://www.nasa.gov/press-release/nasa-awards-400000-to-top-teams-at-second-phase-of-3d-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>printing-competition http://www.nasa.gov/press-release/nasa-awards-contract-for-center-protective-services-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>for-glenn-research-center http://www.nasa.gov/press-release/nasa-announces-cassini-end-of-mission-media-activities 1 feeds processed</p>

<p>Sanitizing contents</p>

<p>Good. Those are the URLs we want to save to disk. Let’s start by just writing those to a file. We will also use a simple</p>

<p>slug function to make a filesystem-safe name from the feed title and save those files in a pre-determined location:</p>

<p>import logging import os.path from feed2exec.utils import slug ARCHIVE_DIR= ‘/run/user/1000/feed-archives/ ‘</p>

<p>def output(*args, feed= None , item= None , session= None , **kwargs):</p>

<h1 id="make-a-safe-path-from-the-item-name">make a safe path from the item name</h1>

<p>path = slug(item.get( ‘title ‘, ‘no-name ‘))</p>

<h1 id="put-the-file-in-the-archive-directory">put the file in the archive directory</h1>

<p>path = os.path.join(ARCHIVE_DIR, path)</p>

<h1 id="only-operate-on-items-that-actually-have-a-link-1">only operate on items that actually have a link</h1>

<p>if item.get( ‘link ‘):</p>

<h1 id="tell-the-user-what-s-going-on-if-verbose--otherwise-we-try-to-stay-silent-if-all-goes-well">tell the user what ‘s going on, if verbose # otherwise, we try to stay silent if all goes well</h1>

<p>logging.info( ‘saving feed item %s to %s from %s ‘,item.get( ‘title ‘), path, item.get( ‘link ‘))</p>

<h1 id="open-the-file">open the file</h1>

<p>with open(path, ‘w’) as archive:</p>

<h1 id="write-the-response">write the response</h1>

<p>archive.write(item.get( ‘link ‘))</p>

<blockquote>
  <p>(continues on next page)</p>
</blockquote>

<p>28 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<blockquote>
  <p>(continued from previous page)</p>
</blockquote>

<p>else :logging.info( ‘no link for feed item %s , not archiving ‘, item.get( ‘title ‘))</p>

<p>Now I know this may look like a huge step from the previous one but I’m sorry, I couldn’t find a simpler second step. :) The output now looks like this:</p>

<p>$ python3 -m feed2exec –config ./doc/ –verbose fetch –force opening local file /home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml parsing feed file:///home/anarcat/src/feed2exec/feed2exec/tests/files/breaking_news.xml␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>(10355 bytes) connecting to database at ./doc/feed2exec.db saving feed item President Drumpf Welcomes Home Record-breaking NASA Astronaut Peggy␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Whitson to /run/user/1000/president-drumpf-welcomes-home-record-breaking-nasa-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>astronaut-peggy-whitson from http://www.nasa.gov/press-release/president-trump-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>welcomes-home-record-breaking-nasa-astronaut-peggy-whitson saving feed item Three International Space Station Crewmates Safely Return to Earth to /</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>run/user/1000/three-international-space-station-crewmates-safely-return-to-earth from␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>http://www.nasa.gov/press-release/three-international-space-station-crewmates-safely-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>return-to-earth saving feed item NASA Statement on Nomination for Agency Administrator to /run/user/1000/</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>nasa-statement-on-nomination-for-agency-administrator from http://www.nasa.gov/press-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>release/nasa-statement-on-nomination-for-agency-administrator saving feed item NASA Television to Air Return of Three International Space Station Crew␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Members to /run/user/1000/nasa-television-to-air-return-of-three-international-space-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>station-crew-members from http://www.nasa.gov/press-release/nasa-television-to-air-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>return-of-three-international-space-station-crew-members saving feed item NASA and Iconic Museum Honor Voyager Spacecraft 40th Anniversary to /</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>run/user/1000/nasa-and-iconic-museum-honor-voyager-spacecraft-40th-anniversary from␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>http://www.nasa.gov/press-release/nasa-and-iconic-museum-honor-voyager-spacecraft-40th-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>anniversary saving feed item NASA’s Johnson Space Center Closes Through Labor Day for Tropical Storm␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Harvey to /run/user/1000/nasa-s-johnson-space-center-closes-through-labor-day-for-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>tropical-storm-harvey from http://www.nasa.gov/press-release/nasa-s-johnson-space-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>center-closes-through-labor-day-for-tropical-storm-harvey saving feed item NASA Cancels Planned Media Availabilities with Astronauts to /run/user/</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>1000/nasa-cancels-planned-media-availabilities-with-astronauts from http://www.nasa.</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>gov/press-release/nasa-cancels-planned-media-availabilities-with-astronauts saving feed item NASA Awards $400,000 to Top Teams at Second Phase of 3D-Printing␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Competition to /run/user/1000/nasa-awards-400-000-to-top-teams-at-second-phase-of-3d-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>printing-competition from http://www.nasa.gov/press-release/nasa-awards-400000-to-top-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>teams-at-second-phase-of-3d-printing-competition saving feed item NASA Awards Contract for Center Protective Services for Glenn Research␣</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>Center to /run/user/1000/nasa-awards-contract-for-center-protective-services-for-glenn-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>research-center from http://www.nasa.gov/press-release/nasa-awards-contract-for-center-</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>protective-services-for-glenn-research-center saving feed item NASA Announces Cassini End-of-Mission Media Activities to /run/user/</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>1000/nasa-announces-cassini-end-of-mission-media-activities from http://www.nasa.gov/</p>

<blockquote>
  <p>˓→</p>
</blockquote>

<p>press-release/nasa-announces-cassini-end-of-mission-media-activities</p>

<p>Sweet! Now it’s not really nice to save this in /run/user/1000 . I just chose this directory because it was a safe place to write but it’s not a persistent directory. Best make that configurable, which is where plugin arguments come in.</p>

<p>3.4. Plugins 29 feed2exec Documentation, Release ??? User configuration</p>

<p>You see that *args parameter? That comes straight from the configuration file. So you could set the path in the configuration file, like this:</p>

<p>[NASA] url = https://www.nasa.gov/rss/dyn/breaking_news.rss output = trumpery args = /srv/archives/nasa/ filter = trumpery</p>

<p>We also need to modify the plugin to fetch that configuration, like this:</p>

<p>def output(*args, feed= None , item= None , session= None , **kwargs):</p>

<h1 id="make-a-safe-path-from-the-item-name-1">make a safe path from the item name</h1>

<p>path = slug(item.get( ‘title ‘, ‘no-name ‘))</p>

<h1 id="take-the-archive-dir-from-the-user-or-use-the-default">take the archive dir from the user or use the default</h1>

<p>archive_dir = ‘ ‘ .join(args) if args else DEFAULT_ARCHIVE_DIR</p>

<h1 id="put-the-file-in-the-archive-directory-1">put the file in the archive directory</h1>

<p>path = os.path.join(archive_dir, path)</p>

<h1 id="--rest-of-the-function-unchanged">[…] # rest of the function unchanged</h1>

<p>Making HTTP requests</p>

<p>And now obviously, we only saved the link itself, not the link content . For that we need some help from the requests</p>

<p>module, and do something like this:</p>

<h1 id="fetch-the-url-in-memory">fetch the URL in memory</h1>

<p>result = session.get(item.get( ‘link ‘))</p>

<p>if result.status_code != requests.codes.ok: logging.warning( ‘failed to fetch link %s : %s ‘,item.get( ‘link ‘), result.status_code)</p>

<h1 id="make-sure-we-retry-next-time">make sure we retry next time</h1>

<p>return False</p>

<h1 id="open-the-file-1">open the file</h1>

<p>with open(path, ‘w’) as archive:</p>

<h1 id="write-the-response-1">write the response</h1>

<p>archive.write(result.text)</p>

<p>This will save the actual link content ( result.text ) to the file. The important statement here is:</p>

<h1 id="fetch-the-url-in-memory-1">fetch the URL in memory</h1>

<p>result = session.get(item.get( ‘link ‘))</p>

<p>which fetches the URL in memory and checks for errors. The other change in the final plugin is simply:</p>

<p>archive.write(result.text)</p>

<p>which writes the article content instead of the link. Notice how the session argument is used here instead of talking directly to the requests module. This leverages a caching system we already have, alongside configuration like user-agent and so on.</p>

<p>30 Chapter 3. Why the name? feed2exec Documentation, Release ??? Plugin return values</p>

<p>Notice how we return False here: this makes the plugin system avoid adding the item to the cache, so it is retried on the next run. If the plugin returns True or nothing ( None ), the plugin is considered to have succeeded and the entry is added to the cache. That logic is defined in feed2exec.controller.FeedManager.fetch() .</p>

<p>Catchup</p>

<p>A final thing that is missing that is critical in all plugins is to respect the catchup setting. It is propagated up from the commandline or configuration all the way down to plugins, through the feed parameters. How you handle it varies from plugin to plugin, but the basic idea is to give feedback (when verbose) of activity when the plugin is run but to not actually do anything. In our case, we simply return success, right before we fetch the URL:</p>

<p>if feed.get( ‘catchup ‘):</p>

<p>return True</p>

<h1 id="fetch-the-url-in-memory-2">fetch the URL in memory</h1>

<p>result = session.get(item.get( ‘link ‘))</p>

<p>Notice how we still fetch the actual feed content but stop before doing any permanent operation. That is the spirit of the “catchup” operation: we not only skip “write” operation, but also any operation which could slow down the “catchup”: fetching stuff over the network takes time and while it can be considered a “readonly” operation as far as the local machine is concerned, we are effectively writing to the network so that operation shouldn’t occur. Hopefully that should get you going with most of the plugins you are thinking of writing!</p>

<p>Writing tests</p>

<p>Writing tests is essential in ensuring that the code will stay maintainable in the future. It allows for easy refactoring and can find bugs that manual testing may not, especially when you get complete coverage (although that is no guarantee either). We’ll take our archive plugin as an example. The first step is to edit the tests/test/test_plugins.py file, where other plugins are tests as well. We start by creating a function named test_archive so that Pytest, our test bed, will find it:</p>

<p>def test_archive(tmpdir, betamax): # noqa</p>

<p>pass</p>

<p>Notice the two arguments named tmpdir and betamax . Both of those are fixtures, a pytest concept that allows to simulate an environment. In particular, the tmpdir fixture, shipped with pytest, allows you to easily manage (and automatically remove) temporary directories. The betamax fixtures is a uses the betamax module to record then replay HTTP requests. Then we need to do something. We need to create a feed and a feed item that we can then send into the plugin. We could also directly parse an existing feed and indeed some plugins do exactly that. But our plugin is simple and we can afford to skip full feed parsing and just synthesize what we need:</p>

<p>feed = Feed( ‘test archive ‘, test_sample) item = feedparser.FeedParserDict({ ‘link ‘: ‘http://example.com/ ‘,</p>

<p>‘title ‘: ‘example site ‘})</p>

<p>This creates a new feed based on the test_sample feed. This is necessary so that the session is properly re-initialized in the feed item (otherwise the betamax fixture will not work). Then it creates a fake feed entry simply with one link</p>

<p>3.4. Plugins 31 feed2exec Documentation, Release ???</p>

<p>and a title. Then we can call our plugin, and verify that it saves the file as we expected. The test for the most common case looks like this:</p>

<p>def test_archive(tmpdir, betamax): # noqa</p>

<p>dest = tmpdir.join( ‘archive ‘)feed = Feed( ‘test archive ‘, test_sample) item = feedparser.FeedParserDict({ ‘link ‘: ‘http://example.com/ ‘,</p>

<p>‘title ‘: ‘example site ‘})</p>

<p>assert archive_plugin.output(str(dest), feed=feed, item=item)</p>

<p>assert dest.join( ‘example-site ‘).check()</p>

<p>Then we can try to run this with pytest-3 :</p>

<p>[1084]anarcat@curie:feed2exec$ pytest-3 =============================== test session starts =============================== platform linux – Python 3.5.3, pytest-3.0.6, py-1.4.32, pluggy-0.4.0 rootdir: /home/anarcat/src/feed2exec, inifile: setup.cfg plugins: profiling-1.2.11, cov-2.4.0, betamax-0.8.0 collected 26 items feed2exec/utils.py .. feed2exec/plugins/transmission.py . feed2exec/tests/test_feeds.py …….. feed2exec/tests/test_main.py ….. feed2exec/tests/test_opml.py . feed2exec/tests/test_plugins.py ……… ———– coverage: platform linux, python 3.5.3-final-0 ———–Name Stmts Miss Cover —————————————————————-feed2exec/<strong>init</strong>.py 12 0 100% feed2exec/<strong>main</strong>.py 87 1 99% feed2exec/_version.py 1 0 100% feed2exec/email.py 81 7 91% feed2exec/feeds.py 243 8 97% feed2exec/logging.py 31 11 65% feed2exec/plugins/<strong>init</strong>.py 47 6 87% feed2exec/plugins/archive.py 23 5 78% feed2exec/plugins/droptitle.py 2 0 100% feed2exec/plugins/echo.py 8 0 100% feed2exec/plugins/emptysummary.py 5 0 100% feed2exec/plugins/error.py 2 0 100% feed2exec/plugins/exec.py 7 0 100% feed2exec/plugins/html2text.py 20 4 80% feed2exec/plugins/ikiwiki_recentchanges.py 9 5 44% feed2exec/plugins/maildir.py 28 0 100% feed2exec/plugins/mbox.py 29 1 97% feed2exec/plugins/null.py 5 1 80% feed2exec/plugins/transmission.py 20 0 100% feed2exec/plugins/wayback.py 20 0 100% feed2exec/tests/<strong>init</strong>.py 0 0 100% feed2exec/tests/conftest.py 3 0 100% feed2exec/tests/fixtures.py 19 0 100% feed2exec/tests/test_feeds.py 124 0 100% (continues on next page)</p>

<p>32 Chapter 3. Why the name? feed2exec Documentation, Release ???</p>

<blockquote>
  <p>(continued from previous page)</p>
</blockquote>

<p>feed2exec/tests/test_main.py 90 0 100% feed2exec/tests/test_opml.py 17 0 100% feed2exec/tests/test_plugins.py 162 0 100% feed2exec/utils.py 41 12 71% —————————————————————-TOTAL 1136 61 95% =========================== 26 passed in 10.83 seconds ============================</p>

<p>Notice the test coverage: we only have 78% test coverage for our plugin. This means that some branches of the code were not executed at all! Let’s see if we can improve that. Looking at the code, I see there are some conditionals for error handling. So let’s simulate an error, and make sure that we don’t create a file on error:</p>

<p>dest.remove() item = feedparser.FeedParserDict({ ‘link ‘: ‘http://example.com/404 ‘,</p>

<p>‘title ‘: ‘example site ‘})</p>

<p>assert not archive_plugin.output(str(dest), feed=feed, item=item)</p>

<p>assert not dest.join( ‘example-site ‘).check()</p>

<p>There. Let’s see the effect on the test coverage:</p>

<p>[1085]anarcat@curie:feed2exec2$ pytest-3 feed2exec/tests/test_plugins.py::test_archive =============================== test session starts =============================== platform linux – Python 3.5.3, pytest-3.0.6, py-1.4.32, pluggy-0.4.0 rootdir: /home/anarcat/src/feed2exec, inifile: setup.cfg plugins: profiling-1.2.11, cov-2.4.0, betamax-0.8.0 collected 10 items feed2exec/tests/test_plugins.py . ———– coverage: platform linux, python 3.5.3-final-0 ———–Name Stmts Miss Cover —————————————————————-feed2exec/<strong>init</strong>.py 12 0 100% feed2exec/<strong>main</strong>.py 87 87 0% feed2exec/_version.py 1 0 100% feed2exec/email.py 81 64 21% feed2exec/feeds.py 243 172 29% feed2exec/logging.py 31 31 0% feed2exec/plugins/<strong>init</strong>.py 47 38 19% feed2exec/plugins/archive.py 23 3 87% feed2exec/plugins/droptitle.py 2 2 0% feed2exec/plugins/echo.py 8 3 62% feed2exec/plugins/emptysummary.py 5 5 0% feed2exec/plugins/error.py 2 2 0% feed2exec/plugins/exec.py 7 7 0% feed2exec/plugins/html2text.py 20 13 35% feed2exec/plugins/ikiwiki_recentchanges.py 9 9 0% feed2exec/plugins/maildir.py 28 19 32% feed2exec/plugins/mbox.py 29 29 0% feed2exec/plugins/null.py 5 5 0%</p>

<blockquote>
  <p>(continues on next page)</p>
</blockquote>

<p>3.4. Plugins 33 feed2exec Documentation, Release ???</p>

<blockquote>
  <p>(continued from previous page)</p>
</blockquote>

<p>feed2exec/plugins/transmission.py 20 12 40% feed2exec/plugins/wayback.py 20 20 0% feed2exec/tests/<strong>init</strong>.py 0 0 100% feed2exec/tests/conftest.py 3 0 100% feed2exec/tests/fixtures.py 19 6 68% feed2exec/tests/test_feeds.py 124 101 19% feed2exec/tests/test_main.py 90 90 0% feed2exec/tests/test_opml.py 17 17 0% feed2exec/tests/test_plugins.py 166 123 26% feed2exec/utils.py 41 16 61% —————————————————————-TOTAL 1140 874 23% ============================ 1 passed in 2.46 seconds =============================</p>

<p>Much better! Only 3 lines left to cover!</p>

<p>Note: Notice how I explicitly provided a path to my test. This is entirely optional. You can just run pytest-3 and it will run the whole test suite: this method is just faster. Notice also how the coverage ratio is very low: this is normal; we are testing, after all, only one plugin here. The only branches left to test in the code is the other possible error (“no link in the feed”) and to test the “catchup” mode. You can see this in the actual test_plugins.py file distributed with this documentation.</p>

<p>Note: If you discover a bug associated with a single feed, you can use the betamax session and the feed2exec.model. Feed.parse() function to manually parse a feed and fire your plugin. This is how email functionality is tested: see the feed2exec.tests.test_plugins.test_email() function for an example.</p>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">ARC PRIZE</title><link href="https://ib.bsb.br/arc-prize/" rel="alternate" type="text/html" title="ARC PRIZE" /><published>2025-09-29T00:00:00+00:00</published><updated>2025-09-29T17:16:05+00:00</updated><id>https://ib.bsb.br/arc-prize</id><content type="html" xml:base="https://ib.bsb.br/arc-prize/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Subtask 1 — System Role &amp; Run Flow
Objective  Define the pipeline goal and top-level control flow.
Inputs  ARC JSON challenges; RunConfig; env vars (keys, MAX_CONCURRENCY, flags).
Procedure / Prompt Spec or Algorithm  Use src/run.py::run() → run_from_json() to load challenges, choose preset, orchestrate solve_challenges() which schedules solve_challenge(); within it call get_answer_grids() to generate/score instructions and finalize guesses.
Outputs / Artifacts  Attempts JSON in attempts/arc-prize-20XX/...; per-task temp_solutions/*.json.
Dependencies / Anchors  README.md (How the system works, Running a solve); src/run.py::{run, run_from_json, solve_challenges, solve_challenge, get_answer_grids, return_answer}.
Acceptance Criteria  Running python src/run.py produces an attempts file and (if truth present) printed accuracy; no unhandled exceptions.
Failure Modes &amp; Mitigations  Missing env keys → set per README; rate limits → lower concurrency flags.
:::

Subtask 2 — Data Models &amp; Grid Representation
Objective  Normalize grid IO for prompts and scoring.
Inputs  Challenge, Example, Input models; integer grids; COLOR_MAP.
Procedure / Prompt Spec or Algorithm  Render grids via Challenge.grid_to_str; optionally embed base64 PNG via Challenge.grid_to_base64/viz.base64_from_grid.
Outputs / Artifacts  Text matrices; optional images.
Dependencies / Anchors  src/models.py::{Challenge, Example, Input, grid_to_str, grid_to_base64, COLOR_MAP}; src/viz.py::{base64_from_grid}.
Acceptance Criteria  Stringified grids are rectangular, integer-only; base64 generation does not raise.
Failure Modes &amp; Mitigations  Image encoding errors → log and fall back to text-only (see main.contents_from_grid try/except).
:::

Subtask 3 — Prompt Packing
Objective  Build consistent message content for LLMs.
Inputs  Training examples, optional attempts, test inputs; toggles: include_base64, use_diffs.
Procedure / Prompt Spec or Algorithm  Use contents_from_grid, contents_from_example, contents_from_challenge to produce a list of typed parts with labeled sections and (optionally) diffs.
Outputs / Artifacts  Structured message content lists.
Dependencies / Anchors  src/main.py::{contents_from_grid, contents_from_example, contents_from_challenge}; diff text via src/run.py::generate_grid_diff.
Acceptance Criteria  Message list order matches training→tests; when diffs enabled, mismatches include ASCII diff.
Failure Modes &amp; Mitigations  Context bloat → disable images/diffs; reduce step sizes.
:::

Subtask 4 — Instruction Synthesis
Objective  Derive a general rule as stepwise instructions.
Inputs  INTUITIVE_PROMPT; packed examples/tests.
Procedure / Prompt Spec or Algorithm  Call get_next_structure(InstructionsResponse, model=step.instruction_model, messages=…) from get_instructions_from_challenge.
Outputs / Artifacts  InstructionsResponse.instructions.
Dependencies / Anchors  src/main.py::{INTUITIVE_PROMPT, InstructionsResponse}; src/run.py::{get_instructions_from_challenge}.
Acceptance Criteria  Non-empty string; avoids example-specific indices per prompt guidance.
Failure Modes &amp; Mitigations  Empty/invalid → retry via step sampling; move to revision/pooling.
:::

Subtask 5 — Executor (Follow Instructions)
Objective  Apply instructions to a test grid to produce an output grid.
Inputs  Instructions; training examples (as reference); one test input; flags is_perfect, include_base64, use_diffs.
Procedure / Prompt Spec or Algorithm  Use AGENT_FOLLOW_INSTRUCTIONS_PROMPT and optional PERFECT_PROMPT; call output_grid_from_instructions which invokes get_next_structure(GridResponse, …).
Outputs / Artifacts  GridResponse.grid (2D ints).
Dependencies / Anchors  src/main.py::{AGENT_FOLLOW_INSTRUCTIONS_PROMPT, PERFECT_PROMPT, GridResponse, output_grid_from_instructions}.
Acceptance Criteria  Grid shape matches target’s shape during scoring; integers only.
Failure Modes &amp; Mitigations  Free text instead of grid → schema enforcement via get_next_structure.
:::

Subtask 6 — Structured Output Enforcement
Objective  Ensure parseable, schema-validated LLM outputs.
Inputs  Pydantic schemas; provider selection.
Procedure / Prompt Spec or Algorithm  Route via get_next_structure which dispatches to _get_next_structure_* per model (OpenAI/Anthropic/Gemini/DeepSeek/xAI/OpenRouter), using JSON/object or tool use as supported.
Outputs / Artifacts  Parsed Pydantic instances.
Dependencies / Anchors  src/llms/structured.py::{get_next_structure, _get_next_structure_openai, _get_next_structure_anthropic, _get_next_structure_gemini, _get_next_structure_deepseek, _get_next_structure_xai, _get_next_structure_openrouter}; src/llms/models.py::Model.
Acceptance Criteria  Successful parse or controlled failure with retries; no downstream string parsing needed.
Failure Modes &amp; Mitigations  Model lacks json_schema → use json_object path; retry with backoff.
:::

Subtask 7 — Scoring (Leave-One-Out)
Objective  Quantify instruction generalization.
Inputs  Candidate instructions; full training set; follow_model.
Procedure / Prompt Spec or Algorithm  For each training example i, hold out i, execute on its input, compare to its output using get_grid_similarity (exact cell-wise match proportion); average over examples.
Outputs / Artifacts  InstructionsScore with example_scores and aggregate score.
Dependencies / Anchors  src/run.py::{get_example_score, get_grid_similarity, score_instructions_on_challenge}.
Acceptance Criteria  Score in [0,1]; 1.0 iff all cells match on all held-out examples.
Failure Modes &amp; Mitigations  Dimensional mismatch → similarity 0; optional viz when VIZ=1.
:::

Subtask 8 — Revision (Self-Repair)
Objective  Improve weak instructions using feedback from mismatches.
Inputs  Prior InstructionsScore; diffs/attempts; StepRevision config.
Procedure / Prompt Spec or Algorithm  Prompt with REVISION_PROMPT via InstructionsScore.get_revised_instructions; rescore revised outputs.
Outputs / Artifacts  New InstructionsScore items.
Dependencies / Anchors  src/run.py::{REVISION_PROMPT, InstructionsScore.get_revised_instructions}; src/configs/models.py::StepRevision.
Acceptance Criteria  Best revised score ≥ previous best; log improvement metrics.
Failure Modes &amp; Mitigations  No gains → move to pooling; adjust sampling counts.
:::

Subtask 9 — Pooling (Synthesis Across Attempts)
Objective  Fuse strengths from several near-miss instruction sets.
Inputs  Top InstructionsScore samples; StepRevisionPool config.
Procedure / Prompt Spec or Algorithm  Use SYNTHESIS_PROMPT via get_pooling_instruction_from_scores; rescore; merge with candidates.
Outputs / Artifacts  Pooled instruction texts and scores.
Dependencies / Anchors  src/run.py::{SYNTHESIS_PROMPT, get_pooling_instruction_from_scores}; src/configs/models.py::StepRevisionPool.
Acceptance Criteria  At least one pooled instruction’s score ≥ prior top.
Failure Modes &amp; Mitigations  Convergence to weak consensus → increase diversity (times), keep multiple candidates.
:::

Subtask 10 — Step Orchestration &amp; Sampling
Objective  Execute configured Step/Revision/Pool sequence.
Inputs  RunConfig.steps with per-step models, counts, timeouts, flags.
Procedure / Prompt Spec or Algorithm  In get_answer_grids, generate candidates per step, rescore, sort desc, short-list for subsequent steps; exit early on perfect score.
Outputs / Artifacts  Sorted candidate list.
Dependencies / Anchors  src/run.py::{get_instruction_scores, get_score_from_instructions, get_answer_grids}; src/configs/models.py::{RunConfig, Step, StepRevision, StepRevisionPool}.
Acceptance Criteria  Deterministic step flow with early exit on score==1.
Failure Modes &amp; Mitigations  Empty candidate set → log error and halt (exception path).
:::

Subtask 11 — Finalization &amp; Diversity
Objective  Produce up to two distinct final guesses per test grid.
Inputs  Top candidates; final_follow_model; final_follow_times.
Procedure / Prompt Spec or Algorithm  Use get_diverse_attempts to generate multiple outputs; prefer perfect-score instructions; otherwise split attempts between top two candidates; ensure diversity when possible.
Outputs / Artifacts  Two Guess objects; attempts JSON updated.
Dependencies / Anchors  src/run.py::{get_diverse_attempts, return_answer, Guess}.
Acceptance Criteria  Each test input yields ≤2 outputs; if all generated outputs are identical, duplicates are allowed per code.
Failure Modes &amp; Mitigations  Lack of diversity → explicitly bias attempts across top-2 instruction sources (as implemented).
:::

Subtask 12 — Concurrency Control
Objective  Prevent provider overload and monitor saturation.
Inputs  Env MAX_CONCURRENCY; config max_concurrent_tasks.
Procedure / Prompt Spec or Algorithm  Use MonitoredSemaphore in both run loop and get_next_structure (API semaphore); log active/available permits.
Outputs / Artifacts  Saturation logs, orderly task scheduling.
Dependencies / Anchors  src/async_utils/semaphore_monitor.py::MonitoredSemaphore; usage in src/run.py::solve_challenges and src/llms/structured.py::API_SEMAPHORE.
Acceptance Criteria  No storm of RESOURCE_EXHAUSTED; visible saturation percentage logs.
Failure Modes &amp; Mitigations  Starvation/deadlocks → minimal critical sections; staggered starts in solve_challenges.
:::

Subtask 13 — Provider Adapters &amp; Retries
Objective  Uniform structured calls across providers with robust retries.
Inputs  Model enum; messages; schemas.
Procedure / Prompt Spec or Algorithm  Dispatch in get_next_structure; per-provider adapters enforce structured output; retry_with_backoff wraps transient failures (xAI/OpenRouter paths) with jitter and caps.
Outputs / Artifacts  Parsed outputs; usage logs.
Dependencies / Anchors  src/llms/structured.py::{retry_with_backoff, get_next_structure, _get_next_structure_*}; src/llms/models.py::Model.
Acceptance Criteria  Recover from transient failures; bounded retries; logged attempts.
Failure Modes &amp; Mitigations  Non-retryable errors → immediate fail with error logs.
:::

Subtask 14 — Logging &amp; Trace Context
Objective  Record spans and mirror to local file while scrubbing sensitive data.
Inputs  LOGFIRE_API_KEY, LOCAL_LOGS_ONLY, LOG_LEVEL.
Procedure / Prompt Spec or Algorithm  Patch logfire methods to inject run_id/task_id; write rotating file logs/arc.log; wrap spans with start/end/error messages.
Outputs / Artifacts  Local log file; optional remote telemetry.
Dependencies / Anchors  src/logging_config.py::{generate_run_id, set_task_id, logfire patches}; src/log.py::log facade.
Acceptance Criteria  Every major operation creates span logs; no secret leakage (scrubbing callback).
Failure Modes &amp; Mitigations  No token/network → LOCAL_LOGS_ONLY=1 path.
:::

Subtask 15 — Persistence (Files &amp; Optional DB)
Objective  Store attempts and optionally DB rows for analysis.
Inputs  Guesses and instruction scores; NEON_DSN (optional).
Procedure / Prompt Spec or Algorithm  Write attempts / temp files; if DB configured, InstructionsScore.save_to_db and Guess.save_to_db insert rows (JSONB fields) with metadata.
Outputs / Artifacts  JSON files; DB rows when enabled.
Dependencies / Anchors  src/run.py::{solve_challenge, Guess.save_to_db, InstructionsScore.save_to_db}.
Acceptance Criteria  Files are valid JSON; DB insert does not raise.
Failure Modes &amp; Mitigations  DB failure → continue with file artifacts only.
:::

Subtask 16 — Visualization &amp; Base64
Objective  Aid debugging and multimodal prompting.
Inputs  VIZ, LOG_GRIDS env toggles; color map.
Procedure / Prompt Spec or Algorithm  When enabled, viz_many shows mismatches; base64_from_grid generates PNGs for embedding.
Outputs / Artifacts  On-screen figures; base64 strings.
Dependencies / Anchors  src/viz.py::{viz_many, base64_from_grid}; hooks in src/run.py and src/main.py.
Acceptance Criteria  No GUI errors when disabled; images created when requested.
Failure Modes &amp; Mitigations  Headless errors → keep defaults off.
:::

Subtask 17 — Environment &amp; Runbook
Objective  Prepare environment and execute.
Inputs  .env with provider keys; MAX_CONCURRENCY; Python 3.12 target in tooling.
Procedure / Prompt Spec or Algorithm  Load env early (src/__init__.py); install deps per pyproject.toml; run python src/run.py; adjust run() to select preset (grok_config_prod, gpt_config_prod, oss_config, mini_config).
Outputs / Artifacts  Successful run start; log output including run id.
Dependencies / Anchors  README.md (Requirements, Environment configuration, Running a solve); src/__init__.py; src/configs/*.
Acceptance Criteria  Smoke test runs with defaults; no KeyError for MAX_CONCURRENCY.
Failure Modes &amp; Mitigations  Missing var → set in .env per README.
:::

Subtask 18 — Evaluation
Objective  Compute accuracy when ground truth is provided.
Inputs  Attempts JSON; solutions JSON.
Procedure / Prompt Spec or Algorithm  Use evaluate_solutions to compare attempts against truth; count either attempt as correct; report aggregate accuracy.
Outputs / Artifacts  Printed accuracy; logs.
Dependencies / Anchors  src/run.py::{evaluate_solutions}.
Acceptance Criteria  Accuracy matches competition scoring semantics (either attempt counts).
Failure Modes &amp; Mitigations  Schema mismatch → validate inputs before evaluation.
</code></pre></div></div>]]></content><author><name></name></author><category term="AI&gt;prompt" /><category term="scratchpad" /></entry><entry><title type="html">git merge</title><link href="https://ib.bsb.br/git-merge/" rel="alternate" type="text/html" title="git merge" /><published>2025-09-28T00:00:00+00:00</published><updated>2025-10-04T17:33:47+00:00</updated><id>https://ib.bsb.br/git-merge</id><content type="html" xml:base="https://ib.bsb.br/git-merge/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  &lt;purpose&gt;
    You consolidate a chat history summary ([[chat_history_summary]]), its output documentation ([[history_outputs_doc]]), and two approach artifacts ([[first_approach]], [[second_approach]]) into a single coherent framework for the repository [[repo_name]]. Produce: a harmonized framework spec, a generation policy, three worked examples, and a JSON manifest for CI.
  &lt;/purpose&gt;
  &lt;context&gt;
    &lt;audience&gt;engineer&lt;/audience&gt;
    &lt;style&gt;
      &lt;tone&gt;extensive, technical, testable&lt;/tone&gt;
      &lt;format&gt;Markdown for specs; JSON for manifest&lt;/format&gt;
    &lt;/style&gt;
    &lt;constraints&gt;
      &lt;constraint&gt;Do not invent facts beyond inputs.&lt;/constraint&gt;
      &lt;constraint&gt;Respect safety and neutrality; no PII enrichment.&lt;/constraint&gt;
      &lt;constraint&gt;Outputs must be self-contained.&lt;/constraint&gt;
    &lt;/constraints&gt;
  &lt;/context&gt;
  &lt;instructions&gt;
    &lt;instruction&gt;Extract goals/constraints from [[chat_history_summary]].&lt;/instruction&gt;
    &lt;instruction&gt;Capture contracts/edge-cases from [[history_outputs_doc]].&lt;/instruction&gt;
    &lt;instruction&gt;Derive principles/workflows from [[first_approach]].&lt;/instruction&gt;
    &lt;instruction&gt;Derive principles/workflows from [[second_approach]].&lt;/instruction&gt;
    &lt;instruction&gt;Harmonize via precedence: documentation → first_approach → second_approach.&lt;/instruction&gt;
    &lt;instruction&gt;Synthesize a unified framework (roles, data flow, instructions, constraints, examples).&lt;/instruction&gt;
    &lt;instruction&gt;Emit: Framework Spec (MD), Generation Policy (MD), three worked examples (MD), Manifest (JSON).&lt;/instruction&gt;
    &lt;instruction&gt;Describe repository wiring paths for [[repo_name]].&lt;/instruction&gt;
  &lt;/instructions&gt;
  &lt;input_data&gt;
    &lt;repo_name&gt;[[repo_name]]&lt;/repo_name&gt;
    &lt;chat_history_summary&gt;[[chat_history_summary]]&lt;/chat_history_summary&gt;
    &lt;history_outputs_doc&gt;[[history_outputs_doc]]&lt;/history_outputs_doc&gt;
    &lt;first_approach&gt;[[first_approach]]&lt;/first_approach&gt;
    &lt;second_approach&gt;[[second_approach]]&lt;/second_approach&gt;
  &lt;/input_data&gt;
  &lt;output_format_specification&gt;
    &lt;framework_spec_markdown&gt;# Overview\n# Roles\n# Data Flow\n# Instructions Hierarchy\n# Constraints\n# Examples&lt;/framework_spec_markdown&gt;
    &lt;generation_policy&gt;- Actionable bullets; each with provenance tag.&lt;/generation_policy&gt;
    &lt;worked_examples&gt;Three minimal worked examples covering typical, conflict, and sparse scenarios.&lt;/worked_examples&gt;
    &lt;manifest_json&gt;{"repo_name":"string","artifacts":[{"path":"string","type":"spec|policy|example|schema|test","generated":true}],"metrics":["format_adherence","consistency","task_success","rule_conflict_free"]}&lt;/manifest_json&gt;
  &lt;/output_format_specification&gt;
  &lt;examples&gt;
    &lt;example&gt;Example inputs/outputs for a straightforward merge.&lt;/example&gt;
  &lt;/examples&gt;
</code></pre></div></div>]]></content><author><name></name></author><category term="AI&gt;prompt" /><category term="scratchpad" /></entry><entry><title type="html">grounded theory</title><link href="https://ib.bsb.br/grounded-theory/" rel="alternate" type="text/html" title="grounded theory" /><published>2025-09-28T00:00:00+00:00</published><updated>2025-09-28T18:15:09+00:00</updated><id>https://ib.bsb.br/grounded-theory</id><content type="html" xml:base="https://ib.bsb.br/grounded-theory/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  &lt;purpose&gt;
    You are an AI scientist implementing a two-stage discovery pipeline over [[dataset_raw]].
    Stage 1 (Empirical Exploration): profile [[dataset_raw]], mine patterns, and synthesize candidate hypotheses with evidence.
    Stage 2 (Deductive Validation): formalize claims, choose appropriate tests/proofs (statistical tests, counterexample search, formal reasoning, computational experiments), execute them, and output verdicts with transparent reasoning.
    Return machine-readable JSON per [[output_schema_json]] plus a extensive executive description.
  &lt;/purpose&gt;

  &lt;context&gt;
    &lt;audience&gt;
      &lt;primary&gt;Analyst/Researcher (advanced statistics proficiency)&lt;/primary&gt;
      &lt;secondary&gt;technical reviewer&lt;/secondary&gt;
    &lt;/audience&gt;
    &lt;toolkit&gt;
      &lt;proof_tools&gt;[[proof_tools]]&lt;/proof_tools&gt;
      &lt;enable_code&gt;[[enable_code]]&lt;/enable_code&gt;
      &lt;max_iterations&gt;[[max_iterations]]&lt;/max_iterations&gt;
    &lt;/toolkit&gt;
    &lt;constraints&gt;
      &lt;constraint&gt;Operate offline on provided inputs; do not use external sources.&lt;/constraint&gt;
      &lt;constraint&gt;Be explicit about assumptions and data quality issues.&lt;/constraint&gt;
      &lt;constraint&gt;Prefer non-parametric or exact methods when sample sizes are small.&lt;/constraint&gt;
      &lt;constraint&gt;Apply multiple-testing correction when evaluating many related hypotheses.&lt;/constraint&gt;
      &lt;constraint&gt;Avoid causal language unless justified; label causality explicitly.&lt;/constraint&gt;
      &lt;constraint&gt;Numeric precision: 3–4 significant figures.&lt;/constraint&gt;
    &lt;/constraints&gt;
    &lt;dialectical_notes&gt;
      &lt;note&gt;Parametric tests may be more powerful under distributional assumptions; non-parametric tests are safer when those assumptions are doubtful or n is small.&lt;/note&gt;
      &lt;note&gt;Correlation does not imply causation; consider directed tests or natural experiments only if justified by design.&lt;/note&gt;
    &lt;/dialectical_notes&gt;
  &lt;/context&gt;

  &lt;instructions&gt;
    &lt;instruction&gt;Ingest [[dataset_raw]] and infer schema/types; describe in "schema_inference".&lt;/instruction&gt;
    &lt;instruction&gt;Profile the data (distributions, missingness, correlations/associations, structure/time order/categories/text motifs); record in "profiling".&lt;/instruction&gt;
    &lt;instruction&gt;Mine salient "patterns" broadly, or focus via [[exploration_objectives]] if provided.&lt;/instruction&gt;
    &lt;instruction&gt;Formulate ≥3 "candidate_hypotheses"; each includes: id, claim, formalization, evidence (with references to profiling/patterns), and priority_score ∈ [0,1].&lt;/instruction&gt;
    &lt;instruction&gt;For each hypothesis, choose a method from [[proof_tools]]; state assumptions and a extensive, reproducible procedure.&lt;/instruction&gt;
    &lt;instruction&gt;Execute the checks/proofs; write outcomes in "tests" (include statistics, error bounds, or constructive counterexamples as applicable).&lt;/instruction&gt;
    &lt;instruction&gt;Issue a "verdict" ∈ {Proven, Falsified, Inconclusive} for each hypothesis, with justification tied to results and assumptions.&lt;/instruction&gt;
    &lt;instruction&gt;Compose an "executive_description", then list "caveats" and "next_steps".&lt;/instruction&gt;
    &lt;instruction&gt;Emit only the JSON object matching [[output_schema_json]] followed by the executive description block.&lt;/instruction&gt;
  &lt;/instructions&gt;

  &lt;input_data&gt;
    &lt;dataset_raw&gt;[[dataset_raw]]&lt;/dataset_raw&gt;
    &lt;dataset_description&gt;[[dataset_description]]&lt;/dataset_description&gt;
    &lt;exploration_objectives&gt;[[exploration_objectives]]&lt;/exploration_objectives&gt;
    &lt;output_schema_json&gt;[[output_schema_json]]&lt;/output_schema_json&gt;
    &lt;proof_tools&gt;["stats","counterexample","combinatorial","simulation"]&lt;/proof_tools&gt;
    &lt;enable_code&gt;no&lt;/enable_code&gt;
    &lt;max_iterations&gt;1&lt;/max_iterations&gt;
  &lt;/input_data&gt;

  &lt;output_format_specification&gt;
    &lt;schema&gt;[[output_schema_json]]&lt;/schema&gt;
    &lt;notes&gt;Return the JSON first; then a extensive executive description paragraph.&lt;/notes&gt;
  &lt;/output_format_specification&gt;

  &lt;examples&gt;
    &lt;example&gt;
      &lt;input_data&gt;
        &lt;dataset_raw&gt;
month,value
1,3
2,4
3,3
4,6
5,4
6,8
7,3
8,5
        &lt;/dataset_raw&gt;
        &lt;exploration_objectives&gt;Check seasonality or periodicity.&lt;/exploration_objectives&gt;
      &lt;/input_data&gt;
      &lt;output&gt;
{"stage_1":{"schema_inference":"Two columns: month(int), value(int)","profiling":"Mean≈4.5; sd≈1.7; mild peaks at months 4 and 6; small n=8","patterns":["autocorr hint at lag 6 (weak)","outlier risk low"],"candidate_hypotheses":[{"id":"H1","claim":"Series exhibits 6-month periodicity","formalization":"ACF lag=6 &amp;gt; 2 sd of ACF noise","evidence":"Peaks near months 4–6; small n"},{"id":"H2","claim":"Upward drift from months 1→6","formalization":"Kendall tau &amp;gt; 0","evidence":"Median rises"},{"id":"H3","claim":"Variance stable across halves","formalization":"Levene p&amp;gt;0.05","evidence":"Similar spread"}]},"stage_2":{"tests":[{"hypothesis_id":"H1","method":"stats","assumptions":"Weak stationarity","procedure":"Compute ACF; compare to CI","result":"ACF lag6 within CI; p≈0.28"},{"hypothesis_id":"H2","method":"stats","assumptions":"Monotone trend test","procedure":"Kendall tau","result":"tau≈0.29; p≈0.31"},{"hypothesis_id":"H3","method":"stats","assumptions":"Equal variance","procedure":"Levene","result":"p≈0.62"}],"verdicts":[{"hypothesis_id":"H1","verdict":"Inconclusive","justification":"Insufficient length"},{"hypothesis_id":"H2","verdict":"Inconclusive","justification":"Not significant"},{"hypothesis_id":"H3","verdict":"Proven","justification":"No variance shift detected"}]},"caveats":["Very small sample"],"next_steps":["Collect more periods","Use STL decomposition"]}

Executive description: Small time series shows no reliable periodicity; variance appears stable. More data recommended.
      &lt;/output&gt;
    &lt;/example&gt;

    &lt;example&gt;
      &lt;input_data&gt;
        &lt;dataset_raw&gt;
A,B,count
yes,yes,18
yes,no,2
no,yes,3
no,no,17
        &lt;/dataset_raw&gt;
        &lt;exploration_objectives&gt;Association between A and B&lt;/exploration_objectives&gt;
      &lt;/input_data&gt;
      &lt;output&gt;
{"stage_1":{"schema_inference":"Contingency table 2×2","profiling":"Row/col margins unbalanced","patterns":["High diagonal counts"],"candidate_hypotheses":[{"id":"H1","claim":"A and B are associated","formalization":"P(A,B) ≠ P(A)P(B)","evidence":"Diagonal dominance"}]},"stage_2":{"tests":[{"hypothesis_id":"H1","method":"stats","assumptions":"Fixed margins","procedure":"Fisher exact test","result":"p&amp;lt;0.01; OR≈51"}],"verdicts":[{"hypothesis_id":"H1","verdict":"Proven","justification":"Strong association"}]},"caveats":["Small table; check sampling"],"next_steps":["Validate with holdout"]}

Executive description: 2×2 data show strong association between A and B (Fisher p&amp;lt;0.01; OR≈51). Sampling assumptions should be verified.
      &lt;/output&gt;
    &lt;/example&gt;
  &lt;/examples&gt;

~~~
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "TwoStrokeDiscoveryResult",
  "type": "object",
  "required": ["stage_1", "stage_2", "executive_description"],
  "properties": {
    "stage_1": {
      "type": "object",
      "required": ["schema_inference", "profiling", "patterns", "candidate_hypotheses"],
      "properties": {
        "schema_inference": { "type": "string" },
        "profiling": { "type": "string" },
        "patterns": { "type": "array", "items": { "type": "string" } },
        "candidate_hypotheses": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["id", "claim", "formalization", "evidence"],
            "properties": {
              "id": { "type": "string" },
              "claim": { "type": "string" },
              "formalization": { "type": "string" },
              "evidence": { "type": "string" },
              "priority_score": { "type": "number" }
            }
          }
        }
      }
    },
    "stage_2": {
      "type": "object",
      "required": ["tests", "verdicts"],
      "properties": {
        "tests": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["hypothesis_id", "method", "assumptions", "procedure", "result"],
            "properties": {
              "hypothesis_id": { "type": "string" },
              "method": { "type": "string" },
              "assumptions": { "type": "string" },
              "procedure": { "type": "string" },
              "result": { "type": "string" }
            }
          }
        },
        "verdicts": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["hypothesis_id", "verdict", "justification"],
            "properties": {
              "hypothesis_id": { "type": "string" },
              "verdict": { "type": "string", "enum": ["Proven", "Falsified", "Inconclusive"] },
              "justification": { "type": "string" }
            }
          }
        }
      }
    },
    "executive_description": { "type": "string" },
    "caveats": { "type": "array", "items": { "type": "string" } },
    "next_steps": { "type": "array", "items": { "type": "string" } }
  }
}
~~~
</code></pre></div></div>]]></content><author><name></name></author><category term="AI&gt;prompt" /></entry><entry><title type="html">bnmp</title><link href="https://ib.bsb.br/bnmp/" rel="alternate" type="text/html" title="bnmp" /><published>2025-09-22T00:00:00+00:00</published><updated>2025-09-29T09:47:36+00:00</updated><id>https://ib.bsb.br/bnmp</id><content type="html" xml:base="https://ib.bsb.br/bnmp/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BNMP — UF-wide harvester (final refactor, 415 fix)

Fix in this version
-------------------
* Adds explicit **Content-Type: application/json;charset=UTF-8** for POST requests
  (some BNMP endpoints return HTTP 415 without it).

Goal
----
Harvest **as much data as possible** from portal BNMP **filtered only by a given UF/Estado**, with
strong robustness and good operator UX. This script merges the best ideas from the provided
scripts and adds:

* Resilient HTTP (browser-like headers, gzip handling, retries w/ exponential backoff + jitter).
* Two complementary strategies for **maximum coverage**:
  1) **per-órgão** pagination via `/bnmpportal/api/pesquisa-pecas/pecas` (UF + órgão filters);
  2) **UF-wide** pagination via `/bnmpportal/api/pesquisa-pecas/filter` (buscaOrgaoRecursivo=True).
* Streamed, memory-safe outputs (GZIP NDJSON) + merged deduplicated stream.
* Detailed run manifest (JSON) and a small HTML preview of sample rows.
* Rich progress logging, counters (401 / transient failures), and configurable knobs.
* No external dependencies (stdlib only). Cross-platform.

Outputs (under an auto-created run directory):
  - `raw_pecas_orgao.ndjson.gz`      — flattened records from per-órgão strategy
  - `raw_filter_uf.ndjson.gz`        — flattened records from UF `/filter` strategy
  - `merged_dedup.ndjson.gz`         — flattened, deduplicated union of both
  - `manifest.json`                  — summary metrics and parameters
  - `sample_preview.html`            — HTML preview (first N merged rows)

Authentication
--------------
Provide the BNMP session cookie and fingerprint via flags or environment:
  - `--cookie` or env `BNMP_COOKIE`
  - `--fp`     or env `BNMP_FP`

UF selection
------------
Use `--uf-id` (int) **or** `--state` (name/sigla). Examples: `--state "Goiás"`, `--state GO`, `--uf-id 9`.

Basic usage
-----------
$ python bnmp_uf_harvester.py --state "DF" --cookie "$BNMP_COOKIE" --fp "$BNMP_FP"

Common knobs
------------
  --strategy both|pecas-orgao|filter-uf   (default: both)
  --page-size-pecas 200                   page size for per-órgão calls
  --page-size-filter 500                  page size for UF /filter calls
  --retries 6                             max attempts for transient failures
  --sleep-page 0.25                       throttle between pages
  --sleep-org 0.35                        throttle between órgãos
  --sort-filter "dataExpedicao,DESC"      sort for /filter (optional)
  --sample 200                            number of rows kept for HTML preview
  --out-dir ./bnmp_out                    parent output folder (a timestamped run dir is created inside)

Notes
-----
* To target **only warrants** (mandados), pass `--tipo-peca MANDADO_DE_PRISAO`. Without `--tipo-peca`,
  the script collects **all peças** available for the UF.
* The merged stream is deduped by best-effort key selection among common ID fields; if none found,
  a content hash is used.
* On 401 errors for a given órgão/page, the script skips forward and continues (counts reported).

"""

from __future__ import annotations
import argparse
import gzip
import hashlib
import html
import io
import json
import os
import pathlib
import random
import sys
import time
from collections import defaultdict, deque
from datetime import datetime
from typing import Any, Dict, Iterable, List, Optional, Tuple
from urllib import error, request
import unicodedata as u

BASE = "https://portalbnmp.cnj.jus.br"
PECAS_URL = f"{BASE}/bnmpportal/api/pesquisa-pecas/pecas"
FILTER_URL = f"{BASE}/bnmpportal/api/pesquisa-pecas/filter"
ORGAOS_UF_URL = f"{BASE}/bnmpportal/api/pesquisa-pecas/orgaos/unidade-federativa"

TRANSIENT = {429, 502, 503, 504}

STATE_MAP = {
    'acre':1,'ac':1,
    'alagoas':2,'al':2,
    'amapa':3,'amapá':3,'ap':3,
    'amazonas':4,'am':4,
    'bahia':5,'ba':5,
    'ceara':6,'ceará':6,'ce':6,
    'distrito federal':7,'df':7,
    'espirito santo':8,'espírito santo':8,'es':8,
    'goias':9,'goiás':9,'go':9,
    'maranhao':10,'maranhão':10,'ma':10,
    'mato grosso':11,'mt':11,
    'mato grosso do sul':12,'ms':12,
    'minas gerais':13,'mg':13,
    'para':14,'pará':14,'pa':14,
    'paraiba':15,'paraíba':15,'pb':15,
    'parana':16,'paraná':16,'pr':16,
    'pernambuco':17,'pe':17,
    'piaui':18,'piauí':18,'pi':18,
    'rio de janeiro':19,'rj':19,
    'rio grande do norte':20,'rn':20,
    'rio grande do sul':21,'rs':21,
    'rondonia':22,'rondônia':22,'ro':22,
    'roraima':23,'rr':23,
    'santa catarina':24,'sc':24,
    'sao paulo':25,'são paulo':25,'sp':25,
    'sergipe':26,'se':26,
    'tocantins':27,'to':27,
}

def norm(s: str) -&gt; str:
    return u.normalize('NFKD', (s or '')).encode('ASCII','ignore').decode().strip().lower()

# --- 415 fix: always include JSON content-type for POSTs ---
JSON_CT = 'application/json;charset=UTF-8'

def build_headers(cookie: str, fp: str) -&gt; Dict[str, str]:
    # Browser-like headers to reduce auth/throttle friction
    return {
        'Accept': 'application/json, text/plain, */*',
        'Accept-Encoding': 'gzip',
        'Accept-Language': 'pt-BR,pt;q=0.9',
        'User-Agent': 'Mozilla/5.0',
        'Origin': BASE,
        'Referer': BASE + '/',
        'X-Requested-With': 'XMLHttpRequest',
        'Connection': 'keep-alive',
        'fingerprint': fp,
        'Cookie': f'portalbnmp={cookie}',
        # Tolerated if ignored; some stacks mirror session validation here
        'Authorization': f'Bearer {cookie}',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache',
        # Important for POST endpoints (prevents HTTP 415 on /filter and /pecas)
        'Content-Type': JSON_CT,
    }


def open_json(url: str, method: str, headers: Dict[str, str], payload: Optional[Dict[str, Any]] = None,
              max_tries: int = 6, base_sleep: float = 0.6, timeout: int = 90,
              query: Optional[Dict[str, Any]] = None) -&gt; Dict[str, Any] | List[Any]:
    """Perform an HTTP request and parse JSON, with retries on transient errors."""
    from urllib.parse import urlencode
    body = json.dumps(payload, ensure_ascii=False).encode('utf-8') if payload is not None else None
    full = url
    if query:
        qs = urlencode(query, doseq=True)
        if qs:
            full = f"{url}?{qs}"
    last_exc: Optional[Exception] = None
    for attempt in range(1, max_tries + 1):
        # ensure JSON content-type for POSTs even if caller headers were modified
        req_headers = dict(headers)
        if body is not None:
            req_headers.setdefault('Content-Type', JSON_CT)
        req = request.Request(full, headers=req_headers, data=body, method=method)
        try:
            with request.urlopen(req, timeout=timeout) as r:
                raw = r.read()
                enc = (r.headers.get('Content-Encoding') or '').lower()
                if 'gzip' in enc:
                    raw = gzip.decompress(raw)
                return json.loads(raw.decode('utf-8', 'replace'))
        except error.HTTPError as e:
            last_exc = e
            code = getattr(e, 'code', None)
            if code in TRANSIENT:
                sleep = base_sleep * (2 ** (attempt - 1)) + random.random() * 0.2
                print(f"[retry {attempt}/{max_tries}] {code} {method} {full} — aguardando {sleep:.2f}s…", file=sys.stderr)
                time.sleep(sleep)
                continue
            # Non-transient: bubble up
            raise
        except error.URLError as e:
            last_exc = e
            sleep = base_sleep * (2 ** (attempt - 1)) + random.random() * 0.2
            print(f"[retry {attempt}/{max_tries}] URLError {getattr(e, 'reason', e)} — aguardando {sleep:.2f}s…", file=sys.stderr)
            time.sleep(sleep)
            continue
    if last_exc:
        raise last_exc
    # Should not reach here
    return {}


def autodetect_items(container: Any) -&gt; List[Any]:
    # Broad detection (covers observed envelopes across scripts)
    if isinstance(container, list):
        return container
    if not isinstance(container, dict):
        return []
    for key in (
        'content','resultados','itens','dados','items','lista','data','registros','records',
        'pecas','mandados','result'
    ):
        v = container.get(key)
        if isinstance(v, list):
            return v
    page = container.get('page') or {}
    if isinstance(page, dict):
        for k in ('content','items','itens','dados'):
            v = page.get(k)
            if isinstance(v, list):
                return v
    for v in container.values():
        if isinstance(v, list) and v and isinstance(v[0], dict):
            return v
    return []


def flatten(obj: Any, prefix: str = '') -&gt; Dict[str, Any]:
    out: Dict[str, Any] = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            kk = f"{prefix}.{k}" if prefix else str(k)
            if isinstance(v, (dict, list)):
                out.update(flatten(v, kk))
            else:
                out[kk] = v
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            kk = f"{prefix}[{i}]"
            if isinstance(v, (dict, list)):
                out.update(flatten(v, kk))
            else:
                out[kk] = v
    else:
        out[prefix or 'valor'] = obj
    return out


def enrich_person_columns(flat: Dict[str, Any]) -&gt; Dict[str, Any]:
    # Surface likely person fields into predictable columns (best-effort)
    out = dict(flat)
    for k in list(flat.keys()):
        nk = norm(k)
        if nk.endswith('.nome') and 'pessoa.nome' not in out:
            out['pessoa.nome'] = flat[k]
        if nk.endswith('.cpf') and 'pessoa.cpf' not in out:
            out['pessoa.cpf'] = flat[k]
        if nk.endswith('.rg') and 'pessoa.rg' not in out:
            out['pessoa.rg'] = flat[k]
        if 'nascimento' in nk and 'pessoa.nascimento' not in out:
            out['pessoa.nascimento'] = flat[k]
        if 'envolvido' in nk and 'nome' in nk and 'pessoa.nome' not in out:
            out['pessoa.nome'] = flat[k]
    return out


def make_run_dir(parent: pathlib.Path) -&gt; pathlib.Path:
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    run_dir = parent / f"bnmp_run_{ts}"
    run_dir.mkdir(parents=True, exist_ok=True)
    return run_dir


def ndjson_gzip_writer(path: pathlib.Path):
    f = gzip.open(path, 'wt', encoding='utf-8')
    def write(obj: Dict[str, Any]):
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")
    return f, write


def html_preview(rows: List[Dict[str, Any]], title: str) -&gt; str:
    # Determine columns: person/context first, then others
    preferred = ['pessoa.nome','pessoa.cpf','pessoa.rg','pessoa.nascimento','__orgao.id','__orgao.nome','__uf.id']
    cols, seen = [], set()
    for c in preferred:
        if any(c in r for r in rows) and c not in seen:
            seen.add(c); cols.append(c)
    for r in rows:
        for k in r.keys():
            if k not in seen:
                seen.add(k); cols.append(k)
    thead = ''.join(f'&lt;th&gt;{html.escape(c)}&lt;/th&gt;' for c in cols)
    body = []
    for r in rows:
        tds = []
        for c in cols:
            v = r.get(c)
            if isinstance(v, (dict, list)):
                cell = f"&lt;code&gt;{html.escape(json.dumps(v, ensure_ascii=False))}&lt;/code&gt;"
            else:
                cell = html.escape('' if v is None else str(v))
            tds.append(f'&lt;td&gt;{cell}&lt;/td&gt;')
        body.append(f"&lt;tr&gt;{''.join(tds)}&lt;/tr&gt;")
    head = f"""
&lt;!doctype html&gt;&lt;html lang="pt-br"&gt;&lt;meta charset="utf-8"&gt;
&lt;title&gt;{html.escape(title)}&lt;/title&gt;
&lt;style&gt;
 body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;margin:16px}}
 h1{{font-size:20px;margin:0}}
 .meta{{color:#444;margin:6px 0 14px 0;font-size:12px}}
 table{{border-collapse:collapse;width:100%}}
 th,td{{border:1px solid #ddd;padding:6px 8px;font-size:12px;vertical-align:top}}
 th{{position:sticky;top:0;background:#f6f6f6;text-align:left}}
 tr:nth-child(even){{background:#fafafa}}
 code{{white-space:pre-wrap}}
&lt;/style&gt;
"""
    return head + f"&lt;h1&gt;{html.escape(title)}&lt;/h1&gt;" + f"&lt;table&gt;&lt;thead&gt;&lt;tr&gt;{thead}&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;{''.join(body)}&lt;/tbody&gt;&lt;/table&gt;&lt;/html&gt;"

# --- ID / dedupe helpers ---
CANDIDATE_ID_KEYS = [
    'id',
    'peca.id', 'pecaId', 'idPeca',
    'mandado.id', 'idMandado',
    'processo.id', 'idProcesso',
    'pecas[0].id',  # occasional shapes
]

def best_effort_uid(flat: Dict[str, Any]) -&gt; str:
    for k in CANDIDATE_ID_KEYS:
        if k in flat and flat[k] is not None:
            return f"{k}:{flat[k]}"
    # fallback: stable hash of sorted items
    items = sorted((str(k), json.dumps(v, ensure_ascii=False, sort_keys=True)) for k, v in flat.items())
    h = hashlib.sha1()
    for k, v in items:
        h.update(k.encode('utf-8'))
        h.update(b'\x00')
        h.update(v.encode('utf-8'))
        h.update(b'\x00')
    return f"hash:{h.hexdigest()}"

# --- Harvest strategies ---

def list_orgaos_for_uf(headers: Dict[str, str], uf_id: int, *, retries: int, base_sleep: float, timeout: int) -&gt; List[Dict[str, Any]]:
    url = f"{ORGAOS_UF_URL}/{uf_id}"
    resp = open_json(url, 'GET', headers, None, max_tries=retries, base_sleep=base_sleep, timeout=timeout)
    if not isinstance(resp, list):
        raise SystemExit('Resposta inesperada ao listar órgãos (array esperado).')
    resp.sort(key=lambda o: ((o.get('nome') or '').casefold(), int(o.get('id') or 0)))
    return resp


def harvest_pecas_por_orgao(headers: Dict[str, str], uf_id: int, *, writer, counters: Dict[str, int],
                             page_size: int, retries: int, base_sleep: float, timeout: int,
                             sleep_page: float, sleep_org: float, tipo_peca: Optional[List[str]],
                             sample_buf: deque, sample_cap: int) -&gt; None:
    orgaos = list_orgaos_for_uf(headers, uf_id, retries=retries, base_sleep=base_sleep, timeout=timeout)
    counters['orgaos_total'] = len(orgaos)
    print(f"[pecas-orgao] órgãos encontrados: {len(orgaos)}", file=sys.stderr)
    for org in orgaos:
        org_id = org.get('id'); org_nome = org.get('nome')
        if org_id is None:
            continue
        counters['orgaos_consultados'] += 1
        page = 0
        while True:
            payload = {
                'pagina': page, 'tamanhoPagina': page_size,
                'page': page,   'size': page_size,
                'filtros': {
                    'unidadeFederativaId': uf_id,
                    'orgaoId': org_id,
                },
                'ordenacao': [ {'propriedade':'dataExpedicao','direcao':'DESC'} ]
            }
            if tipo_peca:
                payload['filtros']['tipoPeca'] = tipo_peca
            try:
                resp = open_json(PECAS_URL, 'POST', headers, payload,
                                  max_tries=retries, base_sleep=base_sleep, timeout=timeout)
            except error.HTTPError as e:
                if e.code == 401:
                    counters['count_401'] += 1
                    print(f"[401] órgão {org_id} — {org_nome}: acesso não autorizado; seguindo…", file=sys.stderr)
                    break
                elif e.code in TRANSIENT:
                    counters['transient_failures'] += 1
                    print(f"[WARN] {e.code} persistente em órgão {org_id} — {org_nome}; seguindo…", file=sys.stderr)
                    break
                else:
                    print(f"[WARN] órgão {org_id} — {org_nome}: erro '{e}'; seguindo…", file=sys.stderr)
                    break
            except error.URLError as e:
                counters['transient_failures'] += 1
                print(f"[WARN] URLError em órgão {org_id} — {org_nome}: '{e.reason}'; seguindo…", file=sys.stderr)
                break

            items = autodetect_items(resp)
            if not items:
                break
            added = 0
            for it in items:
                flat = enrich_person_columns(flatten(it))
                flat['__orgao.id'] = org_id
                flat['__orgao.nome'] = org_nome
                flat['__uf.id'] = uf_id
                writer(flat)
                counters['rows_pecas_orgao'] += 1
                added += 1
                if len(sample_buf) &lt; sample_cap:
                    sample_buf.append(dict(flat))
            page += 1
            print(f"[pecas-orgao] UF {uf_id} — órgão {org_id} — página {page} (+{added}) total={counters['rows_pecas_orgao']}", file=sys.stderr)
            time.sleep(sleep_page)
        time.sleep(sleep_org)


def harvest_filter_uf(headers: Dict[str, str], uf_id: int, *, writer, counters: Dict[str, int],
                       page_size: int, retries: int, base_sleep: float, timeout: int,
                       sleep_page: float, sort_filter: Optional[str], sample_buf: deque, sample_cap: int) -&gt; None:
    page = 0
    while True:
        query = {'page': page, 'size': page_size}
        if sort_filter:
            query['sort'] = sort_filter
        payload = {
            'buscaOrgaoRecursivo': True,
            'orgaoExpeditor': {},
            'idEstado': uf_id,
        }
        try:
            resp = open_json(FILTER_URL, 'POST', headers, payload,
                              max_tries=retries, base_sleep=base_sleep, timeout=timeout,
                              query=query)
        except error.HTTPError as e:
            if e.code == 401:
                counters['count_401'] += 1
                print(f"[401] filter UF {uf_id}: acesso não autorizado; encerrando.", file=sys.stderr)
                break
            elif e.code in TRANSIENT:
                counters['transient_failures'] += 1
                print(f"[WARN] {e.code} persistente no filter UF; encerrando.", file=sys.stderr)
                break
            else:
                print(f"[WARN] filter UF erro '{e}'; encerrando.", file=sys.stderr)
                break
        except error.URLError as e:
            counters['transient_failures'] += 1
            print(f"[WARN] URLError no filter UF: '{e.reason}'; encerrando.", file=sys.stderr)
            break

        items = autodetect_items(resp)
        if not items:
            break
        added = 0
        for it in items:
            flat = enrich_person_columns(flatten(it))
            flat['__uf.id'] = uf_id
            writer(flat)
            counters['rows_filter_uf'] += 1
            added += 1
            if len(sample_buf) &lt; sample_cap:
                sample_buf.append(dict(flat))
        last = bool(resp.get('last')) if isinstance(resp, dict) else False
        number = int(resp.get('number', page)) if isinstance(resp, dict) else page
        total_pages = int(resp.get('totalPages', (page + 1))) if isinstance(resp, dict) else None
        print(f"[filter-uf] UF {uf_id} — página {page} (+{added}) total={counters['rows_filter_uf']} last={last} totalPages={total_pages}", file=sys.stderr)
        if last:
            break
        if total_pages is not None and (number + 1) &gt;= total_pages:
            break
        page += 1
        time.sleep(sleep_page)

# --- Merging &amp; HTML preview ---

def merge_streams_dedup(in_paths: List[pathlib.Path], out_path: pathlib.Path, sample_buf: deque, sample_cap: int) -&gt; Dict[str, int]:
    seen: set[str] = set()
    counts = defaultdict(int)
    with gzip.open(out_path, 'wt', encoding='utf-8') as out:
        for p in in_paths:
            if not p or not p.exists():
                continue
            with gzip.open(p, 'rt', encoding='utf-8') as f:
                for line in f:
                    try:
                        obj = json.loads(line)
                    except Exception:
                        counts['decode_errors'] += 1
                        continue
                    uid = best_effort_uid(obj)
                    if uid in seen:
                        counts['dedup_skipped'] += 1
                        continue
                    seen.add(uid)
                    out.write(json.dumps(obj, ensure_ascii=False) + "\n")
                    counts['merged_rows'] += 1
                    if len(sample_buf) &lt; sample_cap:
                        sample_buf.append(dict(obj))
    counts['unique_keys'] = len(seen)
    return counts

# --- CLI &amp; main ---

def resolve_uf_id(args_state: Optional[str], args_uf_id: Optional[int]) -&gt; Tuple[int, str]:
    if args_uf_id is not None:
        return int(args_uf_id), f"UF {int(args_uf_id)}"
    if not args_state:
        raise SystemExit('Defina --state ou --uf-id.')
    uf_id = STATE_MAP.get(norm(args_state))
    if not uf_id:
        raise SystemExit(f"UF/Estado desconhecido: '{args_state}'. Use nome por extenso ou sigla.")
    return uf_id, args_state


def main():
    ap = argparse.ArgumentParser(description='BNMP — UF-wide harvester (both per-órgão and UF filter).')
    ap.add_argument('--state', help='UF name/sigla, e.g., "Goiás", "GO", "DF"')
    ap.add_argument('--uf-id', type=int, help='UF id num (1..27)')
    ap.add_argument('--cookie', default=os.environ.get('BNMP_COOKIE'), help='portalbnmp cookie (JWT) or env BNMP_COOKIE')
    ap.add_argument('--fp', default=os.environ.get('BNMP_FP'), help='fingerprint header or env BNMP_FP')
    ap.add_argument('--strategy', choices=['both','pecas-orgao','filter-uf'], default='both')
    ap.add_argument('--tipo-peca', nargs='*', help='Optional tipoPeca list (e.g., MANDADO_DE_PRISAO). If omitted, collects ALL peças.')
    ap.add_argument('--page-size-pecas', type=int, default=int(os.environ.get('BNMP_PAGE_SIZE_PECAS', '200')))
    ap.add_argument('--page-size-filter', type=int, default=int(os.environ.get('BNMP_PAGE_SIZE_FILTER', '500')))
    ap.add_argument('--retries', type=int, default=int(os.environ.get('BNMP_RETRIES', '6')))
    ap.add_argument('--timeout', type=int, default=int(os.environ.get('BNMP_TIMEOUT', '90')))
    ap.add_argument('--sleep-page', type=float, default=float(os.environ.get('BNMP_SLEEP_PAGE', '0.25')))
    ap.add_argument('--sleep-org', type=float, default=float(os.environ.get('BNMP_SLEEP_ORG', '0.35')))
    ap.add_argument('--base-sleep', type=float, default=float(os.environ.get('BNMP_BASE_SLEEP', '0.6')))
    ap.add_argument('--sort-filter', default=os.environ.get('BNMP_SORT_FILTER', 'dataExpedicao,DESC'))
    ap.add_argument('--sample', type=int, default=int(os.environ.get('BNMP_SAMPLE', '200')))
    ap.add_argument('--out-dir', default=os.environ.get('BNMP_OUT_DIR', './bnmp_out'))
    args = ap.parse_args()

    if not args.cookie or not args.fp:
        raise SystemExit('Defina --cookie/BNMP_COOKIE e --fp/BNMP_FP.')

    uf_id, state_label = resolve_uf_id(args.state, args.uf_id)

    headers = build_headers(args.cookie, args.fp)

    parent = pathlib.Path(args.out_dir)
    run_dir = make_run_dir(parent)

    # Prepare outputs
    f1_path = run_dir / 'raw_pecas_orgao.ndjson.gz'
    f2_path = run_dir / 'raw_filter_uf.ndjson.gz'
    merged_path = run_dir / 'merged_dedup.ndjson.gz'
    sample_path = run_dir / 'sample_preview.html'
    manifest_path = run_dir / 'manifest.json'

    sample_buf: deque = deque()

    counters: Dict[str, int] = defaultdict(int)
    meta: Dict[str, Any] = {
        'started_at': datetime.now().isoformat(timespec='seconds'),
        'uf_id': uf_id,
        'state_label': state_label,
        'strategy': args.strategy,
        'page_size_pecas': args.page_size_pecas,
        'page_size_filter': args.page_size_filter,
        'retries': args.retries,
        'timeout': args.timeout,
        'sleep_page': args.sleep_page,
        'sleep_org': args.sleep_org,
        'base_sleep': args.base_sleep,
        'sort_filter': args.sort_filter,
        'tipo_peca': args.tipo_peca or [],
        'out_dir': str(run_dir),
    }

    print(f"[init] UF {uf_id} / {state_label} — strategy={args.strategy} — out={run_dir}", file=sys.stderr)

    t0 = time.time()

    # Strategy: per-órgão
    if args.strategy in ('both', 'pecas-orgao'):
        f1, write1 = ndjson_gzip_writer(f1_path)
        try:
            harvest_pecas_por_orgao(headers, uf_id,
                                     writer=write1, counters=counters,
                                     page_size=args.page_size_pecas, retries=args.retries,
                                     base_sleep=args.base_sleep, timeout=args.timeout,
                                     sleep_page=args.sleep_page, sleep_org=args.sleep_org,
                                     tipo_peca=(args.tipo_peca or None),
                                     sample_buf=sample_buf, sample_cap=args.sample)
        finally:
            f1.close()

    # Strategy: UF filter
    if args.strategy in ('both', 'filter-uf'):
        f2, write2 = ndjson_gzip_writer(f2_path)
        try:
            harvest_filter_uf(headers, uf_id,
                              writer=write2, counters=counters,
                              page_size=args.page_size_filter, retries=args.retries,
                              base_sleep=args.base_sleep, timeout=args.timeout,
                              sleep_page=args.sleep_page, sort_filter=args.sort_filter,
                              sample_buf=sample_buf, sample_cap=args.sample)
        finally:
            f2.close()

    # Merge &amp; dedup
    merge_counts = merge_streams_dedup(
        [f1_path if f1_path.exists() else None, f2_path if f2_path.exists() else None],
        merged_path, sample_buf, args.sample
    )

    # HTML preview from sample
    title = (
        f"BNMP — UF {uf_id} / {state_label} — merged rows={merge_counts.get('merged_rows',0)} — "
        f"{datetime.now():%Y-%m-%d %H:%M:%S}"
    )
    html_text = html_preview(list(sample_buf), title)
    sample_path.write_text(html_text, encoding='utf-8')

    # Manifest
    meta.update({
        'finished_at': datetime.now().isoformat(timespec='seconds'),
        'duration_sec': round(time.time() - t0, 2),
        'counters': dict(counters),
        'merge_counts': dict(merge_counts),
        'outputs': {
            'per_orgao': str(f1_path) if f1_path.exists() else None,
            'filter_uf': str(f2_path) if f2_path.exists() else None,
            'merged': str(merged_path),
            'html_preview': str(sample_path),
        }
    })
    manifest_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding='utf-8')

    print(json.dumps(meta, ensure_ascii=False, indent=2))


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n[abort] interrupted by user", file=sys.stderr)
        sys.exit(130)
</code></pre></div></div>]]></content><author><name></name></author><category term="scratchpad" /></entry></feed>