<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ib.bsb.br/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ib.bsb.br/" rel="alternate" type="text/html" /><updated>2025-05-16T07:03:01+00:00</updated><id>https://ib.bsb.br/feed.xml</id><title type="html">infoBAG</title><entry><title type="html">installing rescatux/rescapp on Debian</title><link href="https://ib.bsb.br/installing-rescatuxrescapp-on-debian/" rel="alternate" type="text/html" title="installing rescatux/rescapp on Debian" /><published>2025-05-16T00:00:00+00:00</published><updated>2025-05-16T06:59:31+00:00</updated><id>https://ib.bsb.br/installing-rescatuxrescapp-on-debian</id><content type="html" xml:base="https://ib.bsb.br/installing-rescatuxrescapp-on-debian/"><![CDATA[<h1 id="1-update-your-system">1. Update Your System</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt upgrade <span class="nt">-y</span>
</code></pre></div></div>

<hr />

<h1 id="2-install-build-essentials-and-core-utilities">2. Install Build Essentials and Core Utilities</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> build-essential make coreutils
</code></pre></div></div>

<hr />

<h1 id="3-install-all-required-runtime-dependencies">3. Install All Required Runtime Dependencies</h1>

<p>The following list is derived from the <code class="language-plaintext highlighter-rouge">INSTALL</code> file, plugin scripts, and the codebase. Some packages may already be installed by default, but running these commands is safe and ensures completeness.</p>

<h2 id="31-python-3-and-required-python-modules">3.1. Python 3 and Required Python Modules</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> python3 python3-gi python3-dbus python3-pyqt5 python3-pyqt5.qtwebkit python3-parted
</code></pre></div></div>
<blockquote>
  <p><strong>Note:</strong></p>
  <ul>
    <li><code class="language-plaintext highlighter-rouge">python3-pyqt5.qtwebkit</code> is in the <code class="language-plaintext highlighter-rouge">bullseye</code> repo but may be called <code class="language-plaintext highlighter-rouge">python3-pyqt5.qtwebengine</code> in some newer releases. For Bullseye, the above is correct.</li>
    <li><code class="language-plaintext highlighter-rouge">python3-parted</code> provides the <code class="language-plaintext highlighter-rouge">parted</code> Python bindings.</li>
  </ul>
</blockquote>

<h2 id="32-gui-and-desktop-integration">3.2. GUI and Desktop Integration</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> zenity xdg-utils wmctrl
</code></pre></div></div>

<h2 id="33-dbus-system-integration">3.3. DBus System Integration</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> dbus
</code></pre></div></div>

<h2 id="34-disk-filesystem-and-partition-tools">3.4. Disk, Filesystem, and Partition Tools</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> util-linux reiserfsprogs reiser4progs btrfs-progs xfsprogs xfsdump ntfs-3g dosfstools gawk extundelete os-prober
</code></pre></div></div>

<h2 id="35-raid-lvm-and-encryption">3.5. RAID, LVM, and Encryption</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> dmraid lvm2 cryptsetup libcryptsetup12 cryptsetup-bin
</code></pre></div></div>

<h2 id="36-gpt-and-uefi-tools">3.6. GPT and UEFI Tools</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> gdisk efibootmgr mokutil
</code></pre></div></div>

<h2 id="37-bootloader-and-mbr-tools">3.7. Bootloader and MBR Tools</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> syslinux lilo
</code></pre></div></div>

<h2 id="38-terminal-emulator">3.8. Terminal Emulator</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> xterm
</code></pre></div></div>

<h2 id="39-miscellaneous-utilities">3.9. Miscellaneous Utilities</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> pastebinit hexchat gawk extundelete
</code></pre></div></div>

<h2 id="310-inxi-and-boot-info-script">3.10. Inxi and Boot Info Script</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> inxi boot-info-script
</code></pre></div></div>

<h2 id="311-optional-but-recommended-partition-and-recovery-tools">3.11. Optional but Recommended: Partition and Recovery Tools</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> gparted gpart testdisk
</code></pre></div></div>

<h2 id="312-additional-utilities">3.12. Additional Utilities</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> wget curl
</code></pre></div></div>

<hr />

<h1 id="4-install-the-rescatux-chntpw-package-for-windows-passwordaccount-operations">4. Install the Rescatux chntpw Package (for Windows Password/Account Operations)</h1>

<p><strong>IMPORTANT:</strong><br />
The standard <code class="language-plaintext highlighter-rouge">chntpw</code> package in Debian is not sufficient for all Rescapp features.<br />
You should use the Rescatux-provided version.</p>

<h2 id="41-add-the-rescatux-repository">4.1. Add the Rescatux Repository</h2>

<p>Create the file <code class="language-plaintext highlighter-rouge">/etc/apt/sources.list.d/rescatux.list</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"deb http://rescatux.sourceforge.net/repo/ buster-dev main"</span> | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/rescatux.list
</code></pre></div></div>

<h2 id="42-update-and-install-chntpw">4.2. Update and Install chntpw</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nt">-o</span> Acquire::AllowInsecureRepositories<span class="o">=</span><span class="nb">true</span> <span class="nt">-o</span> Acquire::AllowDowngradeToInsecureRepositories<span class="o">=</span><span class="nb">true </span>update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> chntpw
</code></pre></div></div>
<blockquote>
  <p><strong>Note:</strong></p>
  <ul>
    <li>You may be prompted about unauthenticated packages. Accept them.</li>
    <li>The Rescatux repo is for Buster, but the chntpw package is compatible with Bullseye.</li>
  </ul>
</blockquote>

<hr />

<h1 id="5-optional-selinux-support">5. (Optional) SELinux Support</h1>

<p>If you need SELinux support (rare, mostly for Fedora/RedHat/CentOS rescue):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> python3-selinux python3-semanage policycoreutils-python-utils selinux-basics auditd selinux-policy-default setools
</code></pre></div></div>
<blockquote>
  <p><strong>Note:</strong></p>
  <ul>
    <li>These packages are optional and only needed if you plan to rescue SELinux-enabled systems.</li>
  </ul>
</blockquote>

<hr />

<h1 id="6-clone-the-rescapp-repository">6. Clone the Rescapp Repository</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/rescatux/rescapp.git
<span class="nb">cd </span>rescapp
</code></pre></div></div>

<hr />

<h1 id="7-install-rescapp">7. Install Rescapp</h1>

<p>By default, this will install to <code class="language-plaintext highlighter-rouge">/usr/local</code>.<br />
If you want to install to <code class="language-plaintext highlighter-rouge">/usr</code>, use <code class="language-plaintext highlighter-rouge">prefix=/usr make install</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>make <span class="nb">install</span>
</code></pre></div></div>
<p>or, for system-wide <code class="language-plaintext highlighter-rouge">/usr</code> installation:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>make <span class="nv">prefix</span><span class="o">=</span>/usr <span class="nb">install</span>
</code></pre></div></div>

<hr />

<h1 id="8-optional-verify-installation">8. (Optional) Verify Installation</h1>

<p>Check that the <code class="language-plaintext highlighter-rouge">rescapp</code> binary is in your path:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>which rescapp
</code></pre></div></div>

<p>You should see <code class="language-plaintext highlighter-rouge">/usr/local/bin/rescapp</code> or <code class="language-plaintext highlighter-rouge">/usr/bin/rescapp</code> depending on your install prefix.</p>

<hr />

<h1 id="9-optional-desktop-integration">9. (Optional) Desktop Integration</h1>

<p>If you want Rescapp to appear in your desktop menu, ensure the <code class="language-plaintext highlighter-rouge">.desktop</code> file is installed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls</span> /usr/local/share/applications/rescapp.desktop
</code></pre></div></div>
<p>or</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls</span> /usr/share/applications/rescapp.desktop
</code></pre></div></div>

<hr />

<h1 id="10-run-rescapp">10. Run Rescapp</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rescapp
</code></pre></div></div>

<hr />

<h1 id="11-optional-troubleshooting">11. (Optional) Troubleshooting</h1>

<ul>
  <li>If you encounter missing dependencies, re-check the above lists.</li>
  <li>For issues with chntpw, ensure you are using the Rescatux-provided version.</li>
  <li>For graphical issues, ensure you have a working X session and all Qt5 dependencies.</li>
</ul>

<hr />

<h2 id="summary-table-of-all-key-packages"><strong>Summary Table of All Key Packages</strong></h2>

<table>
  <thead>
    <tr>
      <th>Purpose</th>
      <th>Package Names</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Core build/utils</td>
      <td>build-essential make coreutils</td>
    </tr>
    <tr>
      <td>Python &amp; Qt</td>
      <td>python3 python3-gi python3-dbus python3-pyqt5 python3-pyqt5.qtwebkit python3-parted</td>
    </tr>
    <tr>
      <td>GUI/desktop</td>
      <td>zenity xdg-utils wmctrl</td>
    </tr>
    <tr>
      <td>DBus</td>
      <td>dbus</td>
    </tr>
    <tr>
      <td>Disk/FS tools</td>
      <td>util-linux reiserfsprogs reiser4progs btrfs-progs xfsprogs xfsdump ntfs-3g dosfstools</td>
    </tr>
    <tr>
      <td>RAID/LVM/Crypto</td>
      <td>dmraid lvm2 cryptsetup libcryptsetup12 cryptsetup-bin</td>
    </tr>
    <tr>
      <td>GPT/UEFI</td>
      <td>gdisk efibootmgr mokutil</td>
    </tr>
    <tr>
      <td>Bootloader/MBR</td>
      <td>syslinux lilo</td>
    </tr>
    <tr>
      <td>Terminal emulator</td>
      <td>xterm</td>
    </tr>
    <tr>
      <td>Misc utilities</td>
      <td>pastebinit hexchat gawk extundelete wget curl</td>
    </tr>
    <tr>
      <td>Info scripts</td>
      <td>inxi boot-info-script</td>
    </tr>
    <tr>
      <td>Partition/recovery</td>
      <td>gparted gpart testdisk</td>
    </tr>
    <tr>
      <td>Windows password tool</td>
      <td>chntpw (from Rescatux repo)</td>
    </tr>
    <tr>
      <td>SELinux (optional)</td>
      <td>python3-selinux python3-semanage policycoreutils-python-utils selinux-basics auditd selinux-policy-default setools</td>
    </tr>
  </tbody>
</table>

<hr />

<h1 id="references"><strong>References</strong></h1>

<ul>
  <li><a href="https://github.com/rescatux/rescapp">Rescapp GitHub</a></li>
  <li><a href="https://www.rescatux.org">Rescatux Website</a></li>
  <li><a href="https://github.com/rescatux/chntpw">Rescatux chntpw package</a></li>
  <li><a href="https://packages.debian.org/bullseye/">Debian Bullseye Packages</a></li>
</ul>

<h1 id="bash-script-automation">bash script automation</h1>

<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="#!/bin/bash
# Script to install Rescapp and its dependencies on Debian Bullseye x64
# This script automates the steps from the revised guide.
# It should be run with sudo privileges or by a user who can sudo without a password for apt.
# The script will exit immediately if any command fails.
set -e # Exit immediately if a command exits with a non-zero status.
set -u # Treat unset variables as an error when substituting.
set -o pipefail # The return value of a pipeline is the status of the last command to exit with a non-zero status, or zero if no command exited with a non-zero status.
# --- Configuration ---
# Set DEBIAN_FRONTEND to noninteractive to avoid prompts during package installation
export DEBIAN_FRONTEND=noninteractive
# --- Temporary Directory for Build ---
BUILD_DIR=&quot;&quot; # Initialize BUILD_DIR
# Cleanup function to remove temporary directory
cleanup() {
    if [ -n &quot;$BUILD_DIR&quot; ] &amp;&amp; [ -d &quot;$BUILD_DIR&quot; ]; then
        echo &quot;Cleaning up temporary build directory: $BUILD_DIR&quot;
        sudo rm -rf &quot;$BUILD_DIR&quot;
    fi
}
# Register cleanup function to be called on script exit or interruption
trap cleanup EXIT SIGINT SIGTERM
echo &quot;Starting Rescapp installation process...&quot;
echo &quot;This script will install necessary packages and Rescapp.&quot;
echo &quot;Ensure you have an active internet connection.&quot;
echo &quot;-----------------------------------------------------&quot;
# 1. Update System
echo &quot;[Step 1/7] Updating system packages...&quot;
sudo apt update
sudo apt upgrade -y
# 2. Install Build Essentials and Git
echo &quot;[Step 2/7] Installing build essentials, core utilities, and git...&quot;
sudo apt install -y build-essential make coreutils git
# 3. Install All Required Runtime Dependencies
echo &quot;[Step 3/7] Installing Rescapp runtime dependencies...&quot;
# Note: python3-pyqt5.qtwebkit is for Bullseye. Newer distros might use python3-pyqt5.qtwebengine.
# Lilo might produce a warning during installation; this is generally acceptable for newer systems not relying on Lilo.
sudo apt install -y \
    python3 python3-gi python3-dbus python3-pyqt5 python3-pyqt5.qtwebkit python3-parted \
    zenity xdg-utils wmctrl \
    dbus \
    util-linux reiserfsprogs reiser4progs btrfs-progs xfsprogs xfsdump ntfs-3g dosfstools gawk extundelete os-prober \
    dmraid lvm2 cryptsetup libcryptsetup12 cryptsetup-bin \
    gdisk efibootmgr mokutil \
    syslinux lilo \
    xterm \
    pastebinit hexchat \
    inxi boot-info-script \
    gparted gpart testdisk \
    wget curl
# 4. (Optional) SELinux Support
# If you need to rescue SELinux-enabled systems (e.g., Fedora, RHEL, CentOS),
# uncomment the following section.
# echo &quot;[Step 4/7 - Optional] Installing SELinux support packages...&quot;
# sudo apt install -y \
# python3-selinux python3-semanage policycoreutils-python-utils selinux-basics auditd selinux-policy-default setools
# 5. Install the Rescatux chntpw Package
echo &quot;[Step 4/7] Installing Rescatux chntpw package...&quot;
# This uses the Rescatux repository for a version of chntpw with features needed by Rescapp.
# The repository is for Debian Buster but the chntpw package is generally compatible with Bullseye.
RESCATUX_REPO_FILE=&quot;/etc/apt/sources.list.d/rescatux.list&quot;
RESCATUX_REPO_LINE=&quot;deb http://rescatux.sourceforge.net/repo/ buster-dev main&quot;
if ! grep -qF &quot;$RESCATUX_REPO_LINE&quot; &quot;$RESCATUX_REPO_FILE&quot; 2&gt;/dev/null; then
    echo &quot;$RESCATUX_REPO_LINE&quot; | sudo tee &quot;$RESCATUX_REPO_FILE&quot;
else
    echo &quot;Rescatux repository line already exists in $RESCATUX_REPO_FILE.&quot;
fi
# Allow unauthenticated repositories for this specific source if GPG key is not imported.
sudo apt -o Acquire::AllowInsecureRepositories=true -o Acquire::AllowDowngradeToInsecureRepositories=true update
# You might be prompted to accept unauthenticated packages; this is expected for this repo if not running fully noninteractive.
# The DEBIAN_FRONTEND=noninteractive export should handle this.
sudo apt install -y chntpw
# 6. Clone the Rescapp Repository
BUILD_DIR=$(mktemp -d) # Create a temporary directory
echo &quot;[Step 5/7] Cloning the Rescapp repository into $BUILD_DIR/rescapp...&quot;
git clone https://github.com/rescatux/rescapp.git &quot;$BUILD_DIR/rescapp&quot;
cd &quot;$BUILD_DIR/rescapp&quot;
# 7. Install Rescapp
echo &quot;[Step 6/7] Installing Rescapp (default to /usr/local)...&quot;
# To install to /usr instead, you would use: sudo make prefix=/usr install
sudo make install
# 8. Verify Installation
echo &quot;[Step 7/7] Verifying installation...&quot;
RESCAPP_PATH=$(which rescapp || echo &quot;not_found&quot;) # Avoid error if not found when set -u is active
if [ &quot;$RESCAPP_PATH&quot; != &quot;not_found&quot; ] &amp;&amp; [ -n &quot;$RESCAPP_PATH&quot; ]; then
    echo &quot;Rescapp executable found at: $RESCAPP_PATH&quot;
else
    echo &quot;ERROR: Rescapp executable not found in PATH after installation.&quot;
    # The script will exit here if set -e is active and which fails,
    # but this explicit check is for clarity.
fi
DESKTOP_FILE_USR_LOCAL=&quot;/usr/local/share/applications/rescapp.desktop&quot;
DESKTOP_FILE_USR=&quot;/usr/share/applications/rescapp.desktop&quot;
if [ -f &quot;$DESKTOP_FILE_USR_LOCAL&quot; ]; then
    echo &quot;Rescapp desktop file found at: $DESKTOP_FILE_USR_LOCAL&quot;
elif [ -f &quot;$DESKTOP_FILE_USR&quot; ]; then
    echo &quot;Rescapp desktop file found at: $DESKTOP_FILE_USR&quot;
else
    echo &quot;Warning: Rescapp desktop file not found. Desktop integration might be incomplete.&quot;
fi
echo &quot;-----------------------------------------------------&quot;
echo &quot;Rescapp installation process completed successfully.&quot;
echo &quot;The build files were in the temporary directory $BUILD_DIR and will be cleaned up.&quot;
echo &quot;You can now attempt to run Rescapp by typing &#39;rescapp&#39; in your terminal.&quot;
echo &quot;-----------------------------------------------------&quot;
# Cleanup is handled by the trap EXIT
exit 0" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">#!/bin/bash
# Script to install Rescapp and its dependencies on Debian Bullseye x64
# This script automates the steps from the revised guide.
# It should be run with sudo privileges or by a user who can sudo without a password for apt.
# The script will exit immediately if any command fails.
set -e # Exit immediately if a command exits with a non-zero status.
set -u # Treat unset variables as an error when substituting.
set -o pipefail # The return value of a pipeline is the status of the last command to exit with a non-zero status, or zero if no command exited with a non-zero status.
# --- Configuration ---
# Set DEBIAN_FRONTEND to noninteractive to avoid prompts during package installation
export DEBIAN_FRONTEND=noninteractive
# --- Temporary Directory for Build ---
BUILD_DIR=&quot;&quot; # Initialize BUILD_DIR
# Cleanup function to remove temporary directory
cleanup() {
    if [ -n &quot;$BUILD_DIR&quot; ] &amp;&amp; [ -d &quot;$BUILD_DIR&quot; ]; then
        echo &quot;Cleaning up temporary build directory: $BUILD_DIR&quot;
        sudo rm -rf &quot;$BUILD_DIR&quot;
    fi
}
# Register cleanup function to be called on script exit or interruption
trap cleanup EXIT SIGINT SIGTERM
echo &quot;Starting Rescapp installation process...&quot;
echo &quot;This script will install necessary packages and Rescapp.&quot;
echo &quot;Ensure you have an active internet connection.&quot;
echo &quot;-----------------------------------------------------&quot;
# 1. Update System
echo &quot;[Step 1/7] Updating system packages...&quot;
sudo apt update
sudo apt upgrade -y
# 2. Install Build Essentials and Git
echo &quot;[Step 2/7] Installing build essentials, core utilities, and git...&quot;
sudo apt install -y build-essential make coreutils git
# 3. Install All Required Runtime Dependencies
echo &quot;[Step 3/7] Installing Rescapp runtime dependencies...&quot;
# Note: python3-pyqt5.qtwebkit is for Bullseye. Newer distros might use python3-pyqt5.qtwebengine.
# Lilo might produce a warning during installation; this is generally acceptable for newer systems not relying on Lilo.
sudo apt install -y \
    python3 python3-gi python3-dbus python3-pyqt5 python3-pyqt5.qtwebkit python3-parted \
    zenity xdg-utils wmctrl \
    dbus \
    util-linux reiserfsprogs reiser4progs btrfs-progs xfsprogs xfsdump ntfs-3g dosfstools gawk extundelete os-prober \
    dmraid lvm2 cryptsetup libcryptsetup12 cryptsetup-bin \
    gdisk efibootmgr mokutil \
    syslinux lilo \
    xterm \
    pastebinit hexchat \
    inxi boot-info-script \
    gparted gpart testdisk \
    wget curl
# 4. (Optional) SELinux Support
# If you need to rescue SELinux-enabled systems (e.g., Fedora, RHEL, CentOS),
# uncomment the following section.
# echo &quot;[Step 4/7 - Optional] Installing SELinux support packages...&quot;
# sudo apt install -y \
# python3-selinux python3-semanage policycoreutils-python-utils selinux-basics auditd selinux-policy-default setools
# 5. Install the Rescatux chntpw Package
echo &quot;[Step 4/7] Installing Rescatux chntpw package...&quot;
# This uses the Rescatux repository for a version of chntpw with features needed by Rescapp.
# The repository is for Debian Buster but the chntpw package is generally compatible with Bullseye.
RESCATUX_REPO_FILE=&quot;/etc/apt/sources.list.d/rescatux.list&quot;
RESCATUX_REPO_LINE=&quot;deb http://rescatux.sourceforge.net/repo/ buster-dev main&quot;
if ! grep -qF &quot;$RESCATUX_REPO_LINE&quot; &quot;$RESCATUX_REPO_FILE&quot; 2&gt;/dev/null; then
    echo &quot;$RESCATUX_REPO_LINE&quot; | sudo tee &quot;$RESCATUX_REPO_FILE&quot;
else
    echo &quot;Rescatux repository line already exists in $RESCATUX_REPO_FILE.&quot;
fi
# Allow unauthenticated repositories for this specific source if GPG key is not imported.
sudo apt -o Acquire::AllowInsecureRepositories=true -o Acquire::AllowDowngradeToInsecureRepositories=true update
# You might be prompted to accept unauthenticated packages; this is expected for this repo if not running fully noninteractive.
# The DEBIAN_FRONTEND=noninteractive export should handle this.
sudo apt install -y chntpw
# 6. Clone the Rescapp Repository
BUILD_DIR=$(mktemp -d) # Create a temporary directory
echo &quot;[Step 5/7] Cloning the Rescapp repository into $BUILD_DIR/rescapp...&quot;
git clone https://github.com/rescatux/rescapp.git &quot;$BUILD_DIR/rescapp&quot;
cd &quot;$BUILD_DIR/rescapp&quot;
# 7. Install Rescapp
echo &quot;[Step 6/7] Installing Rescapp (default to /usr/local)...&quot;
# To install to /usr instead, you would use: sudo make prefix=/usr install
sudo make install
# 8. Verify Installation
echo &quot;[Step 7/7] Verifying installation...&quot;
RESCAPP_PATH=$(which rescapp || echo &quot;not_found&quot;) # Avoid error if not found when set -u is active
if [ &quot;$RESCAPP_PATH&quot; != &quot;not_found&quot; ] &amp;&amp; [ -n &quot;$RESCAPP_PATH&quot; ]; then
    echo &quot;Rescapp executable found at: $RESCAPP_PATH&quot;
else
    echo &quot;ERROR: Rescapp executable not found in PATH after installation.&quot;
    # The script will exit here if set -e is active and which fails,
    # but this explicit check is for clarity.
fi
DESKTOP_FILE_USR_LOCAL=&quot;/usr/local/share/applications/rescapp.desktop&quot;
DESKTOP_FILE_USR=&quot;/usr/share/applications/rescapp.desktop&quot;
if [ -f &quot;$DESKTOP_FILE_USR_LOCAL&quot; ]; then
    echo &quot;Rescapp desktop file found at: $DESKTOP_FILE_USR_LOCAL&quot;
elif [ -f &quot;$DESKTOP_FILE_USR&quot; ]; then
    echo &quot;Rescapp desktop file found at: $DESKTOP_FILE_USR&quot;
else
    echo &quot;Warning: Rescapp desktop file not found. Desktop integration might be incomplete.&quot;
fi
echo &quot;-----------------------------------------------------&quot;
echo &quot;Rescapp installation process completed successfully.&quot;
echo &quot;The build files were in the temporary directory $BUILD_DIR and will be cleaned up.&quot;
echo &quot;You can now attempt to run Rescapp by typing &#39;rescapp&#39; in your terminal.&quot;
echo &quot;-----------------------------------------------------&quot;
# Cleanup is handled by the trap EXIT
exit 0</code>
</section>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">ragflow documentation</title><link href="https://ib.bsb.br/ragflow-documentation/" rel="alternate" type="text/html" title="ragflow documentation" /><published>2025-05-16T00:00:00+00:00</published><updated>2025-05-16T06:26:46+00:00</updated><id>https://ib.bsb.br/ragflow-documentation</id><content type="html" xml:base="https://ib.bsb.br/ragflow-documentation/"><![CDATA[<p><em>category</em>.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Get Started"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"RAGFlow Quick Start"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>configurations.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /configurations
---

# Configuration

Configurations for deploying RAGFlow via Docker.

## Guidelines

When it comes to system configurations, you will need to manage the following files:

- [.env](https://github.com/infiniflow/ragflow/blob/main/docker/.env): Contains important environment variables for Docker.
- [service_conf.yaml.template](https://github.com/infiniflow/ragflow/blob/main/docker/service_conf.yaml.template): Configures the back-end services. It specifies the system-level configuration for RAGFlow and is used by its API server and task executor. Upon container startup, the `service_conf.yaml` file will be generated based on this template file. This process replaces any environment variables within the template, allowing for dynamic configuration tailored to the container's environment.
- [docker-compose.yml](https://github.com/infiniflow/ragflow/blob/main/docker/docker-compose.yml): The Docker Compose file for starting up the RAGFlow service.

To update the default HTTP serving port (80), go to [docker-compose.yml](https://github.com/infiniflow/ragflow/blob/main/docker/docker-compose.yml) and change `80:80`
to `&lt;YOUR_SERVING_PORT&gt;:80`.

:::tip NOTE
Updates to the above configurations require a reboot of all containers to take effect:

```bash
docker compose -f docker/docker-compose.yml up -d
```

:::

## Docker Compose

- **docker-compose.yml**  
  Sets up environment for RAGFlow and its dependencies.
- **docker-compose-base.yml**  
  Sets up environment for RAGFlow's dependencies: Elasticsearch/[Infinity](https://github.com/infiniflow/infinity), MySQL, MinIO, and Redis.

:::danger IMPORTANT
We do not actively maintain **docker-compose-CN-oc9.yml**, **docker-compose-gpu-CN-oc9.yml**, or **docker-compose-gpu.yml**, so use them at your own risk. However, you are welcome to file a pull request to improve any of them.
:::

## Docker environment variables

The [.env](https://github.com/infiniflow/ragflow/blob/main/docker/.env) file contains important environment variables for Docker.

### Elasticsearch

- `STACK_VERSION`  
  The version of Elasticsearch. Defaults to `8.11.3`
- `ES_PORT`  
  The port used to expose the Elasticsearch service to the host machine, allowing **external** access to the service running inside the Docker container.  Defaults to `1200`.
- `ELASTIC_PASSWORD`  
  The password for Elasticsearch.

### Kibana

- `KIBANA_PORT`  
  The port used to expose the Kibana service to the host machine, allowing **external** access to the service running inside the Docker container. Defaults to `6601`.
- `KIBANA_USER`  
  The username for Kibana. Defaults to `rag_flow`.
- `KIBANA_PASSWORD`  
  The password for Kibana. Defaults to `infini_rag_flow`.

### Resource management

- `MEM_LIMIT`  
  The maximum amount of the memory, in bytes, that *a specific* Docker container can use while running. Defaults to `8073741824`.

### MySQL

- `MYSQL_PASSWORD`  
  The password for MySQL.
- `MYSQL_PORT`  
  The port used to expose the MySQL service to the host machine, allowing **external** access to the MySQL database running inside the Docker container. Defaults to `5455`.

### MinIO

RAGFlow utilizes MinIO as its object storage solution, leveraging its scalability to store and manage all uploaded files.

- `MINIO_CONSOLE_PORT`  
  The port used to expose the MinIO console interface to the host machine, allowing **external** access to the web-based console running inside the Docker container. Defaults to `9001`
- `MINIO_PORT`  
  The port used to expose the MinIO API service to the host machine, allowing **external** access to the MinIO object storage service running inside the Docker container. Defaults to `9000`.
- `MINIO_USER`  
  The username for MinIO.
- `MINIO_PASSWORD`  
  The password for MinIO.

### Redis

- `REDIS_PORT`  
  The port used to expose the Redis service to the host machine, allowing **external** access to the Redis service running inside the Docker container. Defaults to `6379`.
- `REDIS_PASSWORD`  
  The password for Redis.

### RAGFlow

- `SVR_HTTP_PORT`  
  The port used to expose RAGFlow's HTTP API service to the host machine, allowing **external** access to the service running inside the Docker container. Defaults to `9380`.
- `RAGFLOW-IMAGE`  
  The Docker image edition. Available editions:  
  
  - `infiniflow/ragflow:v0.18.0-slim` (default): The RAGFlow Docker image without embedding models.  
  - `infiniflow/ragflow:v0.18.0`: The RAGFlow Docker image with embedding models including:
    - Built-in embedding models:
      - `BAAI/bge-large-zh-v1.5` 
      - `maidalun1020/bce-embedding-base_v1`


:::tip NOTE  
If you cannot download the RAGFlow Docker image, try the following mirrors.  

- For the `nightly-slim` edition:  
  - `RAGFLOW_IMAGE=swr.cn-north-4.myhuaweicloud.com/infiniflow/ragflow:nightly-slim` or,
  - `RAGFLOW_IMAGE=registry.cn-hangzhou.aliyuncs.com/infiniflow/ragflow:nightly-slim`.
- For the `nightly` edition:  
  - `RAGFLOW_IMAGE=swr.cn-north-4.myhuaweicloud.com/infiniflow/ragflow:nightly` or,
  - `RAGFLOW_IMAGE=registry.cn-hangzhou.aliyuncs.com/infiniflow/ragflow:nightly`.
:::

### Timezone

- `TIMEZONE`  
  The local time zone. Defaults to `'Asia/Shanghai'`.

### Hugging Face mirror site

- `HF_ENDPOINT`  
  The mirror site for huggingface.co. It is disabled by default. You can uncomment this line if you have limited access to the primary Hugging Face domain.

### MacOS

- `MACOS`  
  Optimizations for macOS. It is disabled by default. You can uncomment this line if your OS is macOS.

### User registration

- `REGISTER_ENABLED`
  - `1`: (Default) Enable user registration.
  - `0`: Disable user registration.

## Service configuration

[service_conf.yaml.template](https://github.com/infiniflow/ragflow/blob/main/docker/service_conf.yaml.template) specifies the system-level configuration for RAGFlow and is used by its API server and task executor.

### `ragflow`

- `host`: The API server's IP address inside the Docker container. Defaults to `0.0.0.0`.
- `port`: The API server's serving port inside the Docker container. Defaults to `9380`.

### `mysql`
  
- `name`: The MySQL database name. Defaults to `rag_flow`.
- `user`: The username for MySQL.
- `password`: The password for MySQL.
- `port`: The MySQL serving port inside the Docker container. Defaults to `3306`.
- `max_connections`: The maximum number of concurrent connections to the MySQL database. Defaults to `100`.
- `stale_timeout`: Timeout in seconds.

### `minio`
  
- `user`: The username for MinIO.
- `password`: The password for MinIO.
- `host`: The MinIO serving IP *and* port inside the Docker container. Defaults to `minio:9000`.

### `oauth`  

The OAuth configuration for signing up or signing in to RAGFlow using a third-party account.  It is disabled by default. To enable this feature, uncomment the corresponding lines in **service_conf.yaml.template**.

- `github`: The GitHub authentication settings for your application. Visit the [GitHub Developer Settings](https://github.com/settings/developers) page to obtain your client_id and secret_key.

#### OAuth/OIDC

RAGFlow supports OAuth/OIDC authentication through the following routes:

- `/login/&lt;channel&gt;`: Initiates the OAuth flow for the specified channel
- `/oauth/callback/&lt;channel&gt;`: Handles the OAuth callback after successful authentication

The callback URL should be configured in your OAuth provider as:
```
https://your-app.com/oauth/callback/&lt;channel&gt;
```

For detailed instructions on configuring **service_conf.yaml.template**, please refer to [Usage](https://github.com/infiniflow/ragflow/blob/main/api/apps/auth/README.md#usage).

### `user_default_llm`  

The default LLM to use for a new RAGFlow user. It is disabled by default. To enable this feature, uncomment the corresponding lines in **service_conf.yaml.template**.  

- `factory`: The LLM supplier. Available options:
  - `"OpenAI"`
  - `"DeepSeek"`
  - `"Moonshot"`
  - `"Tongyi-Qianwen"`
  - `"VolcEngine"`
  - `"ZHIPU-AI"`
- `api_key`: The API key for the specified LLM. You will need to apply for your model API key online.

:::tip NOTE  
If you do not set the default LLM here, configure the default LLM on the **Settings** page in the RAGFlow UI.
:::
</code></pre></div></div>
<p>faq.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 10
slug: /faq
---

# FAQs

Answers to questions about general features, troubleshooting, usage, and more.

---

import TOCInline from '@theme/TOCInline';

&lt;TOCInline toc={toc} /&gt;

## General features

---

### What sets RAGFlow apart from other RAG products?

The "garbage in garbage out" status quo remains unchanged despite the fact that LLMs have advanced Natural Language Processing (NLP) significantly. In response, RAGFlow introduces two unique features compared to other Retrieval-Augmented Generation (RAG) products.

- Fine-grained document parsing: Document parsing involves images and tables, with the flexibility for you to intervene as needed.
- Traceable answers with reduced hallucinations: You can trust RAGFlow's responses as you can view the citations and references supporting them.

---

### Differences between RAGFlow full edition and RAGFlow slim edition?

Each RAGFlow release is available in two editions:

- **Slim edition**: excludes built-in embedding models and is identified by a **-slim** suffix added to the version name. Example: `infiniflow/ragflow:v0.18.0-slim`
- **Full edition**: includes built-in embedding models and has no suffix added to the version name. Example: `infiniflow/ragflow:v0.18.0`

---

### Which embedding models can be deployed locally?

RAGFlow offers two Docker image editions, `v0.18.0-slim` and `v0.18.0`:  
  
- `infiniflow/ragflow:v0.18.0-slim` (default): The RAGFlow Docker image without embedding models.  
- `infiniflow/ragflow:v0.18.0`: The RAGFlow Docker image with embedding models including:
  - Built-in embedding models:
    - `BAAI/bge-large-zh-v1.5`
    - `maidalun1020/bce-embedding-base_v1`
  - Embedding models that will be downloaded once you select them in the RAGFlow UI:
    - `BAAI/bge-base-en-v1.5`
    - `BAAI/bge-large-en-v1.5`
    - `BAAI/bge-small-en-v1.5`
    - `BAAI/bge-small-zh-v1.5`
    - `jinaai/jina-embeddings-v2-base-en`
    - `jinaai/jina-embeddings-v2-small-en`
    - `nomic-ai/nomic-embed-text-v1.5`
    - `sentence-transformers/all-MiniLM-L6-v2`

---

### Where to find the version of RAGFlow? How to interpret it?

You can find the RAGFlow version number on the **System** page of the UI:

![Image](https://github.com/user-attachments/assets/20cf7213-2537-4e18-a88c-4dadf6228c6b)

If you build RAGFlow from source, the version number is also in the system log:

```
        ____   ___    ______ ______ __               
       / __ \ /   |  / ____// ____// /____  _      __
      / /_/ // /| | / / __ / /_   / // __ \| | /| / /
     / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ / 
    /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/                             

2025-02-18 10:10:43,835 INFO     1445658 RAGFlow version: v0.15.0-50-g6daae7f2 full
```

Where:

- `v0.15.0`: The officially published release.
- `50`: The number of git commits since the official release.
- `g6daae7f2`: `g` is the prefix, and `6daae7f2` is the first seven characters of the current commit ID.
- `full`/`slim`: The RAGFlow edition.
  - `full`: The full RAGFlow edition.
  - `slim`: The RAGFlow edition without embedding models and Python packages.

---

### Differences between demo.ragflow.io and a locally deployed open-source RAGFlow service?

demo.ragflow.io demonstrates the capabilities of RAGFlow Enterprise. Its DeepDoc models are pre-trained using proprietary data and it offers much more sophisticated team permission controls. Essentially, demo.ragflow.io serves as a preview of RAGFlow's forthcoming SaaS (Software as a Service) offering.

You can deploy an open-source RAGFlow service and call it from a Python client or through RESTful APIs. However, this is not supported on demo.ragflow.io.

---

### Why does it take longer for RAGFlow to parse a document than LangChain?

We put painstaking effort into document pre-processing tasks like layout analysis, table structure recognition, and OCR (Optical Character Recognition) using our vision models. This contributes to the additional time required.

---

### Why does RAGFlow require more resources than other projects?

RAGFlow has a number of built-in models for document structure parsing, which account for the additional computational resources.

---

### Which architectures or devices does RAGFlow support?

We officially support x86 CPU and nvidia GPU. While we also test RAGFlow on ARM64 platforms, we do not maintain RAGFlow Docker images for ARM. If you are on an ARM platform, follow [this guide](./develop/build_docker_image.mdx) to build a RAGFlow Docker image.

---

### Do you offer an API for integration with third-party applications?

The corresponding APIs are now available. See the [RAGFlow HTTP API Reference](./references/http_api_reference.md) or the [RAGFlow Python API Reference](./references/python_api_reference.md) for more information.

---

### Do you support stream output?

Yes, we do.

---

### Do you support sharing dialogue through URL?

No, this feature is not supported.

---

### Do you support multiple rounds of dialogues, referencing previous dialogues as context for the current query?

Yes, we support enhancing user queries based on existing context of an ongoing conversation:

1. On the **Chat** page, hover over the desired assistant and select **Edit**.
2. In the **Chat Configuration** popup, click the **Prompt engine** tab.
3. Switch on **Multi-turn optimization** to enable this feature.

---

### Key differences between AI search and chat?

- **AI search**: This is a single-turn AI conversation using a predefined retrieval strategy (a hybrid search of weighted keyword similarity and weighted vector similarity) and the system's default chat model. It does not involve advanced RAG strategies like knowledge graph, auto-keyword, or auto-question. Retrieved chunks will be listed below the chat model's response.
- **AI chat**: This is a multi-turn AI conversation where you can define your retrieval strategy (a weighted reranking score can be used to replace the weighted vector similarity in a hybrid search) and choose your chat model. In an AI chat, you can configure advanced RAG strategies, such as knowledge graphs, auto-keyword, and auto-question, for your specific case. Retrieved chunks are not displayed along with the answer.

When debugging your chat assistant, you can use AI search as a reference to verify your model settings and retrieval strategy.

---

## Troubleshooting

---

### How to build the RAGFlow image from scratch?

See [Build a RAGFlow Docker image](./develop/build_docker_image.mdx).

### Cannot access https://huggingface.co

A locally deployed RAGflow downloads OCR and embedding modules from [Huggingface website](https://huggingface.co) by default. If your machine is unable to access this site, the following error occurs and PDF parsing fails:

```
FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/huggingface/hub/models--InfiniFlow--deepdoc/snapshots/be0c1e50eef6047b412d1800aa89aba4d275f997/ocr.res'
```

To fix this issue, use https://hf-mirror.com instead:

1. Stop all containers and remove all related resources:

   ```bash
   cd ragflow/docker/
   docker compose down
   ```

2. Uncomment the following line in **ragflow/docker/.env**:

   ```
   # HF_ENDPOINT=https://hf-mirror.com
   ```

3. Start up the server:

   ```bash
   docker compose up -d 
   ```

---

### `MaxRetryError: HTTPSConnectionPool(host='hf-mirror.com', port=443)`

This error suggests that you do not have Internet access or are unable to connect to hf-mirror.com. Try the following:

1. Manually download the resource files from [huggingface.co/InfiniFlow/deepdoc](https://huggingface.co/InfiniFlow/deepdoc) to your local folder **~/deepdoc**.
2. Add a volumes to **docker-compose.yml**, for example:

   ```
   - ~/deepdoc:/ragflow/rag/res/deepdoc
   ```

---

### `WARNING: can't find /raglof/rag/res/borker.tm`

Ignore this warning and continue. All system warnings can be ignored.

---

### `network anomaly There is an abnormality in your network and you cannot connect to the server.`

![anomaly](https://github.com/infiniflow/ragflow/assets/93570324/beb7ad10-92e4-4a58-8886-bfb7cbd09e5d)

You will not log in to RAGFlow unless the server is fully initialized. Run `docker logs -f ragflow-server`.

*The server is successfully initialized, if your system displays the following:*

```
     ____   ___    ______ ______ __               
    / __ \ /   |  / ____// ____// /____  _      __
   / /_/ // /| | / / __ / /_   / // __ \| | /| / /
  / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ / 
 /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/  

 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:9380
 * Running on http://x.x.x.x:9380
 INFO:werkzeug:Press CTRL+C to quit
```

---

### `Realtime synonym is disabled, since no redis connection`

Ignore this warning and continue. All system warnings can be ignored.

![](https://github.com/infiniflow/ragflow/assets/93570324/ef5a6194-084a-4fe3-bdd5-1c025b40865c)

---

### Why does my document parsing stall at under one percent?

![stall](https://github.com/infiniflow/ragflow/assets/93570324/3589cc25-c733-47d5-bbfc-fedb74a3da50)

Click the red cross beside the 'parsing status' bar, then restart the parsing process to see if the issue remains. If the issue persists and your RAGFlow is deployed locally, try the following:

1. Check the log of your RAGFlow server to see if it is running properly:

   ```bash
   docker logs -f ragflow-server
   ```

2. Check if the **task_executor.py** process exists.
3. Check if your RAGFlow server can access hf-mirror.com or huggingface.com.

---

### Why does my pdf parsing stall near completion, while the log does not show any error?

Click the red cross beside the 'parsing status' bar, then restart the parsing process to see if the issue remains. If the issue persists and your RAGFlow is deployed locally, the parsing process is likely killed due to insufficient RAM. Try increasing your memory allocation by increasing the `MEM_LIMIT` value in **docker/.env**.

:::note
Ensure that you restart up your RAGFlow server for your changes to take effect!

```bash
docker compose stop
```

```bash
docker compose up -d
```

:::

![nearcompletion](https://github.com/infiniflow/ragflow/assets/93570324/563974c3-f8bb-4ec8-b241-adcda8929cbb)

---

### `Index failure`

An index failure usually indicates an unavailable Elasticsearch service.

---

### How to check the log of RAGFlow?

```bash
tail -f ragflow/docker/ragflow-logs/*.log
```

---

### How to check the status of each component in RAGFlow?

1. Check the status of the Elasticsearch Docker container:

   ```bash
   $ docker ps
   ```

   *The following is an example result:*

   ```bash
   5bc45806b680   infiniflow/ragflow:latest     "./entrypoint.sh"        11 hours ago   Up 11 hours               0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp, 0.0.0.0:9380-&gt;9380/tcp, :::9380-&gt;9380/tcp   ragflow-server
   91220e3285dd   docker.elastic.co/elasticsearch/elasticsearch:8.11.3   "/bin/tini -- /usr/lâ€¦"   11 hours ago   Up 11 hours (healthy)     9300/tcp, 0.0.0.0:9200-&gt;9200/tcp, :::9200-&gt;9200/tcp           ragflow-es-01
   d8c86f06c56b   mysql:5.7.18        "docker-entrypoint.sâ€¦"   7 days ago     Up 16 seconds (healthy)   0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp     ragflow-mysql
   cd29bcb254bc   quay.io/minio/minio:RELEASE.2023-12-20T01-00-02Z       "/usr/bin/docker-entâ€¦"   2 weeks ago    Up 11 hours      0.0.0.0:9001-&gt;9001/tcp, :::9001-&gt;9001/tcp, 0.0.0.0:9000-&gt;9000/tcp, :::9000-&gt;9000/tcp     ragflow-minio
   ```

2. Follow [this document](./guides/run_health_check.md) to check the health status of the Elasticsearch service.

:::danger IMPORTANT
The status of a Docker container status does not necessarily reflect the status of the service. You may find that your services are unhealthy even when the corresponding Docker containers are up running. Possible reasons for this include network failures, incorrect port numbers, or DNS issues.
:::

---

### `Exception: Can't connect to ES cluster`

1. Check the status of the Elasticsearch Docker container:

   ```bash
   $ docker ps
   ```

   *The status of a healthy Elasticsearch component should look as follows:*  

   ```
   91220e3285dd   docker.elastic.co/elasticsearch/elasticsearch:8.11.3   "/bin/tini -- /usr/lâ€¦"   11 hours ago   Up 11 hours (healthy)     9300/tcp, 0.0.0.0:9200-&gt;9200/tcp, :::9200-&gt;9200/tcp           ragflow-es-01
   ```

2. Follow [this document](./guides/run_health_check.md) to check the health status of the Elasticsearch service.

:::danger IMPORTANT
The status of a Docker container status does not necessarily reflect the status of the service. You may find that your services are unhealthy even when the corresponding Docker containers are up running. Possible reasons for this include network failures, incorrect port numbers, or DNS issues.
:::

3. If your container keeps restarting, ensure `vm.max_map_count` &gt;= 262144 as per [this README](https://github.com/infiniflow/ragflow?tab=readme-ov-file#-start-up-the-server). Updating the `vm.max_map_count` value in **/etc/sysctl.conf** is required, if you wish to keep your change permanent. Note that this configuration works only for Linux.

---

### Can't start ES container and get `Elasticsearch did not exit normally`

This is because you forgot to update the `vm.max_map_count` value in **/etc/sysctl.conf** and your change to this value was reset after a system reboot.

---

### `{"data":null,"code":100,"message":"&lt;NotFound '404: Not Found'&gt;"}`

Your IP address or port number may be incorrect. If you are using the default configurations, enter `http://&lt;IP_OF_YOUR_MACHINE&gt;` (**NOT 9380, AND NO PORT NUMBER REQUIRED!**) in your browser. This should work.

---

### `Ollama - Mistral instance running at 127.0.0.1:11434 but cannot add Ollama as model in RagFlow`

A correct Ollama IP address and port is crucial to adding models to Ollama:

- If you are on demo.ragflow.io, ensure that the server hosting Ollama has a publicly accessible IP address. Note that 127.0.0.1 is not a publicly accessible IP address.
- If you deploy RAGFlow locally, ensure that Ollama and RAGFlow are in the same LAN and can communicate with each other.

See [Deploy a local LLM](./guides/models/deploy_local_llm.mdx) for more information.

---

### Do you offer examples of using DeepDoc to parse PDF or other files?

Yes, we do. See the Python files under the **rag/app** folder.

---

### `FileNotFoundError: [Errno 2] No such file or directory`

1. Check the status of the MinIO Docker container:

   ```bash
   $ docker ps
   ```

   *The status of a healthy Elasticsearch component should look as follows:*  

   ```bash
   cd29bcb254bc   quay.io/minio/minio:RELEASE.2023-12-20T01-00-02Z       "/usr/bin/docker-entâ€¦"   2 weeks ago    Up 11 hours      0.0.0.0:9001-&gt;9001/tcp, :::9001-&gt;9001/tcp, 0.0.0.0:9000-&gt;9000/tcp, :::9000-&gt;9000/tcp     ragflow-minio
   ```

2. Follow [this document](./guides/run_health_check.md) to check the health status of the Elasticsearch service.

:::danger IMPORTANT
The status of a Docker container status does not necessarily reflect the status of the service. You may find that your services are unhealthy even when the corresponding Docker containers are up running. Possible reasons for this include network failures, incorrect port numbers, or DNS issues.
:::

---

## Usage

---

### How to run RAGFlow with a locally deployed LLM?

You can use Ollama or Xinference to deploy local LLM. See [here](./guides/models/deploy_local_llm.mdx) for more information.

---

### How to add an LLM that is not supported?

If your model is not currently supported but has APIs compatible with those of OpenAI, click **OpenAI-API-Compatible** on the **Model providers** page to configure your model:

![openai-api-compatible](https://github.com/user-attachments/assets/b1e964f2-b86e-41af-8528-fd8a96dc5f6f)

---

### How to integrate RAGFlow with Ollama?

- If RAGFlow is locally deployed, ensure that your RAGFlow and Ollama are in the same LAN.
- If you are using our online demo, ensure that the IP address of your Ollama server is public and accessible.

See [here](./guides/models/deploy_local_llm.mdx) for more information.

---

### How to change the file size limit?

For a locally deployed RAGFlow: the total file size limit per upload is 1GB, with a batch upload limit of 32 files. There is no cap on the total number of files per account. To update this 1GB file size limit:

- In **docker/.env**, upcomment `# MAX_CONTENT_LENGTH=1073741824`, adjust the value as needed, and note that `1073741824` represents 1GB in bytes.
- If you update the value of `MAX_CONTENT_LENGTH` in **docker/.env**, ensure that you update `client_max_body_size` in **nginx/nginx.conf** accordingly.

:::tip NOTE
It is not recommended to manually change the 32-file batch upload limit. However, if you use RAGFlow's HTTP API or Python SDK to upload files, the 32-file batch upload limit is automatically removed.
:::

---

### `Error: Range of input length should be [1, 30000]`

This error occurs because there are too many chunks matching your search criteria. Try reducing the **TopN** and increasing **Similarity threshold** to fix this issue:

1. Click **Chat** in the middle top of the page.
2. Right-click the desired conversation &gt; **Edit** &gt; **Prompt engine**
3. Reduce the **TopN** and/or raise **Similarity threshold**.
4. Click **OK** to confirm your changes.

![topn](https://github.com/infiniflow/ragflow/assets/93570324/7ec72ab3-0dd2-4cff-af44-e2663b67b2fc)

---

### How to get an API key for integration with third-party applications?

See [Acquire a RAGFlow API key](./develop/acquire_ragflow_api_key.md).

---

### How to upgrade RAGFlow?

See [Upgrade RAGFlow](./guides/upgrade_ragflow.mdx) for more information.

---

### How to switch the document engine to Infinity?

To switch your document engine from Elasticsearch to [Infinity](https://github.com/infiniflow/infinity):

1. Stop all running containers:  

   ```bash
   $ docker compose -f docker/docker-compose.yml down -v
   ```
:::caution WARNING
`-v` will delete all Docker container volumes, and the existing data will be cleared.
:::

2. In **docker/.env**, set `DOC_ENGINE=${DOC_ENGINE:-infinity}`
3. Restart your Docker image: 

   ```bash
   $ docker compose -f docker-compose.yml up -d
   ```

---

### Where are my uploaded files stored in RAGFlow's image?

All uploaded files are stored in Minio, RAGFlow's object storage solution. For instance, if you upload your file directly to a knowledge base, it is located at `&lt;knowledgebase_id&gt;/filename`.

---
</code></pre></div></div>
<p>release_notes.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /release_notes
---

# Releases

Key features, improvements and bug fixes in the latest releases.

:::info
Each RAGFlow release is available in two editions:
- **Slim edition**: excludes built-in embedding models and is identified by a **-slim** suffix added to the version name. Example: `infiniflow/ragflow:v0.18.0-slim`
- **Full edition**: includes built-in embedding models and has no suffix added to the version name. Example: `infiniflow/ragflow:v0.18.0`
:::

## v0.18.0

Released on April 23, 2025.

### Compatibility changes

From this release onwards, built-in rerank models have been removed because they have minimal impact on retrieval rates but significantly increase retrieval time.

### New features

- MCP server: enables access to RAGFlow's knowledge bases via MCP.
- DeepDoc supports adopting VLM model as a processing pipeline during document layout recognition, enabling in-depth analysis of images in PDF and DOCX files.
- OpenAI-compatible APIs: Agents can be called via OpenAI-compatible APIs.
- User registration control: administrators can enable or disable user registration through an environment variable.
- Team collaboration: Agents can be shared with team members.
- Agent version control: all updates are continuously logged and can be rolled back to a previous version via export.

![export_agent](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/export_agent_as_json.jpg)

### Improvements

- Enhanced answer referencing: Citation accuracy in generated responses is improved.
- Enhanced question-answering experience: users can now manually stop streaming output during a conversation.

### Documentation

#### Added documents

- [Set page rank](./guides/dataset/set_page_rank.md)
- [Enable RAPTOR](./guides/dataset/enable_raptor.md)
- [Set variables for your chat assistant](./guides/chat/set_chat_variables.md)
- [Launch RAGFlow MCP server](./develop/mcp/launch_mcp_server.md)

## v0.17.2

Released on March 13, 2025.

### Compatibility changes

- Removes the **Max_tokens** setting from **Chat configuration**.
- Removes the **Max_tokens** setting from **Generate**, **Rewrite**, **Categorize**, **Keyword** agent components.

From this release onwards, if you still see RAGFlow's responses being cut short or truncated, check the **Max_tokens** setting of your model provider.

### Improvements

- Adds OpenAI-compatible APIs.
- Introduces a German user interface.
- Accelerates knowledge graph extraction.
- Enables Tavily-based web search in the **Retrieval** agent component.
- Adds Tongyi-Qianwen QwQ models (OpenAI-compatible).
- Supports CSV files in the **General** chunking method.

### Fixed issues

- Unable to add models via Ollama/Xinference, an issue introduced in v0.17.1.

### Related APIs

#### HTTP APIs

- [Create chat completion](./references/http_api_reference.md#openai-compatible-api)

#### Python APIs

- [Create chat completion](./references/python_api_reference.md#openai-compatible-api)

## v0.17.1

Released on March 11, 2025.

### Improvements

- Improves English tokenization quality.
- Improves the table extraction logic in Markdown document parsing.
- Updates SiliconFlow's model list.
- Supports parsing XLS files (Excel 97-2003) with improved corresponding error handling.
- Supports Huggingface rerank models.
- Enables relative time expressions ("now", "yesterday", "last week", "next year", and more) in chat assistant and the **Rewrite** agent component.

### Fixed issues

- A repetitive knowledge graph extraction issue.
- Issues with API calling.
- Options in the **PDF parser**, aka **Document parser**, dropdown are missing.
- A Tavily web search issue.
- Unable to preview diagrams or images in an AI chat.

### Documentation

#### Added documents

- [Use tag set](./guides/dataset/use_tag_sets.md)

## v0.17.0

Released on March 3, 2025.

### New features

- AI chat: Implements Deep Research for agentic reasoning. To activate this, enable the **Reasoning** toggle under the **Prompt engine** tab of your chat assistant dialogue.
- AI chat: Leverages Tavily-based web search to enhance contexts in agentic reasoning. To activate this, enter the correct Tavily API key under the **Assistant settings** tab of your chat assistant dialogue.
- AI chat: Supports starting a chat without specifying knowledge bases.
- AI chat: HTML files can also be previewed and referenced, in addition to PDF files.
- Dataset: Adds a **PDF parser**, aka **Document parser**, dropdown menu to dataset configurations. This includes a DeepDoc model option, which is time-consuming, a much faster **naive** option (plain text), which skips DLA (Document Layout Analysis), OCR (Optical Character Recognition), and TSR (Table Structure Recognition) tasks, and several currently *experimental* large model options.
- Agent component: **(x)** or a forward slash `/` can be used to insert available keys (variables) in the system prompt field of the **Generate** or **Template** component.
- Object storage: Supports using Aliyun OSS (Object Storage Service) as a file storage option.
- Models: Updates the supported model list for Tongyi-Qianwen (Qwen), adding DeepSeek-specific models; adds ModelScope as a model provider.
- APIs: Document metadata can be updated through an API.

The following diagram illustrates the workflow of RAGFlow's Deep Research:

![Image](https://github.com/user-attachments/assets/f65d4759-4f09-4d9d-9549-c0e1fe907525)

The following is a screenshot of a conversation that integrates Deep Research:

![Image](https://github.com/user-attachments/assets/165b88ff-1f5d-4fb8-90e2-c836b25e32e9)

### Related APIs

#### HTTP APIs

Adds a body parameter `"meta_fields"` to the [Update document](./references/http_api_reference.md#update-document) method.

#### Python APIs

Adds a key option `"meta_fields"` to the [Update document](./references/python_api_reference.md#update-document) method.

### Documentation

#### Added documents

- [Run retrieval test](./guides/dataset/run_retrieval_test.md)

## v0.16.0

Released on February 6, 2025.

### New features

- Supports DeepSeek R1 and DeepSeek V3.
- GraphRAG refactor: Knowledge graph is dynamically built on an entire knowledge base (dataset) rather than on an individual file, and automatically updated when a newly uploaded file starts parsing. See [here](https://ragflow.io/docs/dev/construct_knowledge_graph).
- Adds an **Iteration** agent component and a **Research report generator** agent template. See [here](./guides/agent/agent_component_reference/iteration.mdx).
- New UI language: Portuguese.
- Allows setting metadata for a specific file in a knowledge base to enhance AI-powered chats. See [here](./guides/dataset/set_metadata.md).
- Upgrades RAGFlow's document engine [Infinity](https://github.com/infiniflow/infinity) to v0.6.0.dev3.
- Supports GPU acceleration for DeepDoc (see [docker-compose-gpu.yml](https://github.com/infiniflow/ragflow/blob/main/docker/docker-compose-gpu.yml)).
- Supports creating and referencing a **Tag** knowledge base as a key milestone towards bridging the semantic gap between query and response.

:::danger IMPORTANT
The **Tag knowledge base** feature is *unavailable* on the [Infinity](https://github.com/infiniflow/infinity) document engine.
:::

### Documentation

#### Added documents

- [Construct knowledge graph](./guides/dataset/construct_knowledge_graph.md)
- [Set metadata](./guides/dataset/set_metadata.md)
- [Begin component](./guides/agent/agent_component_reference/begin.mdx)
- [Generate component](./guides/agent/agent_component_reference/generate.mdx)
- [Interact component](./guides/agent/agent_component_reference/interact.mdx)
- [Retrieval component](./guides/agent/agent_component_reference/retrieval.mdx)
- [Categorize component](./guides/agent/agent_component_reference/categorize.mdx)
- [Keyword component](./guides/agent/agent_component_reference/keyword.mdx)
- [Message component](./guides/agent/agent_component_reference/message.mdx)
- [Rewrite component](./guides/agent/agent_component_reference/rewrite.mdx)
- [Switch component](./guides/agent/agent_component_reference/switch.mdx)
- [Concentrator component](./guides/agent/agent_component_reference/concentrator.mdx)
- [Template component](./guides/agent/agent_component_reference/template.mdx)
- [Iteration component](./guides/agent/agent_component_reference/iteration.mdx)
- [Note component](./guides/agent/agent_component_reference/note.mdx)

## v0.15.1

Released on December 25, 2024.

### Upgrades

- Upgrades RAGFlow's document engine [Infinity](https://github.com/infiniflow/infinity) to v0.5.2.
- Enhances the log display of document parsing status.

### Fixed issues

This release fixes the following issues:

- The `SCORE not found` and `position_int` errors returned by [Infinity](https://github.com/infiniflow/infinity).
- Once an embedding model in a specific knowledge base is changed, embedding models in other knowledge bases can no longer be changed.
- Slow response in question-answering and AI search due to repetitive loading of the embedding model.
- Fails to parse documents with RAPTOR.
- Using the **Table** parsing method results in information loss.
- Miscellaneous API issues.

### Related APIs

#### HTTP APIs

Adds an optional parameter `"user_id"` to the following APIs:

- [Create session with chat assistant](https://ragflow.io/docs/dev/http_api_reference#create-session-with-chat-assistant)
- [Update chat assistant's session](https://ragflow.io/docs/dev/http_api_reference#update-chat-assistants-session)
- [List chat assistant's sessions](https://ragflow.io/docs/dev/http_api_reference#list-chat-assistants-sessions)
- [Create session with agent](https://ragflow.io/docs/dev/http_api_reference#create-session-with-agent)
- [Converse with chat assistant](https://ragflow.io/docs/dev/http_api_reference#converse-with-chat-assistant)
- [Converse with agent](https://ragflow.io/docs/dev/http_api_reference#converse-with-agent)
- [List agent sessions](https://ragflow.io/docs/dev/http_api_reference#list-agent-sessions)

## v0.15.0

Released on December 18, 2024.

### New features

- Introduces additional Agent-specific APIs.
- Supports using page rank score to improve retrieval performance when searching across multiple knowledge bases.
- Offers an iframe in Chat and Agent to facilitate the integration of RAGFlow into your webpage.
- Adds a Helm chart for deploying RAGFlow on Kubernetes.
- Supports importing or exporting an agent in JSON format.
- Supports step run for Agent components/tools.
- Adds a new UI language: Japanese.
- Supports resuming GraphRAG and RAPTOR from a failure, enhancing task management resilience.
- Adds more Mistral models.
- Adds a dark mode to the UI, allowing users to toggle between light and dark themes.

### Improvements

- Upgrades the Document Layout Analysis model in DeepDoc.
- Significantly enhances the retrieval performance when using [Infinity](https://github.com/infiniflow/infinity) as document engine.

### Related APIs

#### HTTP APIs

- [List agent sessions](https://ragflow.io/docs/dev/http_api_reference#list-agent-sessions)
- [List agents](https://ragflow.io/docs/dev/http_api_reference#list-agents)

#### Python APIs

- [List agent sessions](https://ragflow.io/docs/dev/python_api_reference#list-agent-sessions)
- [List agents](https://ragflow.io/docs/dev/python_api_reference#list-agents)

## v0.14.1

Released on November 29, 2024.

### Improvements

Adds [Infinity's configuration file](https://github.com/infiniflow/ragflow/blob/main/docker/infinity_conf.toml) to facilitate integration and customization of [Infinity](https://github.com/infiniflow/infinity) as a document engine. From this release onwards, updates to Infinity's configuration can be made directly within RAGFlow and will take effect immediately after restarting RAGFlow using `docker compose`. [#3715](https://github.com/infiniflow/ragflow/pull/3715)

### Fixed issues

This release fixes the following issues:

- Unable to display or edit content of a chunk after clicking it.
- A `'Not found'` error in Elasticsearch.
- Chinese text becoming garbled during parsing.
- A compatibility issue with Polars.
- A compatibility issue between Infinity and GraphRAG.

## v0.14.0

Released on November 26, 2024.

### New features

- Supports [Infinity](https://github.com/infiniflow/infinity) or Elasticsearch (default) as document engine for vector storage and full-text indexing. [#2894](https://github.com/infiniflow/ragflow/pull/2894)
- Enhances user experience by adding more variables to the Agent and implementing auto-saving.
- Adds a three-step translation agent template, inspired by [Andrew Ng's translation agent](https://github.com/andrewyng/translation-agent).
- Adds an SEO-optimized blog writing agent template.
- Provides HTTP and Python APIs for conversing with an agent.
- Supports the use of English synonyms during retrieval processes.
- Optimizes term weight calculations, reducing the retrieval time by 50%.
- Improves task executor monitoring with additional performance indicators.
- Replaces Redis with Valkey.
- Adds three new UI languages (*contributed by the community*): Indonesian, Spanish, and Vietnamese.

### Compatibility changes

From this release onwards, **service_config.yaml.template** replaces **service_config.yaml** for configuring backend services. Upon Docker container startup, the environment variables defined in this template file are automatically populated and a **service_config.yaml** is auto-generated from it. [#3341](https://github.com/infiniflow/ragflow/pull/3341)

This approach eliminates the need to manually update **service_config.yaml** after making changes to **.env**, facilitating dynamic environment configurations.

:::danger IMPORTANT
Ensure that you [upgrade **both** your code **and** Docker image to this release](https://ragflow.io/docs/dev/upgrade_ragflow#upgrade-ragflow-to-the-most-recent-officially-published-release) before trying this new approach.
:::

### Related APIs

#### HTTP APIs

- [Create session with agent](https://ragflow.io/docs/dev/http_api_reference#create-session-with-agent)
- [Converse with agent](https://ragflow.io/docs/dev/http_api_reference#converse-with-agent)

#### Python APIs

- [Create session with agent](https://ragflow.io/docs/dev/python_api_reference#create-session-with-agent)
- [Converse with agent](https://ragflow.io/docs/dev/python_api_reference#create-session-with-agent)

### Documentation

#### Added documents

- [Configurations](https://ragflow.io/docs/dev/configurations)
- [Manage team members](./guides/team/manage_team_members.md)
- [Run health check on RAGFlow's dependencies](https://ragflow.io/docs/dev/run_health_check)

## v0.13.0

Released on October 31, 2024.

### New features

- Adds the team management functionality for all users.
- Updates the Agent UI to improve usability.
- Adds support for Markdown chunking in the **General** chunking method.
- Introduces an **invoke** tool within the Agent UI.
- Integrates support for Dify's knowledge base API.
- Adds support for GLM4-9B and Yi-Lightning models.
- Introduces HTTP and Python APIs for dataset management, file management within dataset, and chat assistant management.

:::tip NOTE
To download RAGFlow's Python SDK:

```bash
pip install ragflow-sdk==0.13.0
```
:::

### Documentation

#### Added documents

- [Acquire a RAGFlow API key](./develop/acquire_ragflow_api_key.md)
- [HTTP API Reference](./references/http_api_reference.md)
- [Python API Reference](./references/python_api_reference.md)

## v0.12.0

Released on September 30, 2024.

### New features

- Offers slim editions of RAGFlow's Docker images, which do not include built-in BGE/BCE embedding or reranking models.
- Improves the results of multi-round dialogues.
- Enables users to remove added LLM vendors.
- Adds support for **OpenTTS** and **SparkTTS** models.
- Implements an **Excel to HTML** toggle in the **General** chunking method, allowing users to parse a spreadsheet into either HTML tables or key-value pairs by row.
- Adds agent tools **YahooFinance** and **Jin10**.
- Adds an investment advisor agent template.

### Compatibility changes

From this release onwards, RAGFlow offers slim editions of its Docker images to improve the experience for users with limited Internet access. A slim edition of RAGFlow's Docker image does not include built-in BGE/BCE embedding models and has a size of about 1GB; a full edition of RAGFlow is approximately 9GB and includes both built-in embedding models and embedding models that will be downloaded once you select them in the RAGFlow UI.

The default Docker image edition is `nightly-slim`. The following list clarifies the differences between various editions:

- `nightly-slim`: The slim edition of the most recent tested Docker image.
- `v0.12.0-slim`: The slim edition of the most recent **officially released** Docker image.
- `nightly`: The full edition of the most recent tested Docker image.
- `v0.12.0`: The full edition of the most recent **officially released** Docker image.

See [Upgrade RAGFlow](https://ragflow.io/docs/dev/upgrade_ragflow) for instructions on upgrading.

### Documentation

#### Added documents

- [Upgrade RAGFlow](https://ragflow.io/docs/dev/upgrade_ragflow)

## v0.11.0

Released on September 14, 2024.

### New features

-  Introduces an AI search interface within the RAGFlow UI.
-  Supports audio output via **FishAudio** or **Tongyi Qwen TTS**.
-  Allows the use of Postgres for metadata storage, in addition to MySQL.
-  Supports object storage options with S3 or Azure Blob.
-  Supports model vendors: **Anthropic**, **Voyage AI**, and **Google Cloud**.
-  Supports the use of **Tencent Cloud ASR** for audio content recognition.
-  Adds finance-specific agent components: **WenCai**, **AkShare**, **YahooFinance**, and **TuShare**.
-  Adds a medical consultant agent template.
-  Supports running retrieval benchmarking on the following datasets:
    - [ms_marco_v1.1](https://huggingface.co/datasets/microsoft/ms_marco)
    - [trivia_qa](https://huggingface.co/datasets/mandarjoshi/trivia_qa)
    - [miracl](https://huggingface.co/datasets/miracl/miracl)

## v0.10.0

Released on August 26, 2024.

### New features

- Introduces a text-to-SQL template in the Agent UI.
- Implements Agent APIs.
- Incorporates monitoring for the task executor.
- Introduces Agent tools **GitHub**, **DeepL**, **BaiduFanyi**, **QWeather**, and **GoogleScholar**.
- Supports chunking of EML files.
- Supports more LLMs or model services: **GPT-4o-mini**, **PerfXCloud**, **TogetherAI**, **Upstage**, **Novita AI**, **01.AI**, **SiliconFlow**, **PPIO**, **XunFei Spark**, **Baidu Yiyan**, and **Tencent Hunyuan**.

## v0.9.0

Released on August 6, 2024.

### New features

- Supports GraphRAG as a chunking method.
- Introduces Agent component **Keyword** and search tools, including **Baidu**, **DuckDuckGo**, **PubMed**, **Wikipedia**, **Bing**, and **Google**.
- Supports speech-to-text recognition for audio files.
- Supports model vendors **Gemini** and **Groq**.
- Supports inference frameworks, engines, and services including **LM studio**, **OpenRouter**, **LocalAI**, and **Nvidia API**.
- Supports using reranker models in Xinference.

## v0.8.0

Released on July 8, 2024.

### New features

- Supports Agentic RAG, enabling graph-based workflow construction for RAG and agents.
- Supports model vendors **Mistral**, **MiniMax**, **Bedrock**, and **Azure OpenAI**.
- Supports DOCX files in the MANUAL chunking method.
- Supports DOCX, MD, and PDF files in the Q&amp;A chunking method.

## v0.7.0

Released on May 31, 2024.

### New features

- Supports the use of reranker models.
- Integrates reranker and embedding models: [BCE](https://github.com/netease-youdao/BCEmbedding), [BGE](https://github.com/FlagOpen/FlagEmbedding), and [Jina](https://jina.ai/embeddings/).
- Supports LLMs Baichuan and VolcanoArk.
- Implements [RAPTOR](https://arxiv.org/html/2401.18059v1) for improved text retrieval.
- Supports HTML files in the GENERAL chunking method.
- Provides HTTP and Python APIs for deleting documents by ID.
- Supports ARM64 platforms.

:::danger IMPORTANT
While we also test RAGFlow on ARM64 platforms, we do not maintain RAGFlow Docker images for ARM.

If you are on an ARM platform, follow [this guide](./develop/build_docker_image.mdx) to build a RAGFlow Docker image.
:::

### Related APIs

#### HTTP API

- [Delete documents](https://ragflow.io/docs/dev/http_api_reference#delete-documents)

#### Python API

- [Delete documents](https://ragflow.io/docs/dev/python_api_reference#delete-documents)

## v0.6.0

Released on May 21, 2024.

### New features

- Supports streaming output.
- Provides HTTP and Python APIs for retrieving document chunks.
- Supports monitoring of system components, including Elasticsearch, MySQL, Redis, and MinIO.
- Supports disabling **Layout Recognition** in the GENERAL chunking method to reduce file chunking time.

### Related APIs

#### HTTP API

- [Retrieve chunks](https://ragflow.io/docs/dev/http_api_reference#retrieve-chunks)

#### Python API

- [Retrieve chunks](https://ragflow.io/docs/dev/python_api_reference#retrieve-chunks)

## v0.5.0

Released on May 8, 2024.

### New features

- Supports LLM DeepSeek.

</code></pre></div></div>
<p>develop_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Developers"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides for hardcore developers"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>develop\acquire_ragflow_api_key.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /acquire_ragflow_api_key
---

# Acquire RAGFlow API key

An API key is required for the RAGFlow server to authenticate your HTTP/Python or MCP requests. This documents provides instructions on obtaining a RAGFlow API key.

1. Click your avatar in the top right corner of the RAGFlow UI to access the configuration page.
2. Click **API** to switch to the **API** page.
3. Obtain a RAGFlow API key:

![ragflow_api_key](https://github.com/user-attachments/assets/f461ed61-04c6-4faf-b3d8-6b5fa56be4e7)

:::tip NOTE
See the [RAGFlow HTTP API reference](../references/http_api_reference.md) or the [RAGFlow Python API reference](../references/python_api_reference.md) for a complete reference of RAGFlow's HTTP or Python APIs.
:::
</code></pre></div></div>
<p>develop\launch_ragflow_from_source.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /launch_ragflow_from_source
---

# Launch service from source

A guide explaining how to set up a RAGFlow service from its source code. By following this guide, you'll be able to debug using the source code.

## Target audience

Developers who have added new features or modified existing code and wish to debug using the source code, *provided that* their machine has the target deployment environment set up.

## Prerequisites

- CPU &amp;ge; 4 cores
- RAM &amp;ge; 16 GB
- Disk &amp;ge; 50 GB
- Docker &amp;ge; 24.0.0 &amp; Docker Compose &amp;ge; v2.26.1

:::tip NOTE
If you have not installed Docker on your local machine (Windows, Mac, or Linux), see the [Install Docker Engine](https://docs.docker.com/engine/install/) guide.
:::

## Launch a service from source

To launch a RAGFlow service from source code:

### Clone the RAGFlow repository

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
```

### Install Python dependencies

1. Install uv:
   
   ```bash
   pipx install uv
   ```

2. Install Python dependencies:
   - slim:
   ```bash
   uv sync --python 3.10 # install RAGFlow dependent python modules
   ```
   - full:
   ```bash
   uv sync --python 3.10 --all-extras # install RAGFlow dependent python modules
   ```
   *A virtual environment named `.venv` is created, and all Python dependencies are installed into the new environment.*

### Launch third-party services

The following command launches the 'base' services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:

```bash
docker compose -f docker/docker-compose-base.yml up -d
```

### Update `host` and `port` Settings for Third-party Services

1. Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/service_conf.yaml.template** to `127.0.0.1`:

   ```
   127.0.0.1       es01 infinity mysql minio redis
   ```

2. In **docker/service_conf.yaml.template**, update mysql port to `5455` and es port to `1200`, as specified in **docker/.env**.

### Launch the RAGFlow backend service

1. Comment out the `nginx` line in **docker/entrypoint.sh**.

   ```
   # /usr/sbin/nginx
   ```

2. Activate the Python virtual environment:

   ```bash
   source .venv/bin/activate
   export PYTHONPATH=$(pwd)
   ```

3. **Optional:** If you cannot access HuggingFace, set the HF_ENDPOINT environment variable to use a mirror site:
 
   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```

4. Check the configuration in **conf/service_conf.yaml**, ensuring all hosts and ports are correctly set.
   
5. Run the **entrypoint.sh** script to launch the backend service:

   ```shell
   JEMALLOC_PATH=$(pkg-config --variable=libdir jemalloc)/libjemalloc.so;
   LD_PRELOAD=$JEMALLOC_PATH python rag/svr/task_executor.py 1;
   ```
   ```shell
   python api/ragflow_server.py;
   ```

### Launch the RAGFlow frontend service

1. Navigate to the `web` directory and install the frontend dependencies:

   ```bash
   cd web
   npm install
   ```

2. Update `proxy.target` in **.umirc.ts** to `http://127.0.0.1:9380`:

   ```bash
   vim .umirc.ts
   ```

3. Start up the RAGFlow frontend service:

   ```bash
   npm run dev 
   ```

   *The following message appears, showing the IP address and port number of your frontend service:*  

   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)

### Access the RAGFlow service

In your web browser, enter `http://127.0.0.1:&lt;PORT&gt;/`, ensuring the port number matches that shown in the screenshot above.

### Stop the RAGFlow service when the development is done

1. Stop the RAGFlow frontend service:
   ```bash
   pkill npm
   ```

2. Stop the RAGFlow backend service:
   ```bash
   pkill -f "docker/entrypoint.sh"
   ```

</code></pre></div></div>
<p>develop\mcp_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"MCP"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides and references on accessing RAGFlow's knowledge bases via MCP."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>develop\mcp\mcp_client_example.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /mcp_client
---

# RAGFlow MCP client example

We provide a *prototype* MCP client example for testing [here](https://github.com/infiniflow/ragflow/blob/main/mcp/client/client.py).

:::danger IMPORTANT
If your MCP server is running in host mode, include your acquired API key in your client's `headers` as shown below:
```python
async with sse_client("http://localhost:9382/sse", headers={"api_key": "YOUR_KEY_HERE"}) as streams:
    # Rest of your code...
```
:::
</code></pre></div></div>
<p>develop\mcp\mcp_tools.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /mcp_tools
---

# RAGFlow MCP tools

The MCP server currently offers a specialized tool to assist users in searching for relevant information powered by RAGFlow DeepDoc technology:

- **retrieve**: Fetches relevant chunks from specified `dataset_ids` and optional `document_ids` using the RAGFlow retrieve interface, based on a given question. Details of all available datasets, namely, `id` and `description`, are provided within the tool description for each individual dataset.

For more information, see our Python implementation of the [MCP server](https://github.com/infiniflow/ragflow/blob/main/mcp/server/server.py).
</code></pre></div></div>
<p>guides_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides for RAGFlow users and developers."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\manage_files.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 6
slug: /manage_files
---

# Files

Knowledge base, hallucination-free chat, and file management are the three pillars of RAGFlow. RAGFlow's file management allows you to upload files individually or in bulk. You can then link an uploaded file to multiple target knowledge bases. This guide showcases some basic usages of the file management feature.

:::info IMPORTANT
Compared to uploading files directly to various knowledge bases, uploading them to RAGFlow's file management and then linking them to different knowledge bases is *not* an unnecessary step, particularly when you want to delete some parsed files or an entire knowledge base but retain the original files.
:::

## Create folder

RAGFlow's file management allows you to establish your file system with nested folder structures. To create a folder in the root directory of RAGFlow: 

![create new folder](https://github.com/infiniflow/ragflow/assets/93570324/3a37a5f4-43a6-426d-a62a-e5cd2ff7a533)

:::caution NOTE
Each knowledge base in RAGFlow has a corresponding folder under the **root/.knowledgebase** directory. You are not allowed to create a subfolder within it.
:::

## Upload file

RAGFlow's file management supports file uploads from your local machine, allowing both individual and bulk uploads: 

![upload file](https://github.com/infiniflow/ragflow/assets/93570324/5d7ded14-ce2b-4703-8567-9356a978f45c)

![bulk upload](https://github.com/infiniflow/ragflow/assets/93570324/def0db55-824c-4236-b809-a98d8c8674e3)

## Preview file

RAGFlow's file management supports previewing files in the following formats:

- Documents (PDF, DOCS)
- Tables (XLSX)
- Pictures (JPEG, JPG, PNG, TIF, GIF)

![preview](https://github.com/infiniflow/ragflow/assets/93570324/2e931362-8bbf-482c-ac86-b68b09d331bc)

## Link file to knowledge bases

RAGFlow's file management allows you to *link* an uploaded file to multiple knowledge bases, creating a file reference in each target knowledge base. Therefore, deleting a file in your file management will AUTOMATICALLY REMOVE all related file references across the knowledge bases.

![link knowledgebase](https://github.com/infiniflow/ragflow/assets/93570324/6c6b8db4-3269-4e35-9434-6089887e3e3f)

You can link your file to one knowledge base or multiple knowledge bases at one time: 

![link multiple kb](https://github.com/infiniflow/ragflow/assets/93570324/6c508803-fb1f-435d-b688-683066fd7fff)

## Move file to a specific folder

![move files](https://github.com/user-attachments/assets/3a2db469-6811-4ea0-be80-403b61ffe257)

## Search files or folders

**File Management** only supports file name and folder name filtering in the current directory (files or folders in the child directory will not be retrieved).

![search file](https://github.com/infiniflow/ragflow/assets/93570324/77ffc2e5-bd80-4ed1-841f-068e664efffe)

## Rename file or folder

RAGFlow's file management allows you to rename a file or folder:

![rename_file](https://github.com/infiniflow/ragflow/assets/93570324/5abb0704-d9e9-4b43-9ed4-5750ccee011f)


## Delete files or folders

RAGFlow's file management allows you to delete files or folders individually or in bulk. 

To delete a file or folder: 

![delete file](https://github.com/infiniflow/ragflow/assets/93570324/85872728-125d-45e9-a0ee-21e9d4cedb8b)

To bulk delete files or folders:

![bulk delete](https://github.com/infiniflow/ragflow/assets/93570324/519b99ab-ec7f-4c8a-8cea-e0b6dcb3cb46)

&gt; - You are not allowed to delete the **root/.knowledgebase** folder. 
&gt; - Deleting files that have been linked to knowledge bases will **AUTOMATICALLY REMOVE** all associated file references across the knowledge bases.

## Download uploaded file

RAGFlow's file management allows you to download an uploaded file:

![download_file](https://github.com/infiniflow/ragflow/assets/93570324/cf3b297f-7d9b-4522-bf5f-4f45743e4ed5)

&gt; As of RAGFlow v0.18.0, bulk download is not supported, nor can you download an entire folder. 

</code></pre></div></div>
<p>guides\run_health_check.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 8
slug: /run_health_check
---

# Monitoring

Double-check the health status of RAGFlow's dependencies.

---

The operation of RAGFlow depends on four services:

- **Elasticsearch** (default) or [Infinity](https://github.com/infiniflow/infinity) as the document engine
- **MySQL**
- **Redis**
- **MinIO** for object storage

If an exception or error occurs related to any of the above services, such as `Exception: Can't connect to ES cluster`, refer to this document to check their health status.

You can also click you avatar in the top right corner of the page **&gt;** System to view the visualized health status of RAGFlow's core services. The following screenshot shows that all services are 'green' (running healthily). The task executor displays the *cumulative* number of completed and failed document parsing tasks from the past 30 minutes:

![system_status_page](https://github.com/user-attachments/assets/b0c1a11e-93e3-4947-b17a-1bfb4cdab6e4)

Services with a yellow or red light are not running properly. The following is a screenshot of the system page after running `docker stop ragflow-es-10`:

![es_failed](https://github.com/user-attachments/assets/06056540-49f5-48bf-9cc9-a7086bc75790)

You can click on a specific 30-second time interval to view the details of completed and failed tasks:

![done_tasks](https://github.com/user-attachments/assets/49b25ec4-03af-48cf-b2e5-c892f6eaa261)

![done_vs_failed](https://github.com/user-attachments/assets/eaa928d0-a31c-4072-adea-046091e04599)

</code></pre></div></div>
<p>guides\upgrade_ragflow.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 11
slug: /upgrade_ragflow
---

# Upgrading
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Upgrade RAGFlow to `nightly-slim`/`nightly` or the latest, published release.

:::info NOTE
Upgrading RAGFlow in itself will *not* remove your uploaded/historical data. However, be aware that `docker compose -f docker/docker-compose.yml down -v` will remove Docker container volumes, resulting in data loss.
:::

## Upgrade RAGFlow to `nightly-slim`/`nightly`, the most recent, tested Docker image

`nightly-slim` refers to the RAGFlow Docker image *without* embedding models, while `nightly` refers to the RAGFlow Docker image with embedding models. For details on their differences, see [ragflow/docker/.env](https://github.com/infiniflow/ragflow/blob/main/docker/.env).

To upgrade RAGFlow, you must upgrade **both** your code **and** your Docker image:

1. Clone the repo

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   ```

2. Update **ragflow/docker/.env**:

&lt;Tabs
  defaultValue="nightly-slim"
  values={[
    {label: 'nightly-slim', value: 'nightly-slim'},
    {label: 'nightly', value: 'nightly'},
  ]}&gt;
  &lt;TabItem value="nightly-slim"&gt;

```bash
RAGFLOW_IMAGE=infiniflow/ragflow:nightly-slim
```

  &lt;/TabItem&gt;
  &lt;TabItem value="nightly"&gt;

```bash
RAGFLOW_IMAGE=infiniflow/ragflow:nightly
```

  &lt;/TabItem&gt;
&lt;/Tabs&gt;

3. Update RAGFlow image and restart RAGFlow:

   ```bash
   docker compose -f docker/docker-compose.yml pull
   docker compose -f docker/docker-compose.yml up -d
   ```

## Upgrade RAGFlow to the most recent, officially published release

To upgrade RAGFlow, you must upgrade **both** your code **and** your Docker image:

1. Clone the repo

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   ```

2. Switch to the latest, officially published release, e.g., `v0.18.0`:

   ```bash
   git checkout -f v0.18.0
   ```

3. Update **ragflow/docker/.env** as follows:

   ```bash
   RAGFLOW_IMAGE=infiniflow/ragflow:v0.18.0
   ```

4. Update the RAGFlow image and restart RAGFlow:

   ```bash
   docker compose -f docker/docker-compose.yml pull
   docker compose -f docker/docker-compose.yml up -d
   ```

## Frequently asked questions

### Upgrade RAGFlow in an offline environment (without Internet access)

1. From an environment with Internet access, pull the required Docker image.
2. Save the Docker image to a **.tar** file.
   ```bash
   docker save -o ragflow.v0.18.0.tar infiniflow/ragflow:v0.18.0
   ```
3. Copy the **.tar** file to the target server.
4. Load the **.tar** file into Docker:
   ```bash
   docker load -i ragflow.v0.18.0.tar
   ```

</code></pre></div></div>
<p>guides\agent_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Agents"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"RAGFlow v0.8.0 introduces an agent mechanism, featuring a no-code workflow editor on the front end and a comprehensive graph-based task orchestration framework on the backend."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\agent\agent_introduction.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /agent_introduction
---

# Introduction to agents

Key concepts, basic operations, a quick view of the agent editor.

---

## Key concepts

Agents and RAG are complementary techniques, each enhancing the otherâ€™s capabilities in business applications. RAGFlow v0.8.0 introduces an agent mechanism, featuring a no-code workflow editor on the front end and a comprehensive graph-based task orchestration framework on the back end. This mechanism is built on top of RAGFlow's existing RAG solutions and aims to orchestrate search technologies such as query intent classification, conversation leading, and query rewriting to:

- Provide higher retrievals and,
- Accommodate more complex scenarios.

## Create an agent

:::tip NOTE

Before proceeding, ensure that:  

1. You have properly set the LLM to use. See the guides on [Configure your API key](../models/llm_api_key_setup.md) or [Deploy a local LLM](../models/deploy_local_llm.mdx) for more information.
2. You have a knowledge base configured and the corresponding files properly parsed. See the guide on [Configure a knowledge base](../dataset/configure_knowledge_base.md) for more information.

:::

Click the **Agent** tab in the middle top of the page to show the **Agent** page. As shown in the screenshot below, the cards on this page represent the created agents, which you can continue to edit.

![agent_mainpage](https://github.com/user-attachments/assets/5c0bb123-8f4e-42ea-b250-43f640dc6814)

We also provide templates catered to different business scenarios. You can either generate your agent from one of our agent templates or create one from scratch:

1. Click **+ Create agent** to show the **agent template** page:

   ![agent_templates](https://github.com/user-attachments/assets/73bd476c-4bab-4c8c-82f8-6b00fb2cd044)

2. To create an agent from scratch, click the **Blank** card. Alternatively, to create an agent from one of our templates, hover over the desired card, such as **General-purpose chatbot**, click **Use this template**, name your agent in the pop-up dialogue, and click **OK** to confirm.  

   *You are now taken to the **no-code workflow editor** page. The left panel lists the components (operators): Above the dividing line are the RAG-specific components; below the line are tools. We are still working to expand the component list.*

   ![workflow_editor](https://github.com/user-attachments/assets/47b4d5ce-b35a-4d6b-b483-ba495a75a65d)

3. General speaking, now you can do the following:
   - Drag and drop a desired component to your workflow,
   - Select the knowledge base to use,
   - Update settings of specific components,
   - Update LLM settings
   - Sets the input and output for a specific component, and more.
4. Click **Save** to apply changes to your agent and **Run** to test it.

## Components

Please review the flowing description of the RAG-specific components before you proceed:

| Component      | Description                                                                                                                                                                                              |
|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Retrieval**  | A component that retrieves information from specified knowledge bases and returns 'Empty response' if no information is found. Ensure the correct knowledge bases are selected.                          |
| **Generate**   | A component that prompts the LLM to generate responses. You must ensure the prompt is set correctly.                                                                                                     |
| **Interact**   | A component that serves as the interface between human and the bot, receiving user inputs and displaying the agent's responses.                                                                          |
| **Categorize** | A component that uses the LLM to classify user inputs into predefined categories. Ensure you specify the name, description, and examples for each category, along with the corresponding next component. |
| **Message**    | A component that sends out a static message. If multiple messages are supplied, it randomly selects one to send. Ensure its downstream is **Interact**, the interface component.                         |
| **Rewrite**    | A component that rewrites a user query from the **Interact** component, based on the context of previous dialogues.                                                                                      |
| **Keyword**    | A component that extracts keywords from a user query, with TopN specifying the number of keywords to extract.                                                                                            |

:::caution NOTE

- Ensure **Rewrite**'s upstream component is **Relevant** and downstream component is **Retrieval**.
- Ensure the downstream component of **Message** is **Interact**.
- The downstream component of **Begin** is always **Interact**.

:::

## Basic operations

| Operation                 | Description                                                                                                                              |
|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------|
| Add a component           | Drag and drop the desired component from the left panel onto the canvas.                                                                 |
| Delete a component        | On the canvas, hover over the three dots (...) of the component to display the delete option, then select it to remove the component.    |
| Copy a component          | On the canvas, hover over the three dots (...) of the component to display the copy option, then select it to make a copy the component. |
| Update component settings | On the canvas, click the desired component to display the component settings.                                                            |

</code></pre></div></div>
<p>guides\agent\embed_agent_into_webpage.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /embed_agent_into_webpage
---

# Embed agent into webpage

You can use iframe to embed an agent into a third-party webpage.

:::caution WARNING
If your agent's **Begin** component takes a variable, you *cannot* embed it into a webpage.
:::

1. Before proceeding, you must [acquire an API key](../models/llm_api_key_setup.md); otherwise, an error message would appear.
2. On the **Agent** page, click an intended agent **&gt;** **Edit** to access its editing page.
3. Click **Embed into webpage** on the top right corner of the canvas to show the **iframe** window:

   ![agent_embed](https://github.com/user-attachments/assets/f748bb91-1a48-45ca-89ea-5b1c257407cb)

4. Copy the iframe and embed it into a specific location on your webpage.

</code></pre></div></div>
<p>guides\agent\general_purpose_chatbot.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /general_purpose_chatbot
---

# Create chatbot

Create a general-purpose chatbot.

---

Chatbot is one of the most common AI scenarios. However, effectively understanding user queries and responding appropriately remains a challenge. RAGFlow's general-purpose chatbot agent is our attempt to tackle this longstanding issue.  

This chatbot closely resembles the chatbot introduced in [Start an AI chat](../chat/start_chat.md), but with a key difference - it introduces a reflective mechanism that allows it to improve the retrieval from the target knowledge bases by rewriting the user's query.

This document provides guides on creating such a chatbot using our chatbot template.

## Prerequisites

1. Ensure you have properly set the LLM to use. See the guides on [Configure your API key](../models/llm_api_key_setup.md) or [Deploy a local LLM](../models/deploy_local_llm.mdx) for more information.
2. Ensure you have a knowledge base configured and the corresponding files properly parsed. See the guide on [Configure a knowledge base](../dataset/configure_knowledge_base.md) for more information.
3. Make sure you have read the [Introduction to Agentic RAG](./agent_introduction.md).

## Create a chatbot agent from template

To create a general-purpose chatbot agent using our template:

1. Click the **Agent** tab in the middle top of the page to show the **Agent** page.
2. Click **+ Create agent** on the top right of the page to show the **agent template** page.
3. On the **agent template** page, hover over the card on **General-purpose chatbot** and click **Use this template**.  
   *You are now directed to the **no-code workflow editor** page.*

   ![workflow_editor](https://github.com/user-attachments/assets/52e7dc62-4bf5-4fbb-ab73-4a6e252065f0)

:::tip NOTE
RAGFlow's no-code editor spares you the trouble of coding, making agent development effortless.
:::

## Understand each component in the template

Hereâ€™s a breakdown of each component and its role and requirements in the chatbot template:

- **Begin**
  - Function: Sets an opening greeting for users.
  - Purpose: Establishes a welcoming atmosphere and prepares the user for interaction.

- **Interact**
  - Function: Serves as the interface between human and the bot.
  - Role: Acts as the downstream component of **Begin**.  

- **Retrieval**
  - Function: Retrieves information from specified knowledge base(s).
  - Requirement: Must have `knowledgebases` set up to function.

- **Relevant**
  - Function: Assesses the relevance of the retrieved information from the **Retrieval** component to the user query.
  - Process:  
    - If relevant, it directs the data to the **Generate** component for final response generation.
    - Otherwise, it triggers the **Rewrite** component to refine the user query and redo the retrival process.

- **Generate**
  - Function: Prompts the LLM to generate responses based on the retrieved information.  
  - Note: The prompt settings allow you to control the way in which the LLM generates responses. Be sure to review the prompts and make necessary changes.

- **Rewrite**:  
  - Function: Refines a user query when no relevant information from the knowledge base is retrieved.  
  - Usage: Often used in conjunction with **Relevant** and **Retrieval** to create a reflective/feedback loop.  

## Configure your chatbot agent

1. Click **Begin** to set an opening greeting:  
   ![opener](https://github.com/user-attachments/assets/4416bc16-2a84-4f24-a19b-6dc8b1de0908)

2. Click **Retrieval** to select the right knowledge base(s) and make any necessary adjustments:  
   ![setting_knowledge_bases](https://github.com/user-attachments/assets/5f694820-5651-45bc-afd6-cf580ca0228d)

3. Click **Generate** to configure the LLM's summarization behavior:  
   3.1. Confirm the model.  
   3.2. Review the prompt settings. If there are variables, ensure they match the correct component IDs:  
   ![prompt_settings](https://github.com/user-attachments/assets/19e94ea7-7f62-4b73-b526-32fcfa62f1e9)

4. Click **Relevant** to review or change its settings:  
   *You may retain the current settings, but feel free to experiment with changes to understand how the agent operates.*
   ![relevant_settings](https://github.com/user-attachments/assets/9ff7fdd8-7a69-4ee2-bfba-c7fb8029150f)

5. Click **Rewrite** to select a different model for query rewriting or update the maximum loop times for query rewriting:  
   ![choose_model](https://github.com/user-attachments/assets/2bac1d6c-c4f1-42ac-997b-102858c3f550)
   ![loop_time](https://github.com/user-attachments/assets/09a4ce34-7aac-496f-aa59-d8aa33bf0b1f)

:::danger NOTE
Increasing the maximum loop times may significantly extend the time required to receive the final response.
:::

1. Update your workflow where you see necessary.

2. Click to **Save** to apply your changes.  
   *Your agent appears as one of the agent cards on the **Agent** page.*

## Test your chatbot agent

1. Find your chatbot agent on the **Agent** page:  
   ![find_chatbot](https://github.com/user-attachments/assets/6e6382c6-9a86-4190-9fdd-e363b7f64ba9)

2. Experiment with your questions to verify if this chatbot functions as intended:  
   ![test_chatbot](https://github.com/user-attachments/assets/c074d3bd-4c39-4b05-a68b-1fd361f256b3)
</code></pre></div></div>
<p>guides\agent\agent_component_reference_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Agent Components"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"A complete reference for RAGFlow's agent components."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\agent\agent_component_reference\begin.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /begin_component
---

# Begin component

The starting component in a workflow.

---

The **Begin** component sets an opening greeting or accepts inputs from the user. It is automatically populated onto the canvas when you create an agent, whether from a template or from scratch (from a blank template). There should be only one **Begin** component in the workflow.

## Scenarios

A **Begin** component is essential in all cases. Every agent includes a **Begin** component, which cannot be deleted.

## Configurations

Click the component to display its **Configuration** window. Here, you can set an opening greeting and the input parameters (global variables) for the agent.

### ID

The ID is the unique identifier for the component within the workflow. Unlike the IDs of other components, the ID of the **Begin** component *cannot* be changed.

### Opening greeting

An opening greeting is the agent's first message to the user. It can be a welcoming remark or an instruction to guide the user forward.

### Global variables

You can set global variables within the **Begin** component, which can be either required or optional. Once established, users will need to provide values for these variables when interacting or chatting with the agent. Click **+ Add variable** to add a global variable, each with the following attributes:

- **Key**: *Required*  
  The unique variable name.
- **Name**: *Required*  
  A descriptive name providing additional details about the variable.  
  For example, if **Key** is set to `lang`, you can set its **Name** to `Target language`.
- **Type**: *Required*  
  The type of the variable:  
  - **line**: Accepts a single line of text without line breaks.
  - **paragraph**: Accepts multiple lines of text, including line breaks.
  - **options**: Requires the user to select a value for this variable from a dropdown menu. And you are required to set *at least* one option for the dropdown menu.
  - **file**: Requires the user to upload one or multiple files.
  - **integer**: Accepts an integer as input.
  - **boolean**: Requires the user to toggle between on and off.
- **Optional**: A toggle indicating whether the variable is optional. 

:::tip NOTE
To pass in parameters from a client, call:
- HTTP method [Converse with agent](../../../references/http_api_reference.md#converse-with-agent), or
- Python method [Converse with agent](../../../references/python_api_reference.md#converse-with-agent).
:::

:::danger IMPORTANT
- If you set the key type as **file**, ensure the token count of the uploaded file does not exceed your model provider's maximum token limit; otherwise, the plain text in your file will be truncated and incomplete.
- If your agent's **Begin** component takes a variable, you *cannot* embed it into a webpage.
:::

## Examples

As mentioned earlier, the **Begin** component is indispensable for an agent. Still, you can take a look at our three-step interpreter agent template, where the **Begin** component takes two global variables:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Interpreter** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Begin** component to display its **Configuration** window.

## Frequently asked questions

### Is the uploaded file in a knowledge base?

No. Files uploaded to an agent as input are not stored in a knowledge base and hence will not be processed using RAGFlow's built-in OCR, DLR or TSR models, or chunked using RAGFlow's built-in chunking methods. 

### How to upload a webpage or file from a URL?

If you set the type of a variable as **file**, your users will be able to upload a file either from their local device or from an accessible URL. For example:

![upload_file](https://github.com/user-attachments/assets/7ad2a352-0807-4b74-b8d1-d09e5cd37997)

### File size limit for an uploaded file

There is no *specific* file size limit for a file uploaded to an agent. However, note that model providers typically have a default or explicit maximum token setting, which can range from 8196 to 128k: The plain text part of the uploaded file will be passed in as the key value, but if the file's token count exceeds this limit, the string will be truncated and incomplete.

:::tip NOTE
The variables `MAX_CONTENT_LENGTH` in `/docker/.env` and `client_max_body_size` in `/docker/nginx/nginx.conf` set the file size limit for each upload to a knowledge base or **File Management**. These settings DO NOT apply in this scenario.
:::
</code></pre></div></div>
<p>guides\agent\agent_component_reference\categorize.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 5
slug: /categorize_component
---

# Categorize component

A component that classifies user inputs and applies strategies accordingly. 

---

A **Categorize** component is usually the downstream of the **Interact** component.

## Scenarios

A **Categorize** component is essential when you need the LLM to help you identify user intentions and apply appropriate processing strategies.

## Configurations

### Input

The **Categorize** component relies on input variables to specify its data inputs (queries). Click **+ Add variable** in the **Input** section to add the desired input variables. There are two types of input variables: **Reference** and **Text**.

- **Reference**: Uses a component's output or a user input as the data source. You are required to select from the dropdown menu:
  - A component ID under **Component Output**, or 
  - A global variable under **Begin input**, which is defined in the **Begin** component.
- **Text**: Uses fixed text as the query. You are required to enter static text.

### Model

Click the dropdown menu of **Model** to show the model configuration window.

- **Model**: The chat model to use.  
  - Ensure you set the chat model correctly on the **Model providers** page.
  - You can use different models for different components to increase flexibility or improve overall performance.
- **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.  
  This parameter has three options:  
  - **Improvise**: Produces more creative responses.
  - **Precise**: (Default) Produces more conservative responses.
  - **Balance**: A middle ground between **Improvise** and **Precise**.
- **Temperature**: The randomness level of the model's output.  
  Defaults to 0.1.  
  - Lower values lead to more deterministic and predictable outputs.
  - Higher values lead to more creative and varied outputs.
  - A temperature of zero results in the same output for the same prompt.
- **Top P**: Nucleus sampling.  
  - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
  - Defaults to 0.3.
- **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
  - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
  - Defaults to 0.4.
- **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
  - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
  - Defaults to 0.7.

:::tip NOTE
- It is not necessary to stick with the same model for all components. If a specific model is not performing well for a particular task, consider using a different one.
- If you are uncertain about the mechanism behind **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**, simply choose one of the three options of **Preset configurations**.
:::

### Message window size

An integer specifying the number of previous dialogue rounds to input into the LLM. For example, if it is set to 12, the tokens from the last 12 dialogue rounds will be fed to the LLM. This feature consumes additional tokens.

Defaults to 1.

:::tip IMPORTANT
This feature is used for multi-turn dialogue *only*. If your **Categorize** component is not part of a multi-turn dialogue (i.e., it is not in a loop), leave this field as-is.
:::

### Category name

A **Categorize** component must have at least two categories. This field sets the name of the category. Click **+ Add Item** to include the intended categories. 

:::tip NOTE
You will notice that the category name is auto-populated. No worries. Each category is assigned a random name upon creation. Feel free to change it to a name that is understandable to the LLM.
:::

#### Description

Description of this category.  

You can input criteria, situation, or information that may help the LLM determine which inputs belong in this category.

#### Examples

Additional examples that may help the LLM determine which inputs belong in this category.

:::danger IMPORTANT
Examples are more helpful than the description if you want the LLM to classify particular cases into this category.
:::

#### Next step

Specifies the downstream component of this category.

- Once you specify the ID of the downstream component, a link is established between this category and the corresponding component.
- If you manually link this category to a downstream component on the canvas, the ID of that component is auto-populated.

## Examples

You can explore our customer service agent template, where a **Categorize** component (component ID: **Question Categorize**) has four defined categories and takes data inputs from an **Interact** component (component ID: **Interface**):

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Interpreter** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.


</code></pre></div></div>
<p>guides\agent\agent_component_reference\concentrator.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 10
slug: /concentrator_component
---

# Concentrator component

A component that directs execution flow to multiple downstream components.

---

The **Concentrator** component acts as a "repeater" of execution flow, transmitting a flow to multiple downstream components.


## Scenarios

A **Concentrator** component enhances the current UX design. For a component originally designed to support only one downstream component, you can append a **Concentrator**, enabling it to have multiple downstream components.

## Examples

Explore our general-purpose chatbot agent template, featuring a **Concentrator** component (component ID: **medical**) that relays an execution flow from category 2 of the **Categorize** component to two translator components:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **General-purpose chatbot** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\generate.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /generate_component
---

# Generate component

The component that prompts the LLM to respond appropriately.

---

A **Generate** component fine-tunes the LLM and sets its prompt.

## Scenarios

A **Generate** component is essential when you need the LLM to assist with summarizing, translating, or controlling various tasks. 

## Configurations

### Model

Click the dropdown menu of **Model** to show the model configuration window.

- **Model**: The chat model to use.  
  - Ensure you set the chat model correctly on the **Model providers** page.
  - You can use different models for different components to increase flexibility or improve overall performance.
- **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.   
  This parameter has three options:
  - **Improvise**: Produces more creative responses.
  - **Precise**: (Default) Produces more conservative responses.
  - **Balance**: A middle ground between **Improvise** and **Precise**.
- **Temperature**: The randomness level of the model's output.  
  Defaults to 0.1.
  - Lower values lead to more deterministic and predictable outputs.
  - Higher values lead to more creative and varied outputs.
  - A temperature of zero results in the same output for the same prompt.
- **Top P**: Nucleus sampling.  
  - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
  - Defaults to 0.3.
- **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
  - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
  - Defaults to 0.4.
- **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
  - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
  - Defaults to 0.7.

:::tip NOTE
- It is not necessary to stick with the same model for all components. If a specific model is not performing well for a particular task, consider using a different one.
- If you are uncertain about the mechanism behind **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**, simply choose one of the three options of **Preset configurations**.
:::

### System prompt

Typically, you use the system prompt to describe the task for the LLM, specify how it should respond, and outline other miscellaneous requirements. We do not plan to elaborate on this topic, as it can be as extensive as prompt engineering. However, please be aware that the system prompt is often used in conjunction with keys (variables), which serve as various data inputs for the LLM. 

:::danger IMPORTANT
A **Generate** component relies on keys (variables) to specify its data inputs. Its immediate upstream component is *not* necessarily its data input, and the arrows in the workflow indicate *only* the processing sequence. Keys in a **Generate** component are used in conjunction with the system prompt to specify data inputs for the LLM. Use a forward slash `/` or the **(x)** button to show the keys to use.
:::

Below is a prompt excerpt of a **Generate** component from the **Interpreter** template (component ID: **Reflect**):

```text
Your task is to read a source text and a translation to {target_lang}, and give constructive suggestions to improve the translation. The source text and initial translation, delimited by XML tags &lt;SOURCE_TEXT&gt;&lt;/SOURCE_TEXT&gt; and &lt;TRANSLATION&gt;&lt;/TRANSLATION&gt;, are as follows:

&lt;SOURCE_TEXT&gt;
{source_text}
&lt;/SOURCE_TEXT&gt;

&lt;TRANSLATION&gt;
{translation_1}
&lt;/TRANSLATION&gt;

When writing suggestions, pay attention to whether there are ways to improve the translation's fluency, by applying {target_lang} grammar, spelling and punctuation rules, and ensuring there are no unnecessary repetitions.
- Each suggestion should address one specific part of the translation.
- Output the suggestions only.
```

Where `{source_text}` and `{target_lang}` are global variables defined by the **Begin** component, while `{translation_1}` is the output of another **Generate** component with the component ID **Translate directly**.

### Cite 

This toggle sets whether to cite the original text as reference. 


:::tip NOTE
This feature applies *only* after the original documents have been uploaded to the corresponding knowledge base(s) and file parsing is complete.
:::

### Message window size

An integer specifying the number of previous dialogue rounds to input into the LLM. For example, if it is set to 12, the tokens from the last 12 dialogue rounds will be fed to the LLM. This feature consumes additional tokens.

:::tip IMPORTANT
This feature is used for multi-turn dialogue *only*.
:::


## Examples

You can explore our three-step interpreter agent template, where a **Generate** component (component ID: **Reflect**) takes three global variables:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Interpreter** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on component **Reflect**, to display its **Configuration** window, where:
   - `{target_lang}` and `{source_text}` are defined in the **Begin** component and require user input.
   - `{translation_1}` is the output from the upstream component **Translate directly**.

</code></pre></div></div>
<p>guides\agent\agent_component_reference\interact.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /interact_component
---

# Interact component

A component that accepts user inputs and displays responses.

---

An **Interact** component serves as the interface between human and bot, receiving user inputs and displaying the agent's responses.


## Scenarios

An **Interact** component is essential where you need to display the agent's responses or require user-computer interaction.

## Examples

You can explore our three-step interpreter agent template, where the **Interact** component is used to display the final translation, or our customer service agent template, where the **Interact** component is the immediate downstream of **Begin** and is used to display multi-turn dialogue between the user and the agent.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\iteration.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 12
slug: /iteration_component
---

# Iteration component

A component that splits text input into text segments and iterates a predefined workflow for each one.

---

An **Interaction** component can divide text input into text segments and apply its built-in component workflow to each segment. 


## Scenario

An **Iteration** component is essential when a workflow loop is required and the loop count is *not* fixed but depends on number of segments created from the output of specific agent components. 

- If, for instance, you plan to feed several paragraphs into an LLM for content generation, each with its own focus, and feeding them to the LLM all at once could create confusion or contradictions, then you can use an **Iteration** component, which encapsulates a **Generate** component, to repeat the content generation process for each paragraph.
- Another example: If you wish to use the LLM to translate a lengthy paper into a target language without exceeding its token limit, consider using an **Iteration** component, which encapsulates a **Generate** component, to break the paper into smaller pieces and repeat the translation process for each one.

## Internal components

### IterationItem

Each **Iteration** component includes an internal **IterationItem** component. The **IterationItem** component serves as both the starting point and input node of the workflow within the **Iteration** component. It manages the loop of the workflow for all text segments created from the input.

:::tip NOTE
The **IterationItem** component is visible *only* to the components encapsulated by the current **Iteration** components.
:::

![Iterationitem](https://github.com/user-attachments/assets/97117ceb-76c4-432e-aa86-48f253bcb886)

### Build an internal workflow 

You are allowed to pull other components into the **Iteration** component to build an internal workflow, and these "added internal components" are no longer visible to components outside of the current **Iteration** component.

:::danger IMPORTANT
To reference the created text segments from an added internal component, simply add a **Reference** variable that equals **IterationItem** within the **Input** section of that internal component. There is no need to reference the corresponding external component, as the **IterationItem** component manages the loop of the workflow for all created text segments. 
:::

:::tip NOTE
An added internal component can reference an external component when necessary.
:::

## Configurations

### Input

The **Iteration** component uses input variables to specify its data inputs, namely the texts to be segmented. You are allowed to specify multiple input sources for the **Iteration** component. Click **+ Add variable** in the **Input** section to include the desired input variables. There are two types of input variables: **Reference** and **Text**.

- **Reference**: Uses a component's output or a user input as the data source. You are required to select from the dropdown menu:
  - A component ID under **Component Output**, or 
  - A global variable under **Begin input**, which is defined in the **Begin** component.
- **Text**: Uses fixed text as the query. You are required to enter static text.

### Delimiter

The delimiter to use to split the text input into segments:

- Comma (Default)
- Line break
- Tab
- Underline
- Forward slash
- Dash
- Semicolon

## Examples

Explore our research report generator agent template, where the **Iteration** component (component ID: **Sections**) takes subtitles from the **Subtitles** component and generates sections for them:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Iteration** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\keyword.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 6
slug: /keyword_component
---

# Keyword component

A component that extracts keywords from a user query.

---

A **Keyword** component uses the specified LLM to extract keywords from a user query.

## Scenarios

A **Keyword** component is essential where you need to prepare keywords for a potential keyword search.

## Configurations

### Input

The **Keyword** component relies on input variables to specify its data inputs (queries). Click **+ Add variable** in the **Input** section to add the desired input variables. There are two types of input variables: **Reference** and **Text**.

- **Reference**: Uses a component's output or a user input as the data source. You are required to select from the dropdown menu:
  - A component ID under **Component Output**, or 
  - A global variable under **Begin input**, which is defined in the **Begin** component.
- **Text**: Uses fixed text as the query. You are required to enter static text.


### Model

Click the dropdown menu of **Model** to show the model configuration window.

- **Model**: The chat model to use.  
  - Ensure you set the chat model correctly on the **Model providers** page.
  - You can use different models for different components to increase flexibility or improve overall performance.
- **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.   
  This parameter has three options:
  - **Improvise**: Produces more creative responses.
  - **Precise**: (Default) Produces more conservative responses.
  - **Balance**: A middle ground between **Improvise** and **Precise**.
- **Temperature**: The randomness level of the model's output.  
  Defaults to 0.1.
  - Lower values lead to more deterministic and predictable outputs.
  - Higher values lead to more creative and varied outputs.
  - A temperature of zero results in the same output for the same prompt.
- **Top P**: Nucleus sampling.  
  - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
  - Defaults to 0.3.
- **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
  - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
  - Defaults to 0.4.
- **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
  - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
  - Defaults to 0.7.

:::tip NOTE
- It is not necessary to stick with the same model for all components. If a specific model is not performing well for a particular task, consider using a different one.
- If you are uncertain about the mechanism behind **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**, simply choose one of the three options of **Preset**.
:::


### Number of keywords

An integer specifying the number of keywords to extract from the user query. Defaults to 3. Please note that the number of extracted keywords depends on the LLM's capabilities and the token count in the user query, and may *not* match the integer you set.


## Examples

Explore our general-purpose chatbot agent template, where the **Keyword** component (component ID: **keywords**) is used to extract keywords from financial inputs for a potential stock search in the **akshare** component:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **General-purpose chatbot** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Keyword** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\message.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 7
slug: /message_component
---

# Message component

A component that sends out a static message.

---

A **Message** component sends out a static message. If multiple messages are supplied, it randomly selects one to send.

## Configurations

### Messages

The message to send out. 

Click **+ Add message** to add message options. When multiple messages are supplied, the **Message** component randomly selects one to send.

## Examples

Explore our customer service agent template, where the **Message** component (component ID: **What else?**) randomly sends out a message to the user interface if the user inputs is related to personal contact information:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Message** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\note.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 13
slug: /note_component
---

# Note component

The component that keeps design notes.

---

A **note** component allows you to keep design notes, including details about an agent, the output of specific components, the rationale of a particular design, or any information that may assist you, your users, or your fellow developers understand the agent.

## Examples

Explore our customer service agent template, which has five **Note** components:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **note** component to add or update notes.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\retrieval.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 4
slug: /retrieval_component
---

# Retrieval component

A component that retrieves information from specified datasets.

## Scenarios

A **Retrieval** component is essential in most RAG scenarios, where information is extracted from designated knowledge bases before being sent to the LLM for content generation.

## Configurations

Click on a **Retrieval** component to open its configuration window.

### Input

The **Retrieval** component relies on input variables to specify its data inputs (queries). Click **+ Add variable** in the **Input** section to add the desired input variables. There are two types of input variables: **Reference** and **Text**.

- **Reference**: Uses a component's output or a user input as the data source. You are required to select from the dropdown menu:
  - A component ID under **Component Output**, or 
  - A global variable under **Begin input**, which is defined in the **Begin** component.
- **Text**: Uses fixed text as the query. You are required to enter static text.

### Similarity threshold

RAGFlow employs a combination of weighted keyword similarity and weighted vector cosine similarity during retrieval. This parameter sets the threshold for similarities between the user query and chunks stored in the datasets. Any chunk with a similarity score below this threshold will be excluded from the results.

Defaults to 0.2.

### Keyword similarity weight

This parameter sets the weight of keyword similarity in the combined similarity score. The total of the two weights must equal 1.0. Its default value is 0.7, which means the weight of vector similarity in the combined search is 1 - 0.7 = 0.3.

### Top N

This parameter selects the "Top N" chunks from retrieved ones and feed them to the LLM.

Defaults to 8.


### Rerank model

*Optional*

If a rerank model is selected, a combination of weighted keyword similarity and weighted reranking score will be used for retrieval.

:::caution WARNING
Using a rerank model will *significantly* increase the system's response time.
:::

### Tavily API key

*Optional*

Enter your Tavily API key here to enable Tavily web search during retrieval. See [here](https://app.tavily.com/home) for instructions on getting a Tavily API key.

### Use knowledge graph

Whether to use knowledge graph(s) in the specified knowledge base(s) during retrieval for multi-hop question answering. When enabled, this would involve iterative searches across entity, relationship, and community report chunks, greatly increasing retrieval time.

### Knowledge bases 

*Optional*

Select the knowledge base(s) to retrieve data from.

- If no knowledge base is selected, meaning conversations with the agent will not be based on any knowledge base, ensure that the **Empty response** field is left blank to avoid an error.
- If you select multiple knowledge bases, you must ensure that the knowledge bases (datasets) you select use the same embedding model; otherwise, an error message would occur.

### Empty response

- Set this as a response if no results are retrieved from the knowledge base(s) for your query, or 
- Leave this field blank to allow the chat model to improvise when nothing is found.

:::caution WARNING
If you do not specify a knowledge base, you must leave this field blank; otherwise, an error would occur.
:::

## Examples

Explore our customer service agent template, where the **Retrieval** component (component ID: **Search product info**) is used to search the dataset and send the Top N results to the LLM:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Retrieval** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\rewrite.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 8
slug: /rewrite_component
---

# Rewrite component

A component that rewrites a user query.

---

A **Rewrite** component uses a specified LLM to rewrite a user query from the **Interact** component, based on the context of previous dialogues.

## Scenarios

A **Rewrite** component is essential when you need to optimize a user query based on the context of previous conversations. It is usually the upstream component of a **Retrieval** component.

:::tip NOTE
See also the [Keyword](./keyword.mdx) component, a similar component used for multi-turn optimization.
:::

## Configurations

:::tip NOTE
The **Rewrite** component uses the user-agent interaction from the **Interact** component as its data input. Therefore, there is no need to specify its data inputs in the Configurations.
:::

### Model

Click the dropdown menu of **Model** to show the model configuration window.

- **Model**: The chat model to use.  
  - Ensure you set the chat model correctly on the **Model providers** page.
  - You can use different models for different components to increase flexibility or improve overall performance.
- **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.   
  This parameter has three options:
  - **Improvise**: Produces more creative responses.
  - **Precise**: (Default) Produces more conservative responses.
  - **Balance**: A middle ground between **Improvise** and **Precise**.
- **Temperature**: The randomness level of the model's output.  
  Defaults to 0.1.
  - Lower values lead to more deterministic and predictable outputs.
  - Higher values lead to more creative and varied outputs.
  - A temperature of zero results in the same output for the same prompt.
- **Top P**: Nucleus sampling.  
  - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
  - Defaults to 0.3.
- **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
  - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
  - Defaults to 0.4.
- **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
  - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
  - Defaults to 0.7.

:::tip NOTE
- It is not necessary to stick with the same model for all components. If a specific model is not performing well for a particular task, consider using a different one.
- If you are uncertain about the mechanism behind **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**, simply choose one of the three options of **Preset configurations**.
:::


### Message window size

An integer specifying the number of previous dialogue rounds to input into the LLM. For example, if it is set to 12, the tokens from the last 12 dialogue rounds will be fed to the LLM. This feature consumes additional tokens.

Defaults to 1.

:::tip IMPORTANT
This feature is used for multi-turn dialogue *only*. If your **Categorize** component is not part of a multi-turn dialogue (i.e., it is not in a loop), leave this field as-is.
:::

## Examples

Explore our customer service agent template, where the **Rewrite** component (component ID: **Refine Question**) is used to optimize a product-specific user query based on context of previous dialogues before passing it on to the **Retrieval** component.

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Customer service** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Rewrite** component to display its **Configuration** window.
</code></pre></div></div>
<p>guides\agent\agent_component_reference\switch.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 9
slug: /switch_component
---

# Switch component

A component that evaluates whether specified conditions are met and directs the follow of execution accordingly. 

---

A **Switch** component evaluates conditions based on the output of specific components, directing the flow of execution accordingly to enable complex branching logic.

## Scenarios

A **Switch** component is essential for condition-based direction of execution flow. While it shares similarities with the [Categorize](./categorize.mdx) component, which is also used in multi-pronged strategies, the key distinction lies in their approach: the evaluation of the **Switch** component is rule-based, whereas the **Categorize** component involves AI and uses an LLM for decision-making. 

## Configurations

### Case n

A **Switch** component must have at least one case, each with multiple specified conditions and *only one* downstream component. When multiple conditions are specified for a case, you must set the logical relationship between them to either AND or OR.

#### Next step

Specifies the downstream component of this case.

- *Once you specify the ID of the downstream component, a link is established between this case and the corresponding component.*
- *If you manually link this case to a downstream component on the canvas, the ID of that component is auto-populated.*

#### Condition

Evaluates whether the output of specific components meets certain conditions, with **Component ID**, **Operator**, and **Value** together forming a conditional expression.

:::danger IMPORTANT
When you have added multiple conditions for a specific case, a **Logical operator** field appears, requiring you to set the logical relationship between these conditions as either AND or OR.
![Image](https://github.com/user-attachments/assets/102f006e-9906-49c2-af43-de6af03d5074)
:::

- **Component ID**: The ID of the corresponding component.
- **Operator**: The operator required to form a conditional expression.
  - Equals
  - Not equal
  - Greater than
  - Greater equal
  - Less than
  - Less equal
  - Contains 
  - Not contains 
  - Starts with
  - Ends with
  - Is empty
  - Not empty
- **Value**: A single value, which can be an integer, float, or string.  
  - Delimiters, multiple values, or expressions are *not* supported.
  - Strings need not be wrapped in `""` or `''`.

### ELSE 

**Required**. Specifies the downstream component if none of the conditions defined above are met.

*Once you specify the ID of the downstream component, a link is established between ELSE and the corresponding component.*

</code></pre></div></div>
<p>guides\agent\agent_component_reference\template.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 11
slug: /template_component
---

# Template component

A component that formats user inputs or the outputs of other components.

---

A **Template** component acts as a content formatter. It is usually the upstream component of an **Interact** component.


## Scenarios

A **Template** component is useful for organizing various sources of data or information into specific formats.

## Configurations

### Content 

Used together with Keys to organize various data or information sources into desired formats. Example:

```text
&lt;h2&gt;{subtitle}&lt;/h2&gt;
&lt;div&gt;{content}&lt;/div&gt;
```

Where `{subtitle}` and `{content}` are defined keys.

### Key 

A **Template** component relies on keys (variables) to specify its data or information sources. Its immediate upstream component is *not* necessarily its input, and the arrows in the workflow indicate *only* the processing sequence.

Values of keys are categorized into two groups:

- **Component Output**: The value of the key should be a component ID.
- **Begin Input**: The value of the key should be the name of a global variable defined in the **Begin** component.

## Examples

Explore our research report generator agent template, where the **Template** component (component ID: **Article**) organizes user input and the outputs of the **Sections** component into HTML format:

1. Click the **Agent** tab at the top center of the page to access the **Agent** page.
2. Click **+ Create agent** on the top right of the page to open the **agent template** page.
3. On the **agent template** page, hover over the **Research report generator** card and click **Use this template**.
4. Name your new agent and click **OK** to enter the workflow editor.
5. Click on the **Template** component to display its **Configuration** window

</code></pre></div></div>
<p>guides\chat_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Chat"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Chat-specific guides."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\chat\implement_deep_research.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /implement_deep_research
---

# Implement deep research

Implements deep research for agentic reasoning.

---

From v0.17.0 onward, RAGFlow supports integrating agentic reasoning in an AI chat. The following diagram illustrates the workflow of RAGFlow's deep research:

![Image](https://github.com/user-attachments/assets/f65d4759-4f09-4d9d-9549-c0e1fe907525)

To activate this feature:

1. Enable the **Reasoning** toggle under the **Prompt engine** tab of your chat assistant dialogue.

![Image](https://github.com/user-attachments/assets/4a1968d0-0128-4371-879f-77f3a70197f5)

2. Enter the correct Tavily API key under the **Assistant settings** tab of your chat assistant dialogue to leverage Tavily-based web search

![Image](https://github.com/user-attachments/assets/e8787532-7e72-49ef-8951-169ae544512f)

*The following is a screenshot of a conversation that integrates Deep Research:*

![Image](https://github.com/user-attachments/assets/165b88ff-1f5d-4fb8-90e2-c836b25e32e9)
</code></pre></div></div>
<p>guides\chat\set_chat_variables.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 4
slug: /set_chat_variables
---

# Set variables

Set variables to be used together with the system prompt for your LLM.

---

When configuring the system prompt for a chat model, variables play an important role in enhancing flexibility and reusability. With variables, you can dynamically adjust the system prompt to be sent to your model. In the context of RAGFlow, if you have defined variables in the **Chat Configuration** dialogue, except for the system's reserved variable `{knowledge}`, you are required to pass in values for them from RAGFlow's [HTTP API](../../references/http_api_reference.md#converse-with-chat-assistant) or through its [Python SDK](../../references/python_api_reference.md#converse-with-chat-assistant).

:::danger IMPORTANT
In RAGFlow, variables are closely linked with the system prompt. When you add a variable in the **Variable** section, include it in the system prompt. Conversely, when deleting a variable, ensure it is removed from the system prompt; otherwise, an error would occur.
:::

## Where to set variables

Hover your mouse over your chat assistant, click **Edit** to open its **Chat Configuration** dialogue, then click the **Prompt engine** tab. Here, you can work on your variables in the **System prompt** field and the **Variable** section:

![set_variables](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/prompt_engine.jpg)

## 1. Manage variables

In the **Variable** section, you add, remove, or update variables.

### `{knowledge}` - a reserved variable

`{knowledge}` is the system's reserved variable, representing the chunks retrieved from the knowledge base(s) specified by **Knowledge bases** under the **Assistant settings** tab. If your chat assistant is associated with certain knowledge bases, you can keep it as is.

:::info NOTE
It does not currently make a difference whether you set `{knowledge}` to optional or mandatory, but note that this design will be updated at a later point.
:::

From v0.17.0 onward, you can start an AI chat without specifying knowledge bases. In this case, we recommend removing the `{knowledge}` variable to prevent unnecessary reference and keeping the **Empty response** field empty to avoid errors.

### Custom variables

Besides `{knowledge}`, you can also define your own variables to pair with the system prompt. To use these custom variables, you must pass in their values through RAGFlow's official APIs. The **Optional** toggle determines whether these variables are required in the corresponding APIs:

- **Disabled** (Default): The variable is mandatory and must be provided.
- **Enabled**: The variable is optional and can be omitted if not needed.



## 2. Update system prompt

After you add or remove variables in the **Variable** section, ensure your changes are reflected in the system prompt to avoid inconsistencies or errors. Here's an example:

```
You are an intelligent assistant. Please answer the question by summarizing chunks from the specified knowledge base(s)...

Your answers should follow a professional and {style} style.

...

Here is the knowledge base:
{knowledge}
The above is the knowledge base.
```

:::tip NOTE
If you have removed `{knowledge}`, ensure that you thoroughly review and update the entire system prompt to achieve optimal results.
:::

## APIs

The *only* way to pass in values for the custom variables defined in the **Chat Configuration** dialogue is to call RAGFlow's [HTTP API](../../references/http_api_reference.md#converse-with-chat-assistant) or through its [Python SDK](../../references/python_api_reference.md#converse-with-chat-assistant).

### HTTP API

See [Converse with chat assistant](../../references/http_api_reference.md#converse-with-chat-assistant). Here's an example:

```json {9}
curl --request POST \
     --url http://{address}/api/v1/chats/{chat_id}/completions \
     --header 'Content-Type: application/json' \
     --header 'Authorization: Bearer &lt;YOUR_API_KEY&gt;' \
     --data-binary '
     {
          "question": "xxxxxxxxx",
          "stream": true,
          "style":"hilarious"
     }'
```

### Python API

See [Converse with chat assistant](../../references/python_api_reference.md#converse-with-chat-assistant). Here's an example:

```python {18}
from ragflow_sdk import RAGFlow

rag_object = RAGFlow(api_key="&lt;YOUR_API_KEY&gt;", base_url="http://&lt;YOUR_BASE_URL&gt;:9380")
assistant = rag_object.list_chats(name="Miss R")
assistant = assistant[0]
session = assistant.create_session()    

print("\n==================== Miss R =====================\n")
print("Hello. What can I do for you?")

while True:
    question = input("\n==================== User =====================\n&gt; ")
    style = input("Please enter your preferred style (e.g., formal, informal, hilarious): ")
    
    print("\n==================== Miss R =====================\n")
    
    cont = ""
    for ans in session.ask(question, stream=True, style=style):
        print(ans.content[len(cont):], end='', flush=True)
        cont = ans.content
```


</code></pre></div></div>
<p>guides\chat\start_chat.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /start_chat
---

# Start AI chat

Initiate an AI-powered chat with a configured chat assistant.

---

Knowledge base, hallucination-free chat, and file management are the three pillars of RAGFlow. Chats in RAGFlow are based on a particular knowledge base or multiple knowledge bases. Once you have created your knowledge base, finished file parsing, and [run a retrieval test](../dataset/run_retrieval_test.md), you can go ahead and start an AI conversation.

## Start an AI chat

You start an AI conversation by creating an assistant.

1. Click the **Chat** tab in the middle top of the page **&gt;** **Create an assistant** to show the **Chat Configuration** dialogue *of your next dialogue*.

   &gt; RAGFlow offers you the flexibility of choosing a different chat model for each dialogue, while allowing you to set the default models in **System Model Settings**.

2. Update **Assistant settings**:

   - **Assistant name** is the name of your chat assistant. Each assistant corresponds to a dialogue with a unique combination of knowledge bases, prompts, hybrid search configurations, and large model settings.
   - **Empty response**:
     - If you wish to *confine* RAGFlow's answers to your knowledge bases, leave a response here. Then, when it doesn't retrieve an answer, it *uniformly* responds with what you set here.
     - If you wish RAGFlow to *improvise* when it doesn't retrieve an answer from your knowledge bases, leave it blank, which may give rise to hallucinations.
   - **Show quote**: This is a key feature of RAGFlow and enabled by default. RAGFlow does not work like a black box. Instead, it clearly shows the sources of information that its responses are based on.
   - Select the corresponding knowledge bases. You can select one or multiple knowledge bases, but ensure that they use the same embedding model, otherwise an error would occur.

3. Update **Prompt engine**:

   - In **System**, you fill in the prompts for your LLM, you can also leave the default prompt as-is for the beginning.
   - **Similarity threshold** sets the similarity "bar" for each chunk of text. The default is 0.2. Text chunks with lower similarity scores are filtered out of the final response.
   - **Keyword similarity weight** is set to 0.7 by default. RAGFlow uses a hybrid score system to evaluate the relevance of different text chunks. This value sets the weight assigned to the keyword similarity component in the hybrid score.
     - If **Rerank model** is left empty, the hybrid score system uses keyword similarity and vector similarity, and the default weight assigned to the vector similarity component is 1-0.7=0.3.
     - If **Rerank model** is selected, the hybrid score system uses keyword similarity and reranker score, and the default weight assigned to the reranker score is 1-0.7=0.3.
   - **Top N** determines the *maximum* number of chunks to feed to the LLM. In other words, even if more chunks are retrieved, only the top N chunks are provided as input.
   - **Multi-turn optimization** enhances user queries using existing context in a multi-round conversation. It is enabled by default. When enabled, it will consume additional LLM tokens and significantly increase the time to generate answers.
   - **Use knowledge graph** indicates whether to use knowledge graph(s) in the specified knowledge base(s) during retrieval for multi-hop question answering. When enabled, this would involve iterative searches across entity, relationship, and community report chunks, greatly increasing retrieval time.
   - **Reasoning** indicates whether to generate answers through reasoning processes like Deepseek-R1/OpenAI o1. Once enabled, the chat model autonomously integrates Deep Research during question answering when encountering an unknown topic. This involves the chat model dynamically searching external knowledge and generating final answers through reasoning.
   - **Rerank model** sets the reranker model to use. It is left empty by default.
     - If **Rerank model** is left empty, the hybrid score system uses keyword similarity and vector similarity, and the default weight assigned to the vector similarity component is 1-0.7=0.3.
     - If **Rerank model** is selected, the hybrid score system uses keyword similarity and reranker score, and the default weight assigned to the reranker score is 1-0.7=0.3.
   - **Variable** refers to the variables (keys) to be used in the system prompt. `{knowledge}` is a reserved variable. Click **Add** to add more variables for the system prompt.
      - If you are uncertain about the logic behind **Variable**, leave it *as-is*.
      - As of v0.18.0, if you add custom variables here, the only way you can pass in their values is to call:
         - HTTP method [Converse with chat assistant](../../references/http_api_reference.md#converse-with-chat-assistant), or
         - Python method [Converse with chat assistant](../../references/python_api_reference.md#converse-with-chat-assistant).

4. Update **Model Setting**:

   - In **Model**: you select the chat model. Though you have selected the default chat model in **System Model Settings**, RAGFlow allows you to choose an alternative chat model for your dialogue.
   - **Freedom**: A shortcut to **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty** settings, indicating the freedom level of the model. From **Improvise**, **Precise**, to **Balance**, each preset configuration corresponds to a unique combination of **Temperature**, **Top P**, **Presence penalty**, and **Frequency penalty**.   
   This parameter has three options:
      - **Improvise**: Produces more creative responses.
      - **Precise**: (Default) Produces more conservative responses.
      - **Balance**: A middle ground between **Improvise** and **Precise**.
   - **Temperature**: The randomness level of the model's output.  
   Defaults to 0.1.
      - Lower values lead to more deterministic and predictable outputs.
      - Higher values lead to more creative and varied outputs.
      - A temperature of zero results in the same output for the same prompt.
   - **Top P**: Nucleus sampling.  
      - Reduces the likelihood of generating repetitive or unnatural text by setting a threshold *P* and restricting the sampling to tokens with a cumulative probability exceeding *P*.
      - Defaults to 0.3.
   - **Presence penalty**: Encourages the model to include a more diverse range of tokens in the response.  
      - A higher **presence penalty** value results in the model being more likely to generate tokens not yet been included in the generated text.
      - Defaults to 0.4.
   - **Frequency penalty**: Discourages the model from repeating the same words or phrases too frequently in the generated text.  
      - A higher **frequency penalty** value results in the model being more conservative in its use of repeated tokens.
      - Defaults to 0.7.

5. Now, let's start the show:

   ![question1](https://github.com/user-attachments/assets/c4114a3d-74ff-40a3-9719-6b47c7b11ab1)

:::tip NOTE

1. Click the light bulb icon above the answer to view the expanded system prompt:

![](https://github.com/user-attachments/assets/515ab187-94e8-412a-82f2-aba52cd79e09)

   *The light bulb icon is available only for the current dialogue.*

2. Scroll down the expanded prompt to view the time consumed for each task:

![enlighten](https://github.com/user-attachments/assets/fedfa2ee-21a7-451b-be66-20125619923c)
:::

## Update settings of an existing chat assistant

Hover over an intended chat assistant **&gt;** **Edit** to show the chat configuration dialogue:

![edit_chat](https://github.com/user-attachments/assets/5c514cf0-a959-4cfe-abad-5e42a0e23974)

![chat_config](https://github.com/user-attachments/assets/1a4eaed2-5430-4585-8ab6-930549838c5b)

## Integrate chat capabilities into your application or webpage

RAGFlow offers HTTP and Python APIs for you to integrate RAGFlow's capabilities into your applications. Read the following documents for more information:

- [Acquire a RAGFlow API key](../../develop/acquire_ragflow_api_key.md)
- [HTTP API reference](../../references/http_api_reference.md)
- [Python API reference](../../references/python_api_reference.md)

You can use iframe to embed the created chat assistant into a third-party webpage:

1. Before proceeding, you must [acquire an API key](../models/llm_api_key_setup.md); otherwise, an error message would appear.
2. Hover over an intended chat assistant **&gt;** **Edit** to show the **iframe** window:

   ![chat-embed](https://github.com/user-attachments/assets/13ea3021-31c4-4a14-9b32-328cd3318fb5)

3. Copy the iframe and embed it into a specific location on your webpage.

</code></pre></div></div>
<p>guides\chat\best_practices_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Best practices"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Best practices on chat assistant configuration."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\chat\best_practices\accelerate_question_answering.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /accelerate_question_answering
---

# Accelerate answering
import APITable from '@site/src/components/APITable';

A checklist to speed up question answering.

---

Please note that some of your settings may consume a significant amount of time. If you often find that your question answering is time-consuming, here is a checklist to consider:

- In the **Prompt engine** tab of your **Chat Configuration** dialogue, disabling **Multi-turn optimization** will reduce the time required to get an answer from the LLM.
- In the **Prompt engine** tab of your **Chat Configuration** dialogue, leaving the **Rerank model** field empty will significantly decrease retrieval time.
- When using a rerank model, ensure you have a GPU for acceleration; otherwise, the reranking process will be *prohibitively* slow.

:::tip NOTE 
Please note that rerank models are essential in certain scenarios. There is always a trade-off between speed and performance; you must weigh the pros against cons for your specific case.
:::

- In the **Assistant settings** tab of your **Chat Configuration** dialogue, disabling **Keyword analysis** will reduce the time to receive an answer from the LLM.
- When chatting with your chat assistant, click the light bulb icon above the *current* dialogue and scroll down the popup window to view the time taken for each task:  
   ![enlighten](https://github.com/user-attachments/assets/fedfa2ee-21a7-451b-be66-20125619923c)  


```mdx-code-block
&lt;APITable&gt;
```

| Item name         | Description                                                                                   |
| ----------------- | --------------------------------------------------------------------------------------------- |
| Total             | Total time spent on this conversation round, including chunk retrieval and answer generation. |
| Check LLM         | Time to validate the specified LLM.                                                           |
| Create retriever  | Time to create a chunk retriever.                                                             |
| Bind embedding    | Time to initialize an embedding model instance.                                               |
| Bind LLM          | Time to initialize an LLM instance.                                                           |
| Tune question     | Time to optimize the user query using the context of the mult-turn conversation.              |
| Bind reranker     | Time to initialize an reranker model instance for chunk retrieval.                            |
| Generate keywords | Time to extract keywords from the user query.                                                 |
| Retrieval         | Time to retrieve the chunks.                                                                  |
| Generate answer   | Time to generate the answer.                                                                  |

```mdx-code-block
&lt;/APITable&gt;
```
</code></pre></div></div>
<p>guides\dataset_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Datasets"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides on configuring a knowledge base."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\dataset\configure_knowledge_base.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 0
slug: /configure_knowledge_base
---

# Configure knowledge base

Knowledge base, hallucination-free chat, and file management are the three pillars of RAGFlow. RAGFlow's AI chats are based on knowledge bases. Each of RAGFlow's knowledge bases serves as a knowledge source, *parsing* files uploaded from your local machine and file references generated in **File Management** into the real 'knowledge' for future AI chats. This guide demonstrates some basic usages of the knowledge base feature, covering the following topics:

- Create a knowledge base
- Configure a knowledge base
- Search for a knowledge base
- Delete a knowledge base

## Create knowledge base

With multiple knowledge bases, you can build more flexible, diversified question answering. To create your first knowledge base:

![create knowledge base](https://github.com/infiniflow/ragflow/assets/93570324/110541ed-6cea-4a03-a11c-414a0948ba80)

_Each time a knowledge base is created, a folder with the same name is generated in the **root/.knowledgebase** directory._

## Configure knowledge base

The following screenshot shows the configuration page of a knowledge base. A proper configuration of your knowledge base is crucial for future AI chats. For example, choosing the wrong embedding model or chunking method would cause unexpected semantic loss or mismatched answers in chats. 

![knowledge base configuration](https://github.com/infiniflow/ragflow/assets/93570324/384c671a-8b9c-468c-b1c9-1401128a9b65)

This section covers the following topics:

- Select chunking method
- Select embedding model
- Upload file
- Parse file
- Intervene with file parsing results
- Run retrieval testing

### Select chunking method

RAGFlow offers multiple chunking template to facilitate chunking files of different layouts and ensure semantic integrity. In **Chunking method**, you can choose the default template that suits the layouts and formats of your files. The following table shows the descriptions and the compatible file formats of each supported chunk template:

| **Template** | Description                                                           | File format                                                                                   |
|--------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|
| General      | Files are consecutively chunked based on a preset chunk token number. | DOCX, XLSX, XLS (Excel 97-2003), PPT, PDF, TXT, JPEG, JPG, PNG, TIF, GIF, CSV, JSON, EML, HTML |
| Q&amp;A          |                                                                       | XLSX, XLS (Excel 97-2003), CSV/TXT                                                             |
| Resume       | Enterprise edition only. You can also try it out on demo.ragflow.io.  | DOCX, PDF, TXT                                                                                |
| Manual       |                                                                       | PDF                                                                                           |
| Table        |                                                                       | XLSX, XLS (Excel 97-2003), CSV/TXT                                                             |
| Paper        |                                                                       | PDF                                                                                           |
| Book         |                                                                       | DOCX, PDF, TXT                                                                                |
| Laws         |                                                                       | DOCX, PDF, TXT                                                                                |
| Presentation |                                                                       | PDF, PPTX                                                                                     |
| Picture      |                                                                       | JPEG, JPG, PNG, TIF, GIF                                                                      |
| One          | Each document is chunked in its entirety (as one).                    | DOCX, XLSX, XLS (Excel 97-2003), PDF, TXT                                                      |
| Tag          | The knowledge base functions as a tag set for the others.             | XLSX, CSV/TXT                                                                                 |

You can also change a file's chunking method on the **Datasets** page.

![change chunking method](https://github.com/infiniflow/ragflow/assets/93570324/ac116353-2793-42b2-b181-65e7082bed42)

### Select embedding model

An embedding model converts chunks into embeddings. It cannot be changed once the knowledge base has chunks. To switch to a different embedding model, you must delete all existing chunks in the knowledge base. The obvious reason is that we *must* ensure that files in a specific knowledge base are converted to embeddings using the *same* embedding model (ensure that they are compared in the same embedding space).

The following embedding models can be deployed locally:

- BAAI/bge-large-zh-v1.5
- maidalun1020/bce-embedding-base_v1

### Upload file

- RAGFlow's **File Management** allows you to link a file to multiple knowledge bases, in which case each target knowledge base holds a reference to the file.
- In **Knowledge Base**, you are also given the option of uploading a single file or a folder of files (bulk upload) from your local machine to a knowledge base, in which case the knowledge base holds file copies. 

While uploading files directly to a knowledge base seems more convenient, we *highly* recommend uploading files to **File Management** and then linking them to the target knowledge bases. This way, you can avoid permanently deleting files uploaded to the knowledge base. 

### Parse file

File parsing is a crucial topic in knowledge base configuration. The meaning of file parsing in RAGFlow is twofold: chunking files based on file layout and building embedding and full-text (keyword) indexes on these chunks. After having selected the chunking method and embedding model, you can start parsing a file:

![parse file](https://github.com/infiniflow/ragflow/assets/93570324/5311f166-6426-447f-aa1f-bd488f1cfc7b)

- Click the play button next to **UNSTART** to start file parsing.
- Click the red-cross icon and then refresh, if your file parsing stalls for a long time. 
- As shown above, RAGFlow allows you to use a different chunking method for a particular file, offering flexibility beyond the default method. 
- As shown above, RAGFlow allows you to enable or disable individual files, offering finer control over knowledge base-based AI chats. 

### Intervene with file parsing results

RAGFlow features visibility and explainability, allowing you to view the chunking results and intervene where necessary. To do so: 

1. Click on the file that completes file parsing to view the chunking results: 

   _You are taken to the **Chunk** page:_

   ![chunks](https://github.com/infiniflow/ragflow/assets/93570324/0547fd0e-e71b-41f8-8e0e-31649c85fd3d)

2. Hover over each snapshot for a quick view of each chunk.

3. Double-click the chunked texts to add keywords or make *manual* changes where necessary:

   ![update chunk](https://github.com/infiniflow/ragflow/assets/93570324/1d84b408-4e9f-46fd-9413-8c1059bf9c76)

:::caution NOTE
You can add keywords to a file chunk to increase its ranking for queries containing those keywords. This action increases its keyword weight and can improve its position in search list.  
:::

4. In Retrieval testing, ask a quick question in **Test text** to double-check if your configurations work:

   _As you can tell from the following, RAGFlow responds with truthful citations._

   ![retrieval test](https://github.com/infiniflow/ragflow/assets/93570324/c03f06f6-f41f-4b20-a97e-ae405d3a950c)

### Run retrieval testing

RAGFlow uses multiple recall of both full-text search and vector search in its chats. Prior to setting up an AI chat, consider adjusting the following parameters to ensure that the intended information always turns up in answers:

- Similarity threshold: Chunks with similarities below the threshold will be filtered. By default, it is set to 0.2.
- Vector similarity weight: The percentage by which vector similarity contributes to the overall score. By default, it is set to 0.3.

See [Run retrieval test](./run_retrieval_test.md) for details.

![retrieval test](https://github.com/infiniflow/ragflow/assets/93570324/c03f06f6-f41f-4b20-a97e-ae405d3a950c)

## Search for knowledge base

As of RAGFlow v0.18.0, the search feature is still in a rudimentary form, supporting only knowledge base search by name.

![search knowledge base](https://github.com/infiniflow/ragflow/assets/93570324/836ae94c-2438-42be-879e-c7ad2a59693e)

## Delete knowledge base

You are allowed to delete a knowledge base. Hover your mouse over the three dot of the intended knowledge base card and the **Delete** option appears. Once you delete a knowledge base, the associated folder under **root/.knowledge** directory is AUTOMATICALLY REMOVED. The consequence is:

- The files uploaded directly to the knowledge base are gone;  
- The file references, which you created from within **File Management**, are gone, but the associated files still exist in **File Management**. 

![delete knowledge base](https://github.com/infiniflow/ragflow/assets/93570324/fec7a508-6cfe-4bca-af90-81d3fdb94098)

</code></pre></div></div>
<p>guides\dataset\construct_knowledge_graph.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 8
slug: /construct_knowledge_graph
---

# Construct knowledge graph

Generate a knowledge graph for your knowledge base.

---

To enhance multi-hop question-answering, RAGFlow adds a knowledge graph construction step between data extraction and indexing, as illustrated below. This step creates additional chunks from existing ones generated by your specified chunking method.

![Image](https://github.com/user-attachments/assets/1ec21d8e-f255-4d65-9918-69b72dfa142b)

From v0.16.0 onward, RAGFlow supports constructing a knowledge graph on a knowledge base, allowing you to construct a *unified* graph across multiple files within your knowledge base. When a newly uploaded file starts parsing, the generated graph will automatically update.

:::danger WARNING
Constructing a knowledge graph requires significant memory, computational resources, and tokens.
:::

## Scenarios

Knowledge graphs are especially useful for multi-hop question-answering involving *nested* logic. They outperform traditional extraction approaches when you are performing question answering on books or works with complex entities and relationships.

:::tip NOTE
RAPTOR (Recursive Abstractive Processing for Tree Organized Retrieval) can also be used for multi-hop question-answering tasks. See [Enable RAPTOR](./enable_raptor.md) for details. You may use either approach or both, but ensure you understand the memory, computational, and token costs involved.
:::

## Prerequisites

The system's default chat model is used to generate knowledge graph. Before proceeding, ensure that you have a chat model properly configured:

![Image](https://github.com/user-attachments/assets/6bc34279-68c3-4d99-8d20-b7bd1dafc1c1)

## Configurations

### Entity types (*Required*)

The types of the entities to extract from your knowledge base. The default types are: **organization**, **person**, **event**, and **category**. Add or remove types to suit your specific knowledge base.

### Method

The method to use to construct knowledge graph:

- **General**: Use prompts provided by [GraphRAG](https://github.com/microsoft/graphrag) to extract entities and relationships.
- **Light**: (Default) Use prompts provided by [LightRAG](https://github.com/HKUDS/LightRAG) to extract entities and relationships. This option consumes fewer tokens, less memory, and fewer computational resources.

### Entity resolution

Whether to enable entity resolution. You can think of this as an entity deduplication switch. When enabled, the LLM will combine similar entities - e.g., '2025' and 'the year of 2025', or 'IT' and 'Information Technology' - to construct a more effective graph.

- (Default) Disable entity resolution.
- Enable entity resolution. This option consumes more tokens.

### Community report generation

In a knowledge graph, a community is a cluster of entities linked by relationships. You can have the LLM generate an abstract for each community, known as a community report. See [here](https://www.microsoft.com/en-us/research/blog/graphrag-improving-global-search-via-dynamic-community-selection/) for more information. This indicates whether to generate community reports:

- Generate community reports. This option consumes more tokens.
- (Default) Do not generate community reports.

## Procedure

1. On the **Configuration** page of your knowledge base, switch on **Extract knowledge graph** or adjust its settings as needed, and click **Save** to confirm your changes.

   - *The default knowledge graph configurations for your knowledge base are now set and files uploaded from this point onward will automatically use these settings during parsing.*
   - *Files parsed before this update will retain their original knowledge graph settings.*

2. The knowledge graph of your knowledge base does *not* automatically update *until* a newly uploaded file is parsed.

   _A **Knowledge graph** entry appears under **Configuration** once a knowledge graph is created._

3. Click **Knowledge graph** to view the details of the generated graph.
4. To use the created knowledge graph, do either of the following:
   
   - In your **Chat Configuration** dialogue, click the **Assistant settings** tab to add the corresponding knowledge base(s) and click the **Prompt engine** tab to switch on the **Use knowledge graph** toggle.
   - If you are using an agent, click the **Retrieval** agent component to specify the knowledge base(s) and switch on the **Use knowledge graph** toggle.

## Frequently asked questions

### Can I have different knowledge graph settings for different files in my knowledge base?

Yes, you can. Just one graph is generated per knowledge base. The smaller graphs of your files will be *combined* into one big, unified graph at the end of the graph extraction process.

### Does the knowledge graph automatically update when I remove a related file?

Nope. The knowledge graph does *not* automatically update *until* a newly uploaded document is parsed.

### How to remove a generated knowledge graph?

To remove the generated knowledge graph, delete all related files in your knowledge base. Although the **Knowledge graph** entry will still be visible, the graph has actually been deleted.

### Where is the created knowledge graph stored?

All chunks of the created knowledge graph are stored in RAGFlow's document engine: either Elasticsearch or [Infinity](https://github.com/infiniflow/infinity).
</code></pre></div></div>
<p>guides\dataset\enable_excel2html.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 4
slug: /enable_excel2html
---

# Enable Excel2HTML

Convert complex Excel spreadsheets into HTML tables.

---

When using the General chunking method, you can enable the **Excel to HTML** toggle to convert spreadsheet files into HTML tables. If it is disabled, spreadsheet tables will be represented as key-value pairs. For complex tables that cannot be simply represented this way, you must enable this feature.

:::caution WARNING
The feature is disabled by default. If your knowledge base contains spreadsheets with complex tables and you do not enable this feature, RAGFlow will not throw an error but your tables are likely to be garbled.
:::

## Scenarios

Works with complex tables that cannot be represented as key-value pairs. Examples include spreadsheet tables with multiple columns, tables with merged cells, or multiple tables within one sheet. In such cases, consider converting these spreadsheet tables into HTML tables.

## Considerations

- The Excel2HTML feature applies only to spreadsheet files (XLSX or XLS (Excel 97-2003)).
- This feature is associated with the General chunking method. In other words, it is available *only when* you select the General chunking method.
- When this feature is enabled, spreadsheet tables with more than 12 rows will be split into chunks of 12 rows each.

## Procedure

1. On your knowledge base's **Configuration** page, select **General** as the chunking method.

   _The **Excel to HTML** toggle appears._

2. Enable **Excel to HTML** if your knowledge base contains complex spreadsheet tables that cannot be represented as key-value pairs.
3. Leave **Excel to HTML** disabled if your knowledge base has no spreadsheet tables or if its spreadsheet tables can be represented as key-value pairs.
4. If question-answering regarding complex tables is unsatisfactory, check if **Excel to HTML** is enabled.

## Frequently asked questions

### Should I enable this feature for PDFs with complex tables?

Nope. This feature applies to spreadsheet files only. Enabling **Excel to HTML** does not affect your PDFs.
</code></pre></div></div>
<p>guides\dataset\enable_raptor.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 7
slug: /enable_raptor
---

# Enable RAPTOR

A recursive abstractive method used in long-context knowledge retrieval and summarization, balancing broad semantic understanding with fine details.

---

RAPTOR (Recursive Abstractive Processing for Tree Organized Retrieval) is an enhanced document preprocessing technique introduced in a [2024 paper](https://arxiv.org/html/2401.18059v1). Designed to tackle multi-hop question-answering issues, RAPTOR performs recursive clustering and summarization of document chunks to build a hierarchical tree structure. This enables more context-aware retrieval across lengthy documents. RAGFlow v0.6.0 integrates RAPTOR for document clustering as part of its data preprocessing pipeline between data extraction and indexing, as illustrated below.

![document_clustering](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/document_clustering_as_preprocessing.jpg)

Our tests with this new approach demonstrate state-of-the-art (SOTA) results on question-answering tasks requiring complex, multi-step reasoning. By combining RAPTOR retrieval with our built-in chunking methods and/or other retrieval-augmented generation (RAG) approaches, you can further improve your question-answering accuracy.

:::danger WARNING
Enabling RAPTOR requires significant memory, computational resources, and tokens.
:::

## Basic principles

After the original documents are divided into chunks, the chunks are clustered by semantic similarity rather than by their original order in the text. Clusters are then summarized into higher-level chunks by your system's default chat model. This process is applied recursively, forming a tree structure with various levels of summarization from the bottom up. As illustrated in the figure below, the initial chunks form the leaf nodes (shown in blue) and are recursively summarized into a root node (shown in orange).

![raptor](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/clustering_and_summarizing.jpg)

The recursive clustering and summarization capture a broad understanding (by the root node) as well as fine details (by the leaf nodes) necessary for multi-hop question-answering.

## Scenarios

For multi-hop question-answering tasks involving complex, multi-step reasoning, a semantic gap often exists between the question and its answer. As a result, searching with the question often fails to retrieve the relevant chunks that contribute to the correct answer. RAPTOR addresses this challenge by providing the chat model with richer and more context-aware and relevant chunks to summarize, enabling a holistic understanding without losing granular details.

:::tip NOTE
Knowledge graphs can also be used for multi-hop question-answering tasks. See [Construct knowledge graph](./construct_knowledge_graph.md) for details. You may use either approach or both, but ensure you understand the memory, computational, and token costs involved.
:::

## Prerequisites

The system's default chat model is used to summarize clustered content. Before proceeding, ensure that you have a chat model properly configured:

![Image](https://github.com/user-attachments/assets/6bc34279-68c3-4d99-8d20-b7bd1dafc1c1)

## Configurations

The RAPTOR feature is disabled by default. To enable it, manually switch on the **Use RAPTOR to enhance retrieval** toggle on your knowledge base's **Configuration** page.

### Prompt

The following prompt will be applied recursively for cluster summarization, with `{cluster_content}` serving as an internal parameter. We recommend that you keep it as-is for now. The design will be updated at a later point.

```
Please summarize the following paragraphs... Paragraphs as following:
      {cluster_content}
The above is the content you need to summarize.
```

### Max token

The maximum number of tokens per generated summary chunk. Defaults to 256, with a maximum limit of 2048.

### Threshold

In RAPTOR, chunks are clustered by their semantic similarity. The **Threshold** parameter sets the minimum similarity required for chunks to be grouped together.

It defaults to 0.1, with a maximum limit of 1. A higher **Threshold** means fewer chunks in each cluster, while a lower one means more.

### Max cluster

The maximum number of clusters to create. Defaults to 64, with a maximum limit of 1024.

### Random seed

A random seed. Click **+** to change the seed value.

</code></pre></div></div>
<p>guides\dataset\run_retrieval_test.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 10
slug: /run_retrieval_test
---

# Run retrieval test

Conduct a retrieval test on your knowledge base to check whether the intended chunks can be retrieved.

---

After your files are uploaded and parsed, it is recommended that you run a retrieval test before proceeding with the chat assistant configuration. Running a retrieval test is *not* an unnecessary or superfluous step at all! Just like fine-tuning a precision instrument, RAGFlow requires careful tuning to deliver optimal question answering performance. Your knowledge base settings, chat assistant configurations, and the specified large and small models can all significantly impact the final results. Running a retrieval test verifies whether the intended chunks can be recovered, allowing you to quickly identify areas for improvement or pinpoint any issue that needs addressing. For instance, when debugging your question answering system, if you know that the correct chunks can be retrieved, you can focus your efforts elsewhere. For example, in issue [#5627](https://github.com/infiniflow/ragflow/issues/5627), the problem was found to be due to the LLM's limitations.

During a retrieval test, chunks created from your specified chunking method are retrieved using a hybrid search. This search combines weighted keyword similarity with either weighted vector cosine similarity or a weighted reranking score, depending on your settings:

- If no rerank model is selected, weighted keyword similarity will be combined with weighted vector cosine similarity.
- If a rerank model is selected, weighted keyword similarity will be combined with weighted vector reranking score.

In contrast, chunks created from [knowledge graph construction](./construct_knowledge_graph.md) are retrieved solely using vector cosine similarity.

## Prerequisites

- Your files are uploaded and successfully parsed before running a retrieval test.
- A knowledge graph must be successfully built before enabling **Use knowledge graph**.

## Configurations

### Similarity threshold

This sets the bar for retrieving chunks: chunks with similarities below the threshold will be filtered out. By default, the threshold is set to 0.2. This means that only chunks with hybrid similarity score of 20 or higher will be retrieved.

### Keyword similarity weight

This sets the weight of keyword similarity in the combined similarity score, whether used with vector cosine similarity or a reranking score. By default, it is set to 0.7, making the weight of the other component 0.3 (1 - 0.7).

### Rerank model

- If left empty, RAGFlow will use a combination of weighted keyword similarity and weighted vector cosine similarity.
- If a rerank model is selected, weighted keyword similarity will be combined with weighted vector reranking score.

:::danger IMPORTANT
Using a rerank model will significantly increase the time to receive a response.
:::

### Use knowledge graph

In a knowledge graph, an entity description, a relationship description, or a community report each exists as an independent chunk. This switch indicates whether to add these chunks to the retrieval.

The switch is disabled by default. When enabled, RAGFlow performs the following during a retrieval test:

1. Extract entities and entity types from your query using the LLM.
2. Retrieve top N entities from the graph based on their PageRank values, using the extracted entity types.
3. Find similar entities and their N-hop relationships from the graph using the embeddings of the extracted query entities.
4. Retrieve similar relationships from the graph using the query embedding.
5. Rank these retrieved entities and relationships by multiplying each one's PageRank value with its similarity score to the query, returning the top n as the final retrieval.
6. Retrieve the report for the community involving the most entities in the final retrieval.  
   *The retrieved entity descriptions, relationship descriptions, and the top 1 community report are sent to the LLM for content generation.*

:::danger IMPORTANT
Using a knowledge graph in a retrieval test will significantly increase the time to receive a response.
:::

### Test text

This field is where you put in your testing query.

## Procedure

1. Navigate to the **Retrieval testing** page of your knowledge base, enter your query in **Test text**, and click **Testing** to run the test.
2. If the results are unsatisfactory, tune the options listed in the Configuration section and rerun the test.

   *The following is a screenshot of a retrieval test conducted without using knowledge graph. It demonstrates a hybrid search combining weighted keyword similarity and weighted vector cosine similarity. The overall hybrid similarity score is 28.56, calculated as 25.17 (term similarity score) x 0.7 + 36.49 (vector similarity score) x 0.3:*  
   ![Image](https://github.com/user-attachments/assets/541554d4-3f3e-44e1-954b-0ae77d7372c6)

   *The following is a screenshot of a retrieval test conducted using a knowledge graph. It shows that only vector similarity is used for knowledge graph-generated chunks:*  
   ![Image](https://github.com/user-attachments/assets/30a03091-0f7b-4058-901a-f4dc5ca5aa6b)

:::caution WARNING
If you have adjusted the default settings, such as keyword similarity weight or similarity threshold, to achieve the optimal results, be aware that these changes will not be automatically saved. You must apply them to your chat assistant settings or the **Retrieval** agent component settings.
:::

## Frequently asked questions

### Is an LLM used when the Use Knowledge Graph switch is enabled?

Yes, your LLM will be involved to analyze your query and extract the related entities and relationship from the knowledge graph. This also explains why additional tokens and time will be consumed.
</code></pre></div></div>
<p>guides\dataset\set_metadata.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /set_metada
---

# Set metadata

Add metadata to an uploaded file

---

On the **Dataset** page of your knowledge base, you can add metadata to any uploaded file. This approach enables you to 'tag' additional information like URL, author, date, and more to an existing file or dataset. In an AI-powered chat, such information will be sent to the LLM with the retrieved chunks for content generation.

For example, if you have a dataset of HTML files and want the LLM to cite the source URL when responding to your query, add a `"url"` parameter to each file's metadata.

![Image](https://github.com/user-attachments/assets/78cb5035-e96c-43f9-82d7-8fef1b68c843)

:::tip NOTE
Ensure that your metadata is in JSON format; otherwise, your updates will not be applied.
:::

![Image](https://github.com/user-attachments/assets/379cf2c5-4e37-4b79-8aeb-53bf8e01d326)
</code></pre></div></div>
<p>guides\dataset\set_page_rank.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /set_page_rank
---

# Set page rank

Create a step-retrieval strategy using page rank.

---

## Scenario

In an AI-powered chat, you can configure a chat assistant or an agent to respond using knowledge retrieved from multiple specified knowledge bases (datasets), provided that they employ the same embedding model. In situations where you prefer information from certain knowledge base(s) to take precedence or to be retrieved first, you can use RAGFlow's page rank feature to increase the ranking of chunks from these knowledge bases. For example, if you have configured a chat assistant to draw from two knowledge bases, knowledge base A for 2024 news and knowledge base B for 2023 news, but wish to prioritize news from year 2024, this feature is particularly useful.

:::info NOTE
It is important to note that this 'page rank' feature operates at the level of the entire knowledge base rather than on individual files or documents.
:::

## Configuration

On the **Configuration** page of your knowledge base, drag the slider under **Page rank** to set the page rank value for your knowledge base. You are also allowed to input the intended page rank value in the field next to the slider.

:::info NOTE
The page rank value must be an integer. Range: [0,100]

- 0: Disabled (Default)
- A specific value: enabled
:::

:::tip NOTE
If you set the page rank value to a non-integer, say 1.7, it will be rounded down to the nearest integer, which in this case is 1.
:::

## Scoring mechanism

If you configure a chat assistant's **similarity threshold** to 0.2, only chunks with a hybrid score greater than 0.2 x 100 = 20 will be retrieved and sent to the chat model for content generation. This initial filtering step is crucial for narrowing down relevant information.

If you have assigned a page rank of 1 to knowledge base A (2024 news) and 0 to knowledge base B (2023 news), the final hybrid scores of the retrieved chunks will be adjusted accordingly. A chunk retrieved from knowledge base A with an initial score of 50 will receive a boost of 1 x 100 = 100 points, resulting in a final score of 50 + 1 x 100 = 150. In this way, chunks retrieved from knowledge base A will always precede chunks from knowledge base B.
</code></pre></div></div>
<p>guides\dataset\use_tag_sets.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 6
slug: /use_tag_sets
---

# Use tag set

Use a tag set to tag chunks in your datasets.

---

Retrieval accuracy is the touchstone for a production-ready RAG framework. In addition to retrieval-enhancing approaches like auto-keyword, auto-question, and knowledge graph, RAGFlow introduces an auto-tagging feature to address semantic gaps. The auto-tagging feature automatically maps tags in the user-defined tag sets to relevant chunks within your knowledge base based on similarity with each chunk. This automation mechanism allows you to apply an additional "layer" of domain-specific knowledge to existing datasets, which is particularly useful when dealing with a large number of chunks.

To use this feature, ensure you have at least one properly configured tag set, specify the tag set(s) on the **Configuration** page of your knowledge base (dataset), and then re-parse your documents to initiate the auto-tagging process. During this process, each chunk in your dataset is compared with every entry in the specified tag set(s), and tags are automatically applied based on similarity.

:::caution NOTE
The auto-tagging feature is *unavailable* on the [Infinity](https://github.com/infiniflow/infinity) document engine.
:::

## Scenarios

Auto-tagging applies in situations where chunks are so similar to each other that the intended chunks cannot be distinguished from the rest. For example, when you have a few chunks about iPhone and a majority about iPhone case or iPhone accessaries, it becomes difficult to retrieve those chunks about iPhone without additional information.

## Create tag set

You can consider a tag set as a closed set, and the tags to attach to the chunks in your dataset (knowledge base) are *exclusively* from the specified tag set. You use a tag set to "inform" RAGFlow which chunks to tag and which tags to apply.

### Prepare a tag table file

A tag set can comprise one or multiple table files in XLSX, CSV, or TXT formats. Each table file in the tag set contains two columns, **Description** and **Tag**:

- The first column provides descriptions of the tags listed in the second column. These descriptions can be example chunks or example queries. Similarity will be calculated between each entry in this column and every chunk in your dataset.
- The **Tag** column includes tags to pair with the description entries. Multiple tags should be separated by a comma (,).

:::tip NOTE
As a rule of thumb, consider including the following entries in your tag table:

- Descriptions of intended chunks, along with their corresponding tags.
- User queries that fail to retrieve the correct responses using other methods, ensuring their tags match the intended chunks in your dataset.
:::

### Create a tag set

1. Click **+ Create knowledge base** to create a knowledge base.
2. Navigate to the **Configuration** page of the created knowledge base and choose **Tag** as the default chunking method.
3. Navigate to the **Dataset** page and upload and parse your table file in XLSX, CSV, or TXT formats.  
   _A tag cloud appears under the **Tag view** section, indicating the tag set is created:_  
   ![Image](https://github.com/user-attachments/assets/abefbcbf-c130-4abe-95e1-267b0d2a0505)
4. Click the **Table** tab to view the tag frequency table:  
   ![Image](https://github.com/user-attachments/assets/af91d10c-5ea5-491f-ab21-3803d5ebf59f)

:::danger IMPORTANT
A tag set is *not* involved in document indexing or retrieval. Do not specify a tag set when configuring your chat assistant or agent.
:::

## Tag chunks

Once a tag set is created, you can apply it to your dataset:

1. Navigate to the **Configuration** page of your knowledge base (dataset).
2. Select the tag set from the **Tag sets** dropdown and click **Save** to confirm.

:::tip NOTE
If the tag set is missing from the dropdown, check that it has been created or configured correctly.
:::

3. Re-parse your documents to start the auto-tagging process.  
   _In an AI chat scenario using auto-tagged datasets, each query will be tagged using the corresponding tag set(s) and chunks with these tags will have a higher chance to be retrieved._

## Update tag set

Creating a tag set is *not* for once and for all. Oftentimes, you may find it necessary to update or delete existing tags or add new entries. 

- You can update the existing tag set in the tag frequency table.
- To add new entries, you can add and parse new table files in XLSX, CSV, or TXT formats.

### Update tag set in tag frequency table

1. Navigate to the **Configuration** page in your tag set.
2. Click the **Table** tab under **Tag view** to view the tag frequncy table, where you can update tag names or delete tags.

:::danger IMPORTANT
When a tag set is updated, you must re-parse the documents in your dataset so that their tags can be updated accordingly.
:::

### Add new table files

1. Navigate to the **Configuration** page in your tag set.
2. Navigate to the **Dataset** page and upload and parse your table file in XLSX, CSV, or TXT formats.

:::danger IMPORTANT
If you add new table files to your tag set, it is at your own discretion whether to re-parse your documents in your datasets.
:::

## Frequently asked questions

### Can I reference more than one tag set?

Yes, you can. Usually one tag set suffices. When using multiple tag sets, ensure they are independent of each other; otherwise, consider merging your tag sets.

### Difference between a tag set and a standard knowledge base?

A standard knowledge base is a dataset. It will be searched by RAGFlow's document engine and the retrieved chunks will be fed to the LLM. In contrast, a tag set is used solely to attach tags to chunks within your dataset. It does not directly participate in the retrieval process, and you should not choose a tag set when selecting datasets for your chat assistant or agent.

### Difference between auto-tag and auto-keyword?

Both features enhance retrieval in RAGFlow. The auto-keyword feature relies on the LLM and consumes a significant number of tokens, whereas the auto-tag feature is based on vector similarity and predefined tag set(s). You can view the keywords applied in the auto-keyword feature as an open set, as they are generated by the LLM. In contrast, a tag set can be considered a user-defined close set, requiring upload tag set(s) in specified formats before use.

</code></pre></div></div>
<p>guides\dataset\best_practices_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Best practices"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Best practices on configuring a knowledge base."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\dataset\best_practices\accelerate_doc_indexing.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /accelerate_doc_indexing
---

# Accelerate indexing
import APITable from '@site/src/components/APITable';

A checklist to speed up document parsing and indexing.

---

Please note that some of your settings may consume a significant amount of time. If you often find that document parsing is time-consuming, here is a checklist to consider:

- Use GPU to reduce embedding time.
- On the configuration page of your knowledge base, switch off **Use RAPTOR to enhance retrieval**.
- Extracting knowledge graph (GraphRAG) is time-consuming.
- Disable **Auto-keyword** and **Auto-question** on the configuration page of your knowledge base, as both depend on the LLM.
- **v0.17.0+:** If your document is plain text PDF and does not require GPU-intensive processes like OCR (Optical Character Recognition), TSR (Table Structure Recognition), or DLA (Document Layout Analysis), you can choose **Naive** over **DeepDoc** or other time-consuming large model options in the **Document parser** dropdown. This will substantially reduce document parsing time.

</code></pre></div></div>
<p>guides\models_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Models"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">-1</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Guides on model settings."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\models\llm_api_key_setup.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /llm_api_key_setup
---

# Configure model API key

An API key is required for RAGFlow to interact with an online AI model. This guide provides information about setting your model API key in RAGFlow.

## Get model API key

RAGFlow supports most mainstream LLMs. Please refer to [Supported Models](../../references/supported_models.mdx) for a complete list of supported models. You will need to apply for your model API key online. Note that most LLM providers grant newly-created accounts trial credit, which will expire in a couple of months, or a promotional amount of free quota.

:::note
If you find your online LLM is not on the list, don't feel disheartened. The list is expanding, and you can [file a feature request](https://github.com/infiniflow/ragflow/issues/new?assignees=&amp;labels=feature+request&amp;projects=&amp;template=feature_request.yml&amp;title=%5BFeature+Request%5D%3A+) with us! Alternatively, if you have customized or locally-deployed models, you can [bind them to RAGFlow using Ollama, Xinference, or LocalAI](./deploy_local_llm.mdx).
:::

## Configure model API key

You have two options for configuring your model API key:

- Configure it in **service_conf.yaml.template** before starting RAGFlow.
- Configure it on the **Model providers** page after logging into RAGFlow.

### Configure model API key before starting up RAGFlow

1. Navigate to **./docker/ragflow**.
2. Find entry **user_default_llm**:
   - Update `factory` with your chosen LLM.
   - Update `api_key` with yours.
   - Update `base_url` if you use a proxy to connect to the remote service.
3. Reboot your system for your changes to take effect.
4. Log into RAGFlow.  
   _After logging into RAGFlow, you will find your chosen model appears under **Added models** on the **Model providers** page._

### Configure model API key after logging into RAGFlow

:::caution WARNING
After logging into RAGFlow, configuring your model API key through the **service_conf.yaml.template** file will no longer take effect.
:::

After logging into RAGFlow, you can *only* configure API Key on the **Model providers** page:

1. Click on your logo on the top right of the page **&gt;** **Model providers**.
2. Find your model card under **Models to be added** and click **Add the model**:
   ![add model](https://github.com/infiniflow/ragflow/assets/93570324/07e43f63-367c-4c9c-8ed3-8a3a24703f4e)
3. Paste your model API key.
4. Fill in your base URL if you use a proxy to connect to the remote service.
5. Click **OK** to confirm your changes.

:::note
To update an existing model API key at a later point:
![update api key](https://github.com/infiniflow/ragflow/assets/93570324/0bfba679-33f7-4f6b-9ed6-f0e6e4b228ad)
:::
</code></pre></div></div>
<p>guides\team_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Team"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Team-specific guides."</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>guides\team\join_or_leave_team.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 2
slug: /join_or_leave_team
---

# Join or leave a team

Accept an invite to join a team, decline an invite, or leave a team.

---

Once you join a team, you can do the following:

- Upload documents to the team owner's shared datasets (knowledge bases).
- Parse documents in the team owner's shared datasets.
- Use the team owner's shared Agents.

:::tip NOTE
You cannot invite users to a team unless you are its owner.
:::

## Prerequisites

1. Ensure that your Email address that received the team invitation is associated with a RAGFlow user account.
2. The team owner should share his knowledge bases by setting their **Permission** to **Team**.

## Accept or decline team invite

1. You will be notified when you receive an invitation to join a team:

![team_notification](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/team_notification.jpg)

2. Click on your avatar in the top right corner of the page, then select **Team** in the left-hand panel to access the **Team** page.

![team](https://github.com/user-attachments/assets/0eac2503-26bc-4568-b3f2-bcd84069a07a)

_On the **Team** page, you can view the information about members of your team and the teams you have joined._

![accept_or_decline_team_invite](https://github.com/user-attachments/assets/6a2cb61f-03d5-4423-9ed1-71df97ff4114)

_After accepting the team invite, you should be able to view and update the team owner's knowledge bases whose **Permissions** is set to **Team**._

## Leave a joined team

![leave_team](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/quit.jpg)
</code></pre></div></div>
<p>guides\team\manage_team_members.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 1
slug: /manage_team_members
---

# Manage team members

Invite or remove team members.

---

By default, each RAGFlow user is assigned a single team named after their name. RAGFlow allows you to invite RAGFlow users to your team. Your team members can help you:

- Upload documents to your shared datasets (knowledge bases).
- Parse documents in your shared datasets.
- Use your shared Agents.

:::tip NOTE
- Your team members are currently *not* allowed to invite users to your team, and only you, the team owner, is permitted to do so.
- Sharing added models with team members is only available in RAGFlow's Enterprise edition.
:::

## Prerequisites

1. Ensure that the invited team member is a RAGFlow user and that the Email address used is associated with a RAGFlow user account.
2. To allow your team members to view and update your knowledge base, ensure that you set **Permissions** on its **Configuration** page from **Only me** to **Team**.

## Invite team members

Click on your avatar in the top right corner of the page, then select **Team** in the left-hand panel to access the **Team** page.

![team](https://github.com/user-attachments/assets/0eac2503-26bc-4568-b3f2-bcd84069a07a)

_On the **Team** page, you can view the information about members of your team and the teams you have joined._

You are, by default, the owner of your own team and the only person permitted to invite users to join your team or remove team members.

![invite_team_member](https://github.com/user-attachments/assets/d85b55c3-7e86-4f04-a414-ca18a9ee8963)

## Remove team members

![remove_members](https://github.com/user-attachments/assets/5c1a6ab5-8862-47a0-ad09-77fe88866508)
</code></pre></div></div>
<p>guides\team\share_agents.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 5
slug: /share_agent
---

# Share Agent

Share an Agent with your team members.

---

When ready, you may share your Agents with your team members so that they can use them. Please note that your Agents are not shared automatically; you must manually enable sharing by selecting the corresponding **Permissions** radio button:

1. Click the intended Agent to open its editing canvas. 
2. Click **Settings** to show the **Agent settings** dialogue.
3. Change **Permissions** from **Only me** to **Team**.
4. Click **Save** to apply your changes.

![share_agent](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/share_agent.jpg)

*When completed, your team members will see your shared Agents like this:*

![shared_agent](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/shared_agent.jpg)
</code></pre></div></div>
<p>guides\team\share_chat_assistant.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 4
slug: /share_chat_assistant
---

# Share chat assistant

Sharing chat assistant is currently exclusive to RAGFlow Enterprise, but will be made available in due course.
</code></pre></div></div>
<p>guides\team\share_knowledge_bases.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 3
slug: /share_datasets
---

# Share knowledge base

Share a knowledge base with team members.

---

When ready, you may share your knowledge bases with your team members so that they can upload and parse files in them. Please note that your knowledge bases are not shared automatically; you must manually enable sharing by selecting the appropriate **Permissions** radio button:

1. Navigate to the knowledge base's **Configuration** page.
2. Change **Permissions** from **Only me** to **Team**.
3. Click **Save** to apply your changes.

![share_knowledge_base](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/share_knowledge_base.jpg)

*Once completed, your team members will see your shared knowledge bases like this:*

![shared_knowledge_base](https://raw.githubusercontent.com/infiniflow/ragflow-docs/main/images/shared_knowledge_base.jpg)
</code></pre></div></div>
<p>guides\team\share_model.md</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 6
slug: /share_model
---

# Share models

Sharing models is currently exclusive to RAGFlow Enterprise.
</code></pre></div></div>
<p>references_category_.json</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"References"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"position"</span><span class="p">:</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w">
  </span><span class="nl">"link"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"generated-index"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Miscellaneous References"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></code></pre></div></div>
<p>references\supported_models.mdx</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
sidebar_position: 0
slug: /supported_models
---

# Supported models
import APITable from '@site/src/components/APITable';

A complete list of models supported by RAGFlow, which will continue to expand.

```mdx-code-block
&lt;APITable&gt;
```

| Provider              | Chat               | Embedding          | Rerank             | Img2txt            | Speech2txt         | TTS                |
| --------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |
| Anthropic             | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Azure-OpenAI          | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: |                    |
| BAAI                  |                    | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |
| BaiChuan              | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| BaiduYiyan            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| Bedrock               | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| Cohere                | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| DeepSeek              | :heavy_check_mark: |                    |                    |                    |                    |                    |
| FastEmbed             |                    | :heavy_check_mark: |                    |                    |                    |                    |
| Fish Audio            |                    |                    |                    |                    |                    | :heavy_check_mark: |
| Gemini                | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |                    |
| Google Cloud          | :heavy_check_mark: |                    |                    |                    |                    |                    |
| GPUStack              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: |
| Groq                  | :heavy_check_mark: |                    |                    |                    |                    |                    |
| HuggingFace           | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| Jina                  |                    | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |
| LeptonAI              | :heavy_check_mark: |                    |                    |                    |                    |                    |
| LocalAI               | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |                    |
| LM-Studio             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| MiniMax               | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Mistral               | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| ModelScope            | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Moonshot              | :heavy_check_mark: |                    |                    | :heavy_check_mark: |                    |                    |
| Novita AI             | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| NVIDIA                | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| Ollama                | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |                    |
| OpenAI                | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |
| OpenAI-API-Compatible | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| OpenRouter            | :heavy_check_mark: |                    |                    | :heavy_check_mark: |                    |                    |
| PerfXCloud            | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| Replicate             | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| PPIO                  | :heavy_check_mark: |                    |                    |                    |                    |                    |
| SILICONFLOW           | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| StepFun               | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Tencent Hunyuan       | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Tencent Cloud         |                    |                    |                    |                    | :heavy_check_mark: |                    |
| TogetherAI            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| Tongyi-Qianwen        | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |
| Upstage               | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |                    |
| VLLM                  | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| VolcEngine            | :heavy_check_mark: |                    |                    |                    |                    |                    |
| Voyage AI             |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |
| Xinference            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |
| XunFei Spark          | :heavy_check_mark: |                    |                    |                    |                    | :heavy_check_mark: |
| Youdao                |                    | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |
| ZHIPU-AI              | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |                    |
| 01.AI                 | :heavy_check_mark: |                    |                    |                    |                    |                    |

```mdx-code-block
&lt;/APITable&gt;
```

:::danger IMPORTANT
If your model is not listed here but has APIs compatible with those of OpenAI, click **OpenAI-API-Compatible** on the **Model providers** page to configure your model.
::: 

:::note
The list of supported models is extracted from [this source](https://github.com/infiniflow/ragflow/blob/main/rag/llm/__init__.py) and may not be the most current. For the latest supported model list, please refer to the Python file.
:::

</code></pre></div></div>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">Copying data in Linux</title><link href="https://ib.bsb.br/copying-data-in-linux/" rel="alternate" type="text/html" title="Copying data in Linux" /><published>2025-05-15T00:00:00+00:00</published><updated>2025-05-15T21:45:24+00:00</updated><id>https://ib.bsb.br/copying-data-in-linux</id><content type="html" xml:base="https://ib.bsb.br/copying-data-in-linux/"><![CDATA[<p>Copying a large volume of data like 500GiB, especially when it consists of thousands of individual files, from an SD card to an external hard disk in Linux requires strategies that minimize overhead and maximize throughput. The key is to leverage parallel processing to utilize multiple CPU cores and choose tools that handle file operations efficiently.</p>

<p><strong>Understanding the Bottlenecks</strong></p>

<p>Before diving into tools, it’s helpful to understand potential bottlenecks:</p>
<ol>
  <li><strong>Per-File Overhead (CPU Bound):</strong> When dealing with thousands of small files, the operating system incurs overhead for each file operation (opening, reading metadata, writing metadata, closing). This can make the CPU a bottleneck even if the drives aren’t saturated. Parallel processing helps here.</li>
  <li><strong>I/O Throughput (Drive Bound):</strong> The read speed of your SD card and the write speed of your external HDD (especially if it’s a mechanical drive vs. an SSD) will ultimately limit transfer rates for large files or when per-file overhead is minimized.</li>
  <li><strong>Single-Threaded Operations:</strong> Standard <code class="language-plaintext highlighter-rouge">cp</code> or <code class="language-plaintext highlighter-rouge">mv</code> commands are typically single-threaded, processing one file at a time, making them inefficient for this scale.</li>
</ol>

<p>Here are several effective Linux tools and techniques to accomplish this task, focusing on speed and resource utilization:</p>

<p><strong>1. <code class="language-plaintext highlighter-rouge">rsync</code> with GNU <code class="language-plaintext highlighter-rouge">parallel</code> (Recommended for Robustness &amp; Parallelism)</strong></p>

<p><code class="language-plaintext highlighter-rouge">rsync</code> is a powerful and versatile tool for copying and synchronizing files. While <code class="language-plaintext highlighter-rouge">rsync</code> itself processes files sequentially within a single instance, you can use it with GNU <code class="language-plaintext highlighter-rouge">parallel</code> to run multiple <code class="language-plaintext highlighter-rouge">rsync</code> jobs concurrently, significantly speeding up the transfer of many files.</p>

<ul>
  <li><strong>How it Works:</strong> <code class="language-plaintext highlighter-rouge">find</code> lists all files and directories. GNU <code class="language-plaintext highlighter-rouge">parallel</code> takes this list and launches multiple <code class="language-plaintext highlighter-rouge">rsync</code> processes, each handling a subset of the files/directories simultaneously. This leverages multiple CPU cores to manage the per-file operations and can better saturate your drive’s I/O capabilities.</li>
  <li><strong>Key Advantages:</strong> Robust error handling, ability to resume interrupted transfers (with <code class="language-plaintext highlighter-rouge">rsync</code>), preserves permissions and metadata, detailed progress.</li>
  <li><strong>Installation:</strong> If <code class="language-plaintext highlighter-rouge">parallel</code> isn’t installed: <code class="language-plaintext highlighter-rouge">sudo apt update &amp;&amp; sudo apt install parallel</code> (Debian/Ubuntu) or <code class="language-plaintext highlighter-rouge">sudo dnf install parallel</code> (Fedora/RHEL).</li>
</ul>

<p><strong>Example Command (Copying contents of source into destination):</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure destination directory exists: mkdir -p /media/user/externalhdd/backup_destination</span>
find /media/user/sdcard/source_folder/ <span class="nt">-mindepth</span> 1 <span class="nt">-print0</span> | <span class="se">\</span>
  parallel <span class="nt">-0</span> <span class="nt">-j</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="nt">--eta</span> <span class="nt">--joblog</span> /tmp/rsync_parallel.log <span class="se">\</span>
  rsync <span class="nt">-aP</span> <span class="o">{}</span> /media/user/externalhdd/backup_destination/
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">/media/user/sdcard/source_folder/</code>: Your source directory on the SD card. The trailing slash means “contents of.”</li>
  <li><code class="language-plaintext highlighter-rouge">-mindepth 1</code>: Excludes the top-level source directory itself from the list, processing its contents.</li>
  <li><code class="language-plaintext highlighter-rouge">-print0</code>: Handles filenames with spaces or special characters safely.</li>
  <li><code class="language-plaintext highlighter-rouge">parallel -0 -j$(nproc) --eta --joblog /tmp/rsync_parallel.log</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">-0</code>: Expects null-terminated input from <code class="language-plaintext highlighter-rouge">find</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">-j$(nproc)</code>: Runs a number of jobs equal to your CPU cores. You can set a specific number, e.g., <code class="language-plaintext highlighter-rouge">-j4</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">--eta</code>: Shows estimated time of arrival.</li>
      <li><code class="language-plaintext highlighter-rouge">--joblog /tmp/rsync_parallel.log</code>: Logs the progress and success/failure of each parallel job.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">rsync -aP {} /media/user/externalhdd/backup_destination/</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">-a</code>: Archive mode (preserves permissions, timestamps, symbolic links, etc.).</li>
      <li><code class="language-plaintext highlighter-rouge">-P</code>: Combines <code class="language-plaintext highlighter-rouge">--progress</code> and <code class="language-plaintext highlighter-rouge">--partial</code> (for resumability).</li>
      <li><code class="language-plaintext highlighter-rouge">{}</code>: Placeholder for the file/directory passed by <code class="language-plaintext highlighter-rouge">parallel</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">/media/user/externalhdd/backup_destination/</code>: The destination. The trailing slash is important for <code class="language-plaintext highlighter-rouge">rsync</code> to copy items <em>into</em> this directory.</li>
    </ul>
  </li>
</ul>

<p><strong>Dry Run (Highly Recommended):</strong> Before running the actual copy, perform a dry run:
Add <code class="language-plaintext highlighter-rouge">rsync -anP</code> (note the <code class="language-plaintext highlighter-rouge">n</code> for dry-run) in the command above, or add <code class="language-plaintext highlighter-rouge">--dry-run</code> to the <code class="language-plaintext highlighter-rouge">parallel</code> command.</p>

<p><strong>2. <code class="language-plaintext highlighter-rouge">tar</code> Pipelined (Efficient for Many Small Files)</strong></p>

<p>This classic method archives the source files into a single stream (<code class="language-plaintext highlighter-rouge">stdout</code>) and pipes this stream directly to another <code class="language-plaintext highlighter-rouge">tar</code> process that extracts it at the destination (<code class="language-plaintext highlighter-rouge">stdin</code>). This significantly reduces the overhead of individual file system operations, especially beneficial for mechanical drives and vast numbers of tiny files.</p>

<ul>
  <li><strong>How it Works:</strong> <code class="language-plaintext highlighter-rouge">tar</code> reads all source files sequentially and writes them as a continuous data stream. The receiving <code class="language-plaintext highlighter-rouge">tar</code> process reads this stream and recreates the files and directory structure.</li>
  <li><strong>Key Advantages:</strong> Can be very fast for scenarios with extreme numbers of small files by minimizing disk head seeking.</li>
  <li><strong>Considerations:</strong> Less easily resumable if interrupted compared to <code class="language-plaintext highlighter-rouge">rsync</code>. Progress indication is often through tools like <code class="language-plaintext highlighter-rouge">pv</code> (Pipe Viewer).</li>
</ul>

<p><strong>Example Command:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure destination directory exists: mkdir -p /media/user/externalhdd/backup_destination</span>
<span class="o">(</span><span class="nb">cd</span> /media/user/sdcard/source_folder/ <span class="o">&amp;&amp;</span> <span class="nb">tar</span> <span class="nt">-cf</span> - .<span class="o">)</span> | pv | <span class="o">(</span><span class="nb">cd</span> /media/user/externalhdd/backup_destination/ <span class="o">&amp;&amp;</span> <span class="nb">tar</span> <span class="nt">-xf</span> -<span class="o">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">(cd /media/user/sdcard/source_folder/ &amp;&amp; tar -cf - .)</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">cd ...</code>: Changes to the source directory. The subshell <code class="language-plaintext highlighter-rouge">(...)</code> ensures this <code class="language-plaintext highlighter-rouge">cd</code> doesn’t affect your main shell’s working directory.</li>
      <li><code class="language-plaintext highlighter-rouge">tar -cf - .</code>: Creates (<code class="language-plaintext highlighter-rouge">c</code>) an archive of the current directory (<code class="language-plaintext highlighter-rouge">.</code>) and writes it to standard output (<code class="language-plaintext highlighter-rouge">f -</code>).</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">pv</code>: (Optional, install with <code class="language-plaintext highlighter-rouge">sudo apt install pv</code>) Pipe Viewer shows progress of data through the pipe.</li>
  <li><code class="language-plaintext highlighter-rouge">(cd /media/user/externalhdd/backup_destination/ &amp;&amp; tar -xf -)</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">cd ...</code>: Changes to the destination directory in a subshell.</li>
      <li><code class="language-plaintext highlighter-rouge">tar -xf -</code>: Extracts (<code class="language-plaintext highlighter-rouge">x</code>) the archive from standard input (<code class="language-plaintext highlighter-rouge">f -</code>).</li>
    </ul>
  </li>
</ul>

<p><strong>3. <code class="language-plaintext highlighter-rouge">find</code> with <code class="language-plaintext highlighter-rouge">xargs</code> and <code class="language-plaintext highlighter-rouge">cp --parents</code> (Parallel Basic Copy)</strong></p>

<p>This method uses <code class="language-plaintext highlighter-rouge">find</code> to locate files, and <code class="language-plaintext highlighter-rouge">xargs</code> to execute <code class="language-plaintext highlighter-rouge">cp</code> commands in parallel. The crucial <code class="language-plaintext highlighter-rouge">--parents</code> option for <code class="language-plaintext highlighter-rouge">cp</code> ensures the source directory structure is replicated at the destination.</p>

<ul>
  <li><strong>How it Works:</strong> <code class="language-plaintext highlighter-rouge">find</code> generates a list of files. <code class="language-plaintext highlighter-rouge">xargs</code> takes this list and runs multiple <code class="language-plaintext highlighter-rouge">cp</code> commands simultaneously. <code class="language-plaintext highlighter-rouge">cp --parents</code> recreates the necessary parent directories at the destination.</li>
  <li><strong>Key Advantages:</strong> Uses standard <code class="language-plaintext highlighter-rouge">cp</code>, can be effective if <code class="language-plaintext highlighter-rouge">rsync</code>’s overhead is a concern for a simple copy.</li>
  <li><strong>Considerations:</strong> <code class="language-plaintext highlighter-rouge">cp</code> doesn’t have <code class="language-plaintext highlighter-rouge">rsync</code>’s advanced resumability or delta-transfer capabilities (though not relevant for an initial full copy).</li>
</ul>

<p><strong>Example Command:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure base destination directory exists: mkdir -p /media/user/externalhdd/backup_destination</span>
<span class="nb">cd</span> /media/user/sdcard/source_folder/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
  find <span class="nb">.</span> <span class="nt">-type</span> f <span class="nt">-print0</span> | <span class="se">\</span>
  xargs <span class="nt">-0</span> <span class="nt">-P</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="nt">-I</span> <span class="o">{}</span> <span class="nb">cp</span> <span class="nt">--parents</span> <span class="nt">-a</span> <span class="o">{}</span> /media/user/externalhdd/backup_destination/
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">cd /media/user/sdcard/source_folder/</code>: Change to the source directory to make relative paths work with <code class="language-plaintext highlighter-rouge">cp --parents</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">find . -type f -print0</code>: Finds only files (<code class="language-plaintext highlighter-rouge">-type f</code>) in the current directory (<code class="language-plaintext highlighter-rouge">.</code>) and its subdirectories.</li>
  <li><code class="language-plaintext highlighter-rouge">xargs -0 -P$(nproc) -I {}</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">-0</code>: Null-terminated input.</li>
      <li><code class="language-plaintext highlighter-rouge">-P$(nproc)</code>: Parallel processes up to the number of CPU cores.</li>
      <li><code class="language-plaintext highlighter-rouge">-I {}</code>: Replaces <code class="language-plaintext highlighter-rouge">{}</code> with each input item. This makes <code class="language-plaintext highlighter-rouge">cp --parents</code> work correctly with paths containing spaces.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">cp --parents -a {} /media/user/externalhdd/backup_destination/</code>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">--parents</code>: Recreates the source directory structure under the destination.</li>
      <li><code class="language-plaintext highlighter-rouge">-a</code>: Archive mode (like <code class="language-plaintext highlighter-rouge">rsync -a</code>, equivalent to <code class="language-plaintext highlighter-rouge">-dR --preserve=all</code>).</li>
      <li><code class="language-plaintext highlighter-rouge">{}</code>: The file to copy.</li>
      <li><code class="language-plaintext highlighter-rouge">/media/user/externalhdd/backup_destination/</code>: The target directory where the structure from <code class="language-plaintext highlighter-rouge">source_folder</code> will be created.</li>
    </ul>
  </li>
</ul>

<p><strong>4. <code class="language-plaintext highlighter-rouge">fpsync</code> (Specialized Parallel <code class="language-plaintext highlighter-rouge">rsync</code> Wrapper)</strong></p>

<p><code class="language-plaintext highlighter-rouge">fpsync</code> is a tool designed to parallelize <code class="language-plaintext highlighter-rouge">rsync</code>. It uses <code class="language-plaintext highlighter-rouge">fpart</code> to partition the file list and then runs multiple <code class="language-plaintext highlighter-rouge">rsync</code> workers.</p>

<ul>
  <li><strong>How it Works:</strong> Automates the process of splitting the workload and managing parallel <code class="language-plaintext highlighter-rouge">rsync</code> instances.</li>
  <li><strong>Key Advantages:</strong> Tailored for this exact scenario; can be very efficient.</li>
  <li><strong>Installation:</strong> May need to be installed via your package manager (e.g., <code class="language-plaintext highlighter-rouge">sudo apt install fpart</code>, as <code class="language-plaintext highlighter-rouge">fpsync</code> is often bundled with it).</li>
</ul>

<p><strong>Example Command:</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure destination directory exists: mkdir -p /media/user/externalhdd/backup_destination</span>
fpsync <span class="nt">-n</span> <span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="nt">-v</span> <span class="se">\</span>
  /media/user/sdcard/source_folder/ /media/user/externalhdd/backup_destination/
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">-n $(nproc)</code>: Number of parallel <code class="language-plaintext highlighter-rouge">rsync</code> workers (e.g., number of CPU cores).</li>
  <li><code class="language-plaintext highlighter-rouge">-v</code>: Verbose mode.</li>
  <li><code class="language-plaintext highlighter-rouge">/media/user/sdcard/source_folder/</code>: Source directory.</li>
  <li><code class="language-plaintext highlighter-rouge">/media/user/externalhdd/backup_destination/</code>: Destination directory.</li>
  <li><strong>Note on batching:</strong> <code class="language-plaintext highlighter-rouge">fpsync</code> uses <code class="language-plaintext highlighter-rouge">fpart</code> underneath. If you need to control batching by number of files per job (rather than just total workers), you might pass <code class="language-plaintext highlighter-rouge">fpart</code> options using <code class="language-plaintext highlighter-rouge">fpsync -o "-f &lt;num_files&gt;"</code>. Check <code class="language-plaintext highlighter-rouge">man fpart</code> for details.</li>
</ul>

<p><strong>5. <code class="language-plaintext highlighter-rouge">mc</code> (Midnight Commander - TUI Alternative)</strong></p>

<p>For users who prefer a Text-based User Interface, Midnight Commander is a powerful console file manager. Its built-in copy operations (<code class="language-plaintext highlighter-rouge">F5</code>) are generally well-optimized and can handle large numbers of files more gracefully than a simple desktop file manager.</p>

<ul>
  <li><strong>How it Works:</strong> Provides an interactive way to select source files/directories and copy them. While it might not offer the same granular parallel control as CLI combinations, it’s often faster than basic <code class="language-plaintext highlighter-rouge">cp</code> for large jobs.</li>
  <li><strong>Installation:</strong> <code class="language-plaintext highlighter-rouge">sudo apt install mc</code> or <code class="language-plaintext highlighter-rouge">sudo dnf install mc</code>.</li>
  <li><strong>Usage:</strong> Run <code class="language-plaintext highlighter-rouge">mc</code>, navigate panels to source and destination, select files (e.g., <code class="language-plaintext highlighter-rouge">Insert</code> key or <code class="language-plaintext highlighter-rouge">*</code>), press <code class="language-plaintext highlighter-rouge">F5</code> to copy.</li>
</ul>

<p><strong>Additional Considerations for Maximizing Speed:</strong></p>

<ul>
  <li><strong>Hardware:</strong> Ensure both SD card reader and external HDD are connected to the fastest available USB ports (USB 3.0+). An SSD external drive will be significantly faster than a mechanical HDD.</li>
  <li><strong>Filesystem Mount Options:</strong> Mounting filesystems with <code class="language-plaintext highlighter-rouge">noatime</code> or <code class="language-plaintext highlighter-rouge">relatime</code> can reduce some disk I/O by not updating file access times on every read.
<code class="language-plaintext highlighter-rouge">sudo mount -o remount,noatime /media/user/sdcard</code> (if applicable and safe for your use case).</li>
  <li><strong>I/O Scheduler:</strong> For mechanical drives, the I/O scheduler can matter. Modern kernels often default to <code class="language-plaintext highlighter-rouge">bfq</code> or <code class="language-plaintext highlighter-rouge">mq-deadline</code>, which are generally good.</li>
  <li><strong>System Load:</strong> Minimize other disk-intensive or CPU-intensive processes during the copy.</li>
  <li><strong>Resource Monitoring:</strong> Use tools like <code class="language-plaintext highlighter-rouge">iotop</code> (to see disk I/O per process), <code class="language-plaintext highlighter-rouge">htop</code> (CPU/memory), <code class="language-plaintext highlighter-rouge">vmstat</code>, or <code class="language-plaintext highlighter-rouge">dstat</code> to identify bottlenecks during the transfer.</li>
  <li><strong>GUI <code class="language-plaintext highlighter-rouge">rsync</code> Front-ends:</strong> If you prefer a GUI but want <code class="language-plaintext highlighter-rouge">rsync</code>’s power, tools like <code class="language-plaintext highlighter-rouge">grsync</code> provide a graphical interface to <code class="language-plaintext highlighter-rouge">rsync</code>.</li>
</ul>

<p><strong>Which Method to Choose?</strong></p>

<ul>
  <li><strong>For general robustness, features, and good parallel performance:</strong> <code class="language-plaintext highlighter-rouge">rsync</code> with GNU <code class="language-plaintext highlighter-rouge">parallel</code> (Method 1) or <code class="language-plaintext highlighter-rouge">fpsync</code> (Method 4) are excellent choices.</li>
  <li><strong>For potentially the highest speed with extreme numbers of very small files (especially to/from mechanical drives):</strong> The <code class="language-plaintext highlighter-rouge">tar</code> pipe (Method 2) can be very effective.</li>
  <li><strong>For a simpler parallel <code class="language-plaintext highlighter-rouge">cp</code> approach:</strong> <code class="language-plaintext highlighter-rouge">find</code> with <code class="language-plaintext highlighter-rouge">xargs</code> and <code class="language-plaintext highlighter-rouge">cp --parents</code> (Method 3) is a solid option.</li>
  <li><strong>For an interactive TUI approach:</strong> <code class="language-plaintext highlighter-rouge">mc</code> (Method 5) is user-friendly.</li>
</ul>

<p>Always test with a smaller subset of your data and use dry-run options where available before committing to the full 500GiB transfer. This allows you to verify commands and estimate performance. Remember to replace placeholder paths with your actual SD card and external HDD mount points.</p>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">GitHub Actions Build Workflow Locally on a Debian</title><link href="https://ib.bsb.br/github-actions-build-workflow-locally-on-a-debian/" rel="alternate" type="text/html" title="GitHub Actions Build Workflow Locally on a Debian" /><published>2025-05-15T00:00:00+00:00</published><updated>2025-05-15T09:21:47+00:00</updated><id>https://ib.bsb.br/github-actions-build-workflow-locally-on-a-debian</id><content type="html" xml:base="https://ib.bsb.br/github-actions-build-workflow-locally-on-a-debian/"><![CDATA[<p><strong>Goal:</strong> To build the OS images and related artifacts as specified in the workflow, using a Debian-based environment like Finnix.</p>

<p><strong>Assumed Environment:</strong></p>
<ul>
  <li>A running Debian-based Linux distribution (e.g., Finnix booted, or a standard Debian/Ubuntu desktop/server). This guide assumes your distribution is reasonably compatible with Debian 12 (Bullseye), as the workflow specifies <code class="language-plaintext highlighter-rouge">image-debian-12</code>.</li>
  <li>Internet connectivity (for downloading packages and tools).</li>
  <li><code class="language-plaintext highlighter-rouge">sudo</code> privileges.</li>
</ul>

<hr />

<h3 id="1-prerequisites--initial-setup">1. Prerequisites &amp; Initial Setup</h3>

<p>Before starting the build, ensure your system and environment are ready.</p>

<p><strong>a. Resource Requirements:</strong>
The original GitHub Actions workflow specifies <code class="language-plaintext highlighter-rouge">runs-on</code> parameters that suggest the following minimum resources. Ensure your local machine or VM has:</p>
<ul>
  <li><strong>CPU:</strong> At least 4 cores.</li>
  <li><strong>Memory (RAM):</strong> At least 4 GB.</li>
  <li><strong>Disk Space:</strong> At least 100 GB free, especially in your build directory and system partitions like <code class="language-plaintext highlighter-rouge">/tmp</code> and <code class="language-plaintext highlighter-rouge">/var</code>.</li>
</ul>

<p><strong>b. System Update (Recommended):</strong>
Open a terminal and update your package lists:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get update
</code></pre></div></div>
<p><em>(On a fresh Finnix boot, this might be less critical but is good practice on persistent systems.)</em></p>

<p><strong>c. Install Essential Tools:</strong>
Some tools might already be present, especially on Finnix, but ensure they are installed:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> git curl ca-certificates gnupg
</code></pre></div></div>

<p><strong>d. Clone Your Repository:</strong>
If you haven’t already, clone the repository containing the workflow and the source code.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone &lt;your-repository-url&gt;
<span class="nb">cd</span> &lt;your-repository-name&gt;
</code></pre></div></div>
<p>Replace <code class="language-plaintext highlighter-rouge">&lt;your-repository-url&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;your-repository-name&gt;</code> with your actual repository details.</p>

<p><strong>e. Checkout the Target Tag:</strong>
The workflow triggers on any tag push (<code class="language-plaintext highlighter-rouge">on: push: tags: - '*'</code>). To reproduce a build for a specific tag (e.g., <code class="language-plaintext highlighter-rouge">v1.2.3</code>), check it out:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Replace v1.2.3 with your actual tag</span>
<span class="nb">export </span><span class="nv">TARGET_TAG</span><span class="o">=</span><span class="s2">"v1.2.3"</span>
git checkout tags/<span class="k">${</span><span class="nv">TARGET_TAG</span><span class="k">}</span> <span class="nt">-b</span> build-<span class="k">${</span><span class="nv">TARGET_TAG</span><span class="k">}</span>
</code></pre></div></div>
<p>This creates a local branch <code class="language-plaintext highlighter-rouge">build-${TARGET_TAG}</code> based on the tag.</p>

<p><strong>f. Define the Tag Name Environment Variable:</strong>
The workflow uses <code class="language-plaintext highlighter-rouge">github.ref_name</code> for the tag. We’ll simulate this:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">GITHUB_REF_NAME</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">TARGET_TAG</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Building for tag: </span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div></div>

<hr />

<h3 id="2-installing-build-tools-and-dependencies">2. Installing Build Tools and Dependencies</h3>

<p>This section mirrors the setup steps from your GitHub Actions workflow.</p>

<p><strong>a. Install Go:</strong>
The workflow uses <code class="language-plaintext highlighter-rouge">actions/setup-go@v5</code> with <code class="language-plaintext highlighter-rouge">go-version: stable</code>.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> golang-go
<span class="c"># Verify installation (optional)</span>
go version
</code></pre></div></div>
<p>For most “stable” use cases, the version of Go provided by Debian’s repositories should suffice.</p>

<p><strong>b. Repository Permissions (Conditional):</strong>
The workflow runs <code class="language-plaintext highlighter-rouge">sudo chown -R $(id -u):$(id -g) .</code>. This is often for GitHub Actions runner environments. Locally, if you cloned as your user and manage <code class="language-plaintext highlighter-rouge">sudo</code> appropriately, you might not need this. If you encounter permission errors during <code class="language-plaintext highlighter-rouge">make</code> or <code class="language-plaintext highlighter-rouge">mkosi</code> operations related to file ownership <em>within your working directory</em>, you might revisit this. Generally, proceed without it first.</p>

<p><strong>c. Install Core Build Dependencies:</strong>
These are the packages listed in the workflow.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">--yes</span> <span class="se">\</span>
    binutils <span class="se">\</span>
    debian-archive-keyring <span class="se">\</span>
    devscripts <span class="se">\</span>
    make <span class="se">\</span>
    parted <span class="se">\</span>
    pipx <span class="se">\</span>
    qemu-utils
</code></pre></div></div>

<p><strong>d. Setup Incus (Daily Build):</strong>
The workflow uses a script to get daily builds of Incus.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"Setting up Incus (daily build)..."</span>
curl https://pkgs.zabbly.com/get/incus-daily | <span class="nb">sudo </span>sh

<span class="c"># The script above should handle repository setup and installation.</span>
<span class="c"># The workflow then initializes Incus and sets socket permissions.</span>
<span class="nb">sudo </span>incus admin init <span class="nt">--auto</span>

<span class="c"># The workflow uses 'sudo chmod 666 /var/lib/incus/unix.socket'.</span>
<span class="c"># This makes the socket world-writable, which is generally not recommended for production.</span>
<span class="c"># A better long-term approach is to add your user to the 'incus' group:</span>
<span class="c">#   sudo usermod -a -G incus $USER</span>
<span class="c">#   # Then log out and back in, or start a new shell: newgrp incus</span>
<span class="c"># This allows running 'incus' commands without sudo.</span>
<span class="c"># For immediate effect in the current script, or if not adding user to group,</span>
<span class="c"># the chmod command from the workflow can be used, but be aware of its implications.</span>
<span class="nb">sudo chmod </span>666 /var/lib/incus/unix.socket <span class="c"># As per workflow; consider security implications</span>

<span class="c"># Verify Incus (you might need sudo if group membership isn't active yet or chmod wasn't run)</span>
incus list
<span class="c"># If 'incus list' fails due to permissions and you haven't run chmod 666, try: sudo incus list</span>
</code></pre></div></div>

<p><strong>e. Setup mkosi:</strong>
<code class="language-plaintext highlighter-rouge">mkosi</code> is installed using <code class="language-plaintext highlighter-rouge">pipx</code> from a specific git commit.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pipx <span class="nb">install </span>git+https://github.com/systemd/mkosi.git@v25.3

<span class="c"># IMPORTANT: Understanding PATH for mkosi and sudo</span>
<span class="c"># pipx installs applications to $HOME/.local/bin by default for the current user.</span>
<span class="c"># Add this to your PATH for the current session if it's not already configured in your .bashrc/.zshrc:</span>
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">HOME</span><span class="k">}</span><span class="s2">/.local/bin:</span><span class="k">${</span><span class="nv">PATH</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Make sure </span><span class="nv">$HOME</span><span class="s2">/.local/bin is in your PATH. Current PATH: </span><span class="nv">$PATH</span><span class="s2">"</span>

<span class="c"># Verify mkosi installation</span>
mkosi <span class="nt">--version</span>
</code></pre></div></div>
<p>If <code class="language-plaintext highlighter-rouge">pipx</code> prompts you to run <code class="language-plaintext highlighter-rouge">pipx ensurepath</code>, do so, and it might require opening a new terminal or sourcing your shell’s profile file (<code class="language-plaintext highlighter-rouge">.bashrc</code>, <code class="language-plaintext highlighter-rouge">.zshrc</code>, etc.).</p>

<hr />

<h3 id="3-configuring-for-the-build">3. Configuring for the Build</h3>

<p>Prepare files and environment variables specific to this build.</p>

<p><strong>a. Create <code class="language-plaintext highlighter-rouge">mkosi.version</code> File:</strong>
This file stores the version (tag name) for the build.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="o">&gt;</span> mkosi.version
<span class="nb">cat </span>mkosi.version <span class="c"># Verify content</span>
</code></pre></div></div>

<p><strong>b. Handle Secrets (<code class="language-plaintext highlighter-rouge">mkosi.crt</code>, <code class="language-plaintext highlighter-rouge">mkosi.key</code>):</strong>
The workflow uses GitHub secrets <code class="language-plaintext highlighter-rouge">secrets.SB_CERT</code> and <code class="language-plaintext highlighter-rouge">secrets.SB_KEY</code>. You must provide these files locally. <strong>These are sensitive files; handle them securely.</strong>
Create <code class="language-plaintext highlighter-rouge">mkosi.crt</code> and <code class="language-plaintext highlighter-rouge">mkosi.key</code> in your repository root with the <em>actual certificate and private key content</em>.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Example: Replace placeholder content with your actual secrets.</span>
<span class="nb">echo</span> <span class="s2">"---BEGIN CERTIFICATE---
This is a placeholder for your SB_CERT.
Replace this with the actual certificate content.
---END CERTIFICATE---"</span> <span class="o">&gt;</span> mkosi.crt
<span class="nb">chmod </span>644 mkosi.crt

<span class="nb">echo</span> <span class="s2">"---BEGIN PRIVATE KEY---
This is a placeholder for your SB_KEY.
Replace this with the actual private key content.
---END PRIVATE KEY---"</span> <span class="o">&gt;</span> mkosi.key
<span class="nb">chmod </span>600 mkosi.key <span class="c"># Restrict permissions for the private key</span>

<span class="nb">echo</span> <span class="s2">"IMPORTANT: Placeholder mkosi.crt and mkosi.key created. REPLACE with actual content."</span>
</code></pre></div></div>

<hr />

<h3 id="4-executing-the-build">4. Executing the Build</h3>

<p>This is where <code class="language-plaintext highlighter-rouge">mkosi</code> and your <code class="language-plaintext highlighter-rouge">Makefile</code> perform the image creation.</p>

<p><strong>a. Run the Main Build Command:</strong>
The workflow uses <code class="language-plaintext highlighter-rouge">make build-iso</code>. This assumes your <code class="language-plaintext highlighter-rouge">Makefile</code> has this target.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># The original workflow exports PATH="${PATH}:/root/.local/bin".</span>
<span class="c"># This implies 'pipx install' might have been run as root, or the runner's $HOME is /root.</span>
<span class="c"># Since we installed mkosi as the current user (via pipx to $HOME/.local/bin),</span>
<span class="c"># and 'make build-iso' might invoke 'mkosi' which often requires root privileges</span>
<span class="c"># for operations like mounting filesystems, you'll likely need to run 'make' with sudo.</span>
<span class="c">#</span>
<span class="c"># Using 'sudo -E' is crucial here:</span>
<span class="c"># -E preserves the existing environment variables, including:</span>
<span class="c">#   1. GITHUB_REF_NAME: Needed by your build scripts.</span>
<span class="c">#   2. PATH: Ensures sudo can find 'mkosi' from $HOME/.local/bin (of the user who ran export).</span>

<span class="nb">echo</span> <span class="s2">"Running the build via 'sudo -E make build-iso'..."</span>
<span class="nb">sudo</span> <span class="nt">-E</span> make build-iso
</code></pre></div></div>

<p><strong>b. Troubleshooting Build Failures:</strong>
If the build fails:</p>
<ul>
  <li><strong>Examine <code class="language-plaintext highlighter-rouge">make</code> output:</strong> Look for the first error message.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">mkosi</code> logs:</strong> <code class="language-plaintext highlighter-rouge">mkosi</code> often creates detailed logs. Check for logs inside the <code class="language-plaintext highlighter-rouge">mkosi.output/</code> directory or its subdirectories (e.g., <code class="language-plaintext highlighter-rouge">mkosi.output/*.log</code>, <code class="language-plaintext highlighter-rouge">mkosi.output/build-*/</code>).</li>
  <li><strong>Verbose <code class="language-plaintext highlighter-rouge">make</code>:</strong> If your <code class="language-plaintext highlighter-rouge">Makefile</code> supports it, try: <code class="language-plaintext highlighter-rouge">sudo -E make build-iso VERBOSE=1</code> (or similar flags like <code class="language-plaintext highlighter-rouge">V=1</code>) for more detailed command output.</li>
  <li><strong>Incus issues:</strong> If <code class="language-plaintext highlighter-rouge">mkosi</code> uses Incus containers for the build:
    <ul>
      <li>Check container status: <code class="language-plaintext highlighter-rouge">sudo incus list</code></li>
      <li>View container logs: <code class="language-plaintext highlighter-rouge">sudo incus logs &lt;container_name_shown_in_list_or_mkosi_output&gt;</code></li>
    </ul>
  </li>
  <li><strong>Permissions:</strong> Ensure <code class="language-plaintext highlighter-rouge">sudo -E</code> was used. Double-check file permissions for scripts or configuration files used by <code class="language-plaintext highlighter-rouge">make</code> or <code class="language-plaintext highlighter-rouge">mkosi</code>.</li>
  <li><strong>Dependencies:</strong> Verify all packages from step 2.c were installed successfully.</li>
</ul>

<p><strong>c. Organize Output Files:</strong>
After a successful build, <code class="language-plaintext highlighter-rouge">mkosi</code> places output files (typically in <code class="language-plaintext highlighter-rouge">mkosi.output/</code>). The workflow then moves these.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> upload
<span class="nb">echo</span> <span class="s2">"Moving build artifacts to 'upload/' directory..."</span>

<span class="c"># Adjust these mv commands if your output filenames differ based on mkosi config or GITHUB_REF_NAME.</span>
<span class="c"># The glob patterns like *.usr-x86-64.* should handle variations.</span>
<span class="nb">mv </span>mkosi.output/debug.raw upload/
<span class="nb">mv </span>mkosi.output/incus.raw upload/

<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.raw upload/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.img
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.iso upload/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.iso
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.efi upload/
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.usr-x86-64.<span class="k">*</span> upload/
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.usr-x86-64-verity.<span class="k">*</span> upload/
<span class="nb">mv </span>mkosi.output/IncusOS_<span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span>.usr-x86-64-verity-sig.<span class="k">*</span> upload/

<span class="nb">echo</span> <span class="s2">"Files moved to 'upload/' directory:"</span>
<span class="nb">ls</span> <span class="nt">-lh</span> upload/
</code></pre></div></div>
<p>If files are missing, review the build logs from <code class="language-plaintext highlighter-rouge">make build-iso</code> to see what <code class="language-plaintext highlighter-rouge">mkosi</code> actually produced in <code class="language-plaintext highlighter-rouge">mkosi.output/</code>.</p>

<hr />

<h3 id="5-compressing-the-artifacts">5. Compressing the Artifacts</h3>

<p>The workflow compresses the built files using <code class="language-plaintext highlighter-rouge">gzip</code>.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install gzip if not already present</span>
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nb">gzip

echo</span> <span class="s2">"Compressing files in 'upload/' directory..."</span>
<span class="nb">cd </span>upload
<span class="k">for </span>i <span class="k">in</span> <span class="k">*</span><span class="p">;</span> <span class="k">do
  if</span> <span class="o">[</span> <span class="nt">-f</span> <span class="s2">"</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span> <span class="c"># Check if it's a regular file</span>
    <span class="nb">echo</span> <span class="s2">"Compressing </span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">..."</span>
    <span class="nb">gzip</span> <span class="nt">-9</span> <span class="s2">"</span><span class="k">${</span><span class="nv">i</span><span class="k">}</span><span class="s2">"</span>
  <span class="k">fi
done
</span><span class="nb">cd</span> .. <span class="c"># Return to repository root</span>

<span class="nb">echo</span> <span class="s2">"Compressed files in 'upload/' directory:"</span>
<span class="nb">ls</span> <span class="nt">-lh</span> upload/
</code></pre></div></div>

<hr />

<h3 id="6-managing-build-artifacts-local-release">6. Managing Build Artifacts (Local “Release”)</h3>

<p>The GitHub Actions workflow uploads these compressed files to a GitHub Release. Locally, your “release” artifacts are in the <code class="language-plaintext highlighter-rouge">upload/</code> directory.</p>

<p>You can:</p>
<ul>
  <li>Copy them to another location for testing or distribution.</li>
  <li>Optionally, create an actual GitHub Release from your local machine using the GitHub CLI (<code class="language-plaintext highlighter-rouge">gh</code>):
    <ol>
      <li><strong>Install GitHub CLI:</strong> (If not already installed. Instructions at <a href="https://cli.github.com/">cli.github.com</a>)
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Example for Debian/Ubuntu:</span>
<span class="nb">type</span> <span class="nt">-p</span> curl <span class="o">&gt;</span>/dev/null <span class="o">||</span> <span class="nb">sudo </span>apt <span class="nb">install </span>curl <span class="nt">-y</span>
curl <span class="nt">-fsSL</span> https://cli.github.com/packages/githubcli-archive-keyring.gpg | <span class="nb">sudo dd </span><span class="nv">of</span><span class="o">=</span>/usr/share/keyrings/githubcli-archive-keyring.gpg <span class="se">\</span>
<span class="o">&amp;&amp;</span> <span class="nb">sudo chmod </span>go+r /usr/share/keyrings/githubcli-archive-keyring.gpg <span class="se">\</span>
<span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"deb [arch=</span><span class="si">$(</span>dpkg <span class="nt">--print-architecture</span><span class="si">)</span><span class="s2"> signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main"</span> | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/github-cli.list <span class="o">&gt;</span> /dev/null <span class="se">\</span>
<span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt update <span class="se">\</span>
<span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt <span class="nb">install </span>gh <span class="nt">-y</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Authenticate <code class="language-plaintext highlighter-rouge">gh</code> CLI:</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gh auth login
</code></pre></div>        </div>
      </li>
      <li><strong>Create Release and Upload Files:</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure you are in the root of your repository directory</span>
gh release create <span class="s2">"</span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span> ./upload/<span class="k">*</span> <span class="nt">--title</span> <span class="s2">"Release </span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--notes</span> <span class="s2">"Locally built release for </span><span class="k">${</span><span class="nv">GITHUB_REF_NAME</span><span class="k">}</span><span class="s2">"</span>
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ul>

<hr />

<h3 id="7-important-considerations">7. Important Considerations</h3>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">Makefile</code> and <code class="language-plaintext highlighter-rouge">mkosi</code> Configurations:</strong> This tutorial heavily relies on your project’s <code class="language-plaintext highlighter-rouge">Makefile</code> (with the <code class="language-plaintext highlighter-rouge">build-iso</code> target) and <code class="language-plaintext highlighter-rouge">mkosi</code> configuration files (<code class="language-plaintext highlighter-rouge">mkosi.conf</code>, <code class="language-plaintext highlighter-rouge">mkosi.local.conf</code>, <code class="language-plaintext highlighter-rouge">mkosi.conf.d/*</code>, <code class="language-plaintext highlighter-rouge">mkosi.build</code> scripts) being correct and present in your repository.</li>
  <li><strong>Root Privileges (<code class="language-plaintext highlighter-rouge">sudo -E</code>):</strong> Be mindful of commands requiring <code class="language-plaintext highlighter-rouge">sudo</code>. Using <code class="language-plaintext highlighter-rouge">sudo -E</code> is critical for <code class="language-plaintext highlighter-rouge">make build-iso</code> to ensure it inherits necessary environment variables like your modified <code class="language-plaintext highlighter-rouge">PATH</code> (for <code class="language-plaintext highlighter-rouge">mkosi</code>) and <code class="language-plaintext highlighter-rouge">GITHUB_REF_NAME</code>.</li>
  <li><strong>Environment Differences:</strong> A local environment will always have subtle differences from a GitHub Actions runner.</li>
  <li><strong>Finnix Specifics:</strong> If using Finnix, remember its live nature means installed packages or system changes are typically not persistent across reboots unless you’ve configured persistence. For a single build session, this is usually fine.</li>
  <li><strong>Clean Builds:</strong> For truly reproducible builds, consider cleaning previous build outputs (e.g., <code class="language-plaintext highlighter-rouge">mkosi.output/</code>, <code class="language-plaintext highlighter-rouge">upload/</code>) before starting a new build. Your <code class="language-plaintext highlighter-rouge">Makefile</code> might have a <code class="language-plaintext highlighter-rouge">clean</code> target: <code class="language-plaintext highlighter-rouge">sudo -E make clean</code>.</li>
  <li><strong>Resource Intensive:</strong> OS image building can be very demanding on CPU, RAM, and disk I/O.</li>
  <li><strong>Alternative: Containerized Builds:</strong> For higher fidelity reproduction and isolation, consider running the entire build process within a Docker or Podman container based on a <code class="language-plaintext highlighter-rouge">debian:12</code> image. This is more complex to set up but offers a cleaner environment.</li>
</ul>

<hr />]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">restic to transfer files</title><link href="https://ib.bsb.br/restic-to-transfer-files/" rel="alternate" type="text/html" title="restic to transfer files" /><published>2025-05-13T00:00:00+00:00</published><updated>2025-05-15T12:39:44+00:00</updated><id>https://ib.bsb.br/restic-to-transfer-files</id><content type="html" xml:base="https://ib.bsb.br/restic-to-transfer-files/"><![CDATA[<p>This guide uses the following specific paths:</p>
<ul>
  <li><strong>Source File:</strong> <code class="language-plaintext highlighter-rouge">/opt/data_files/original_large.img</code></li>
  <li><strong>External Drive Mount Point:</strong> <code class="language-plaintext highlighter-rouge">/media/my_external_drive</code></li>
  <li><strong>Restic Repository on External Drive:</strong> <code class="language-plaintext highlighter-rouge">/media/my_external_drive/restic_img_repo</code></li>
  <li><strong>Final Destination for Restored File:</strong> <code class="language-plaintext highlighter-rouge">/media/my_external_drive/final_copied_image.img</code></li>
</ul>

<p>Adjust these paths to match your actual file locations and drive configuration.</p>

<hr />

<h1 id="using-restic-to-transfer-a-large-img-file-in-debian">Using <code class="language-plaintext highlighter-rouge">restic</code> to Transfer a Large <code class="language-plaintext highlighter-rouge">.img</code> File in Debian</h1>

<h2 id="1-install-restic">1. Install <code class="language-plaintext highlighter-rouge">restic</code></h2>

<p>If <code class="language-plaintext highlighter-rouge">restic</code> is not already installed:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>restic <span class="nt">-y</span>
</code></pre></div></div>

<h2 id="2-set-repository-password">2. Set Repository Password</h2>

<p><code class="language-plaintext highlighter-rouge">restic</code> encrypts all data in its repository. You need to set a password for it. Choose a strong, unique password and <strong>store it securely</strong>. If you lose this password, your backed-up data will be irrecoverable.</p>

<p>Export the password as an environment variable for the current terminal session:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">RESTIC_PASSWORD</span><span class="o">=</span><span class="s1">'your-chosen-strong-password'</span>
</code></pre></div></div>
<p>(Replace <code class="language-plaintext highlighter-rouge">'your-chosen-strong-password'</code> with your actual password). You’ll need to do this in any new terminal or add it to your shell’s startup file (e.g., <code class="language-plaintext highlighter-rouge">~/.bashrc</code>) for persistence.</p>

<h2 id="3-initialize-the-restic-repository">3. Initialize the <code class="language-plaintext highlighter-rouge">restic</code> Repository</h2>

<p>Create the <code class="language-plaintext highlighter-rouge">restic</code> repository on your external drive. This only needs to be done once for a new repository.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>restic <span class="nt">-r</span> /media/my_external_drive/restic_img_repo init
</code></pre></div></div>
<p><strong>Expected output on success:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>created restic repository XXXXXXXX at /media/my_external_drive/restic_img_repo

Please note that knowledge of your password is required to access
the repository. Losing your password means that your data is
irreversibly lost.
</code></pre></div></div>

<h2 id="4-back-up-the-img-file-to-the-repository">4. Back Up the <code class="language-plaintext highlighter-rouge">.img</code> File to the Repository</h2>

<p>This step copies the source file into the <code class="language-plaintext highlighter-rouge">restic</code> repository. <code class="language-plaintext highlighter-rouge">restic</code> shows progress and can resume if interrupted; simply re-run the same command.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>restic <span class="nt">-r</span> /media/my_external_drive/restic_img_repo backup /opt/data_files/original_large.img <span class="nt">--verbose</span>
</code></pre></div></div>
<ul>
  <li><code class="language-plaintext highlighter-rouge">-r /media/my_external_drive/restic_img_repo</code>: Specifies the repository.</li>
  <li><code class="language-plaintext highlighter-rouge">backup /opt/data_files/original_large.img</code>: Backs up the specified file.</li>
  <li><code class="language-plaintext highlighter-rouge">--verbose</code>: Provides detailed progress (percentage, speed, ETA).</li>
</ul>

<p>Note the snapshot ID outputted upon completion (e.g., <code class="language-plaintext highlighter-rouge">snapshot abcdef01 saved</code>). You can usually use <code class="language-plaintext highlighter-rouge">latest</code> instead of the ID.</p>

<h2 id="5-restore-the-img-file-from-the-repository">5. Restore the <code class="language-plaintext highlighter-rouge">.img</code> File from the Repository</h2>

<p>This step extracts the file from the repository to a standard file on your external drive.
<code class="language-plaintext highlighter-rouge">restic</code> restores files with their original path structure relative to the backup root, under the <code class="language-plaintext highlighter-rouge">--target</code> directory.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>restic <span class="nt">-r</span> /media/my_external_drive/restic_img_repo restore latest <span class="nt">--target</span> /media/my_external_drive/restore_temp <span class="nt">--include</span> /opt/data_files/original_large.img <span class="nt">--verbose</span>
</code></pre></div></div>
<ul>
  <li><code class="language-plaintext highlighter-rouge">restore latest</code>: Restores from the most recent snapshot.</li>
  <li><code class="language-plaintext highlighter-rouge">--target /media/my_external_drive/restore_temp</code>: Base directory for restoration. A temporary directory name is used here for clarity before the final move.</li>
  <li><code class="language-plaintext highlighter-rouge">--include /opt/data_files/original_large.img</code>: Specifies only this file should be restored from the snapshot.</li>
</ul>

<p>This command will restore the file to: <code class="language-plaintext highlighter-rouge">/media/my_external_drive/restore_temp/opt/data_files/original_large.img</code>.</p>

<h2 id="6-move-the-restored-file-to-its-final-location">6. Move the Restored File to its Final Location</h2>

<p>Move the restored file from the temporary restoration path to your desired final path and name.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mv</span> /media/my_external_drive/restore_temp/opt/data_files/original_large.img /media/my_external_drive/final_copied_image.img
</code></pre></div></div>
<p>After the move, you can remove the temporary directory structure if it’s empty:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">rmdir</span> <span class="nt">-p</span> /media/my_external_drive/restore_temp/opt/data_files
</code></pre></div></div>
<p>(The <code class="language-plaintext highlighter-rouge">-p</code> option removes parent directories if they become empty).</p>

<h2 id="7-verify-file-integrity">7. Verify File Integrity</h2>

<p>Compare checksums of the original source file and the final restored file.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sha256sum</span> /opt/data_files/original_large.img /media/my_external_drive/final_copied_image.img
</code></pre></div></div>
<p>The two checksums <strong>must match exactly</strong>. You can also use <code class="language-plaintext highlighter-rouge">md5sum</code>.</p>

<h2 id="important-notes">Important Notes</h2>

<ul>
  <li><strong>Password:</strong> Your <code class="language-plaintext highlighter-rouge">RESTIC_PASSWORD</code> is vital. Loss of the password means loss of data.</li>
  <li><strong>Disk Space:</strong> The external drive needs space for the <code class="language-plaintext highlighter-rouge">restic</code> repository (roughly the size of the <code class="language-plaintext highlighter-rouge">.img</code> file) AND for the fully restored <code class="language-plaintext highlighter-rouge">.img</code> file simultaneously during this process.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">restic</code> is a Backup Tool:</strong> This method uses backup/restore. The repository contains a versioned, deduplicated, and encrypted copy.</li>
  <li><strong>Metadata:</strong> <code class="language-plaintext highlighter-rouge">restic</code> preserves file permissions and modification times.</li>
  <li><strong>Interrupts:</strong> <code class="language-plaintext highlighter-rouge">restic backup</code> and <code class="language-plaintext highlighter-rouge">restic restore</code> operations are generally resumable by re-running the same command.</li>
  <li><strong>Cleanup (Optional):</strong> If you no longer need the backup in the <code class="language-plaintext highlighter-rouge">restic</code> repository after confirming the transfer:
    <ol>
      <li>List snapshots: <code class="language-plaintext highlighter-rouge">restic -r /media/my_external_drive/restic_img_repo snapshots</code></li>
      <li>Forget the snapshot(s) and prune data: <code class="language-plaintext highlighter-rouge">restic -r /media/my_external_drive/restic_img_repo forget &lt;SNAPSHOT_ID_or_latest&gt; --prune</code></li>
      <li>To completely remove the repository: <code class="language-plaintext highlighter-rouge">rm -rf /media/my_external_drive/restic_img_repo</code> (Use <code class="language-plaintext highlighter-rouge">rm -rf</code> with extreme caution).</li>
    </ol>
  </li>
</ul>

<h1 id="integrating-restic-with-storj-for-decentralized-cloud-backups">Integrating Restic with Storj for Decentralized Cloud Backups</h1>

<p>Storj is a decentralized cloud object storage platform that offers a compelling backend solution for Restic backups due to its distributed architecture, S3 compatibility, end-to-end encryption principles, and potentially competitive pricing. This section provides a comprehensive guide to configuring and using Restic with Storj, primarily leveraging Rclone as the intermediary.</p>

<h3 id="1-introduction-to-storj-for-restic-users">1. Introduction to Storj for Restic Users</h3>

<p>Storj provides enterprise-grade, globally distributed cloud object storage. Its key features relevant to Restic users include:</p>
<ul>
  <li><strong>Decentralization:</strong> Files are encrypted, erasure-coded, and spread across a vast network of independent storage nodes, enhancing durability and availability.</li>
  <li><strong>Security:</strong> Storj emphasizes zero-trust security. When combined with Restic’s client-side end-to-end encryption, your backup data remains confidential even from Storj and its node operators.</li>
  <li><strong>S3-Compatible API:</strong> Storj offers an Amazon S3-compatible API, allowing tools like Rclone (and by extension, Restic) to interact with it seamlessly.</li>
  <li><strong>Native Integration Potential:</strong> Rclone also supports a native Storj integration (via Uplink CLI concepts), which can offer client-side erasure coding for path encryption and potentially optimized performance.</li>
  <li><strong>Cost-Effectiveness:</strong> Storj’s pricing model (typically per GB stored and per GB downloaded) can be attractive, especially when paired with Restic’s efficient deduplication, which minimizes storage footprint.</li>
</ul>

<p>Using Restic with Storj via Rclone allows you to combine Restic’s robust backup features with Storj’s resilient and secure decentralized storage.</p>

<h3 id="2-prerequisites">2. Prerequisites</h3>

<p>Before you begin, ensure you have the following:</p>
<ol>
  <li><strong>A Storj Account:</strong> Sign up at <a href="https://storj.io/signup?partner=restic">Storj.io</a>. New accounts often come with a free trial allowance.</li>
  <li><strong>Restic Installed:</strong> Download and install the Restic binary for your operating system from the <a href="https://github.com/restic/restic/releases">official Restic releases page</a> or via your system’s package manager. Ensure it’s in your PATH.</li>
  <li><strong>Rclone Installed:</strong> Download and install Rclone from the <a href="https://rclone.org/install/">official Rclone website</a>. Ensure it’s in your PATH.</li>
</ol>

<h3 id="3-rclone-configuration-for-storj">3. Rclone Configuration for Storj</h3>

<p>Rclone is the bridge between Restic and Storj. You need to configure an Rclone “remote” that points to your Storj account. Storj can be accessed by Rclone in two main ways. The <strong>S3-Compatible Gateway</strong> (Method 1) is often simpler to set up if you’re familiar with S3 credentials and offers broad compatibility. The <strong>Native Storj Integration</strong> (Method 2) uses Storj’s Uplink protocols, may offer client-side erasure coding for path encryption, and could have different performance characteristics. Choose the method that best fits your technical comfort and requirements.</p>

<h4 id="31-method-1-configuring-rclone-for-storj-via-s3-compatible-gateway">3.1. Method 1: Configuring Rclone for Storj via S3-Compatible Gateway</h4>

<p>This method uses Storj’s S3-compatible API endpoint and requires S3 credentials (access key and secret key).</p>

<p><strong>Step 1: Generate S3 Credentials in Storj Console</strong></p>
<ol>
  <li>Log in to your Storj account.</li>
  <li>Navigate to <strong>Access Keys</strong> (or a similarly named section for S3 credentials).</li>
  <li>Create a new Access Key.
    <ul>
      <li>Give it a descriptive name (e.g., <code class="language-plaintext highlighter-rouge">restic-backup-s3-creds</code>).</li>
      <li>Set appropriate permissions (Read, Write, List, Delete for the buckets Restic will use).</li>
      <li>Note down the <strong>Access Key ID</strong>, <strong>Secret Access Key</strong>, and the <strong>S3 Gateway Endpoint</strong> (e.g., <code class="language-plaintext highlighter-rouge">gateway.storjshare.io</code>).</li>
    </ul>
  </li>
</ol>

<p><strong>Step 2: Configure Rclone Remote for S3 Gateway</strong>
You can configure Rclone by editing its configuration file directly (find its location with <code class="language-plaintext highlighter-rouge">rclone config file</code>) or by using the interactive <code class="language-plaintext highlighter-rouge">rclone config</code> setup.</p>

<ul>
  <li><strong>Editing <code class="language-plaintext highlighter-rouge">rclone.conf</code> directly:</strong>
Add a new remote section to your <code class="language-plaintext highlighter-rouge">rclone.conf</code> file (e.g., <code class="language-plaintext highlighter-rouge">~/.config/rclone/rclone.conf</code>):
    <div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[storj_s3]</span>
<span class="py">type</span> <span class="p">=</span> <span class="s">s3</span>
<span class="py">provider</span> <span class="p">=</span> <span class="s">Storj</span>
<span class="py">access_key_id</span> <span class="p">=</span> <span class="s">YOUR_STORJ_ACCESS_KEY_ID</span>
<span class="py">secret_access_key</span> <span class="p">=</span> <span class="s">YOUR_STORJ_SECRET_ACCESS_KEY</span>
<span class="py">endpoint</span> <span class="p">=</span> <span class="s">gateway.storjshare.io</span>
<span class="c"># Optional: Define a specific region if needed, though often not required for Storj's global gateway
# region = us-east-1
# Optional: Storj docs sometimes recommend disabling checksum for S3 gateway with rclone
# disable_checksum = true
# Optional: Rclone's 'chunk_size' (e.g., '64M') controls how Rclone itself chunks large files (Restic packs in this case) 
# for multipart upload to S3-compatible storage. This is distinct from Restic's '--pack-size'. 
# Adjusting Rclone's 'chunk_size' might be relevant for optimizing uploads of very large Restic packs 
# over unstable connections, but Restic's default pack handling is usually sufficient.
# chunk_size = 64M 
</span></code></pre></div>    </div>
    <p>Replace <code class="language-plaintext highlighter-rouge">YOUR_STORJ_ACCESS_KEY_ID</code> and <code class="language-plaintext highlighter-rouge">YOUR_STORJ_SECRET_ACCESS_KEY</code> with the credentials you generated.</p>
  </li>
  <li><strong>Using <code class="language-plaintext highlighter-rouge">rclone config</code> (Interactive):</strong>
    <ol>
      <li>Run <code class="language-plaintext highlighter-rouge">rclone config</code>.</li>
      <li>Choose <code class="language-plaintext highlighter-rouge">n</code> for a new remote.</li>
      <li>Enter a name, e.g., <code class="language-plaintext highlighter-rouge">storj_s3</code>.</li>
      <li>For “Type of storage to configure,” choose <code class="language-plaintext highlighter-rouge">s3</code> (Amazon S3 Compliant Storage Providers).</li>
      <li>For “S3 Provider,” choose <code class="language-plaintext highlighter-rouge">Storj</code>.</li>
      <li>Enter your <code class="language-plaintext highlighter-rouge">access_key_id</code> and <code class="language-plaintext highlighter-rouge">secret_access_key</code>.</li>
      <li>For “Endpoint for S3 API,” enter the Storj S3 gateway (e.g., <code class="language-plaintext highlighter-rouge">gateway.storjshare.io</code>).</li>
      <li>You can typically leave other options (region, location_constraint, acl) as default or blank unless you have specific requirements.</li>
      <li>Save the configuration.</li>
    </ol>
  </li>
</ul>

<h4 id="32-method-2-configuring-rclone-for-native-storj-integration-uplink">3.2. Method 2: Configuring Rclone for Native Storj Integration (Uplink)</h4>

<p>This method uses Storj’s native protocols via the Uplink library integrated into Rclone. It might offer client-side erasure coding and potentially different performance characteristics. It typically uses an Access Grant or an API Key and Encryption Passphrase.</p>

<p><strong>Step 1: Obtain Native Access Credentials</strong></p>
<ul>
  <li><strong>Access Grant:</strong> This is a serialized string that bundles permissions, API key, satellite address, and encryption passphrase. You might generate this via the Uplink CLI or receive it if someone is granting you access.</li>
  <li><strong>API Key &amp; Encryption Passphrase:</strong> You can generate an API key from the Storj console and then use it with your chosen encryption passphrase.</li>
</ul>

<p><strong>Step 2: Configure Rclone Remote for Native Storj</strong>
Use the <code class="language-plaintext highlighter-rouge">rclone config</code> interactive setup:</p>
<ol>
  <li>Run <code class="language-plaintext highlighter-rouge">rclone config</code>.</li>
  <li>Choose <code class="language-plaintext highlighter-rouge">n</code> for a new remote.</li>
  <li>Enter a name, e.g., <code class="language-plaintext highlighter-rouge">storj_native</code>.</li>
  <li>For “Type of storage to configure,” find and choose <code class="language-plaintext highlighter-rouge">storj</code> (Storj Decentralized Cloud Storage).</li>
  <li>Rclone will then prompt for the authentication method:
    <ul>
      <li><strong>Existing Access Grant:</strong> If you have an access grant string, choose this option and paste the grant.</li>
      <li><strong>New Access Grant (API Key &amp; Passphrase):</strong> If you have an API key and want to use a new or existing encryption passphrase:
        <ul>
          <li>Select the appropriate satellite address (e.g., <code class="language-plaintext highlighter-rouge">us-central-1.storj.io</code>, <code class="language-plaintext highlighter-rouge">europe-west-1.storj.io</code>, <code class="language-plaintext highlighter-rouge">asia-east-1.storj.io</code>).</li>
          <li>Enter your API Key.</li>
          <li>Enter your encryption passphrase (this is critical for encrypting/decrypting file paths and metadata <em>before</em> Restic’s own encryption layer).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Review the summary and save the configuration.
    <ul>
      <li><em>Note:</em> Remote-specific tuning flags for the native Storj Rclone remote (like a hypothetical <code class="language-plaintext highlighter-rouge">upload_nolarge_chunks = true</code>) would be added directly to this remote’s section in <code class="language-plaintext highlighter-rouge">rclone.conf</code> if needed, rather than typically passed via <code class="language-plaintext highlighter-rouge">restic -o rclone.args</code>.</li>
    </ul>
  </li>
</ol>

<p><strong>Step 3: Verify Rclone Remote</strong>
After configuring, test your Rclone remote:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rclone lsd storj_s3:  <span class="c"># or storj_native:</span>
<span class="c"># This should list any buckets you have. If it's a new account, it will be empty.</span>
</code></pre></div></div>

<h3 id="4-restic-repository-setup-on-storj">4. Restic Repository Setup on Storj</h3>

<p><strong>Step 1: Create a Storj Bucket</strong>
Restic needs a bucket (and optionally a path within it) to store its repository. Use Rclone to create a bucket if it doesn’t exist.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Replace 'your-restic-bucket' with your desired bucket name</span>
<span class="c"># Replace 'storj_s3' with your configured Rclone remote name (storj_s3 or storj_native)</span>
rclone <span class="nb">mkdir </span>storj_s3:your-restic-bucket
</code></pre></div></div>
<p>Bucket names must be globally unique if using the S3 gateway, or unique within your project for native access.</p>

<p><strong>Step 2: Initialize the Restic Repository</strong>
Now, initialize the Restic repository within the bucket using the Rclone backend.
Choose a strong, unique password for your Restic repository. <strong>Losing this Restic password means your backup data is irrecoverably lost, regardless of Storj access.</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Define your Restic repository location using the Rclone remote</span>
<span class="c"># Format: rclone:&lt;rclone-remote-name&gt;:&lt;bucket-name&gt;/&lt;path-within-bucket&gt;</span>
<span class="c"># The path-within-bucket is optional but recommended for organization.</span>
<span class="nb">export </span><span class="nv">RESTIC_REPOSITORY</span><span class="o">=</span><span class="s2">"rclone:storj_s3:your-restic-bucket/my-server-backups"</span>

<span class="c"># Set your Restic password (choose one method)</span>
<span class="nb">export </span><span class="nv">RESTIC_PASSWORD</span><span class="o">=</span><span class="s1">'your-super-strong-restic-password'</span>
<span class="c"># OR use a password file:</span>
<span class="c"># echo 'your-super-strong-restic-password' &gt; ~/.restic-storj-pass</span>
<span class="c"># chmod 600 ~/.restic-storj-pass</span>
<span class="c"># export RESTIC_PASSWORD_FILE=~/.restic-storj-pass</span>

<span class="c"># Initialize the repository</span>
restic init
</code></pre></div></div>
<p>On success, Restic will confirm repository creation.</p>

<h3 id="5-backup-operations-with-storj">5. Backup Operations with Storj</h3>

<p>With the repository initialized, you can start backing up data.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Example: Backing up your home directory</span>
restic backup ~/ <span class="nt">--verbose</span> <span class="nt">--tag</span> home_backup_<span class="si">$(</span><span class="nb">date</span> +%Y%m%d<span class="si">)</span>

<span class="c"># Example: Backing up specific project directories</span>
restic backup /srv/projectA /var/www/projectB <span class="nt">--verbose</span> <span class="nt">--tag</span> web_projects
</code></pre></div></div>

<p><strong>Storj-Specific Optimizations and Considerations:</strong></p>

<ul>
  <li><strong>Pack Size (<code class="language-plaintext highlighter-rouge">--pack-size</code>):</strong>
The Storj documentation (from <code class="language-plaintext highlighter-rouge">raw_data</code>) recommends setting Restic’s pack size to 60 MiB. This is suggested because it aligns well with Storj’s underlying segment sizes (often around 64MiB), potentially minimizing padding or overhead per segment and improving storage efficiency or transfer performance on their network.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>restic backup /data/to/backup <span class="nt">--pack-size</span><span class="o">=</span>60
</code></pre></div>    </div>
    <p>While Restic might be “not very precise” and actual pack files may be slightly larger, this is a good target.</p>
  </li>
  <li><strong>Rclone Options via Restic (<code class="language-plaintext highlighter-rouge">-o</code> or <code class="language-plaintext highlighter-rouge">--option</code>):</strong>
You can pass arguments to the underlying <code class="language-plaintext highlighter-rouge">rclone serve restic</code> command that Restic invokes. These are best used for flags affecting the <code class="language-plaintext highlighter-rouge">serve</code> command itself or general Rclone behavior during that operation.
    <ul>
      <li><strong>Connection Parallelism (<code class="language-plaintext highlighter-rouge">rclone.connections</code>):</strong>
The Storj documentation draft mentions: <code class="language-plaintext highlighter-rouge">Passing -o rclone.connections=1 reduces the Rclone parallelism to a single upload. The Storj backend will still open multiple connections to storage nodes. Use this option to reduce the stress on your router in case of failing uploads with the default parallelism.</code>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>restic <span class="nt">-o</span> rclone.connections<span class="o">=</span>1 backup /data/to/backup
</code></pre></div>        </div>
        <p>Conversely, on very stable, high-bandwidth connections, you <em>might</em> experiment with higher values (e.g., <code class="language-plaintext highlighter-rouge">rclone.connections=4</code> or <code class="language-plaintext highlighter-rouge">8</code>), but monitor for errors. The default for Restic’s Rclone backend is 5 connections.</p>
      </li>
      <li><strong>Bandwidth Limiting (via <code class="language-plaintext highlighter-rouge">rclone.args</code>):</strong>
If backups saturate your internet connection or cause instability, you can limit Rclone’s bandwidth by passing the <code class="language-plaintext highlighter-rouge">--bwlimit</code> flag to Rclone:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Limit to 5 MiB/s</span>
restic <span class="nt">-o</span> rclone.args<span class="o">=</span><span class="s2">"serve restic --stdio --bwlimit 5M"</span> backup /data/to/backup
</code></pre></div>        </div>
      </li>
      <li><strong>Other Rclone Arguments (<code class="language-plaintext highlighter-rouge">rclone.args</code>):</strong>
For other Rclone flags specific to the <code class="language-plaintext highlighter-rouge">rclone serve restic</code> command:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>restic <span class="nt">-o</span> rclone.args<span class="o">=</span><span class="s2">"serve restic --stdio --stats 1m"</span> backup /data/to/backup
<span class="c"># --stats 1m would make rclone print transfer stats every minute during the 'serve restic' operation.</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>Unix Root Backups:</strong>
When backing up the root directory (<code class="language-plaintext highlighter-rouge">/</code>) on Unix systems, always use <code class="language-plaintext highlighter-rouge">--one-file-system</code> to prevent Restic from trying to back up virtual filesystems like <code class="language-plaintext highlighter-rouge">/proc</code>, <code class="language-plaintext highlighter-rouge">/sys</code>, or other mounted filesystems you don’t intend to include:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>restic backup / <span class="nt">--one-file-system</span> <span class="nt">--exclude</span> /mnt <span class="nt">--exclude</span> /media <span class="nt">--exclude</span> /var/cache <span class="nt">--exclude</span> /tmp
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="6-restore-operations-from-storj">6. Restore Operations from Storj</h3>

<p>Restoring data is straightforward:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Restore the latest snapshot to ~/restore_dir</span>
restic restore latest <span class="nt">--target</span> ~/restore_dir <span class="nt">--verbose</span>

<span class="c"># Restore a specific snapshot by ID</span>
restic restore &lt;snapshot_ID&gt; <span class="nt">--target</span> ~/restore_dir

<span class="c"># Restore specific files/folders from the latest snapshot</span>
restic restore latest <span class="nt">--target</span> ~/restore_dir <span class="nt">--include</span> <span class="s2">"/path/within/snapshot/to/file.txt"</span> <span class="nt">--include</span> <span class="s2">"/another/path/*"</span>
</code></pre></div></div>
<p><strong>Considerations:</strong></p>
<ul>
  <li><strong>Egress Costs:</strong> Restoring large amounts of data from any cloud storage, including Storj, will incur download (egress) bandwidth charges. Factor this into your recovery plan.</li>
  <li><strong>Time:</strong> Download speeds will depend on your internet connection, Storj network performance, and the number/size of files.</li>
</ul>

<h3 id="7-repository-maintenance-on-storj">7. Repository Maintenance on Storj</h3>

<p>Regular maintenance is crucial for managing storage space and ensuring repository health.</p>

<ul>
  <li><strong>Forgetting Old Snapshots (<code class="language-plaintext highlighter-rouge">restic forget</code>):</strong>
Define a retention policy to remove old, unneeded snapshots. Always use <code class="language-plaintext highlighter-rouge">--prune</code> with <code class="language-plaintext highlighter-rouge">forget</code> when interacting with cloud backends to reclaim space.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Keep the last 7 daily, 4 weekly, 6 monthly, 1 yearly snapshots, and prune data</span>
restic forget <span class="nt">--keep-daily</span> 7 <span class="nt">--keep-weekly</span> 4 <span class="nt">--keep-monthly</span> 6 <span class="nt">--keep-yearly</span> 1 <span class="nt">--prune</span>

<span class="c"># Forget a specific snapshot and prune</span>
<span class="c"># restic forget &lt;snapshot_ID&gt; --prune</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Checking Repository Integrity (<code class="language-plaintext highlighter-rouge">restic check</code>):</strong>
Periodically verify the repository’s consistency.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>restic check
</code></pre></div>    </div>
    <p>For a more thorough check that reads all data packs (slower and incurs more download operations/costs on cloud storage):</p>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>restic check <span class="nt">--read-data</span>
<span class="c"># Or check a subset of data:</span>
<span class="c"># restic check --read-data-subset=10%</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="8-performance-considerations-and-troubleshooting-with-storj">8. Performance Considerations and Troubleshooting with Storj</h3>

<p>Based on community discussions (e.g., the Storj forum thread in <code class="language-plaintext highlighter-rouge">raw_data</code>):</p>
<ul>
  <li><strong>“Successful puts less than success threshold” Errors:</strong> This error, sometimes seen with Rclone and Storj, indicates that Rclone couldn’t upload enough erasure-coded pieces of a segment to enough storage nodes to meet the success threshold.
    <ul>
      <li><strong>Possible Causes:</strong> Network instability, router issues (too many connections), very slow individual nodes, or aggressive Rclone transfer settings.</li>
      <li><strong>Mitigation:</strong>
        <ul>
          <li>Try reducing Rclone’s parallelism: <code class="language-plaintext highlighter-rouge">restic -o rclone.connections=1 ...</code></li>
          <li>Limit bandwidth: <code class="language-plaintext highlighter-rouge">restic -o rclone.args="serve restic --stdio --bwlimit &lt;rate&gt;" ...</code></li>
          <li>Ensure your Rclone and Restic versions are up-to-date.</li>
          <li>Check the Storj status page and community forums for any ongoing network issues.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Upload/Download Speeds:</strong>
    <ul>
      <li>Heavily dependent on your internet connection’s upload/download capacity.</li>
      <li>Storj’s decentralized nature means performance can vary.</li>
      <li>Small files can have higher per-file overhead than large files. Restic’s packing helps, but very numerous tiny files can still be slower.</li>
      <li>Initial backups are often slower than subsequent incremental backups due to Restic’s deduplication.</li>
    </ul>
  </li>
  <li><strong>Rclone &amp; Restic Logging:</strong>
    <ul>
      <li>To get detailed logs from Rclone during a Restic operation, you can instruct Restic to pass logging flags to Rclone:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>restic <span class="nt">-o</span> rclone.args<span class="o">=</span><span class="s2">"serve restic --stdio -vv --log-file /tmp/rclone-restic.log"</span> backup ...
</code></pre></div>        </div>
        <p>This will make Rclone itself very verbose. Inspect <code class="language-plaintext highlighter-rouge">/tmp/rclone-restic.log</code>.</p>
      </li>
      <li>For debugging Restic’s own operations and progress, use Restic’s verbosity flags (e.g., <code class="language-plaintext highlighter-rouge">restic -vv backup ...</code>) or set <code class="language-plaintext highlighter-rouge">export RESTIC_PROGRESS_FPS=10</code> for more frequent progress updates from Restic.</li>
    </ul>
  </li>
</ul>

<p>Refer to the <a href="https://forum.storj.io/">Storj Community Forum</a> for ongoing discussions and support, particularly the thread mentioned in the initial draft: <a href="https://forum.storj.io/t/two-more-tech-previews-rclone-and-restic/6072">Two more Tech Previews - RClone and Restic</a> (though note this thread is from 2020 and information may have evolved).</p>

<h3 id="9-security-and-cost-considerations">9. Security and Cost Considerations</h3>

<ul>
  <li><strong>Security:</strong>
    <ul>
      <li><strong>Restic’s End-to-End Encryption:</strong> Your data is encrypted on your machine <em>before</em> it’s sent to Rclone and then to Storj. Only you, with the Restic repository password, can decrypt it. A key benefit of Restic’s client-side encryption is that Storj (or any cloud provider used as a backend) only ever stores encrypted blobs of data. They cannot access your actual file contents or metadata, ensuring a high degree of privacy.</li>
      <li><strong>Storj’s Security:</strong> Storj further encrypts data segments and distributes them. If using native Rclone integration with an encryption passphrase, that adds another layer for metadata path encryption.</li>
      <li><strong>Access Credentials:</strong> Protect your Storj S3 credentials, Access Grants, API keys, and especially your Restic repository password diligently.</li>
    </ul>
  </li>
  <li><strong>Cost:</strong>
    <ul>
      <li><strong>Storage:</strong> Storj charges for the amount of data stored per month (after erasure coding, so 1GB of your data might consume ~2.7GB on the network, but you are typically billed for the 1GB). Restic’s deduplication significantly reduces the actual data stored over time for versioned backups.</li>
      <li><strong>Egress (Download):</strong> Storj charges for data downloaded from the network. This is relevant for restores.</li>
      <li><strong>Operations:</strong> There might be minor charges for certain API operations, but these are usually less significant for typical Restic usage.</li>
      <li>Always refer to the <a href="https://storj.io/dcs/pricing">official Storj pricing page</a> for current details. Restic’s efficiency helps in making Storj a cost-effective option for long-term, secure backups.</li>
    </ul>
  </li>
</ul>]]></content><author><name></name></author><category term="aid&gt;software" /></entry><entry><title type="html">backup bash script</title><link href="https://ib.bsb.br/backup-bash-script/" rel="alternate" type="text/html" title="backup bash script" /><published>2025-05-12T00:00:00+00:00</published><updated>2025-05-12T18:04:46+00:00</updated><id>https://ib.bsb.br/backup-bash-script</id><content type="html" xml:base="https://ib.bsb.br/backup-bash-script/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#!/bin/bash

# ==============================================================================
# Script Name: comprehensive_backup_v2.sh
# Description: Archives (tar) and compresses (zstd) files from a source
#              directory to a destination file. Offers configuration for
#              compression level, progress view (pv), I/O buffering (mbuffer),
#              and exclusions. Includes pre-checks and basic verification steps.
# Author:      AI Assistant
# Version:     2.0
# Usage:       sudo bash comprehensive_backup_v2.sh
# Requirements: bash, sudo, tar, zstd, pv, mbuffer, coreutils (du, df, awk, numfmt, stat)
# ==============================================================================

# --- Script Setup ---
# Exit immediately if a command exits with a non-zero status.
# set -e
# Treat unset variables as an error when substituting.
# set -u
# Cause pipelines to return the exit status of the last command that exited with a non-zero status,
# or zero if no command exited with a non-zero status.
set -o pipefail

# --- Configuration ---
DEFAULT_SOURCE_DIR="/media/usb0"
DEFAULT_DEST_DIR="/media/usb5"
DEFAULT_FILENAME_BASE="backup_usb0"
DEFAULT_COMPRESSION_LEVEL=9 # Good balance (1-19)
DEFAULT_USE_PV="y"
DEFAULT_USE_MBUFFER="y"
DEFAULT_MBUFFER_SIZE="6G" # e.g., 128M, 256M, 512M, 1G
DEFAULT_EXCLUDE_PATTERNS=() # Add patterns like: ('./cache/*' '*.tmp')

# --- Helper Functions ---
check_command() {
  if ! command -v "$1" &amp;&gt; /dev/null; then
    echo "Error: Required command '$1' not found. Please install it (e.g., using 'sudo apt install $1')." &gt;&amp;2
    exit 1
  fi
}

print_separator() {
  printf -- '-%.0s' {1..70}; printf '\n'
}

# --- Root Check ---
if [[ $EUID -ne 0 ]]; then
   echo "Error: This script must be run with sudo privileges for accurate size checks and potentially writing to protected locations." &gt;&amp;2
   exit 1
fi

# --- Dependency Check ---
print_separator
echo "Checking required commands..."
check_command tar
check_command zstd
check_command pv
check_command mbuffer
check_command du
check_command df
check_command awk
check_command numfmt
check_command stat
echo "All required commands found."

# --- User Input / Configuration Override ---
print_separator
echo "Configure Backup Parameters (Press Enter for defaults):"

read -p "Source directory [${DEFAULT_SOURCE_DIR}]: " SOURCE_DIR
SOURCE_DIR=${SOURCE_DIR:-$DEFAULT_SOURCE_DIR}

read -p "Destination directory [${DEFAULT_DEST_DIR}]: " DEST_DIR
DEST_DIR=${DEST_DIR:-$DEFAULT_DEST_DIR}

read -p "Base filename for archive [${DEFAULT_FILENAME_BASE}]: " FILENAME_BASE
FILENAME_BASE=${FILENAME_BASE:-$DEFAULT_FILENAME_BASE}

read -p "Compression level (1=fastest, 19=best, default=${DEFAULT_COMPRESSION_LEVEL}): " COMPRESSION_LEVEL
COMPRESSION_LEVEL=${COMPRESSION_LEVEL:-$DEFAULT_COMPRESSION_LEVEL}
# Basic validation for compression level
if ! [[ "$COMPRESSION_LEVEL" =~ ^[1-9]$|^1[0-9]$ ]]; then # Regex for 1-19
  echo "Invalid compression level. Using default: ${DEFAULT_COMPRESSION_LEVEL}"
  COMPRESSION_LEVEL=$DEFAULT_COMPRESSION_LEVEL
fi

read -p "Use 'pv' for progress monitoring? (Y/n) [${DEFAULT_USE_PV}]: " USE_PV
USE_PV=${USE_PV:-$DEFAULT_USE_PV}

read -p "Use 'mbuffer' for I/O buffering? (y/N) [${DEFAULT_USE_MBUFFER}]: " USE_MBUFFER
USE_MBUFFER=${USE_MBUFFER:-$DEFAULT_USE_MBUFFER}
MBUFFER_SIZE=$DEFAULT_MBUFFER_SIZE
if [[ "${USE_MBUFFER,,}" == "y" ]]; then
  read -p "mbuffer size (e.g., 256M, 1G) [${DEFAULT_MBUFFER_SIZE}]: " MBUFFER_SIZE_INPUT
  MBUFFER_SIZE=${MBUFFER_SIZE_INPUT:-$DEFAULT_MBUFFER_SIZE}
fi

# Exclude pattern input
echo "Enter exclude patterns one by one (relative to source, e.g., './cache/*', '*.tmp'). Press Enter on empty line to finish."
EXCLUDE_PATTERNS=()
while true; do
    read -p "Exclude pattern (or Enter to finish): " pattern
    if [[ -z "$pattern" ]]; then
        break
    fi
    EXCLUDE_PATTERNS+=("$pattern")
done
if [ ${#EXCLUDE_PATTERNS[@]} -eq 0 ]; then
    EXCLUDE_PATTERNS=("${DEFAULT_EXCLUDE_PATTERNS[@]}") # Use default if none entered
fi


# --- Pre-Checks ---
print_separator
echo "Performing Pre-Checks..."

# Check directories
if [ ! -d "$SOURCE_DIR" ]; then
  echo "Error: Source directory '$SOURCE_DIR' not found or not a directory." &gt;&amp;2
  exit 1
fi
if [ ! -d "$DEST_DIR" ]; then
  echo "Error: Destination directory '$DEST_DIR' not found or not a directory." &gt;&amp;2
  exit 1
fi
# Optional: Check if they are mount points (informational)
if ! mountpoint -q "$SOURCE_DIR"; then
    echo "Info: Source directory '$SOURCE_DIR' does not appear to be a distinct mount point."
fi
if ! mountpoint -q "$DEST_DIR"; then
    echo "Info: Destination directory '$DEST_DIR' does not appear to be a distinct mount point."
fi

# Check source size
echo "Calculating source size (this may take a while)..."
SRC_SIZE_BYTES=$(du -sb "$SOURCE_DIR" 2&gt;/dev/null)
DU_EXIT_CODE=$?
if [ $DU_EXIT_CODE -ne 0 ] || [ -z "$SRC_SIZE_BYTES" ]; then
    echo "Error: Could not determine source size (du exit code: $DU_EXIT_CODE). Check permissions for '$SOURCE_DIR'." &gt;&amp;2
    exit 1
fi
SRC_SIZE_BYTES=$(echo "$SRC_SIZE_BYTES" | awk '{print $1}') # Extract number
SRC_SIZE_HUMAN=$(numfmt --to=iec $SRC_SIZE_BYTES)
echo "Source size: ${SRC_SIZE_HUMAN} (${SRC_SIZE_BYTES} bytes)"

# Check if source seems empty
if [ "$SRC_SIZE_BYTES" -le 4096 ]; then # 4096 is typical size of an empty directory metadata
    echo "Warning: Source directory size is very small. It might be empty or contain only empty subdirectories."
fi

# Check destination space
DEST_AVAIL_BYTES=$(df --output=avail -B1 "$DEST_DIR" 2&gt;/dev/null | awk 'NR==2{print $1}')
DF_EXIT_CODE=$?
if [ $DF_EXIT_CODE -ne 0 ] || [ -z "$DEST_AVAIL_BYTES" ]; then
    echo "Error: Could not determine destination available space (df exit code: $DF_EXIT_CODE). Check path '$DEST_DIR'." &gt;&amp;2
    exit 1
fi
DEST_AVAIL_HUMAN=$(numfmt --to=iec $DEST_AVAIL_BYTES)
echo "Destination available space: ${DEST_AVAIL_HUMAN} (${DEST_AVAIL_BYTES} bytes)"

# Space warning
if [ "$SRC_SIZE_BYTES" -gt "$DEST_AVAIL_BYTES" ]; then
  print_separator
  echo "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
  echo "!!! WARNING: Source size (${SRC_SIZE_HUMAN}) is LARGER than available"
  echo "!!!          destination space (${DEST_AVAIL_HUMAN})."
  echo "!!! This operation will ONLY succeed if the data compresses"
  echo "!!! significantly (below ${DEST_AVAIL_HUMAN}). Consider using a higher"
  echo "!!! compression level (current: ${COMPRESSION_LEVEL}) or freeing up space."
  echo "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
  print_separator
elif [ "$DEST_AVAIL_BYTES" -lt "$((SRC_SIZE_BYTES / 2))" ]; then # Heuristic: Warn if available space is less than half the source size
  echo "Warning: Available destination space (${DEST_AVAIL_HUMAN}) is less than half the source size. Ensure compression is effective."
fi

# Construct final destination path and check for collision
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
DEST_FILENAME="${FILENAME_BASE}_L${COMPRESSION_LEVEL}_${TIMESTAMP}.tar.zst"
DEST_FILE_PATH="${DEST_DIR}/${DEST_FILENAME}"

if [ -e "$DEST_FILE_PATH" ]; then
    echo "Error: Destination file '$DEST_FILE_PATH' already exists." &gt;&amp;2
    read -p "Do you want to overwrite it? (y/N): " OVERWRITE_CONFIRM
    if [[ "${OVERWRITE_CONFIRM,,}" != "y" ]]; then
        echo "Aborted by user."
        exit 0
    fi
    echo "Overwriting existing file."
fi

# Final confirmation
print_separator
echo "Backup Summary:"
echo "  Source:      $SOURCE_DIR ($SRC_SIZE_HUMAN)"
echo "  Destination: $DEST_FILE_PATH"
echo "  Compression: Level $COMPRESSION_LEVEL"
echo "  Progress View: ${USE_PV}"
echo "  IO Buffering: ${USE_MBUFFER} (Size: $MBUFFER_SIZE)"
if [ ${#EXCLUDE_PATTERNS[@]} -gt 0 ]; then
    echo "  Exclusions:  ${EXCLUDE_PATTERNS[*]}"
fi
print_separator

read -p "Do you want to proceed with the backup? (y/N): " CONFIRM_PROCEED
if [[ "${CONFIRM_PROCEED,,}" != "y" ]]; then
  echo "Aborted by user."
  exit 0
fi

# --- Build and Execute the Command Pipeline ---
print_separator
echo "Building and executing the command pipeline..."

# Start with tar command (relative paths within archive)
TAR_CMD=("tar" "-cf" "-")

# Add exclude patterns
for pattern in "${EXCLUDE_PATTERNS[@]}"; do
    TAR_CMD+=("--exclude=${pattern}")
done

# Specify source directory context and content
TAR_CMD+=("-C" "$SOURCE_DIR" ".")

# Build the pipeline string array for clarity and execution
PIPELINE_STAGES=()
PIPELINE_STAGES+=("$(printf '%q ' "${TAR_CMD[@]}")") # Stage 0: tar

# Add pv if requested
if [[ "${USE_PV,,}" == "y" ]]; then
  PIPELINE_STAGES+=("| pv -s $SRC_SIZE_BYTES") # Stage 1: pv
fi

# Add mbuffer if requested
if [[ "${USE_MBUFFER,,}" == "y" ]]; then
  PIPELINE_STAGES+=("| mbuffer -m $MBUFFER_SIZE") # Stage 2 (or 1 if no pv): mbuffer
fi

# Add zstd command
PIPELINE_STAGES+=("| zstd -T0 -${COMPRESSION_LEVEL} -o '${DEST_FILE_PATH}'") # Final Stage: zstd

# Combine stages into a single command string for execution
FULL_PIPELINE_CMD="${PIPELINE_STAGES[*]}"

# Check if running inside screen/tmux
if [[ -z "$STY" &amp;&amp; -z "$TMUX" ]]; then
    echo "Warning: Not running inside screen or tmux."
    echo "For long operations, it's highly recommended to run this script"
    echo "within a 'screen' or 'tmux' session to prevent interruptions."
    read -p "Press Enter to continue anyway, or Ctrl+C to stop and restart in screen/tmux."
fi

echo "Starting backup process... This may take a very long time."
echo "Executing: ${FULL_PIPELINE_CMD}"

# Execute the pipeline using bash -c
# Capture PIPESTATUS immediately after execution
bash -c "$FULL_PIPELINE_CMD"; PIPE_STATUS=("${PIPESTATUS[@]}")
EXECUTION_EXIT_CODE=$? # Overall exit code (affected by pipefail)

# --- Post-Execution ---
print_separator
echo "Backup process finished."

# Check exit codes from the pipeline
FINAL_EXIT_CODE=0
STAGE_NAMES=("tar") # Start with tar
if [[ "${USE_PV,,}" == "y" ]]; then STAGE_NAMES+=("pv"); fi
if [[ "${USE_MBUFFER,,}" == "y" ]]; then STAGE_NAMES+=("mbuffer"); fi
STAGE_NAMES+=("zstd")

echo "Checking pipeline exit codes: ${PIPE_STATUS[*]}"
for i in "${!PIPE_STATUS[@]}"; do
    stage_name=${STAGE_NAMES[$i]:-"unknown_stage_$i"}
    exit_code=${PIPE_STATUS[$i]}
    if [ "$exit_code" -ne 0 ]; then
        echo "Error: Pipeline stage '${stage_name}' failed with exit code $exit_code." &gt;&amp;2
        FINAL_EXIT_CODE=$exit_code # Report the first non-zero exit code
    fi
done

# Double check overall exit code if pipefail was active
if [ "$FINAL_EXIT_CODE" -eq 0 ] &amp;&amp; [ "$EXECUTION_EXIT_CODE" -ne 0 ]; then
     echo "Warning: Overall pipeline check reported failure (exit code $EXECUTION_EXIT_CODE), but individual stages seemed okay. Check results carefully." &gt;&amp;2
     # Use the overall code if it indicates failure and individual checks didn't
     FINAL_EXIT_CODE=$EXECUTION_EXIT_CODE
fi


if [ "$FINAL_EXIT_CODE" -eq 0 ]; then
    echo "Pipeline completed successfully (basic check)."
    COMPRESSED_SIZE_BYTES=$(stat -c%s "$DEST_FILE_PATH" 2&gt;/dev/null)
    COMPRESSED_SIZE_HUMAN=$(numfmt --to=iec $COMPRESSED_SIZE_BYTES 2&gt;/dev/null || echo "N/A")
    echo "Archive saved to: ${DEST_FILE_PATH}"
    echo "Compressed size:  ${COMPRESSED_SIZE_HUMAN} (${COMPRESSED_SIZE_BYTES:-N/A} bytes)"
    print_separator
    echo "IMPORTANT: Please verify the integrity of the archive:"
    echo "1. Test compression: zstd -t '${DEST_FILE_PATH}'"
    echo "2. List contents (optional, takes time/CPU): tar --list -I zstd -f '${DEST_FILE_PATH}' | less"
    print_separator
else
    echo "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!" &gt;&amp;2
    echo "!!! ERRORS DETECTED during the backup process (Exit Code: ${FINAL_EXIT_CODE})." &gt;&amp;2
    echo "!!! The archive file '${DEST_FILE_PATH}' may be incomplete or corrupt." &gt;&amp;2
    echo "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!" &gt;&amp;2
    exit $FINAL_EXIT_CODE
fi

exit 0
</code></pre></div></div>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">Fixing rk3588 Init Scripts</title><link href="https://ib.bsb.br/fixing-rk3588-init-scripts/" rel="alternate" type="text/html" title="Fixing rk3588 Init Scripts" /><published>2025-05-12T00:00:00+00:00</published><updated>2025-05-12T21:01:34+00:00</updated><id>https://ib.bsb.br/fixing-rk3588-init-scripts</id><content type="html" xml:base="https://ib.bsb.br/fixing-rk3588-init-scripts/"><![CDATA[<h2 id="whats-happening">What’s Happening</h2>

<p>You encountered warnings about two scripts in your <code class="language-plaintext highlighter-rouge">/etc/init.d/</code> directory:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">mount_usb.sh</code></li>
  <li><code class="language-plaintext highlighter-rouge">gobinet_boot.sh</code></li>
</ul>

<p>These warnings occurred because:</p>

<ol>
  <li>When enabling a service, Debian uses a tool called <code class="language-plaintext highlighter-rouge">insserv</code> to analyze all init scripts and determine their proper boot order.</li>
  <li>This tool requires LSB (Linux Standard Base) headers in each script to understand dependencies and run order.</li>
  <li>Your scripts lack these headers, causing the warnings.</li>
</ol>

<p><strong>Understanding the Core Problem</strong></p>

<ul>
  <li><strong>LSB Tags:</strong> SysV init scripts (those in <code class="language-plaintext highlighter-rouge">/etc/init.d/</code>) use special comment blocks (<code class="language-plaintext highlighter-rouge">### BEGIN INIT INFO ... ### END INIT INFO</code>) called LSB headers. These headers tell the system (via tools like <code class="language-plaintext highlighter-rouge">insserv</code> or <code class="language-plaintext highlighter-rouge">systemd</code>’s compatibility layer) about the service’s dependencies, what runlevels it should start/stop in, and provide descriptions. The warnings you saw mean these headers are missing or incomplete.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">mount_usb.sh</code>:</strong> This script’s logic (<code class="language-plaintext highlighter-rouge">if [ $ACTION == "add" ]</code>) indicates it’s designed to react to dynamic hardware events (USB stick plugged in/out). This is the job of <code class="language-plaintext highlighter-rouge">udev</code>, not the static boot sequence managed by init scripts.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">gobinet_boot.sh</code>:</strong> This script, which runs <code class="language-plaintext highlighter-rouge">quectel-CM</code> after a delay, is a more traditional candidate for a boot service, but it needs the proper LSB structure.</li>
</ul>

<hr />

<p><strong>Part 1: Refactoring <code class="language-plaintext highlighter-rouge">mount_usb.sh</code> (The <code class="language-plaintext highlighter-rouge">udev</code> Approach)</strong></p>

<p>This script should not be in <code class="language-plaintext highlighter-rouge">/etc/init.d/</code>. We’ll move its logic to a helper script called by a <code class="language-plaintext highlighter-rouge">udev</code> rule.</p>

<p><strong>Step 1.1: Create/Move the Helper Script</strong></p>

<p>Let’s place the helper script in <code class="language-plaintext highlighter-rouge">/usr/local/sbin/</code>, a standard location for locally installed system administration scripts.</p>

<p>Original script location: <code class="language-plaintext highlighter-rouge">/etc/init.d/mount_usb.sh</code>
New helper script location: <code class="language-plaintext highlighter-rouge">/usr/local/sbin/mount_usb_helper.sh</code></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo mv</span> /etc/init.d/mount_usb.sh /usr/local/sbin/mount_usb_helper.sh
<span class="nb">sudo chmod</span> +x /usr/local/sbin/mount_usb_helper.sh
</code></pre></div></div>

<p>Now, replace the content of <code class="language-plaintext highlighter-rouge">/usr/local/sbin/mount_usb_helper.sh</code> with this improved version:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c"># Helper script to auto-mount/unmount USB VFAT drives, intended to be called by udev.</span>
<span class="c"># $1 (DEVNAME): Device name (e.g., sdb1) passed by the udev rule via %k.</span>
<span class="c"># $ACTION:      Environment variable (add/remove) set by udev.</span>

<span class="nv">DEVNAME</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">1</span><span class="k">}</span><span class="s2">"</span>
<span class="nv">MOUNT_BASE</span><span class="o">=</span><span class="s2">"/mnt/media"</span> <span class="c"># Or your preferred base path</span>
<span class="nv">MOUNT_POINT</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_BASE</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2">"</span>
<span class="nv">LOG_TAG</span><span class="o">=</span><span class="s2">"usb-mount-helper"</span> <span class="c"># For syslog</span>

<span class="c"># Function for logging to syslog (and optionally a dedicated file)</span>
log_message<span class="o">()</span> <span class="o">{</span>
    logger <span class="nt">-t</span> <span class="s2">"</span><span class="k">${</span><span class="nv">LOG_TAG</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--</span> <span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span> <span class="c"># -- ensures message isn't mistaken for options</span>
    <span class="c"># Optional: echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" &gt;&gt; "/var/log/${LOG_TAG}.log"</span>
<span class="o">}</span>

<span class="k">if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$DEVNAME</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span>log_message <span class="s2">"Error: Device name not provided. Exiting."</span>
    <span class="nb">exit </span>1
<span class="k">fi</span>

<span class="c"># Create the base mount directory if it doesn't exist</span>
<span class="k">if</span> <span class="o">[</span> <span class="o">!</span> <span class="nt">-d</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_BASE</span><span class="k">}</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_BASE</span><span class="k">}</span><span class="s2">"</span>
    <span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> <span class="nt">-ne</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
        </span>log_message <span class="s2">"Error: Could not create base mount directory </span><span class="k">${</span><span class="nv">MOUNT_BASE</span><span class="k">}</span><span class="s2">. Exiting."</span>
        <span class="nb">exit </span>1
    <span class="k">fi
fi

if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$ACTION</span><span class="s2">"</span> <span class="o">==</span> <span class="s2">"add"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span>log_message <span class="s2">"Add event for /dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2">."</span>

    <span class="c"># Check if already mounted using findmnt for reliability</span>
    <span class="k">if </span>findmnt <span class="nt">--source</span> <span class="s2">"/dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--target</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span> <span class="o">&gt;</span> /dev/null<span class="p">;</span> <span class="k">then
        </span>log_message <span class="s2">"/dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2"> already mounted at </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">. Skipping."</span>
        <span class="nb">exit </span>0
    <span class="k">fi

    if</span> <span class="o">[</span> <span class="o">!</span> <span class="nt">-d</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
        </span><span class="nb">mkdir</span> <span class="nt">-p</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span>
        <span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> <span class="nt">-ne</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
            </span>log_message <span class="s2">"Error: Could not create mount point </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">. Exiting."</span>
            <span class="nb">exit </span>1
        <span class="k">fi
    fi</span>

    <span class="c"># Give the system a moment if needed (sometimes helpful for newly appeared devices)</span>
    <span class="nb">sleep </span>1

    <span class="c"># Mount options:</span>
    <span class="c"># - iocharset=utf8: For correct filename encoding.</span>
    <span class="c"># - uid=$(id -u linaro), gid=$(id -g linaro): Mounts as user 'linaro'.</span>
    <span class="c">#   Replace 'linaro' with your desired username or a system user.</span>
    <span class="c">#   Alternatively, use gid=plugdev (if users are in plugdev group) and appropriate fmask/dmask.</span>
    <span class="c"># - fmask=0137, dmask=0027: File/Dir permissions.</span>
    <span class="c">#   fmask=0137 -&gt; owner=rw, group=r, other= --- (640)</span>
    <span class="c">#   dmask=0027 -&gt; owner=rwx, group=rx, other= --- (750)</span>
    <span class="c">#   Adjust as needed. E.g., fmask=0117, dmask=0007 for rwx for owner, rwx for group.</span>
    <span class="c"># - nofail: Prevents boot errors if USB device is problematic or not mountable.</span>
    <span class="c"># - flush: Mounts VFAT with frequent flushing, good for removable media.</span>
    <span class="c"># - sync: Can be used, but 'flush' is often preferred for VFAT on removable media.</span>
    <span class="c">#         Using 'sync' for all I/O can slow things down significantly.</span>
    <span class="c">#         The original script used 'sync' *after* mount, which is less effective.</span>
    <span class="nv">USER_NAME</span><span class="o">=</span><span class="s2">"linaro"</span> <span class="c"># CHANGE THIS to your target user if needed</span>
    <span class="nv">USER_ID</span><span class="o">=</span><span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span> <span class="s2">"</span><span class="nv">$USER_NAME</span><span class="s2">"</span><span class="si">)</span>
    <span class="nv">GROUP_ID</span><span class="o">=</span><span class="si">$(</span><span class="nb">id</span> <span class="nt">-g</span> <span class="s2">"</span><span class="nv">$USER_NAME</span><span class="s2">"</span><span class="si">)</span>

    <span class="k">if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$USER_ID</span><span class="s2">"</span> <span class="o">]</span> <span class="o">||</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$GROUP_ID</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
        </span>log_message <span class="s2">"Error: Could not determine UID/GID for user '</span><span class="nv">$USER_NAME</span><span class="s2">'. Mounting with defaults."</span>
        mount <span class="nt">-t</span> vfat <span class="nt">-o</span> <span class="s2">"iocharset=utf8,nofail,flush"</span> <span class="s2">"/dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span>
    <span class="k">else
        </span>mount <span class="nt">-t</span> vfat <span class="nt">-o</span> <span class="s2">"iocharset=utf8,uid=</span><span class="k">${</span><span class="nv">USER_ID</span><span class="k">}</span><span class="s2">,gid=</span><span class="k">${</span><span class="nv">GROUP_ID</span><span class="k">}</span><span class="s2">,fmask=0137,dmask=0027,nofail,flush"</span> <span class="s2">"/dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span>
    <span class="k">fi

    if</span> <span class="o">[</span> <span class="nv">$?</span> <span class="nt">-eq</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
        </span>log_message <span class="s2">"Successfully mounted /dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2"> to </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">."</span>
    <span class="k">else
        </span>log_message <span class="s2">"Error: Failed to mount /dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2"> to </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">. Cleaning up directory."</span>
        <span class="nb">rmdir</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span> 2&gt;/dev/null <span class="c"># Attempt to remove if empty</span>
        <span class="nb">exit </span>1
    <span class="k">fi

elif</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$ACTION</span><span class="s2">"</span> <span class="o">==</span> <span class="s2">"remove"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span>log_message <span class="s2">"Remove event for device that might be mounted at </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2"> (was /dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2">)."</span>

    <span class="c"># Check if the specific device is mounted at the expected point</span>
    <span class="k">if </span>findmnt <span class="nt">--source</span> <span class="s2">"/dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--target</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span> <span class="o">&gt;</span> /dev/null<span class="p">;</span> <span class="k">then
        </span>umount <span class="nt">-f</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span> <span class="c"># Force unmount</span>
        <span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> <span class="nt">-eq</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
            </span>log_message <span class="s2">"Successfully unmounted </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">."</span>
        <span class="k">else
            </span>log_message <span class="s2">"Warning: Failed to unmount </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">. Attempting lazy unmount."</span>
            umount <span class="nt">-l</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span> <span class="c"># Lazy unmount as a fallback</span>
            <span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> <span class="nt">-eq</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
                </span>log_message <span class="s2">"Successfully lazy unmounted </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">."</span>
            <span class="k">else
                </span>log_message <span class="s2">"Error: Failed to lazy unmount </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">."</span>
            <span class="k">fi
        fi
    elif </span>findmnt <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span> <span class="o">&gt;</span> /dev/null<span class="p">;</span> <span class="k">then</span>
        <span class="c"># The mount point exists but isn't our device, or our device was already unmounted.</span>
        log_message <span class="s2">"Mount point </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2"> is in use by another device or /dev/</span><span class="k">${</span><span class="nv">DEVNAME</span><span class="k">}</span><span class="s2"> already unmounted. Won't force unmount other devices."</span>
    <span class="k">else
        </span>log_message <span class="s2">"Mount point </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2"> not found or not mounted. Skipping unmount logic."</span>
    <span class="k">fi</span>

    <span class="c"># Remove the directory if it exists and is empty</span>
    <span class="k">if</span> <span class="o">[</span> <span class="nt">-d</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
        if</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">ls</span> <span class="nt">-A</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span><span class="si">)</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span> <span class="c"># Check if directory is empty</span>
            <span class="nb">rmdir</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">"</span>
            <span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> <span class="nt">-eq</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
                </span>log_message <span class="s2">"Successfully removed directory </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2">."</span>
            <span class="k">else
                </span>log_message <span class="s2">"Note: Directory </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2"> not empty after unmount or error removing."</span>
            <span class="k">fi
        else
            </span>log_message <span class="s2">"Note: Directory </span><span class="k">${</span><span class="nv">MOUNT_POINT</span><span class="k">}</span><span class="s2"> is not empty. Not removing."</span>
        <span class="k">fi
    fi
else
    </span>log_message <span class="s2">"Warning: Unknown or no ACTION ('</span><span class="nv">$ACTION</span><span class="s2">') for device /dev/</span><span class="nv">$DEVNAME</span><span class="s2">. Nothing to do."</span>
<span class="k">fi

</span><span class="nb">exit </span>0
</code></pre></div></div>

<p><strong>Step 1.2: Create the <code class="language-plaintext highlighter-rouge">udev</code> Rule</strong></p>

<p>Create a new file named <code class="language-plaintext highlighter-rouge">/etc/udev/rules.d/85-automount-usb-vfat.rules</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Udev rule for automatically mounting/unmounting VFAT USB block devices

# Match USB block devices (e.g., partitions on USB sticks) that are VFAT
# SUBSYSTEMS=="usb": Ensures the device is on the USB bus.
# SUBSYSTEM=="block": Ensures it's a block device.
# DRIVERS=="sd": Further refinement for SCSI-like devices (common for USB storage).
# KERNEL=="sd[a-z]*[0-9]": Matches partitions like sdb1, sdc1. Using sd[a-z]* instead of sd[b-z] for more generality if system disk is nvme.
# ENV{ID_FS_TYPE}=="vfat": Only act on VFAT filesystems.
# ACTION=="add": Trigger on device addition.
# RUN+="/usr/local/sbin/mount_usb_helper.sh %k": Execute helper. %k is kernel name (e.g., sdb1).
ACTION=="add", SUBSYSTEMS=="usb", SUBSYSTEM=="block", DRIVERS=="sd", KERNEL=="sd[a-z]*[0-9]", ENV{ID_FS_TYPE}=="vfat", RUN+="/usr/local/sbin/mount_usb_helper.sh %k"

# Rule for removal. Simpler match as we mainly care about the KERNEL name passed to the script.
ACTION=="remove", SUBSYSTEMS=="usb", SUBSYSTEM=="block", DRIVERS=="sd", KERNEL=="sd[a-z]*[0-9]", RUN+="/usr/local/sbin/mount_usb_helper.sh %k"

</code></pre></div></div>
<p><em>Self-correction:</em> The <code class="language-plaintext highlighter-rouge">KERNEL=="sd[a-z]*[0-9]"</code> combined with <code class="language-plaintext highlighter-rouge">SUBSYSTEMS=="usb"</code> is generally safe. If your main system disk is also USB and named e.g. <code class="language-plaintext highlighter-rouge">sda</code>, you might add <code class="language-plaintext highlighter-rouge">ATTRS{removable}=="1"</code> to only target removable USB block devices, or refine the <code class="language-plaintext highlighter-rouge">KERNEL</code> pattern like <code class="language-plaintext highlighter-rouge">sd[b-z]*[0-9]</code>.</p>

<p><strong>Step 1.3: Apply <code class="language-plaintext highlighter-rouge">udev</code> Changes</strong></p>
<ol>
  <li>Reload <code class="language-plaintext highlighter-rouge">udev</code> rules:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>udevadm control <span class="nt">--reload-rules</span>
</code></pre></div>    </div>
  </li>
  <li>Trigger <code class="language-plaintext highlighter-rouge">udev</code> for existing devices (or just re-plug your USB device):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>udevadm trigger
</code></pre></div>    </div>
    <p>Now, when you plug in a VFAT USB drive, it should be automatically mounted by your user <code class="language-plaintext highlighter-rouge">linaro</code> (or the user you configured) under <code class="language-plaintext highlighter-rouge">/mnt/media/DEVICE_NAME</code>. Logs will go to syslog tagged with <code class="language-plaintext highlighter-rouge">usb-mount-helper</code>.</p>
  </li>
</ol>

<hr />

<p><strong>Part 2: Refactoring <code class="language-plaintext highlighter-rouge">gobinet_boot.sh</code> (SysV init script with LSB)</strong></p>

<p>This script will remain in <code class="language-plaintext highlighter-rouge">/etc/init.d/</code> but will be made LSB-compliant.</p>

<p><strong>Step 2.1: Edit <code class="language-plaintext highlighter-rouge">/etc/init.d/gobinet_boot.sh</code></strong></p>

<p>Replace its content with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/sh</span>
<span class="c">### BEGIN INIT INFO</span>
<span class="c"># Provides:          gobinet-boot</span>
<span class="c"># Required-Start:    $remote_fs $syslog</span>
<span class="c"># Required-Stop:     $remote_fs $syslog</span>
<span class="c"># Should-Start:      $network # If quectel-CM *needs* basic networking already up.</span>
<span class="c"># Default-Start:     2 3 4 5</span>
<span class="c"># Default-Stop:      0 1 6</span>
<span class="c"># Short-Description: Initialize Quectel GobiNet device</span>
<span class="c"># Description:       This service waits for a specified time and then runs</span>
<span class="c">#                    the quectel-CM command to initialize GobiNet hardware.</span>
<span class="c">#                    The Quectel module might provide network connectivity.</span>
<span class="c">### END INIT INFO</span>

<span class="c"># PATH should only include /usr/* if it runs after the mountnfs.sh script</span>
<span class="nv">PATH</span><span class="o">=</span>/sbin:/usr/sbin:/bin:/usr/bin
<span class="nv">DESC</span><span class="o">=</span><span class="s2">"Quectel GobiNet initialization"</span>
<span class="nv">NAME</span><span class="o">=</span>gobinet-boot <span class="c"># LSB 'Provides' name</span>
<span class="nv">DAEMON_COMMAND</span><span class="o">=</span>/sbin/quectel-CM <span class="c"># The command to run</span>
<span class="nv">SCRIPTNAME</span><span class="o">=</span>/etc/init.d/gobinet_boot.sh <span class="c"># Actual script name</span>
<span class="nv">PIDFILE</span><span class="o">=</span>/var/run/<span class="k">${</span><span class="nv">NAME</span><span class="k">}</span>.pid
<span class="nv">LOGFILE</span><span class="o">=</span>/var/log/<span class="k">${</span><span class="nv">NAME</span><span class="k">}</span>.log
<span class="nv">SLEEP_DURATION</span><span class="o">=</span>60 <span class="c"># Original sleep duration</span>

<span class="c"># Exit if the command is not installed</span>
<span class="o">[</span> <span class="nt">-x</span> <span class="s2">"</span><span class="nv">$DAEMON_COMMAND</span><span class="s2">"</span> <span class="o">]</span> <span class="o">||</span> <span class="o">{</span> <span class="nb">echo</span> <span class="s2">"</span><span class="nv">$DAEMON_COMMAND</span><span class="s2"> not found or not executable."</span><span class="p">;</span> <span class="nb">exit </span>1<span class="p">;</span> <span class="o">}</span>

<span class="c"># Read configuration variable file if it is present</span>
<span class="o">[</span> <span class="nt">-r</span> /etc/default/<span class="nv">$NAME</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="nb">.</span> /etc/default/<span class="nv">$NAME</span>

<span class="c"># Load the VERBOSE setting and other rcS variables</span>
<span class="nb">.</span> /lib/init/vars.sh

<span class="c"># Define LSB log_* functions.</span>
<span class="nb">.</span> /lib/lsb/init-functions

<span class="c"># Function to log messages with timestamp</span>
_log_msg<span class="o">()</span> <span class="o">{</span>
    <span class="nb">echo</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">date</span> <span class="s1">'+%Y-%m-%d %H:%M:%S'</span><span class="si">)</span><span class="s2"> - </span><span class="nv">$1</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$LOGFILE</span><span class="s2">"</span>
    log_action_msg <span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span> <span class="c"># For console via LSB</span>
<span class="o">}</span>

<span class="c">#</span>
<span class="c"># Function that starts the service</span>
<span class="c">#</span>
do_start<span class="o">()</span>
<span class="o">{</span>
    <span class="c"># Check if already running based on PID file and process</span>
    <span class="k">if</span> <span class="o">[</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> ps <span class="nt">-p</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">cat</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span><span class="si">)</span><span class="s2">"</span> <span class="o">&gt;</span> /dev/null 2&gt;&amp;1<span class="p">;</span> <span class="k">then
        </span>_log_msg <span class="s2">"</span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) is already running (PID </span><span class="si">$(</span><span class="nb">cat</span> <span class="nv">$PIDFILE</span><span class="si">)</span><span class="s2">)."</span>
        <span class="k">return </span>1 <span class="c"># LSB code for already running</span>
    <span class="k">fi

    </span>_log_msg <span class="s2">"Starting </span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">)..."</span>

    <span class="c"># This subshell runs the sleep and the command in the background.</span>
    <span class="c"># IMPORTANT: This PID management is basic. If quectel-CM is a true daemon,</span>
    <span class="c"># it should create its own PID file. If it does, modify this script to use that.</span>
    <span class="o">(</span>
        _log_msg <span class="s2">"Subshell: Sleeping for </span><span class="nv">$SLEEP_DURATION</span><span class="s2"> seconds before running </span><span class="nv">$DAEMON_COMMAND</span><span class="s2">."</span>
        <span class="nb">sleep</span> <span class="s2">"</span><span class="nv">$SLEEP_DURATION</span><span class="s2">"</span>
        _log_msg <span class="s2">"Subshell: Executing </span><span class="nv">$DAEMON_COMMAND</span><span class="s2">."</span>
        <span class="c"># Redirect quectel-CM's stdout and stderr to the log file</span>
        <span class="s2">"</span><span class="nv">$DAEMON_COMMAND</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$LOGFILE</span><span class="s2">"</span> 2&gt;&amp;1
        _log_msg <span class="s2">"Subshell: </span><span class="nv">$DAEMON_COMMAND</span><span class="s2"> finished or daemonized. Subshell exiting."</span>
        <span class="c"># If quectel-CM daemonizes and creates its own PID, this outer PID is not useful after this point.</span>
    <span class="o">)</span> &amp;
    <span class="c"># Store the PID of the backgrounded subshell</span>
    <span class="nb">echo</span> <span class="nv">$!</span> <span class="o">&gt;</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span>

    <span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> <span class="nt">-eq</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
        </span>_log_msg <span class="s2">"</span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) started (subshell PID </span><span class="nv">$!</span><span class="s2">). Check </span><span class="nv">$LOGFILE</span><span class="s2"> for </span><span class="nv">$DAEMON_COMMAND</span><span class="s2"> output."</span>
        <span class="k">return </span>0 <span class="c"># Success</span>
    <span class="k">else
        </span>_log_msg <span class="s2">"Failed to start </span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) subshell."</span>
        <span class="nb">rm</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span> <span class="c"># Clean up PID file on failure</span>
        <span class="k">return </span>2 <span class="c"># LSB code for generic error</span>
    <span class="k">fi</span>
<span class="o">}</span>

<span class="c">#</span>
<span class="c"># Function that stops the service</span>
<span class="c">#</span>
do_stop<span class="o">()</span>
<span class="o">{</span>
    _log_msg <span class="s2">"Stopping </span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">)..."</span>

    <span class="k">if</span> <span class="o">[</span> <span class="o">!</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
        </span>_log_msg <span class="s2">"</span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) PID file not found. Assuming not running or already stopped."</span>
        <span class="k">return </span>1 <span class="c"># LSB code for not running</span>
    <span class="k">fi

    </span><span class="nv">PID_TO_KILL</span><span class="o">=</span><span class="si">$(</span><span class="nb">cat</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span><span class="si">)</span>
    <span class="k">if</span> <span class="o">!</span> ps <span class="nt">-p</span> <span class="s2">"</span><span class="nv">$PID_TO_KILL</span><span class="s2">"</span> <span class="o">&gt;</span> /dev/null 2&gt;&amp;1<span class="p">;</span> <span class="k">then
        </span>_log_msg <span class="s2">"</span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) (PID </span><span class="nv">$PID_TO_KILL</span><span class="s2"> from PID file) not running. Removing stale PID file."</span>
        <span class="nb">rm</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span>
        <span class="k">return </span>1 <span class="c"># LSB code for not running</span>
    <span class="k">fi</span>

    <span class="c"># Attempt to stop the process found in the PID file.</span>
    <span class="c"># This will kill the subshell. If quectel-CM daemonized and detached,</span>
    <span class="c"># this won't stop the actual quectel-CM daemon unless it's a child of the subshell</span>
    <span class="c"># and gets killed when the subshell (its parent) is terminated.</span>
    <span class="c"># For a true daemon, you'd need to know its actual PID or have it respond to signals.</span>
    <span class="c"># The `start-stop-daemon --stop` utility is more robust if the process behaves like a daemon.</span>
    <span class="c"># However, we are targeting the subshell's PID here.</span>
    <span class="nb">kill</span> <span class="s2">"</span><span class="nv">$PID_TO_KILL</span><span class="s2">"</span>
    <span class="c"># Allow some time for the process to terminate</span>
    <span class="nb">sleep </span>2

    <span class="k">if</span> <span class="o">!</span> ps <span class="nt">-p</span> <span class="s2">"</span><span class="nv">$PID_TO_KILL</span><span class="s2">"</span> <span class="o">&gt;</span> /dev/null 2&gt;&amp;1<span class="p">;</span> <span class="k">then
        </span>_log_msg <span class="s2">"</span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) (PID </span><span class="nv">$PID_TO_KILL</span><span class="s2">) stopped successfully."</span>
        <span class="nb">rm</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span>
        <span class="k">return </span>0 <span class="c"># Success</span>
    <span class="k">else
        </span>_log_msg <span class="s2">"Failed to stop </span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) (PID </span><span class="nv">$PID_TO_KILL</span><span class="s2">) with SIGTERM. Sending SIGKILL."</span>
        <span class="nb">kill</span> <span class="nt">-9</span> <span class="s2">"</span><span class="nv">$PID_TO_KILL</span><span class="s2">"</span>
        <span class="nb">sleep </span>1
        <span class="k">if</span> <span class="o">!</span> ps <span class="nt">-p</span> <span class="s2">"</span><span class="nv">$PID_TO_KILL</span><span class="s2">"</span> <span class="o">&gt;</span> /dev/null 2&gt;&amp;1<span class="p">;</span> <span class="k">then
            </span>_log_msg <span class="s2">"</span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) (PID </span><span class="nv">$PID_TO_KILL</span><span class="s2">) killed successfully."</span>
            <span class="nb">rm</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span>
            <span class="k">return </span>0 <span class="c"># Success</span>
        <span class="k">else
            </span>_log_msg <span class="s2">"Error: Failed to kill </span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) (PID </span><span class="nv">$PID_TO_KILL</span><span class="s2">) even with SIGKILL."</span>
            <span class="k">return </span>2 <span class="c"># LSB code for generic error</span>
        <span class="k">fi
    fi</span>
<span class="o">}</span>

<span class="c">#</span>
<span class="c"># Function that gets the status of the service</span>
<span class="c">#</span>
do_status<span class="o">()</span>
<span class="o">{</span>
    <span class="c"># status_of_proc -p "$PIDFILE" "$DAEMON_COMMAND" "$NAME"</span>
    <span class="c"># The above LSB function is good if DAEMON_COMMAND is the actual long-running process name.</span>
    <span class="c"># Since we are managing a subshell, we'll check the PID from the file.</span>
    <span class="k">if</span> <span class="o">[</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
        </span><span class="nv">SERVICE_PID</span><span class="o">=</span><span class="si">$(</span><span class="nb">cat</span> <span class="s2">"</span><span class="nv">$PIDFILE</span><span class="s2">"</span><span class="si">)</span>
        <span class="k">if </span>ps <span class="nt">-p</span> <span class="s2">"</span><span class="nv">$SERVICE_PID</span><span class="s2">"</span> <span class="o">&gt;</span> /dev/null 2&gt;&amp;1<span class="p">;</span> <span class="k">then
            </span>log_success_msg <span class="s2">"</span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) is running with PID </span><span class="nv">$SERVICE_PID</span><span class="s2">."</span>
            <span class="c"># You could add more info here, e.g., check if quectel-CM is also running if it's a child.</span>
            <span class="nb">exit </span>0 <span class="c"># LSB code for running</span>
        <span class="k">else
            </span>log_failure_msg <span class="s2">"</span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) PID file exists, but process </span><span class="nv">$SERVICE_PID</span><span class="s2"> is not running. Stale PID file?"</span>
            <span class="nb">exit </span>1 <span class="c"># LSB code for not running but PID file exists (stale)</span>
        <span class="k">fi
    else
        </span>log_failure_msg <span class="s2">"</span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">) is not running (no PID file)."</span>
        <span class="nb">exit </span>3 <span class="c"># LSB code for not running</span>
    <span class="k">fi</span>
<span class="o">}</span>


<span class="k">case</span> <span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span> <span class="k">in
  </span>start<span class="p">)</span>
    do_start
    <span class="nb">exit</span> <span class="nv">$?</span>
    <span class="p">;;</span>
  stop<span class="p">)</span>
    do_stop
    <span class="nb">exit</span> <span class="nv">$?</span>
    <span class="p">;;</span>
  status<span class="p">)</span>
    do_status
    <span class="p">;;</span>
  restart|force-reload<span class="p">)</span>
    _log_msg <span class="s2">"Restarting </span><span class="nv">$DESC</span><span class="s2"> (</span><span class="nv">$NAME</span><span class="s2">)..."</span>
    do_stop
    <span class="c"># Allow some time for graceful shutdown before restarting</span>
    <span class="nb">sleep </span>2
    do_start
    <span class="nb">exit</span> <span class="nv">$?</span>
    <span class="p">;;</span>
  <span class="k">*</span><span class="p">)</span>
    <span class="nb">echo</span> <span class="s2">"Usage: </span><span class="nv">$SCRIPTNAME</span><span class="s2"> {start|stop|status|restart}"</span> <span class="o">&gt;</span>&amp;2
    <span class="nb">exit </span>3
    <span class="p">;;</span>
<span class="k">esac</span>

<span class="nb">exit </span>0
</code></pre></div></div>

<p><strong>Step 2.2: Make the script executable</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chmod</span> +x /etc/init.d/gobinet_boot.sh
</code></pre></div></div>

<p><strong>Step 2.3: Test the script</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo</span> /etc/init.d/gobinet_boot.sh start
<span class="nb">sudo</span> /etc/init.d/gobinet_boot.sh status
<span class="c"># Wait for more than SLEEP_DURATION to see if quectel-CM runs</span>
<span class="nb">sudo</span> /etc/init.d/gobinet_boot.sh status
<span class="nb">sudo</span> /etc/init.d/gobinet_boot.sh stop
</code></pre></div></div>
<p>Check <code class="language-plaintext highlighter-rouge">/var/log/gobinet-boot.log</code> for output.</p>

<p><strong>Step 2.4: Update System’s Understanding of the Service</strong>
If you had previously enabled it with <code class="language-plaintext highlighter-rouge">systemd-sysv-install</code> or <code class="language-plaintext highlighter-rouge">update-rc.d</code>, systemd might still have an old version of its generated unit.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl daemon-reload
<span class="c"># If you want to ensure it's enabled for boot (using systemd's SysV generator):</span>
<span class="nb">sudo </span>systemctl <span class="nb">enable </span>gobinet_boot.sh <span class="c"># Or it might be gobinet-boot based on 'Provides'</span>
</code></pre></div></div>
<p>The <code class="language-plaintext highlighter-rouge">insserv</code> warnings for <code class="language-plaintext highlighter-rouge">gobinet_boot.sh</code> should now be gone.</p>

<p><strong>Important Notes for <code class="language-plaintext highlighter-rouge">gobinet_boot.sh</code>:</strong></p>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">quectel-CM</code> Behavior:</strong> The provided script assumes <code class="language-plaintext highlighter-rouge">quectel-CM</code> is a command that either:
    <ol>
      <li>Initializes hardware and then exits.</li>
      <li>Starts its own daemon process and detaches correctly (in which case the PID in <code class="language-plaintext highlighter-rouge">/var/run/gobinet-boot.pid</code> is only for the initial subshell).
If <code class="language-plaintext highlighter-rouge">quectel-CM</code> is meant to run continuously as a foreground process <em>managed by the init script</em>, the init script would need to be more complex, likely using <code class="language-plaintext highlighter-rouge">start-stop-daemon</code> to manage it directly.</li>
    </ol>
  </li>
  <li><strong>Dependencies (<code class="language-plaintext highlighter-rouge">Required-Start</code>, <code class="language-plaintext highlighter-rouge">Should-Start</code>):</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">$remote_fs $syslog</code>: Standard, almost always needed.</li>
      <li><code class="language-plaintext highlighter-rouge">$network</code>: Added as <code class="language-plaintext highlighter-rouge">Should-Start</code>. If <code class="language-plaintext highlighter-rouge">/sbin/quectel-CM</code> <em>needs</em> the network to be up before it can configure the Quectel modem, this is appropriate. If the Quectel modem <em>provides</em> a primary network interface, then this script should likely start <em>before</em> the generic <code class="language-plaintext highlighter-rouge">$network</code> target, or it might be part of a more specific modem management service. You might need to remove <code class="language-plaintext highlighter-rouge">$network</code> or change it to something like <code class="language-plaintext highlighter-rouge">Before=network.target</code> in a systemd unit.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">sleep 60</code>:</strong> This mimics the original script’s delay. In a production environment, especially with systemd, you’d ideally look for ways to trigger <code class="language-plaintext highlighter-rouge">quectel-CM</code> based on device availability or other events rather than a fixed sleep, if possible.</li>
</ul>

<hr />

<p><strong>Part 3: The Systemd Native Unit Approach (Recommended for Debian Bullseye)</strong></p>

<p>While the above steps fix the LSB warnings for your SysV init scripts, Debian Bullseye uses <code class="language-plaintext highlighter-rouge">systemd</code> as its primary init system. Creating native <code class="language-plaintext highlighter-rouge">systemd</code> unit files is the modern, more robust, and flexible way to manage services.</p>

<p><strong>3.1 For <code class="language-plaintext highlighter-rouge">gobinet_boot.sh</code> functionality -&gt; <code class="language-plaintext highlighter-rouge">gobinet-boot.service</code></strong></p>

<p>Create <code class="language-plaintext highlighter-rouge">/etc/systemd/system/gobinet-boot.service</code>:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">Quectel GobiNet Initialization Service</span>
<span class="py">Documentation</span><span class="p">=</span><span class="s">man:quectel-CM(8) # If a man page exists</span>
<span class="c"># If it needs to run after basic network configuration is attempted:
# After=network.target syslog.target
# If it needs to run before network-online.target is reached:
# Before=network-online.target
# If the GobiNet modem *provides* network, dependencies might be more complex
# or it might be better integrated with ModemManager or netplan/ifupdown.
# For now, let's assume it runs after basic system services are up.
</span><span class="py">After</span><span class="p">=</span><span class="s">syslog.target local-fs.target</span>

<span class="nn">[Service]</span>
<span class="py">Type</span><span class="p">=</span><span class="s">oneshot # Assumes quectel-CM initializes and exits, or is a self-daemonizing script.</span>
<span class="py">RemainAfterExit</span><span class="p">=</span><span class="s">yes # If Type=oneshot, and we consider the 'service' up after the command.</span>
<span class="py">ExecStartPre</span><span class="p">=</span><span class="s">/bin/sleep 60 # The original delay</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/sbin/quectel-CM</span>
<span class="py">StandardOutput</span><span class="p">=</span><span class="s">journal+console # Log to systemd journal and console</span>
<span class="py">StandardError</span><span class="p">=</span><span class="s">journal+console</span>

<span class="c"># If quectel-CM is a true daemon that forks and manages its own PID:
# Type=forking
# PIDFile=/var/run/quectel-cm.pid # Path to the PID file quectel-CM creates
# ExecStart=/sbin/quectel-CM &lt;options_if_any&gt;
# GuessMainPID=no # If PIDFile is accurate
</span>
<span class="c"># If quectel-CM runs in foreground and systemd should manage it:
# Type=simple
# ExecStart=/sbin/quectel-CM
# Restart=on-failure # Optional: restart if it fails
</span>
<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">multi-user.target # Start when multi-user target is reached</span>
</code></pre></div></div>

<p><strong>To use this systemd service:</strong></p>
<ol>
  <li>Create the file <code class="language-plaintext highlighter-rouge">/etc/systemd/system/gobinet-boot.service</code> with the content above.</li>
  <li><strong>Disable the SysV init script if you previously enabled it:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl disable gobinet_boot.sh <span class="c"># Or update-rc.d gobinet_boot.sh remove</span>
<span class="nb">sudo rm</span> /etc/init.d/gobinet_boot.sh <span class="c"># Optionally remove the old script</span>
</code></pre></div>    </div>
  </li>
  <li>Reload systemd: <code class="language-plaintext highlighter-rouge">sudo systemctl daemon-reload</code></li>
  <li>Enable the new service: <code class="language-plaintext highlighter-rouge">sudo systemctl enable gobinet-boot.service</code></li>
  <li>Start it: <code class="language-plaintext highlighter-rouge">sudo systemctl start gobinet-boot.service</code></li>
  <li>Check status: <code class="language-plaintext highlighter-rouge">sudo systemctl status gobinet-boot.service</code> and <code class="language-plaintext highlighter-rouge">journalctl -u gobinet-boot.service</code></li>
</ol>

<p><strong>Advantages of systemd unit for <code class="language-plaintext highlighter-rouge">gobinet-boot</code>:</strong></p>
<ul>
  <li><strong>Clearer Dependencies:</strong> <code class="language-plaintext highlighter-rouge">After=</code>, <code class="language-plaintext highlighter-rouge">Before=</code>, <code class="language-plaintext highlighter-rouge">Wants=</code>, <code class="language-plaintext highlighter-rouge">Requires=</code> offer fine-grained control.</li>
  <li><strong>Better Process Management:</strong> <code class="language-plaintext highlighter-rouge">Type=</code> (simple, forking, oneshot, etc.) correctly defines how systemd handles the process.</li>
  <li><strong>Resource Control:</strong> Can set CPU/memory limits, etc.</li>
  <li><strong>Integrated Logging:</strong> Output goes directly to the systemd journal.</li>
  <li><strong>Simpler Syntax:</strong> Often more straightforward than complex shell scripting for service management.</li>
</ul>

<p><strong>3.2 For <code class="language-plaintext highlighter-rouge">mount_usb.sh</code> functionality (udev is still key)</strong></p>

<p>The <code class="language-plaintext highlighter-rouge">udev</code> approach for <code class="language-plaintext highlighter-rouge">mount_usb.sh</code> is already good and leverages <code class="language-plaintext highlighter-rouge">systemd-udevd.service</code>. You don’t typically create a separate <code class="language-plaintext highlighter-rouge">.service</code> file for each <code class="language-plaintext highlighter-rouge">udev</code> <code class="language-plaintext highlighter-rouge">RUN+=</code> script unless the script needs to start a long-running service managed by systemd. Your helper script is short-lived, so the current udev method is fine.</p>

<p><em>Advanced Systemd Automount:</em>
For truly on-demand mounting managed by systemd (where the mount only happens when the directory is accessed), you could explore systemd <code class="language-plaintext highlighter-rouge">.automount</code> and <code class="language-plaintext highlighter-rouge">.mount</code> units. This is more complex to set up than the udev script but offers different benefits. For your current VFAT use case, the udev + helper script is a good balance of simplicity and functionality.</p>

<hr />

<p><strong>Summary of Recommendations:</strong></p>

<ol>
  <li><strong>For <code class="language-plaintext highlighter-rouge">mount_usb.sh</code>:</strong>
    <ul>
      <li>Definitely move it out of <code class="language-plaintext highlighter-rouge">/etc/init.d/</code>.</li>
      <li>Use the <code class="language-plaintext highlighter-rouge">udev</code> rule and the improved <code class="language-plaintext highlighter-rouge">/usr/local/sbin/mount_usb_helper.sh</code> script provided above. This addresses the LSB warnings by removing the script from <code class="language-plaintext highlighter-rouge">insserv</code>’s scope and makes it function correctly.</li>
    </ul>
  </li>
  <li><strong>For <code class="language-plaintext highlighter-rouge">gobinet_boot.sh</code>:</strong>
    <ul>
      <li><strong>Option A (Improved SysV):</strong> Use the refactored <code class="language-plaintext highlighter-rouge">/etc/init.d/gobinet_boot.sh</code> provided. This will fix the LSB warnings.</li>
      <li><strong>Option B (Recommended - Systemd Native):</strong> Create the <code class="language-plaintext highlighter-rouge">gobinet-boot.service</code> unit file as described. Disable/remove the old SysV script. This is the more modern and robust solution for Debian Bullseye.</li>
    </ul>
  </li>
</ol>

<p>By implementing these changes, your system will be cleaner, the <code class="language-plaintext highlighter-rouge">insserv</code> warnings will be resolved, and your scripts (or their systemd equivalents) will function more reliably and correctly. Remember to carefully test the behavior of <code class="language-plaintext highlighter-rouge">quectel-CM</code> to choose the best <code class="language-plaintext highlighter-rouge">Type=</code> and PID management strategy if you opt for the systemd unit.</p>]]></content><author><name></name></author><category term="aid&gt;linux&gt;hardware&gt;rockchip" /></entry><entry><title type="html">compare</title><link href="https://ib.bsb.br/compare/" rel="alternate" type="text/html" title="compare" /><published>2025-05-10T00:00:00+00:00</published><updated>2025-05-10T17:22:49+00:00</updated><id>https://ib.bsb.br/compare</id><content type="html" xml:base="https://ib.bsb.br/compare/"><![CDATA[<section class="code-block-container" role="group" aria-label="Markdown Code Block" data-filename="markdown_code_block.md" data-code="The task is to compare several scripts for their &quot;robustness&quot; and &quot;featurefullness&quot; and then identify the &quot;best one, the most effective one.&quot;

To perform the requested comparison and identify the most effective script, here it is the content of those scripts:

```script1

```

~~~script2

~~~

```script3

```

~~~script4

~~~

Here it is the outline of the methodology for you, the AI ASSISTANT, to employ to approach that task. Your analysis must focus on the following key criteria and extend to other critical aspects of script quality:

**1. Assessing Robustness:**

Robustness refers to how well a script handles errors, unexpected inputs, and varying operational conditions without failing, producing incorrect results, or causing unintended side effects. The AI ASSITANT must examine:

*   **Error Handling and Propagation:**
    *   Does the script explicitly check for command execution failures (e.g., using `if ! command; then ...` or checking the exit status `$?`)?
    *   Does it utilize options like `set -e` (exit immediately if a command exits with a non-zero status), `set -u` (treat unset variables as an error when performing expansion), and `set -o pipefail` (a pipeline&#39;s return status is the value of the last command to exit with a non-zero status, or zero if all commands exit successfully)?
    *   Are `trap` commands used effectively for cleanup (e.g., removing temporary files) on script exit, interruption, or specific signals?
*   **Input Validation:**
    *   If the script accepts arguments or user input, does it rigorously validate them? This includes checking for the correct number of arguments, expected data types/formats, valid value ranges, and sanitizing inputs to prevent security issues (see Security section).
    *   How does it behave with missing, malformed, excessive, or unexpected inputs? Does it provide clear error messages and exit gracefully?
*   **Edge Case and Boundary Condition Handling:**
    *   Does the script account for potential edge cases, such as empty input files, files with special characters in names, zero-value inputs, or specific environmental conditions that might affect its logic?
*   **Idempotence (if applicable):**
    *   If the script is intended to make system changes (e.g., configuration, file creation), can it be run multiple times with the same initial state and produce the same end state without causing errors or unintended cumulative effects on subsequent runs?
*   **Resource Management:**
    *   If the script handles resources like temporary files, network connections, or background processes, does it manage them correctly, ensuring they are released or cleaned up appropriately to prevent leaks or conflicts?

**2. Assessing Featurefullness:**

Featurefullness relates to the breadth, depth, and relevance of the script&#39;s capabilities in relation to its intended purpose. The AI ASSITANT must evaluate:

*   **Scope and Relevance of Functionality:**
    *   What specific tasks does the script perform? How many distinct operations or functionalities does it offer?
    *   Does the feature set directly support the script&#39;s core purpose, or does it include extraneous features that add complexity without significant value?
    *   Does it comprehensively address the problem it&#39;s designed to solve?
*   **Flexibility and Configurability:**
    *   Does the script offer command-line options or arguments to modify its behavior in meaningful ways?
    *   Can its operation be customized through well-documented configuration files or environment variables?
*   **User Experience (UX):**
    *   If interactive, are prompts clear and unambiguous?
    *   Does it provide helpful output, status messages, and clear usage instructions (e.g., a `--help` or `-h` option)?
*   **Integration Capabilities:**
    *   Can the script easily integrate with other tools or workflows? For example, does it correctly handle standard input (stdin), produce parseable standard output (stdout), and use standard error (stderr) appropriately for diagnostic messages?

**3. Assessing Readability and Maintainability:**

A script that is difficult to understand is also difficult to debug, modify, and verify for correctness, impacting its long-term robustness and utility.

*   **Code Structure and Organization:**
    *   Is the script logically structured, perhaps using functions for modularity and to avoid code duplication?
    *   Is the flow of execution easy to follow?
*   **Clarity of Naming:**
    *   Are variable names, function names, and comments clear, descriptive, and consistent?
*   **Use of Comments:**
    *   Are there sufficient comments to explain complex logic, non-obvious operations, or the purpose of different sections, without cluttering the code?
*   **Consistency in Style:**
    *   Does the script follow a consistent coding style (indentation, spacing, quoting)?
*   **Simplicity:**
    *   Does the script achieve its goals in a straightforward manner, or is it overly complex and convoluted?

**4. Assessing Performance and Efficiency:**

Depending on the script&#39;s purpose (e.g., processing large datasets, running frequently in automated systems), performance can be a critical factor.

*   **Choice of Commands and Techniques:**
    *   Does it use efficient commands and code-native builtins where appropriate (e.g., avoiding unnecessary external process forks by using code-native builtins like `read` or parameter expansions instead of `sed`/`awk` for simple tasks)?
    *   For text processing, are efficient tools like `awk`, `sed`, or `grep` used effectively, rather than less efficient shell loops for large data?
*   **Handling of Large Data/Files:**
    *   If the script processes large files or data volumes, does it do so in a memory-efficient way (e.g., processing line-by-line instead of reading entire files into memory if not needed)?
*   **Resource Consumption:**
    *   Are there any obvious bottlenecks or excessive consumption of CPU, memory, or I/O resources?

**5. Assessing Portability and Dependencies:**

A script&#39;s utility can be enhanced if it can run reliably across different environments.

*   **Dependency Management:**
    *   Does the script clearly state its dependencies on external commands or tools?
    *   Does it check for the existence and (if necessary) the correct version of these dependencies at runtime, providing informative errors if they are not met?
    *   Does it rely on common utilities or more obscure ones that might not be universally available?

**Determining the &quot;Best&quot; or &quot;Most Effective&quot; Script:**

After assessing each script against these criteria, determining the &quot;best&quot; or &quot;most effective&quot; one is not always an absolute judgment. It heavily depends on the **specific requirements, priorities, and context** for which the script is intended.

*   **Criticality:** For a script running in a critical production system, **robustness and security** would likely be the highest priorities, even if it means sacrificing some featurefullness or development speed.
*   **User Expertise &amp; Use Case:** A quick utility script for personal use by an expert might prioritize development speed and featurefullness for a specific task, with less emphasis on exhaustive error handling for every conceivable edge case.
*   **Lifespan and Maintenance:** For a script intended for long-term use and potential modification by multiple people, **readability and maintainability** become crucial.
*   **Performance Needs:** If a script processes large data volumes or runs very frequently, **performance and efficiency** might outweigh other factors, provided core correctness is maintained.

Generally, the &quot;most effective&quot; script is one that:
*   Reliably and securely performs its intended functions.
*   Offers a feature set that is well-aligned with its purpose without unnecessary complexity.
*   Is understandable, maintainable, and performs adequately for its context.
*   Handles errors gracefully and provides useful feedback.

To provide a definitive comparison, the AI ASSITANT might also conceptually involve the use of static analysis tools and a systematic testing strategy, if possible." data-download-link="" data-download-label="Download Markdown">
  <code class="language-markdown">The task is to compare several scripts for their &quot;robustness&quot; and &quot;featurefullness&quot; and then identify the &quot;best one, the most effective one.&quot;

To perform the requested comparison and identify the most effective script, here it is the content of those scripts:

```script1

```

~~~script2

~~~

```script3

```

~~~script4

~~~

Here it is the outline of the methodology for you, the AI ASSISTANT, to employ to approach that task. Your analysis must focus on the following key criteria and extend to other critical aspects of script quality:

**1. Assessing Robustness:**

Robustness refers to how well a script handles errors, unexpected inputs, and varying operational conditions without failing, producing incorrect results, or causing unintended side effects. The AI ASSITANT must examine:

*   **Error Handling and Propagation:**
    *   Does the script explicitly check for command execution failures (e.g., using `if ! command; then ...` or checking the exit status `$?`)?
    *   Does it utilize options like `set -e` (exit immediately if a command exits with a non-zero status), `set -u` (treat unset variables as an error when performing expansion), and `set -o pipefail` (a pipeline&#39;s return status is the value of the last command to exit with a non-zero status, or zero if all commands exit successfully)?
    *   Are `trap` commands used effectively for cleanup (e.g., removing temporary files) on script exit, interruption, or specific signals?
*   **Input Validation:**
    *   If the script accepts arguments or user input, does it rigorously validate them? This includes checking for the correct number of arguments, expected data types/formats, valid value ranges, and sanitizing inputs to prevent security issues (see Security section).
    *   How does it behave with missing, malformed, excessive, or unexpected inputs? Does it provide clear error messages and exit gracefully?
*   **Edge Case and Boundary Condition Handling:**
    *   Does the script account for potential edge cases, such as empty input files, files with special characters in names, zero-value inputs, or specific environmental conditions that might affect its logic?
*   **Idempotence (if applicable):**
    *   If the script is intended to make system changes (e.g., configuration, file creation), can it be run multiple times with the same initial state and produce the same end state without causing errors or unintended cumulative effects on subsequent runs?
*   **Resource Management:**
    *   If the script handles resources like temporary files, network connections, or background processes, does it manage them correctly, ensuring they are released or cleaned up appropriately to prevent leaks or conflicts?

**2. Assessing Featurefullness:**

Featurefullness relates to the breadth, depth, and relevance of the script&#39;s capabilities in relation to its intended purpose. The AI ASSITANT must evaluate:

*   **Scope and Relevance of Functionality:**
    *   What specific tasks does the script perform? How many distinct operations or functionalities does it offer?
    *   Does the feature set directly support the script&#39;s core purpose, or does it include extraneous features that add complexity without significant value?
    *   Does it comprehensively address the problem it&#39;s designed to solve?
*   **Flexibility and Configurability:**
    *   Does the script offer command-line options or arguments to modify its behavior in meaningful ways?
    *   Can its operation be customized through well-documented configuration files or environment variables?
*   **User Experience (UX):**
    *   If interactive, are prompts clear and unambiguous?
    *   Does it provide helpful output, status messages, and clear usage instructions (e.g., a `--help` or `-h` option)?
*   **Integration Capabilities:**
    *   Can the script easily integrate with other tools or workflows? For example, does it correctly handle standard input (stdin), produce parseable standard output (stdout), and use standard error (stderr) appropriately for diagnostic messages?

**3. Assessing Readability and Maintainability:**

A script that is difficult to understand is also difficult to debug, modify, and verify for correctness, impacting its long-term robustness and utility.

*   **Code Structure and Organization:**
    *   Is the script logically structured, perhaps using functions for modularity and to avoid code duplication?
    *   Is the flow of execution easy to follow?
*   **Clarity of Naming:**
    *   Are variable names, function names, and comments clear, descriptive, and consistent?
*   **Use of Comments:**
    *   Are there sufficient comments to explain complex logic, non-obvious operations, or the purpose of different sections, without cluttering the code?
*   **Consistency in Style:**
    *   Does the script follow a consistent coding style (indentation, spacing, quoting)?
*   **Simplicity:**
    *   Does the script achieve its goals in a straightforward manner, or is it overly complex and convoluted?

**4. Assessing Performance and Efficiency:**

Depending on the script&#39;s purpose (e.g., processing large datasets, running frequently in automated systems), performance can be a critical factor.

*   **Choice of Commands and Techniques:**
    *   Does it use efficient commands and code-native builtins where appropriate (e.g., avoiding unnecessary external process forks by using code-native builtins like `read` or parameter expansions instead of `sed`/`awk` for simple tasks)?
    *   For text processing, are efficient tools like `awk`, `sed`, or `grep` used effectively, rather than less efficient shell loops for large data?
*   **Handling of Large Data/Files:**
    *   If the script processes large files or data volumes, does it do so in a memory-efficient way (e.g., processing line-by-line instead of reading entire files into memory if not needed)?
*   **Resource Consumption:**
    *   Are there any obvious bottlenecks or excessive consumption of CPU, memory, or I/O resources?

**5. Assessing Portability and Dependencies:**

A script&#39;s utility can be enhanced if it can run reliably across different environments.

*   **Dependency Management:**
    *   Does the script clearly state its dependencies on external commands or tools?
    *   Does it check for the existence and (if necessary) the correct version of these dependencies at runtime, providing informative errors if they are not met?
    *   Does it rely on common utilities or more obscure ones that might not be universally available?

**Determining the &quot;Best&quot; or &quot;Most Effective&quot; Script:**

After assessing each script against these criteria, determining the &quot;best&quot; or &quot;most effective&quot; one is not always an absolute judgment. It heavily depends on the **specific requirements, priorities, and context** for which the script is intended.

*   **Criticality:** For a script running in a critical production system, **robustness and security** would likely be the highest priorities, even if it means sacrificing some featurefullness or development speed.
*   **User Expertise &amp; Use Case:** A quick utility script for personal use by an expert might prioritize development speed and featurefullness for a specific task, with less emphasis on exhaustive error handling for every conceivable edge case.
*   **Lifespan and Maintenance:** For a script intended for long-term use and potential modification by multiple people, **readability and maintainability** become crucial.
*   **Performance Needs:** If a script processes large data volumes or runs very frequently, **performance and efficiency** might outweigh other factors, provided core correctness is maintained.

Generally, the &quot;most effective&quot; script is one that:
*   Reliably and securely performs its intended functions.
*   Offers a feature set that is well-aligned with its purpose without unnecessary complexity.
*   Is understandable, maintainable, and performs adequately for its context.
*   Handles errors gracefully and provides useful feedback.

To provide a definitive comparison, the AI ASSITANT might also conceptually involve the use of static analysis tools and a systematic testing strategy, if possible.</code>
</section>]]></content><author><name></name></author><category term="AI&gt;prompt" /></entry><entry><title type="html">NetBird Cloud &amp;amp; TigerVNC: Secure SSH &amp;amp; VNC Access</title><link href="https://ib.bsb.br/tigerbird/" rel="alternate" type="text/html" title="NetBird Cloud &amp;amp; TigerVNC: Secure SSH &amp;amp; VNC Access" /><published>2025-05-10T00:00:00+00:00</published><updated>2025-05-14T09:59:33+00:00</updated><id>https://ib.bsb.br/tigerbird</id><content type="html" xml:base="https://ib.bsb.br/tigerbird/"><![CDATA[<h2 id="netbird-cloud--tigervnc-for-secure-remote-access">NetBird Cloud &amp; TigerVNC for Secure Remote Access</h2>

<p>This guide details how to use NetBird Cloud in conjunction with TigerVNC to establish secure command-line (SSH) and graphical desktop (VNC) access to your Debian-based servers. It refactors a previous guide that focused on Tailscale, adapting the principles and steps for NetBird Cloud.</p>

<p><strong>Always refer to the official NetBird Cloud documentation (<a href="https://netbird.io">netbird.io</a>) for the most current installation commands and feature details, as cloud services and their tools evolve.</strong></p>

<p><strong>Part 1: Secure Command-Line Access (SSH via NetBird Cloud)</strong></p>

<p>Secure command-line access via SSH is a cornerstone of server management. NetBird Cloud significantly enhances its security and ease of use by creating a private, peer-to-peer encrypted network.</p>

<p><strong>1.1. Key NetBird Cloud Benefits for SSH:</strong></p>
<ul>
  <li><strong>No Public SSH Port Exposure</strong>: Your server’s SSH port (default 22) does not need to be open on your internet router. NetBird creates a secure overlay network, meaning only authenticated and authorized peers on your NetBird network can reach the server.</li>
  <li><strong>Simplified Firewall Management</strong>: While the local firewall on the Debian machine (e.g., <code class="language-plaintext highlighter-rouge">ufw</code>) still needs to allow SSH traffic (e.g., <code class="language-plaintext highlighter-rouge">sudo ufw allow ssh</code>), NetBird Access Controls become the primary gatekeeper for <em>which peers</em> on your NetBird network can even attempt to connect.</li>
  <li><strong>Identity-Based Access</strong>: Authenticate using your NetBird identity, which supports Single Sign-On (SSO) with providers like Google, Microsoft, and GitHub. NetBird Access Controls define which peers (associated with users/groups) can connect to which machines.</li>
  <li><strong>Private DNS</strong>: Access servers using simple, stable hostnames (e.g., <code class="language-plaintext highlighter-rouge">my-debian-server.netbird.self</code>) within your NetBird network, instead of relying on potentially changing IP addresses.</li>
  <li><strong>Automated Setup</strong>: NetBird agents can be brought online automatically using setup keys, facilitating non-interactive joining to your NetBird network, which is ideal for scripted deployments.</li>
  <li><strong>Peer-to-peer connections &amp; encryption</strong>: All traffic between your devices on the NetBird network is end-to-end encrypted using WireGuard®.</li>
</ul>

<p><strong>1.2. Core Components for SSH Access:</strong></p>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">openssh-server</code></strong>: This must be installed and running on the Debian server. NetBird provides the secure transport layer for your existing SSH connections; <code class="language-plaintext highlighter-rouge">openssh-server</code> handles the SSH protocol itself.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>openssh-server
<span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> ssh
</code></pre></div>    </div>
  </li>
  <li><strong>NetBird Agent</strong>: Installed on both the server and client machines.</li>
</ul>

<p><strong>1.3. Implementation Steps for SSH over NetBird Cloud:</strong></p>

<ol>
  <li><strong>On the Debian Server:</strong>
    <ul>
      <li>Install <code class="language-plaintext highlighter-rouge">openssh-server</code> (as shown above).</li>
      <li><strong>Install NetBird Agent:</strong>
The recommended method for Debian is often to add NetBird’s package repository and install via <code class="language-plaintext highlighter-rouge">apt</code>. Always check the <a href="https://docs.netbird.io/how-to/installation">official NetBird installation guide</a> for the latest instructions.
<em>Example (verify on netbird.io):</em>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add NetBird repository (command may vary)</span>
<span class="c"># sudo curl -sSL https://pkgs.netbird.io/debian/gpg.key | sudo gpg --dearmor -o /usr/share/keyrings/netbird-archive-keyring.gpg</span>
<span class="c"># echo "deb [signed-by=/usr/share/keyrings/netbird-archive-keyring.gpg] https://pkgs.netbird.io/debian stable main" | sudo tee /etc/apt/sources.list.d/netbird.list</span>
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>netbird
</code></pre></div>        </div>
        <p>Alternatively, a script-based installation might be available:</p>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># curl -fsSL https://pkgs.netbird.io/install.sh | sudo bash</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Obtain a NetBird Setup Key:</strong> Go to your NetBird Management Portal (e.g., <code class="language-plaintext highlighter-rouge">app.netbird.io</code>), navigate to the “Setup Keys” tab, and create a new key.</li>
      <li><strong>Join NetBird:</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>netbird up <span class="nt">--setup-key</span> YOUR_NETBIRD_SETUP_KEY <span class="nt">--hostname</span> my-debian-server
</code></pre></div>        </div>
        <p>The <code class="language-plaintext highlighter-rouge">--hostname</code> flag helps identify the peer in the NetBird UI and can influence its DNS name. You can also assign the server to <strong>Groups</strong> in the NetBird Management Portal for easier policy management, especially as your network grows.</p>
      </li>
    </ul>
  </li>
  <li><strong>In Your NetBird Management Portal (e.g., <code class="language-plaintext highlighter-rouge">app.netbird.io</code>):</strong>
    <ul>
      <li>Verify the machine (<code class="language-plaintext highlighter-rouge">my-debian-server</code>) appears in the “Peers” list.</li>
      <li><strong>Define Access Controls (Policies):</strong> Navigate to “Access Control” -&gt; “Policies”. Create a policy to allow SSH traffic. NetBird policies apply to ‘Peers’ (devices), which are enrolled by users.
        <ul>
          <li><strong>Name:</strong> e.g., “Allow SSH to Debian Servers”</li>
          <li><strong>Action:</strong> <code class="language-plaintext highlighter-rouge">Accept</code></li>
          <li><strong>Sources:</strong> Select the NetBird <strong>Groups</strong> (e.g., <code class="language-plaintext highlighter-rouge">group:developers</code>) or individual <strong>Peers</strong> that should be allowed to initiate SSH connections.</li>
          <li><strong>Destinations:</strong> Select the NetBird <strong>Group</strong> containing <code class="language-plaintext highlighter-rouge">my-debian-server</code> (e.g., <code class="language-plaintext highlighter-rouge">group:production-servers</code>) or <code class="language-plaintext highlighter-rouge">my-debian-server</code> itself by its peer name.</li>
          <li><strong>Protocol:</strong> <code class="language-plaintext highlighter-rouge">TCP</code></li>
          <li><strong>Ports (Destination Ports):</strong> <code class="language-plaintext highlighter-rouge">22</code></li>
          <li>Ensure the policy is enabled.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>On Your Client Machine:</strong>
    <ul>
      <li>Install the NetBird Agent (follow the same installation steps as for the server, appropriate for your client’s OS).</li>
      <li><strong>Log in to NetBird:</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>netbird up
</code></pre></div>        </div>
        <p>This command will typically open a browser window for you to authenticate with your chosen SSO provider.</p>
      </li>
      <li><strong>Connect via SSH:</strong> Use the server’s NetBird Private DNS name (usually ending in <code class="language-plaintext highlighter-rouge">.netbird.self</code>) and the local Linux username on the server.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh localuser@my-debian-server.netbird.self
<span class="c"># Replace 'localuser' with the actual username on the Debian server.</span>
<span class="c"># Replace 'my-debian-server.netbird.self' with the server's NetBird DNS name.</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<hr />

<p><strong>Part 2: Secure Graphical Desktop Access (VNC over NetBird Cloud)</strong></p>

<p>For tasks requiring a graphical desktop environment, VNC is a common solution. By routing VNC traffic over your NetBird network, you gain security and connectivity benefits similar to those for SSH, notably avoiding public VNC port exposure.</p>

<p><strong>2.1. Why VNC?</strong>
VNC (Virtual Network Computing) allows you to view and interact with a remote computer’s graphical desktop environment. This is useful for applications that don’t have a command-line interface or for tasks that are easier to perform graphically.</p>

<p><strong>2.2. Key NetBird Cloud Benefits for VNC:</strong></p>
<ul>
  <li><strong>No Public VNC Port Exposure</strong>: The VNC server port (commonly 5900, 5901, etc.) does not need to be open on your internet router. NetBird creates a secure, encrypted tunnel directly to the VNC server peer on your NetBird network.</li>
  <li><strong>Simplified Firewall Management</strong>: Your local firewall on the Debian machine (<code class="language-plaintext highlighter-rouge">ufw</code>) must allow traffic to the VNC port <em>from the NetBird interface</em>. NetBird Access Controls then determine which NetBird peers can reach this port on the server over the NetBird network.</li>
  <li><strong>Private DNS</strong>: Connect to your VNC server using its NetBird Private DNS name (e.g., <code class="language-plaintext highlighter-rouge">my-debian-server.netbird.self:1</code> or <code class="language-plaintext highlighter-rouge">my-debian-server.netbird.self:5901</code>).</li>
</ul>

<p><strong>2.3. Core TigerVNC Components:</strong>
You’ll need a VNC server on the remote machine and a VNC viewer on your local machine. TigerVNC is a high-performance, open-source VNC implementation.</p>

<ul>
  <li><strong>VNC Server Options:</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">Xvnc</code></strong>: Creates a <em>virtual</em> desktop session, independent of any physical display. Ideal for headless servers.</li>
      <li><strong><code class="language-plaintext highlighter-rouge">x0vncserver</code></strong>: Shares an <em>existing</em> X display (e.g., the physical console display or an active X11 session).</li>
    </ul>
  </li>
  <li><strong>VNC Server Management &amp; Configuration:</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">vncsession</code></strong>: A script to manage <code class="language-plaintext highlighter-rouge">Xvnc</code> sessions, often integrated with systemd for user services. Configuration is typically in <code class="language-plaintext highlighter-rouge">$HOME/.config/tigervnc/config</code> (user-specific) or <code class="language-plaintext highlighter-rouge">/etc/tigervnc/vncserver-config-defaults</code>.
<em>Example user config (<code class="language-plaintext highlighter-rouge">$HOME/.config/tigervnc/config</code>):</em>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># geometry=1920x1080
# depth=24
# securitytypes=VncAuth,TLSVnc # Default often includes TLSVnc
# session=xfce # Specify your installed desktop environment
</code></pre></div>        </div>
      </li>
      <li><strong><code class="language-plaintext highlighter-rouge">vncconfig</code></strong>: A utility to configure and control a <em>running</em> instance of <code class="language-plaintext highlighter-rouge">Xvnc</code>.</li>
      <li><strong><code class="language-plaintext highlighter-rouge">vncpasswd</code></strong>: Crucial for setting the password to access VNC desktops. This must be run by the Linux user who will own the VNC session.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># As the VNC user (e.g., 'localuser')</span>
vncpasswd <span class="c"># Stores obfuscated password in $HOME/.vnc/passwd or $HOME/.config/tigervnc/passwd</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li><strong>VNC Client:</strong>
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">vncviewer</code></strong>: The client application used to connect to the VNC server.</li>
    </ul>
  </li>
</ul>

<p><strong>2.4. Implementation Steps for VNC over NetBird Cloud:</strong></p>

<p><strong>2.4.1. Setting up a Virtual Desktop with <code class="language-plaintext highlighter-rouge">Xvnc</code> (via <code class="language-plaintext highlighter-rouge">vncsession</code>)</strong>
This is suitable for headless servers or creating independent graphical sessions. Using a systemd service is highly recommended for persistence.</p>

<ol>
  <li><strong>On the Debian Server (as <code class="language-plaintext highlighter-rouge">root</code> or with <code class="language-plaintext highlighter-rouge">sudo</code>):</strong>
    <ul>
      <li>Ensure the NetBird Agent is installed and running (see section 1.3).</li>
      <li>Install a Desktop Environment (if not already present). XFCE is a good lightweight option:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>xfce4 xfce4-goodies
</code></pre></div>        </div>
      </li>
      <li>Install TigerVNC Server components:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>tigervnc-standalone-server tigervnc-common
</code></pre></div>        </div>
      </li>
      <li><strong>Configure VNC User and Password (as the intended VNC user, e.g., <code class="language-plaintext highlighter-rouge">localuser</code>):</strong>
If you are <code class="language-plaintext highlighter-rouge">root</code>, switch to the user: <code class="language-plaintext highlighter-rouge">su - localuser</code>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vncpasswd <span class="c"># Set the VNC-specific password</span>
<span class="c"># (Optional) Create/edit $HOME/.config/tigervnc/config as shown in section 2.3</span>
</code></pre></div>        </div>
        <p>If you switched user, type <code class="language-plaintext highlighter-rouge">exit</code> to return to your previous shell.</p>
      </li>
      <li><strong>Start the VNC Server Session (Persistent Session with Systemd Recommended):</strong>
As the VNC user (<code class="language-plaintext highlighter-rouge">localuser</code>), create a systemd user service file: <code class="language-plaintext highlighter-rouge">~/.config/systemd/user/tigervnc@.service</code>:
        <div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">TigerVNC remote desktop server for %I</span>
<span class="py">After</span><span class="p">=</span><span class="s">network-online.target netbird.service # Ensure NetBird is up</span>

<span class="nn">[Service]</span>
<span class="py">Type</span><span class="p">=</span><span class="s">forking</span>
<span class="c"># Use %h for user's home directory in ExecStart path
</span><span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/bin/vncsession %i -SecurityTypes VncAuth,TLSVnc -rfbauth %h/.config/tigervnc/passwd -geometry 1920x1080 -localhost no</span>
<span class="py">ExecStop</span><span class="p">=</span><span class="s">/usr/bin/vncsession -kill %i</span>
<span class="py">Restart</span><span class="p">=</span><span class="s">on-failure</span>
<span class="py">RestartSec</span><span class="p">=</span><span class="s">5</span>

<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">default.target</span>
</code></pre></div>        </div>
        <p><em>Note on <code class="language-plaintext highlighter-rouge">-localhost no</code></em>: This ensures <code class="language-plaintext highlighter-rouge">Xvnc</code> listens on all interfaces, including the NetBird virtual interface.
Then, as the VNC user (ensure lingering is enabled: <code class="language-plaintext highlighter-rouge">sudo loginctl enable-linger localuser</code> – this allows systemd user services to run even when the user <code class="language-plaintext highlighter-rouge">localuser</code> is not actively logged in):</p>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl <span class="nt">--user</span> daemon-reload
systemctl <span class="nt">--user</span> <span class="nb">enable</span> <span class="nt">--now</span> tigervnc@:1 <span class="c"># Starts VNC on display :1 (port 5901)</span>
<span class="c"># To check status: systemctl --user status tigervnc@:1</span>
</code></pre></div>        </div>
      </li>
      <li><strong>Configure Local Firewall (<code class="language-plaintext highlighter-rouge">ufw</code>):</strong>
Allow incoming connections to the VNC port (e.g., 5901 for display :1) <em>specifically from the NetBird interface</em>. First, identify your NetBird interface name (e.g., <code class="language-plaintext highlighter-rouge">wt0</code>, <code class="language-plaintext highlighter-rouge">nb-xxxx</code>) using <code class="language-plaintext highlighter-rouge">ip link show</code> or <code class="language-plaintext highlighter-rouge">nmcli device show</code>.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Replace 'YOUR_NETBIRD_INTERFACE' with the actual interface name</span>
<span class="nb">sudo </span>ufw allow <span class="k">in </span>on YOUR_NETBIRD_INTERFACE to any port 5901 proto tcp
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<p><strong>2.4.2. Sharing an Existing X Display with <code class="language-plaintext highlighter-rouge">x0vncserver</code></strong>
This is for sharing a display that is already active (e.g., a physical monitor).</p>

<ol>
  <li><strong>On the Debian Server:</strong>
    <ul>
      <li>Ensure NetBird Agent is installed and running.</li>
      <li>Ensure an X server is running with the session you want to share.</li>
      <li>Install <code class="language-plaintext highlighter-rouge">tigervnc-scraping-server</code> (usually provides <code class="language-plaintext highlighter-rouge">x0vncserver</code>):
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>tigervnc-scraping-server
</code></pre></div>        </div>
      </li>
      <li><strong>Set VNC Password (as the user whose X session will be shared):</strong> <code class="language-plaintext highlighter-rouge">vncpasswd</code></li>
      <li><strong>Start <code class="language-plaintext highlighter-rouge">x0vncserver</code> (as the user owning the X session):</strong>
This command needs to be run from within the active X session or with the <code class="language-plaintext highlighter-rouge">DISPLAY</code> environment variable correctly set.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x0vncserver <span class="nt">-rfbauth</span> <span class="nv">$HOME</span>/.config/tigervnc/passwd <span class="nt">-SecurityTypes</span> VncAuth,TLSVnc <span class="nt">-display</span> :0 <span class="nt">-localhost</span> no
</code></pre></div>        </div>
      </li>
      <li><strong>Configure Local Firewall (<code class="language-plaintext highlighter-rouge">ufw</code>):</strong>
Allow incoming connections to the VNC port (default 5900 if <code class="language-plaintext highlighter-rouge">x0vncserver</code> attaches to display :0) from the NetBird interface.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Replace 'YOUR_NETBIRD_INTERFACE' with the actual interface name</span>
<span class="nb">sudo </span>ufw allow <span class="k">in </span>on YOUR_NETBIRD_INTERFACE to any port 5900 proto tcp
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<p><strong>2.4.3. NetBird Access Controls for VNC Access</strong>
In your NetBird Management Portal (“Access Control” -&gt; “Policies”):</p>
<ul>
  <li>Create a new policy or modify an existing one.</li>
  <li><strong>Name:</strong> e.g., “Allow VNC to Debian Servers”</li>
  <li><strong>Action:</strong> <code class="language-plaintext highlighter-rouge">Accept</code></li>
  <li><strong>Sources:</strong> Select the NetBird <strong>Groups</strong> or individual <strong>Peers</strong> allowed to initiate VNC connections.</li>
  <li><strong>Destinations:</strong> Select the NetBird <strong>Group</strong> containing <code class="language-plaintext highlighter-rouge">my-debian-server</code> or <code class="language-plaintext highlighter-rouge">my-debian-server</code> itself.</li>
  <li><strong>Protocol:</strong> <code class="language-plaintext highlighter-rouge">TCP</code></li>
  <li><strong>Ports (Destination Ports):</strong> <code class="language-plaintext highlighter-rouge">5901</code> (for <code class="language-plaintext highlighter-rouge">Xvnc</code> on display :1) or <code class="language-plaintext highlighter-rouge">5900</code> (for <code class="language-plaintext highlighter-rouge">x0vncserver</code> on display :0). Create separate rules or use a port list if multiple VNC displays are active.</li>
  <li>Ensure the policy is enabled.</li>
</ul>

<p><strong>2.4.4. Connecting from the Client Machine</strong></p>

<ol>
  <li>Ensure the NetBird Agent is installed on your client and you are logged in (<code class="language-plaintext highlighter-rouge">sudo netbird up</code>).</li>
  <li>Install a VNC Viewer (e.g., <code class="language-plaintext highlighter-rouge">tigervnc-viewer</code> on Linux, or RealVNC Viewer, TightVNC Viewer on other OSes).
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># For Debian/Ubuntu clients</span>
<span class="nb">sudo </span>apt <span class="nb">install </span>tigervnc-viewer
</code></pre></div>    </div>
  </li>
  <li>Connect using the VNC Viewer to the server’s NetBird Private DNS name and display/port:
    <ul>
      <li>For <code class="language-plaintext highlighter-rouge">Xvnc</code> on display <code class="language-plaintext highlighter-rouge">:1</code>: <code class="language-plaintext highlighter-rouge">my-debian-server.netbird.self:1</code> (or <code class="language-plaintext highlighter-rouge">my-debian-server.netbird.self:5901</code>)</li>
      <li>For <code class="language-plaintext highlighter-rouge">x0vncserver</code> on display <code class="language-plaintext highlighter-rouge">:0</code>: <code class="language-plaintext highlighter-rouge">my-debian-server.netbird.self:0</code> (or <code class="language-plaintext highlighter-rouge">my-debian-server.netbird.self:5900</code>)
You will be prompted for the VNC password set on the server.</li>
    </ul>
  </li>
</ol>

<p><strong>2.5. VNC Security Considerations:</strong></p>
<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">SecurityTypes</code> Parameter for TigerVNC:</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">VncAuth</code>: Basic password authentication. The password itself is not sent in cleartext but the VNC protocol’s own encryption for this is not considered strong by modern standards.</li>
      <li><code class="language-plaintext highlighter-rouge">TLSVnc</code>: Encapsulates the VNC protocol within TLS, providing strong encryption for the VNC session itself. This requires X.509 certificates on the server (TigerVNC can generate self-signed ones).</li>
    </ul>
  </li>
  <li><strong>Interaction with NetBird Encryption:</strong> NetBird Cloud provides strong, end-to-end WireGuard® encryption for all traffic between peers on your NetBird network. This means your VNC traffic is already well-protected.
    <ul>
      <li>Using <code class="language-plaintext highlighter-rouge">VncAuth</code> over NetBird is often considered sufficient due to NetBird’s robust underlying encryption.</li>
      <li>Opting for <code class="language-plaintext highlighter-rouge">TLSVnc</code> provides an <em>additional layer</em> of security directly within the VNC protocol. This offers defense-in-depth, which could be beneficial if, hypothetically, the NetBird agent on an endpoint were compromised or if you wanted VNC security independent of the overlay network for any reason. However, it adds complexity to the VNC server setup (certificate management).</li>
    </ul>
  </li>
</ul>

<hr />

<p><strong>Key Features, Benefits, and Improvements of this NetBird-based Framework</strong></p>

<ul>
  <li><strong>Comprehensive Access:</strong> Provides secure solutions for both command-line (SSH) and graphical desktop (VNC) access.</li>
  <li><strong>Unified Security Layer:</strong> NetBird Cloud acts as a common security and connectivity backbone, leveraging peer-to-peer encryption, SSO integration, and centralized access policies.</li>
  <li><strong>Reduced Attack Surface:</strong> Eliminates the need to expose SSH or VNC ports directly to the public internet, drastically minimizing vulnerability to external threats.</li>
  <li><strong>Centralized Network Access Control:</strong> NetBird Access Controls (Policies) manage network reachability to services (SSH, VNC ports) based on peer identity and group membership. Application-level authentication (SSH keys, VNC passwords) remains distinct and is handled by the respective services.</li>
  <li><strong>Simplified Connectivity:</strong> NetBird’s Private DNS allows using consistent, human-readable hostnames for all connections within the private network.</li>
  <li><strong>Flexibility &amp; Modernity:</strong> Users can choose the appropriate remote access method, secured by a modern, zero-trust networking solution.</li>
  <li><strong>Builds on Proven Technology:</strong> Leverages NetBird (for secure overlay networking using WireGuard®) and TigerVNC (a robust, open-source VNC implementation).</li>
</ul>

<p><strong>Hypothesis Evaluation:</strong>
The initial hypothesis that a comprehensive Secure Remote Access framework can be constructed by integrating TigerVNC with NetBird Cloud’s security model (analogous to a previous Tailscale-based framework) is <strong>strongly supported</strong>. This guide successfully demonstrates the merging of these components, showcasing how NetBird can secure both SSH and VNC traffic effectively.</p>

<hr />

<p><strong>Limitations and Areas for Further Development</strong></p>

<ul>
  <li><strong>VNC Authentication vs. NetBird Identity:</strong> VNC session authentication (password) is separate from the NetBird SSO identity. There’s no direct SSO into the VNC session itself.</li>
  <li><strong>VNC Performance:</strong> While generally good, VNC performance can vary based on network latency, bandwidth, display resolution, and desktop environment complexity.</li>
  <li><strong>Complexity of VNC Server &amp; Desktop Environment Setup:</strong> Detailed configuration of desktop environments and advanced VNC server tuning can be intricate and OS-dependent.</li>
  <li><strong>Granularity of VNC Session Control:</strong> Fine-grained control <em>within</em> an active VNC session (e.g., clipboard sharing, read-only access) is managed by TigerVNC’s capabilities, not by NetBird Access Controls.</li>
</ul>

<hr />

<p><strong>Broader Implications of this Framework for Secure Remote Access Practices</strong></p>

<ul>
  <li><strong>Enhanced Productivity:</strong> Offers flexible, secure, and easy-to-use access to essential tools, boosting remote work capabilities.</li>
  <li><strong>Simplified IT Administration:</strong> Centralizes network access control via the NetBird Management Portal, reducing the complexity of managing firewalls and individual VPN configurations.</li>
  <li><strong>Improved Security Posture:</strong> Significantly enhances security by adopting zero-trust principles, ensuring that only authenticated and explicitly authorized peers can connect to services.</li>
  <li><strong>Shift Towards Modern Solutions:</strong> Encourages the adoption of modern, software-defined perimeter solutions like NetBird Cloud over traditional VPNs or direct public exposure of services.</li>
</ul>

<hr />

<p><strong>Actionable Next Steps for Implementing, Refining, and Further Developing this Framework</strong></p>

<ol>
  <li><strong>Detailed Implementation Guides:</strong>
    <ul>
      <li>Create specific tutorials for various desktop environments (KDE, GNOME) used with <code class="language-plaintext highlighter-rouge">vncsession</code> and systemd user services over NetBird.</li>
      <li>Develop a more focused guide for <code class="language-plaintext highlighter-rouge">x0vncserver</code> scenarios, perhaps including auto-starting it within a desktop session.</li>
    </ul>
  </li>
  <li><strong>Refinement and Advanced Configuration:</strong>
    <ul>
      <li>Provide advanced NetBird Access Control policy examples (e.g., restricting access based on specific source peer IPs within the NetBird network, though group-based access is generally preferred for scalability).</li>
      <li>Include a dedicated troubleshooting section for common VNC-over-NetBird and SSH-over-NetBird issues.</li>
      <li>Detail the setup of <code class="language-plaintext highlighter-rouge">TLSVnc</code> with self-signed or CA-signed certificates for users desiring that additional VNC-specific encryption layer.</li>
      <li>Explore and document the use of NetBird’s “Network Routes” and “DNS Forwarding” features for scenarios involving access to legacy networks or services through a NetBird peer acting as a gateway.</li>
    </ul>
  </li>
</ol>]]></content><author><name></name></author><category term="aid&gt;linux&gt;software" /></entry><entry><title type="html">ratpoisonrc dot files</title><link href="https://ib.bsb.br/ratpoisonrc/" rel="alternate" type="text/html" title="ratpoisonrc dot files" /><published>2025-05-09T00:00:00+00:00</published><updated>2025-05-13T14:05:11+00:00</updated><id>https://ib.bsb.br/ratpoisonrc</id><content type="html" xml:base="https://ib.bsb.br/ratpoisonrc/"><![CDATA[<h1 id="kkantadas">kkantadas</h1>
<p><code class="language-plaintext highlighter-rouge">https://raw.githubusercontent.com/kkantadas/dotfiles/refs/heads/main/.ratpoisonrc</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>exec /usr/bin/rpws init 2 -k

if egrep -iq 'touchpad' /proc/bus/input/devices; then
exec   synclient VertEdgeScroll=1 &amp;
exec   synclient TapButton1=1 &amp;
exec xsetroot -cursor_name left_ptr &gt; /dev/null &amp;
#exec redshift &amp;
#exec xset r rate 250 25 &amp;
#exec ./bin/conky.sh &amp; ratpoison -c "set padding 0 15 0 0" &amp; sleep 10s &amp;&amp; killall conky ; ratpoison -c "set padding 0 0 0 0" &gt; /dev/null &amp;
#addhook switchwin exec rpthumb
#addhook quit exec rpexpose --clean
#bind v exec rpselect

#addhook switchwin banish
#exec unclutter -idle 1 -jitter 3 -root
#warp on

#RP commands
#aliasarestart "exec ratpoison -c 'restart ~/.config/ratpoison/.ratpoisonrc'"
#set chdir ~/.config/ratpoison/

set bgcolor grey1
set fgcolor grey
#set fwcolor black
set fwcolor #2f2f2f
set bwcolor #1f1f1f
#set bwcolor black

set barborder 0
set border 2

set bargravity n
#set padding 0 0 0 0
#set font fixed-6
#set resizeunit 1
set historysize 100
set historycompaction 1
startup_message off
set wingravity n#ne
#set winliststyle row
#set winfmt %t
#set inputwidth 800

##KeyBindings
newkmap   super-x
#definekey super-x d     exec 'exec' # exec dmenu_run &gt; /dev/null &amp;
#definekey super-x d     exec rofi -show drun -theme dmenu
#definekey super-x d     exec rofi -theme flat-orange -font "hack 11" -show drun
definekey super-x l      exec ./bin/lapower.sh
definekey super-x j      exec urxvt -e alsamixer 
definekey super-x space  exec ./bin/split.sh
definekey super-x R      exec ratpoison -c restart
definekey super-x D      exec dmenumount
#definekey super-x p     exec ./bin/mptest.sh
definekey super-x c      exec dratmenu
definekey top s-x        readkey super-x
definekey super-x Q      exec ratpoison -c quit
definekey super-x e      exec emacs
definekey super-x E      exec emacs -q -l ~/Emacs-project/init.el
#definekey super-x m     exec  urxvt -e mutt &gt; /dev/null &amp;


definekey super-x 1    exec  ratpoison -c "frestore `tail -n 1 .config/ratframe/ratframe1`" ;  ratpoison -c next
definekey super-x 2    exec  ratpoison -c "frestore `tail -n 1 .config/ratframe/ratframe2`" ;  ratpoison -c next
definekey super-x 3    exec  ratpoison -c "frestore `tail -n 1 .config/ratframe/ratframe3`" ;  ratpoison -c next
definekey super-x 4    exec  ./bin/rat-frame4.sh
definekey super-x 5    exec  ./bin/rat-frame5.sh
definekey super-x 6    exec  ./bin/rat-frame6.sh
definekey super-x 7    exec   transset-df 0.7
definekey super-x 8    exec   transset-df 0.8
definekey super-x 9    exec   transset-df 0.9
definekey super-x 0    exec   transset-df 1.0
definekey super-x "apostrophe"  exec ./bin/wifi.sh
definekey super-x F1   exec ratpoison -c "hsplit 21/29" -c focusleft -c "hsplit " -c focusleft -c "resize -190 0" -c  exchangeright
definekey super-x x   exec rat-ranger &amp;
definekey super-x X   exec urxvt -e ranger &amp;
#definekey super-x w exec chromium --process-per-site
definekey super-x w exec rat-or-raise firefox &amp;
definekey super-x m exec scrot -e 'convert $f label:"%x %X" -append $f'
# exec sh ~/.fehbg &amp;

#definekey super-x v  exec ./bin/conky.sh &amp; sleep 9s &amp;&amp; killall conky
#definekey super-x v   exec ./bin/conky.sh &amp; ratpoison -c "set padding 0 15 0 0" &amp; sleep 25s &amp;&amp; killall conky ; ratpoison -c "set padding 0 0 0 0" &amp;
#definekey super-x t   exec ratpoison -c "setenv fs1 `ratpoison -c 'fdump'`"

#restore the frame layout in slot #1
definekey super-x p exec ratpoison -c "frestore `tail -n 1 .config/ratframe/fsPing`" &amp; urxvt -e ping -c4 google.com &amp;&amp; ratpoison -c undo
#definekey super-x p exec ratpoison -c "vsplit 12/13" -c focusdown &amp;  urxvt -e ping -c4 google.com  &amp;&amp; ratpoison -c undo
#definekey super-x p exec ratpoison -c "echo `ping -c2 google.com | grep data`"
#definekey super-x C exec ratpoison -c "echo Connecting Ethernet `con` "
definekey super-x r exec ratpoison -c "frestore `ratpoison -c 'getenv fs1'`"
definekey super-x s exec ratpoison -c "vsplit 5/6" -c focusdown
definekey super-x S exec ratpoison -c "vsplit 5/6" -c focusdown &amp; urxvt  
definekey super-x f exec xterm -e w3m -B

definekey super-x l exec ratpoison -c fdump &gt; dump

definekey super-x f exec xterm -e w3m -B
definekey super-x h exec urxvt -e htop &amp;

definekey super-x Down exec xrandr -o normal &amp;
definekey super-x Left exec xrandr -o right &amp;
definekey super-x Right exec xrandr -o left &amp;

#definekey super-x a exec ratpoison -d :0.0 -c "echo Batt `acpi -b|cut -c-11-25` `acpi -t` `date`"
definekey super-x a exec ratpoison -d :0.0 -c "echo `~/bin/ping-check.sh` Net: `cat /sys/class/net/enp2s0f1/operstate` @  Wifi: `cat /sys/class/net/wlan0/operstate /sys/class/net/wlp3s0/operstate` `iw wlan0 info |grep ssid|cut -c6-50` @ Bat:`acpi -b |cut -c11-27` @  Mem: `free -h | grep Mem | cut -c27-30` @ Themp:`acpi -t|cut -c15-19`°C @ `date -R|cut -c18-22` `date|cut -c1-10`"
#: `connmanctl state|grep State|cut -c11-19`
definekey super-x G exec urxvt -e vim Vedatxt/Bhagavat-Gita/Bhagavad-Gita.txt &amp;
definekey super-x K exec urxvt -e less Vedatxt/A.C.Bhaktivadanta\ Swami\ Prabhupada/Krsna_Book.txt &amp;
definekey super-x B exec urxvt -e ranger Vedatxt/Srimad\ Bhagavatam/ &amp;
definekey super-x V exec urxvt -e ranger Vedatxt &amp;

bind k exec ratpoison -c focusup
bind j exec ratpoison -c focusdown 
bind h exec ratpoison -c focusleft
bind l exec ratpoison -c focusright
bind C-k exec ratpoison -c exchangeup
bind C-j exec ratpoison -c exchangedown
bind C-h exec ratpoison -c exchangeleft
bind C-l exec ratpoison -c exchangeright

bind C exec urxvt -bg grey -fg black &amp; sleep 0.3 &amp;&amp; transset-df -a 0.7
bind s exec ratpoison -c  vsplit -c focusdown
bind parenright exec ratpoison -c "frestore `tail -n 1 .config/ratframe/dump`" &amp; sleep 0.6s &amp;&amp; sudo systemctl suspend  

bind S exec ratpoison -c  hsplit -c focusright
bind O exec ratpoison -c kill 
bind o exec ratpoison -c delete
exec /usr/bin/rpws init 2 -k

if egrep -iq 'touchpad' /proc/bus/input/devices; then
exec   synclient VertEdgeScroll=1 &amp;
exec   synclient TapButton1=1 &amp;
exec xsetroot -cursor_name left_ptr &gt; /dev/null &amp;
#exec redshift &amp;
#exec xset r rate 250 25 &amp;
#exec ./bin/conky.sh &amp; ratpoison -c "set padding 0 15 0 0" &amp; sleep 10s &amp;&amp; killall conky ; ratpoison -c "set padding 0 0 0 0" &gt; /dev/null &amp;
#addhook switchwin exec rpthumb
#addhook quit exec rpexpose --clean
#bind v exec rpselect

#addhook switchwin banish
#exec unclutter -idle 1 -jitter 3 -root
#warp on

#RP commands
#aliasarestart "exec ratpoison -c 'restart ~/.config/ratpoison/.ratpoisonrc'"
#set chdir ~/.config/ratpoison/

set bgcolor grey1
set fgcolor grey
#set fwcolor black
set fwcolor #2f2f2f
set bwcolor #1f1f1f
#set bwcolor black

set barborder 0
set border 2

set bargravity n
#set padding 0 0 0 0
#set font fixed-6
#set resizeunit 1
set historysize 100
set historycompaction 1
startup_message off
set wingravity n#ne
#set winliststyle row
#set winfmt %t
#set inputwidth 800

##KeyBindings
newkmap   super-x
#definekey super-x d     exec 'exec' # exec dmenu_run &gt; /dev/null &amp;
#definekey super-x d     exec rofi -show drun -theme dmenu
#definekey super-x d     exec rofi -theme flat-orange -font "hack 11" -show drun
definekey super-x l      exec ./bin/lapower.sh
definekey super-x j      exec urxvt -e alsamixer 
definekey super-x space  exec ./bin/split.sh
definekey super-x R      exec ratpoison -c restart
definekey super-x D      exec dmenumount
#definekey super-x p     exec ./bin/mptest.sh
#definekey super-x c      exec xterm  -e 'rm -rfv ~/{.cache/{/google-chrome/Default/,common-lisp/,gstreamer-1.0/,vimb/WebKitCache},  .cache/yay/,.local/share/webkitgtk,.pki/} ;read'
#definekey super-x c      exec xterm -e 'rm -rfv .cache/mozilla/firefox/3s57yr41.default-release/{thumbnails/,cache2/,weave/logs/,storage/default} .cache/yay/ .local/share/webkitgtk .pki/ .mozilla/firefox/3s57yr41.default-release/{thumbnails/,cache2/,weave/logs/,storage/default/} ;read'
definekey super-x c      exec xterm -e 'rm -rfv .cache/mozilla/firefox/3s57yr41.default-release/{thumbnails/,cache2/,weave/logs/,storage/default} .cache/yay/ .local/share/webkitgtk .pki/ . ;read'
definekey top s-x        readkey super-x
definekey super-x Q      exec ratpoison -c quit
definekey super-x e      exec emacs
#definekey super-x m     exec  urxvt -e mutt &gt; /dev/null &amp;


definekey super-x 1    exec  ratpoison -c "frestore `tail -n 1 .config/ratframe/ratframe1`" ;  ratpoison -c next
definekey super-x 2    exec  ratpoison -c "frestore `tail -n 1 .config/ratframe/ratframe2`" ;  ratpoison -c next
definekey super-x 3    exec  ratpoison -c "frestore `tail -n 1 .config/ratframe/ratframe3`" ;  ratpoison -c next
definekey super-x 4    exec  ./bin/frame4.sh
definekey super-x 5    exec  ./bin/frame5.sh
definekey super-x 6    exec  ./bin/frame6.sh
definekey super-x 7    exec   transset-df 0.7
definekey super-x 8    exec   transset-df 0.8
definekey super-x 9    exec   transset-df 0.9
definekey super-x 0    exec   transset-df 1.0
definekey super-x "apostrophe"  exec ./bin/wifi.sh
definekey super-x F1   exec ratpoison -c "hsplit 21/29" -c focusleft -c "hsplit " -c focusleft -c "resize -190 0" -c  exchangeright
definekey super-x x    exec urxvt -e ranger &amp;
definekey super-x m    exec scrot -e 'convert $f label:"%x %X" -append $f'
# exec sh ~/.fehbg &amp;

#definekey super-x v  exec ./bin/conky.sh &amp; sleep 9s &amp;&amp; killall conky
#definekey super-x v   exec ./bin/conky.sh &amp; ratpoison -c "set padding 0 15 0 0" &amp; sleep 25s &amp;&amp; killall conky ; ratpoison -c "set padding 0 0 0 0" &amp;

#definekey super-x t   exec ratpoison -c "setenv fs1 `ratpoison -c 'fdump'`"

#restore the frame layout in slot #1
definekey super-x p exec ratpoison -c "frestore `tail -n 1 .config/ratframe/fsPing`" &amp; urxvt -fn "xft:monaco:pixelsize=10" -e ping -c4 google.com &amp;&amp; ratpoison -c undo
#definekey super-x p exec ratpoison -c "frestore `tail -n 1 .config/ratframe/fsPing`" &amp; urxvt -e ping -c4 google.com &amp;&amp; ratpoison -c undo

#definekey super-x p exec ratpoison -c "vsplit 12/13" -c focusdown &amp;  urxvt -e ping -c4 google.com  &amp;&amp; ratpoison -c undo
#definekey super-x p exec ratpoison -c "echo `ping -c2 google.com | grep data`"
#definekey super-x C exec ratpoison -c "echo Connecting Ethernet `con` "
definekey super-x r exec ratpoison -c "frestore `ratpoison -c 'getenv fs1'`"
definekey super-x s exec ratpoison -c "vsplit 5/6" -c focusdown
definekey super-x f exec xterm -e w3m -B

definekey super-x l exec ratpoison -c fdump &gt; dump

definekey super-x f exec xterm -e w3m -B
definekey super-x h exec urxvt -e htop &amp;

definekey super-x Down exec xrandr -o normal &amp;
definekey super-x Left exec xrandr -o right &amp;
definekey super-x Right exec xrandr -o left &amp;

#definekey super-x a exec ratpoison -d :0.0 -c "echo Batt `acpi -b|cut -c-11-25` `acpi -t` `date`"
definekey super-x a exec ratpoison -d :0.0 -c "echo `~/bin/ping-check.sh` Net: `cat /sys/class/net/enp2s0f1/operstate` @  Wifi: `cat /sys/class/net/wlan0/operstate /sys/class/net/wlp3s0/operstate` `iw wlan0 info |grep ssid|cut -c6-50` @ Bat:`acpi -b |cut -c11-27` @  Mem: `free -h | grep Mem | cut -c27-30` @ Themp:`acpi -t|cut -c15-19`°C @ `date -R|cut -c18-22` `date|cut -c1-10`"
#: `connmanctl state|grep State|cut -c11-19`

definekey super-x G exec urxvt -e vim Vedatxt/Bhagavat-Gita/Bhagavad-Gita.txt &amp;
definekey super-x K exec urxvt -e less Vedatxt/A.C.Bhaktivadanta\ Swami\ Prabhupada/Krsna_Book.txt &amp;
definekey super-x B exec urxvt -e ranger Vedatxt/Srimad\ Bhagavatam/ &amp;
definekey super-x V exec urxvt -e ranger Vedatxt &amp;

#bind j exec amixer -q set Master 10+
#bind h exec amixer -q set Master 10-
#bind C exec urxvt -bg grey -fg black &amp; sleep 0.3 &amp;&amp; transset-df -a 0.7
bind s exec ratpoison -c  vsplit -c focusdown
#bind parenright exec ratpoison -c "frestore `tail -n 1 .config/ratframe/dump`" &amp; sleep 0.5s &amp;&amp; sudo systemctl suspend  

bind S exec ratpoison -c  hsplit -c focusright
bind C exec  urxvt -bg '#F8C888' -fg '#392613' -fn "xft:comic shanns:pixelsize=16"
#bind h exec amixer -q set Speaker unmute # 100%
#bind l exec amixer -q set Speaker mute   # 0%
bind t exec ratpoison -d :0.0 -c "echo `ratpoison -c windows`"
bind m exec ratpoison -c undo "title mutt" ; urxvt -e mutt &gt; /dev/null &amp;
bind c exec urxvt
bind d exec xterm &amp; sleep .3s &amp;&amp; transset-df -a 0.7
#set font ScaBenguit 
#set font terminus
set font ter-13n
#set font Liberation Serif
#set font "-*-fixed-bold-r-normal-*-5-*-*-*-c-*-*-*"
set inputwidth 250

GPG_TTY=$(tty)
export GPG_TTY
exec xset b 0
</code></pre></div></div>

<h1 id="dimatura">dimatura</h1>
<p><code class="language-plaintext highlighter-rouge">https://raw.githubusercontent.com/dimatura/dot_ratpoison/refs/heads/master/.ratpoisonrc</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># vim: commentstring=#%s

# replacement for this hack: set PATH in .profile
# setenv PATH /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/home/dmaturan/bin

# start up progs ***********************************************#{{{

## misc daemons
#exec gnome-power-manager
#exec gnome-settings-daemon
#exec cinnamon-settings-daemon
#exec nm-applet
#exec urxvtd -q -o -f

## widgets
# set bg
exec xsetroot -solid gray8 -cursor_name left_ptr
# conky status monitor
#exec conky
# also: alpha 128, monitor
#exec trayer --align left --edge top --distance 0 --expand true --transparent true --SetDockType true --SetPartialStrut true --height 14 --width 50 --widthtype percent
# MIT coords
#exec redshift -l 40.44:-80.0
#exec redshift -l 42.35391379569696:-71.09029769897461

## ui
#exec easystroke
# set up keyboard layout switching (press both shifts)
#exec setxkbmap -option grp:switch,grp:shifts_toggle,grp_led:scroll us,es
# also run xmodmap
#exec xmodmap ~/.Xmodmaprc
# thinkpad button
#exec tpb -d

#unmanage rpbar
#exec rpbar
unmanage panel

#exec ssh-add .ssh/id_rsa
#exec daemon mpdas
#}}}

# appearance/ui ***************************************************#{{{

set startupmessage 0
set border 2
# left top right bottom, leave px for bars
#set padding 0 14 0 13
set padding 0 14 0 0
#set padding 0 28 0 0

# bar at south and middle, fits well with 15 padding at bottom
set bargravity c
set barpadding 4 4
# some colors:
# DarkTurquoise, lightskyblue, Gold, Goldenrod, Lavender,
# LightSlateGray, LightSteelBlue, PowderBlue, SkyBlue, palegreen
# DarkSeaGreen, Navy, MidnightBlue, DarkSlateGray, gray12
set bgcolor #6a9fb5
set fgcolor #151515
# from vim mustang colorthem
# set fwcolor #b1d631
set fwcolor #d28445

#set font -*-snap-*-*-*-*-*-*-*-*-*-*-*-*
#set font -*-terminus-*-*-*-*-20-*-*-*-*-*-*-*
#set font -*-terminus-medium-r-normal-*-14-*-*-*-*-*-*-*
set font -*-helvetica-*-r-*-*-*-*-*-*-*-*-*-*
set inputwidth 600
set historysize 1000

set msgwait 1
# don't move mouse cursor around
set warp 0

#}}}

# hooks ********************************************************#{{{

# get rid of mouse cursor
# addhook key banish

# for rpbar
#addhook switchwin exec rpbarsend
#addhook switchframe exec rpbarsend
#addhook switchgroup exec rpbarsend
#addhook deletewindow exec rpbarsend
#addhook titlechanged exec rpbarsend
#addhook newwindow exec rpbarsend

#addhook switchwin exec ratpoison -c "windows %n %t%s" &gt; /tmp/rpbarfifo
#addhook switchframe exec ratpoison -c "windows %n %t%s" &gt; /tmp/rpbarfifo
#addhook switchgroup exec ratpoison -c "windows %n %t%s" &gt; /tmp/rpbarfifo
#addhook deletewindow exec ratpoison -c "windows %n %t%s" &gt; /tmp/rpbarfifo
# TODO use this
# echo -e "`ratpoison -c "windows %n %t%s"`\00"

#addhook switchwin exec echo r &gt; /tmp/rpbarfifo
#addhook switchframe exec echo r &gt; /tmp/rpbarfifo
#addhook switchgroup exec echo r &gt; /tmp/rpbarfifo
#addhook deletewindow exec echo r &gt; /tmp/rpbarfifo

# kill programs there should be only one of
addhook restart exec killall conky
#addhook restart exec killall rpbar
#addhook restart exec killall redshift
addhook quit exec killall conky
#addhook quit exec killall rpbar
#addhook quit exec killall redshift

# verbose group switching
addhook switchgroup groups
#addhook switchwin windows %n %c %t

#}}}

# aliases ******************************************************#{{{
#alias ipython exec urxvt -e ipython
alias ipythonq exec ipython-qtconsole
alias bpython exec urxvt -e bpython
alias firefox exec firefox
#alias chromium exec chromium-browser
alias chrome exec google-chrome
alias mc exec urxvt -e mc
alias mocp exec urxvt -e mocp
alias ncmpc exec urxvt -e ncmpc -c
alias alsamixer exec urxvt -e alsamixer
#alias wicd exec wicd-client -n
alias vimwiki exec gvim ~/repos/notes/vimwiki/index.wiki
alias bash exec urxvt -e bash
alias fmlove exec fmlove.sh
alias shell exec urxvt -e

#}}}

# key bindings *************************************************#{{{

# escape key
escape C-a

bind Return nextscreen
#abort key sequence
bind Escape abort
# TODO interactive group selection with dwm or something like it
bind g groups
bind semicolon colon

# example of creating keymap
#newkmap ctrl-x
#definekey ctrl-x n next

# running apps#{{{

# TODO consider execa, execf
# TODO see archlinux wiki for various dwm-based launchers
bind space exec dmenu_run
bind e colon exec gvim
bind c exec urxvt -e fish
unbind b
bind b exec urxvt -e bash
#bind b exec urxvt -e st
# execute in terminal; "!" used to be for execute,
# but dmenu is better for that
unbind exclam
bind exclam colon exec urxvt -e
# i is 'info' but I don't use it
unbind i
bind i exec urxvt -e ipython
#bind c exec urxvt -is +sb -fg '#51a366' -bg '#111111' -fn 'xft:DejaVu Sans Mono:pixelsize=11:antialias=false:autohinting=true'
#}}}

# window management#{{{

bind W exec dratmenu.py
# a window selector using rpselect
#bind w exec rpselect
# a window selector using ratmen
#bind w exec ratmenwin
bind w exec dratmenu.py
bind C-w exec dratmenu.py

# some vi-like bindings
bind v hsplit
bind s split
bind q remove
bind o only

bind j focusdown
bind h focusleft
bind l focusright
bind k focusup
#bind j exec ratpy focus down
#bind h exec ratpy focus left
#bind l exec ratpy focus right
#bind k exec ratpy focus up

bind J exchangedown
bind K exchangeup
bind H exchangeleft
bind L exchangeright

bind r resize
bind R resize
#bind Q kill
bind Q delete

# workspaces
#definekey top s-F1 rpws1
#definekey top s-F2 rpws2
#definekey top s-F3 rpws3
#definekey top s-F4 rpws4
#definekey top s-F5 rpws5

# just go with raw groups for now
# TODO start with 1?
definekey top s-F1 gselect 0
definekey top s-F2 gselect 1
definekey top s-F3 gselect 2
definekey top s-F4 gselect 3
definekey top s-F5 gselect 4
definekey top s-F6 gselect 5

#definekey top s-F1 sselect 0
#definekey top s-F2 sselect 1
#definekey top s-F3 sselect 2

#definekey top s-Left prevscreen
#definekey top s-Right nextscreen
definekey top s-Return nextscreen

#}}}

# music and audio#{{{

# 'm' is bound to last message by default but I don't use that
unbind m
bind m exec st -e ncmpcpp
#bind greater exec mpc next
#bind less exec mpc prev
#bind slash exec mpc toggle
bind greater exec pytify -n
bind less exec pytify -p
bind slash exec pytify -pp

#volume bindings
# chose these F-keys because they correspond to fn-keys in eee.
#bind F10 exec amixer sset PCM toggle
#bind F11 exec bin/ratpy_audio.py amixer_volume -
#bind F12 exec bin/ratpy_audio.py amixer_volume +
bind F11 exec bin/ratpy_audio.py pamixer_volume -
bind F12 exec bin/ratpy_audio.py pamixer_volume +

#}}}

# links to window key#{{{
# use describekey to find these !!
definekey top s-n link n
definekey top s-p link p
definekey top s-b link b
definekey top s-j link j
definekey top s-k link k
definekey top s-l link l
definekey top s-h link h
definekey top s-o link o
definekey top s-q link q
definekey top s-w link w
definekey top s-x link x
definekey top s-r link r
definekey top s-r link R
definekey top s-s link s
definekey top s-v link v
#definekey top s-u link u
definekey top s-0 link 0
definekey top s-1 link 1
definekey top s-2 link 2
definekey top s-3 link 3
definekey top s-4 link 4
definekey top s-5 link 5
definekey top s-6 link 6
definekey top s-7 link 7
definekey top s-8 link 8
definekey top s-9 link 9
definekey top s-S link S
definekey top s-N link N
definekey top s-P link P
# audio controls
definekey top s-F10 link F10
definekey top s-F11 link F11
definekey top s-F12 link F12
# a couple of important keys
definekey top s-space link space
definekey top s-Return link Return
#}}}

# rat emulation#{{{

definekey top s-Up ratrelwarp 0 -15
definekey top s-Down ratrelwarp 0 15
definekey top s-Left ratrelwarp -15 0
definekey top s-Right ratrelwarp 15 0
definekey top s-Menu ratclick 1

#definekey top Home ratrelwarp 0 -15
#definekey top End ratrelwarp 0 15
#definekey top Delete ratrelwarp -15 0
#definekey top Next ratrelwarp 15 0
# weird menu key. Also possible: Insert, Backslash. KP_Insert, asterisk
#definekey top s-Menu ratclick 1

#}}}

#}}}
</code></pre></div></div>]]></content><author><name></name></author><category term="scratchpad" /><category term="linux&gt;dotfile" /></entry></feed>