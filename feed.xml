<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ib.bsb.br/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ib.bsb.br/" rel="alternate" type="text/html" /><updated>2025-05-22T15:34:31+00:00</updated><id>https://ib.bsb.br/feed.xml</id><title type="html">infoBAG</title><entry><title type="html">Diagnostic script for disks and mount with `exfat-fuse`</title><link href="https://ib.bsb.br/diagnostic-script-for-disks-and-mount-with-exfat-fuse/" rel="alternate" type="text/html" title="Diagnostic script for disks and mount with `exfat-fuse`" /><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T15:14:10+00:00</updated><id>https://ib.bsb.br/diagnostic-script-for-disks-and-mount-with-exfat-fuse</id><content type="html" xml:base="https://ib.bsb.br/diagnostic-script-for-disks-and-mount-with-exfat-fuse/"><![CDATA[<ol>
  <li><strong>Install <code class="language-plaintext highlighter-rouge">exfat-fuse</code>:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>exfat-fuse
</code></pre></div>    </div>
  </li>
  <li><strong>Mount using FUSE:</strong>
    <ul>
      <li><strong>For root ownership:</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>mount <span class="nt">-t</span> fuse.exfat <span class="nt">-o</span> <span class="nv">uid</span><span class="o">=</span>0,gid<span class="o">=</span>0,rw,noatime,allow_other <span class="nv">UUID</span><span class="o">=</span><span class="s2">"69AF-5F99"</span> /mnt/my_external_hdd
</code></pre></div>        </div>
        <p>Or directly:</p>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>mount.exfat-fuse <span class="nt">-o</span> <span class="nv">uid</span><span class="o">=</span>0,gid<span class="o">=</span>0,rw,noatime,allow_other /dev/sdb /mnt/my_external_hdd
</code></pre></div>        </div>
      </li>
      <li><strong>For specific non-root user ownership (e.g., UID/GID 1000):</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>mount <span class="nt">-t</span> fuse.exfat <span class="nt">-o</span> <span class="nv">uid</span><span class="o">=</span>1000,gid<span class="o">=</span>1000,rw,noatime,allow_other <span class="nv">UUID</span><span class="o">=</span><span class="s2">"69AF-5F99"</span> /mnt/my_external_hdd
</code></pre></div>        </div>
        <p>Or directly:</p>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>mount.exfat-fuse <span class="nt">-o</span> <span class="nv">uid</span><span class="o">=</span>1000,gid<span class="o">=</span>1000,rw,noatime,allow_other /dev/sdb /mnt/my_external_hdd
</code></pre></div>        </div>
      </li>
      <li><code class="language-plaintext highlighter-rouge">allow_other</code>: This FUSE-specific option is important if you want users other than the one who mounted the filesystem (in this case, root, even if <code class="language-plaintext highlighter-rouge">uid</code>/<code class="language-plaintext highlighter-rouge">gid</code> are set for appearance) to access it.</li>
    </ul>
  </li>
  <li><strong>Unmounting FUSE filesystems:</strong>
To unmount a filesystem mounted with <code class="language-plaintext highlighter-rouge">exfat-fuse</code> (or <code class="language-plaintext highlighter-rouge">fuse.exfat</code>), you use:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>fusermount <span class="nt">-u</span> /mnt/my_external_hdd
</code></pre></div>    </div>
  </li>
</ol>

<h1 id="script-to-gather-extensive-diagnostic-information-about-a-specified-disk-device">Script to gather extensive diagnostic information about a specified disk device.</h1>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c"># All outputs will be concatenated into a single text file.</span>

<span class="c"># --- Configuration ---</span>
<span class="nv">DEVICE</span><span class="o">=</span><span class="s2">"/dev/sdb"</span>
<span class="nv">OUTPUT_FILE</span><span class="o">=</span><span class="s2">"/home/linaro/sdb.txt"</span>
<span class="nv">TESTDISK_CWD_LOG_FILE</span><span class="o">=</span><span class="s2">"testdisk.log"</span> <span class="c"># TestDisk creates this in the Current Working Directory</span>

<span class="c"># --- Pre-flight Checks ---</span>

<span class="c"># Ensure the script is executed with root privileges</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">id</span> <span class="nt">-u</span><span class="si">)</span><span class="s2">"</span> <span class="nt">-ne</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
  </span><span class="nb">echo</span> <span class="s2">"This script requires root privileges to access raw disk devices."</span> <span class="o">&gt;</span>&amp;2
  <span class="nb">echo</span> <span class="s2">"Please run it using sudo: sudo </span><span class="nv">$0</span><span class="s2">"</span> <span class="o">&gt;</span>&amp;2
  <span class="nb">exit </span>1
<span class="k">fi</span>

<span class="c"># Initialize the output file (do this before device check so errors can be logged to it)</span>
<span class="nb">echo</span> <span class="s2">"Disk Diagnostics for </span><span class="nv">$DEVICE</span><span class="s2"> - Report generated on </span><span class="si">$(</span><span class="nb">date</span><span class="si">)</span><span class="s2">"</span> <span class="o">&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"========================================================================"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>

<span class="c"># Verify that the specified device exists and is a block device</span>
<span class="k">if</span> <span class="o">[</span> <span class="o">!</span> <span class="nt">-b</span> <span class="s2">"</span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"Error: The device </span><span class="nv">$DEVICE</span><span class="s2"> does not exist or is not a block device."</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span> <span class="o">&gt;</span>&amp;2
    <span class="nb">exit </span>1
<span class="k">fi</span>

<span class="c"># --- Helper Function ---</span>

<span class="c"># Function to execute a command, log its output/errors, and check its exit status</span>
run_and_log<span class="o">()</span> <span class="o">{</span>
  <span class="nb">local </span><span class="nv">description</span><span class="o">=</span><span class="s2">"</span><span class="nv">$1</span><span class="s2">"</span>
  <span class="nb">local </span><span class="nv">command_to_run</span><span class="o">=</span><span class="s2">"</span><span class="nv">$2</span><span class="s2">"</span>
  <span class="nb">local </span><span class="nv">tool_name</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">3</span><span class="k">:-}</span><span class="s2">"</span> <span class="c"># Optional: simple name of the tool for 'command -v' check</span>

  <span class="nb">echo</span> <span class="s2">"Executing: </span><span class="nv">$description</span><span class="s2">"</span> <span class="c"># Console feedback</span>

  <span class="c"># If a tool name is provided, check if it's installed</span>
  <span class="k">if</span> <span class="o">[</span> <span class="nt">-n</span> <span class="s2">"</span><span class="nv">$tool_name</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    if</span> <span class="o">!</span> <span class="nb">command</span> <span class="nt">-v</span> <span class="s2">"</span><span class="nv">$tool_name</span><span class="s2">"</span> &amp;&gt; /dev/null<span class="p">;</span> <span class="k">then
      </span><span class="nb">echo</span> <span class="s2">"------------------------------------------------------------------------"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="nb">echo</span> <span class="s2">"Command: </span><span class="nv">$command_to_run</span><span class="s2"> (SKIPPED)"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="nb">echo</span> <span class="s2">"Warning: Tool '</span><span class="nv">$tool_name</span><span class="s2">' not found. Please install it and try again."</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="nb">echo</span> <span class="s2">"Skipping: </span><span class="nv">$description</span><span class="s2"> (tool '</span><span class="nv">$tool_name</span><span class="s2">' not found)"</span>
      <span class="nb">echo</span> <span class="s2">"------------------------------------------------------------------------"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="k">return
    fi
  fi
  
  </span><span class="nb">echo</span> <span class="s2">"------------------------------------------------------------------------"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
  <span class="nb">echo</span> <span class="s2">"Command: </span><span class="nv">$command_to_run</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
  <span class="nb">echo</span> <span class="s2">"Output:"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
  
  <span class="c"># Using eval to correctly handle commands with pipes, redirections, and other shell constructs</span>
  <span class="c"># This is generally safe when command_to_run strings are hardcoded within the script.</span>
  <span class="nb">eval</span> <span class="s2">"</span><span class="nv">$command_to_run</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span> 2&gt;&amp;1
  <span class="nb">local </span><span class="nv">exit_status</span><span class="o">=</span><span class="nv">$?</span>
  
  <span class="k">if</span> <span class="o">[</span> <span class="nv">$exit_status</span> <span class="nt">-ne</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span> <span class="c"># Ensure warning is on a new line if command produced output</span>
    <span class="nb">echo</span> <span class="s2">"Warning: Command exited with status </span><span class="nv">$exit_status</span><span class="s2">."</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
  <span class="k">fi
  
  </span><span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
  <span class="nb">echo</span> <span class="s2">"------------------------------------------------------------------------"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
  <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="o">}</span>

<span class="c"># --- Script Main Body ---</span>

<span class="nb">echo</span> <span class="s2">"Starting diagnostic data collection for </span><span class="nv">$DEVICE</span><span class="s2">."</span>
<span class="nb">echo</span> <span class="s2">"All output will be directed to </span><span class="nv">$OUTPUT_FILE</span><span class="s2">."</span>
<span class="nb">echo</span> <span class="s2">"Please note: Some operations may take a significant amount of time."</span>
<span class="nb">echo</span> <span class="s2">""</span>

<span class="c"># --- Section 1: Core Investigation Tools ---</span>
<span class="nb">echo</span> <span class="s2">"&gt;&gt;&gt; Section: Core Investigation Tools &lt;&lt;&lt;"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span><span class="p">;</span> <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>

run_and_log <span class="s2">"fdisk -l (List partition table)"</span> <span class="s2">"fdisk -l </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"fdisk"</span>
run_and_log <span class="s2">"parted print (Display detailed partition information)"</span> <span class="s2">"parted -s </span><span class="nv">$DEVICE</span><span class="s2"> print"</span> <span class="s2">"parted"</span>
run_and_log <span class="s2">"sfdisk -d (Dump partition table structure)"</span> <span class="s2">"sfdisk -d </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"sfdisk"</span>
run_and_log <span class="s2">"sfdisk --verify (Verify partition table consistency)"</span> <span class="s2">"sfdisk --verify </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"sfdisk"</span>
run_and_log <span class="s2">"blkid </span><span class="nv">$DEVICE</span><span class="s2"> (Show block device attributes for </span><span class="nv">$DEVICE</span><span class="s2">)"</span> <span class="s2">"blkid </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"blkid"</span>
run_and_log <span class="s2">"blkid (Show block device attributes for all devices - for context)"</span> <span class="s2">"blkid"</span> <span class="s2">"blkid"</span>
run_and_log <span class="s2">"file -s </span><span class="nv">$DEVICE</span><span class="s2"> (Determine data type/filesystem signature of </span><span class="nv">$DEVICE</span><span class="s2">)"</span> <span class="s2">"file -s </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"file"</span>
run_and_log <span class="s2">"lsblk -f </span><span class="nv">$DEVICE</span><span class="s2"> (List block devices with filesystem info for </span><span class="nv">$DEVICE</span><span class="s2">)"</span> <span class="s2">"lsblk -f </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"lsblk"</span>

<span class="c"># --- Section 2: Disk Health and Low-Level Analysis Tools ---</span>
<span class="nb">echo</span> <span class="s2">"&gt;&gt;&gt; Section: Disk Health and Low-Level Analysis Tools &lt;&lt;&lt;"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span><span class="p">;</span> <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>

<span class="nb">echo</span> <span class="s2">"Note: For 'smartctl', the 'smartmontools' package is typically required."</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
run_and_log <span class="s2">"smartctl -a (S.M.A.R.T. health data)"</span> <span class="s2">"smartctl -a </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"smartctl"</span>

run_and_log <span class="s2">"dd + hexdump (Inspect first 512 bytes - MBR area)"</span> <span class="s2">"dd if=</span><span class="nv">$DEVICE</span><span class="s2"> bs=512 count=1 | hexdump -C"</span> <span class="s2">"dd"</span> <span class="c"># hexdump is part of bsdmainutils or similar</span>
<span class="c"># Reduced count for strings scan to 10MB to keep script execution time reasonable</span>
run_and_log <span class="s2">"dd + strings (Scan first 10MB for printable strings)"</span> <span class="s2">"dd if=</span><span class="nv">$DEVICE</span><span class="s2"> bs=1M count=10 status=none | strings"</span> <span class="s2">"dd"</span> <span class="c"># strings is part of binutils</span>

<span class="nb">echo</span> <span class="s2">"Note: For 'gdisk', the 'gdisk' package is typically required."</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
run_and_log <span class="s2">"gdisk -l (GPT partition table list - also checks MBR)"</span> <span class="s2">"gdisk -l </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"gdisk"</span>

<span class="c"># --- Section 3: Filesystem-Specific and Recovery-Oriented Tools ---</span>
<span class="nb">echo</span> <span class="s2">"&gt;&gt;&gt; Section: Filesystem-Specific and Recovery-Oriented Tools &lt;&lt;&lt;"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span><span class="p">;</span> <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>

<span class="nb">echo</span> <span class="s2">"Note: For 'gpart', the 'gpart' package is typically required."</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
run_and_log <span class="s2">"gpart (Attempt to guess PC-type hard disk partitions)"</span> <span class="s2">"gpart </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"gpart"</span>

<span class="nb">echo</span> <span class="s2">"Note: For 'mmls', the 'sleuthkit' package is typically required."</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
run_and_log <span class="s2">"mmls (Display partition layout using The Sleuth Kit)"</span> <span class="s2">"mmls </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="s2">"mmls"</span>

<span class="c"># --- Section 4: TestDisk Logging ---</span>
<span class="nb">echo</span> <span class="s2">"&gt;&gt;&gt; Section: TestDisk Logging &lt;&lt;&lt;"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span><span class="p">;</span> <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Note: For 'testdisk', the 'testdisk' package is typically required."</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Attempting to run TestDisk with logging to capture initial analysis..."</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"TestDisk will create '</span><span class="nv">$TESTDISK_CWD_LOG_FILE</span><span class="s2">' in the current working directory: </span><span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span><span class="s2">"</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>

<span class="c"># Remove old log file if it exists to ensure a fresh log for this run</span>
<span class="nb">rm</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$TESTDISK_CWD_LOG_FILE</span><span class="s2">"</span>

<span class="c"># Check if testdisk command exists before trying to run</span>
<span class="k">if </span><span class="nb">command</span> <span class="nt">-v</span> testdisk &amp;&gt; /dev/null<span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"------------------------------------------------------------------------"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    <span class="nb">echo</span> <span class="s2">"Command: testdisk /debug /log </span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    <span class="nb">echo</span> <span class="s2">"Output (TestDisk's own log '</span><span class="nv">$TESTDISK_CWD_LOG_FILE</span><span class="s2">' will be appended below if created):"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    
    <span class="c"># Run TestDisk. It should perform analysis, log to its file, and exit.</span>
    <span class="c"># Suppress its direct stdout/stderr as the primary log data goes to its own file.</span>
    testdisk /debug /log <span class="nv">$DEVICE</span> <span class="o">&gt;</span> /dev/null 2&gt;&amp;1
    <span class="nv">testdisk_exit_status</span><span class="o">=</span><span class="nv">$?</span>

    <span class="k">if</span> <span class="o">[</span> <span class="nv">$testdisk_exit_status</span> <span class="nt">-ne</span> 0 <span class="o">]</span><span class="p">;</span> <span class="k">then
        </span><span class="nb">echo</span> <span class="s2">"Warning: TestDisk command may not have completed successfully (exit status: </span><span class="nv">$testdisk_exit_status</span><span class="s2">)."</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    <span class="k">fi

    if</span> <span class="o">[</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$TESTDISK_CWD_LOG_FILE</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
      </span><span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="nb">echo</span> <span class="s2">"Appending content of </span><span class="nv">$TESTDISK_CWD_LOG_FILE</span><span class="s2"> (created by TestDisk):"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="nb">cat</span> <span class="s2">"</span><span class="nv">$TESTDISK_CWD_LOG_FILE</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="nb">echo</span> <span class="s2">"Temporary log file '</span><span class="nv">$TESTDISK_CWD_LOG_FILE</span><span class="s2">' has been appended and will now be removed."</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span> <span class="c"># Also to console</span>
      <span class="nb">rm</span> <span class="s2">"</span><span class="nv">$TESTDISK_CWD_LOG_FILE</span><span class="s2">"</span>
    <span class="k">else
      </span><span class="nb">echo</span> <span class="s2">"Warning: TestDisk log file ('</span><span class="nv">$TESTDISK_CWD_LOG_FILE</span><span class="s2">') was not found in the current directory after execution."</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
      <span class="nb">echo</span> <span class="s2">"TestDisk may not have run as expected or created the log file."</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    <span class="k">fi
    </span><span class="nb">echo</span> <span class="s2">"------------------------------------------------------------------------"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="k">else
    </span><span class="nb">echo</span> <span class="s2">"------------------------------------------------------------------------"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    <span class="nb">echo</span> <span class="s2">"Command: testdisk /debug /log </span><span class="nv">$DEVICE</span><span class="s2"> (SKIPPED)"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    <span class="nb">echo</span> <span class="s2">"Warning: Tool 'testdisk' not found. Please install it and try again."</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    <span class="nb">echo</span> <span class="s2">"Skipping: TestDisk logging (tool 'testdisk' not found)"</span>
    <span class="nb">echo</span> <span class="s2">"------------------------------------------------------------------------"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
    <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="k">fi</span>

<span class="c"># --- Section 5: Kernel Messages ---</span>
<span class="nb">echo</span> <span class="s2">"&gt;&gt;&gt; Section: Kernel Messages (dmesg) &lt;&lt;&lt;"</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span><span class="p">;</span> <span class="nb">echo</span> <span class="s2">""</span> <span class="o">&gt;&gt;</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="c"># Construct a grep pattern to match the base device name (e.g., sdb) and the full path (e.g., /dev/sdb)</span>
<span class="c"># This helps capture messages that might refer to the disk in different ways.</span>
<span class="nv">DEVICE_BASENAME</span><span class="o">=</span><span class="si">$(</span><span class="nb">basename</span> <span class="s2">"</span><span class="nv">$DEVICE</span><span class="s2">"</span><span class="si">)</span>
run_and_log <span class="s2">"dmesg (Kernel messages related to </span><span class="nv">$DEVICE</span><span class="s2">)"</span> <span class="se">\</span>
            <span class="s2">"dmesg | grep -Ei --binary-files=text </span><span class="se">\"</span><span class="s2">(</span><span class="nv">$DEVICE_BASENAME</span><span class="s2">|</span><span class="nv">$DEVICE</span><span class="s2">)</span><span class="se">\"</span><span class="s2"> || echo 'No specific dmesg entries found for </span><span class="nv">$DEVICE</span><span class="s2"> or </span><span class="nv">$DEVICE_BASENAME</span><span class="s2">'"</span> <span class="se">\</span>
            <span class="s2">"dmesg"</span> <span class="c"># grep is usually available</span>

<span class="c"># --- Completion ---</span>
<span class="nb">echo</span> <span class="s2">""</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span> <span class="c"># Final blank line in file for spacing</span>
<span class="nb">echo</span> <span class="s2">"Diagnostic script for </span><span class="nv">$DEVICE</span><span class="s2"> has completed."</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"All collected information has been saved to: </span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span> | <span class="nb">tee</span> <span class="nt">-a</span> <span class="s2">"</span><span class="nv">$OUTPUT_FILE</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Please review the contents of this file carefully."</span>

<span class="nb">exit </span>0
</code></pre></div></div>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">Generate `.bashrc` aliases from .desktop files</title><link href="https://ib.bsb.br/generate-bashrc-aliases-from-desktop-files/" rel="alternate" type="text/html" title="Generate `.bashrc` aliases from .desktop files" /><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T15:31:11+00:00</updated><id>https://ib.bsb.br/generate-bashrc-aliases-from-desktop-files</id><content type="html" xml:base="https://ib.bsb.br/generate-bashrc-aliases-from-desktop-files/"><![CDATA[<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="#!/bin/bash

# Script to extract Exec= lines from .desktop files
# and create a list of bash aliases in a deterministic order.

# --- Configuration ---
DESKTOP_DIR=&quot;/home/linaro/.local/share/applications&quot;
OUTPUT_FILE=&quot;/home/linaro/desktop-exec-alias.txt&quot;
ALIAS_COUNTER=0
# --- End Configuration ---

echo &quot;--- Script Starting ---&quot;
echo &quot;Desktop directory: $DESKTOP_DIR&quot;
echo &quot;Output file: $OUTPUT_FILE&quot;

# Ensure the target directory for .desktop files exists
if [ ! -d &quot;$DESKTOP_DIR&quot; ]; then
    echo &quot;Error: Directory $DESKTOP_DIR does not exist.&quot; &gt;&amp;2
    exit 1
fi
echo &quot;Desktop directory confirmed to exist.&quot;

# Clear or create the output file for a fresh list
# This ensures that if the script is run multiple times,
# the output file contains only the latest aliases.
&gt; &quot;$OUTPUT_FILE&quot;
echo &quot;Output file initialized (cleared or created).&quot;

echo &quot;Finding .desktop files...&quot;
# Find all files ending with .desktop in the specified directory.
# -print0 outputs filenames null-terminated.
# sort -z sorts null-terminated input (ensures deterministic order of aliases).
# The while loop with IFS= and read -r -d $&#39;\0&#39;
# robustly handles filenames that might contain spaces or special characters.
find &quot;$DESKTOP_DIR&quot; -name &quot;*.desktop&quot; -type f -print0 | sort -z | while IFS= read -r -d $&#39;\0&#39; desktop_file; do
    echo &quot;----------------------------------------&quot; # Separator for each file
    echo &quot;Processing file: $desktop_file&quot; # DEBUG

    # Try to get the Exec line using grep first for debugging
    # This helps see if the line is even present in a way grep recognizes
    echo &quot;Attempting to grep &#39;^Exec=&#39; from file...&quot; # DEBUG
    grep_exec_line=$(grep &#39;^Exec=&#39; &quot;$desktop_file&quot;) # DEBUG
    if [ -n &quot;$grep_exec_line&quot; ]; then
        echo &quot;DEBUG: grep found the following Exec line(s):&quot; # DEBUG
        echo &quot;$grep_exec_line&quot; # DEBUG
    else
        echo &quot;DEBUG: grep did NOT find any line starting with &#39;Exec=&#39;&quot; # DEBUG
    fi

    # Original sed command to extract the value
    echo &quot;Attempting to extract Exec value with sed...&quot; # DEBUG
    exec_value=$(sed -n &#39;s/^Exec=//p&#39; &quot;$desktop_file&quot; | head -n 1)
    
    if [ -n &quot;$exec_value&quot; ]; then
        echo &quot;DEBUG: sed extracted value: &#39;$exec_value&#39;&quot; # DEBUG
        # Append the alias command to the output file.
        # Single quotes around &#39;$exec_value&#39; are important to preserve
        # the command exactly as it is, including spaces and special characters,
        # when writing to the alias file.
        echo &quot;alias $ALIAS_COUNTER=&#39;$exec_value&#39;&quot; &gt;&gt; &quot;$OUTPUT_FILE&quot;
        ALIAS_COUNTER=$((ALIAS_COUNTER + 1)) # Increment the alias number
    else
        echo &quot;DEBUG: sed did NOT extract any value for Exec=&quot; # DEBUG
        # Output a warning to the standard error stream if an Exec line
        # couldn&#39;t be found or its value was empty in a specific .desktop file.
        echo &quot;Warning: Could not extract Exec value from $desktop_file (or line was not found/empty after &#39;Exec=&#39;)&quot; &gt;&amp;2
    fi
done

echo &quot;----------------------------------------&quot;
echo &quot;File processing loop finished.&quot;
echo &quot;Alias list generation complete.&quot;
echo &quot;Output saved to: $OUTPUT_FILE&quot;
echo &quot;Total aliases generated: $ALIAS_COUNTER&quot; # Shows actual count
echo &quot;--- Script Finished ---&quot;

exit 0" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">#!/bin/bash

# Script to extract Exec= lines from .desktop files
# and create a list of bash aliases in a deterministic order.

# --- Configuration ---
DESKTOP_DIR=&quot;/home/linaro/.local/share/applications&quot;
OUTPUT_FILE=&quot;/home/linaro/desktop-exec-alias.txt&quot;
ALIAS_COUNTER=0
# --- End Configuration ---

echo &quot;--- Script Starting ---&quot;
echo &quot;Desktop directory: $DESKTOP_DIR&quot;
echo &quot;Output file: $OUTPUT_FILE&quot;

# Ensure the target directory for .desktop files exists
if [ ! -d &quot;$DESKTOP_DIR&quot; ]; then
    echo &quot;Error: Directory $DESKTOP_DIR does not exist.&quot; &gt;&amp;2
    exit 1
fi
echo &quot;Desktop directory confirmed to exist.&quot;

# Clear or create the output file for a fresh list
# This ensures that if the script is run multiple times,
# the output file contains only the latest aliases.
&gt; &quot;$OUTPUT_FILE&quot;
echo &quot;Output file initialized (cleared or created).&quot;

echo &quot;Finding .desktop files...&quot;
# Find all files ending with .desktop in the specified directory.
# -print0 outputs filenames null-terminated.
# sort -z sorts null-terminated input (ensures deterministic order of aliases).
# The while loop with IFS= and read -r -d $&#39;\0&#39;
# robustly handles filenames that might contain spaces or special characters.
find &quot;$DESKTOP_DIR&quot; -name &quot;*.desktop&quot; -type f -print0 | sort -z | while IFS= read -r -d $&#39;\0&#39; desktop_file; do
    echo &quot;----------------------------------------&quot; # Separator for each file
    echo &quot;Processing file: $desktop_file&quot; # DEBUG

    # Try to get the Exec line using grep first for debugging
    # This helps see if the line is even present in a way grep recognizes
    echo &quot;Attempting to grep &#39;^Exec=&#39; from file...&quot; # DEBUG
    grep_exec_line=$(grep &#39;^Exec=&#39; &quot;$desktop_file&quot;) # DEBUG
    if [ -n &quot;$grep_exec_line&quot; ]; then
        echo &quot;DEBUG: grep found the following Exec line(s):&quot; # DEBUG
        echo &quot;$grep_exec_line&quot; # DEBUG
    else
        echo &quot;DEBUG: grep did NOT find any line starting with &#39;Exec=&#39;&quot; # DEBUG
    fi

    # Original sed command to extract the value
    echo &quot;Attempting to extract Exec value with sed...&quot; # DEBUG
    exec_value=$(sed -n &#39;s/^Exec=//p&#39; &quot;$desktop_file&quot; | head -n 1)
    
    if [ -n &quot;$exec_value&quot; ]; then
        echo &quot;DEBUG: sed extracted value: &#39;$exec_value&#39;&quot; # DEBUG
        # Append the alias command to the output file.
        # Single quotes around &#39;$exec_value&#39; are important to preserve
        # the command exactly as it is, including spaces and special characters,
        # when writing to the alias file.
        echo &quot;alias $ALIAS_COUNTER=&#39;$exec_value&#39;&quot; &gt;&gt; &quot;$OUTPUT_FILE&quot;
        ALIAS_COUNTER=$((ALIAS_COUNTER + 1)) # Increment the alias number
    else
        echo &quot;DEBUG: sed did NOT extract any value for Exec=&quot; # DEBUG
        # Output a warning to the standard error stream if an Exec line
        # couldn&#39;t be found or its value was empty in a specific .desktop file.
        echo &quot;Warning: Could not extract Exec value from $desktop_file (or line was not found/empty after &#39;Exec=&#39;)&quot; &gt;&amp;2
    fi
done

echo &quot;----------------------------------------&quot;
echo &quot;File processing loop finished.&quot;
echo &quot;Alias list generation complete.&quot;
echo &quot;Output saved to: $OUTPUT_FILE&quot;
echo &quot;Total aliases generated: $ALIAS_COUNTER&quot; # Shows actual count
echo &quot;--- Script Finished ---&quot;

exit 0</code>
</section>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">Google Drive API and `rclone`</title><link href="https://ib.bsb.br/google-drive-api-and-rclone/" rel="alternate" type="text/html" title="Google Drive API and `rclone`" /><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T15:16:46+00:00</updated><id>https://ib.bsb.br/google-drive-api-and-rclone</id><content type="html" xml:base="https://ib.bsb.br/google-drive-api-and-rclone/"><![CDATA[<p><strong>Step 1: Install <code class="language-plaintext highlighter-rouge">rclone</code></strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>rclone
</code></pre></div></div>

<p><strong>Step 2: Configure <code class="language-plaintext highlighter-rouge">rclone</code> for Google Drive (with Own API Credentials - Highly Recommended)</strong></p>

<p>Using rclone’s default API credentials can lead to rate-limiting errors during large transfers. Creating your own is more reliable.</p>

<ol>
  <li>
    <p><strong>Create your own Google API Client ID and Secret for <code class="language-plaintext highlighter-rouge">rclone</code>:</strong>
Follow the official <code class="language-plaintext highlighter-rouge">rclone</code> guide: <a href="https://rclone.org/drive/#making-your-own-client-id">https://rclone.org/drive/#making-your-own-client-id</a>
This process involves using the Google Cloud Console. It might seem complex, but it’s a one-time setup that significantly improves reliability for large transfers. Keep your generated <code class="language-plaintext highlighter-rouge">client_id</code> and <code class="language-plaintext highlighter-rouge">client_secret</code> handy.</p>
  </li>
  <li>
    <p><strong>Run <code class="language-plaintext highlighter-rouge">rclone config</code>:</strong></p>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rclone config
</code></pre></div>    </div>
    <p>Follow the interactive prompts:</p>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">n</code> (New remote)</li>
      <li><code class="language-plaintext highlighter-rouge">name&gt;</code>: Enter a short name (e.g., <code class="language-plaintext highlighter-rouge">gdrive_backup</code>)</li>
      <li><code class="language-plaintext highlighter-rouge">Storage&gt;</code>: Type <code class="language-plaintext highlighter-rouge">drive</code> or select the number for Google Drive.</li>
      <li><code class="language-plaintext highlighter-rouge">client_id&gt;</code>: <strong>Enter the Client ID you created.</strong></li>
      <li><code class="language-plaintext highlighter-rouge">client_secret&gt;</code>: <strong>Enter the Client Secret you created.</strong></li>
      <li><code class="language-plaintext highlighter-rouge">scope&gt;</code>: Choose <code class="language-plaintext highlighter-rouge">1</code> (Full access all files).</li>
      <li><code class="language-plaintext highlighter-rouge">root_folder_id&gt;</code>: Press Enter (leave blank for full Drive access, or specify a folder ID if desired).</li>
      <li><code class="language-plaintext highlighter-rouge">service_account_file&gt;</code>: Press Enter (leave blank).</li>
      <li><code class="language-plaintext highlighter-rouge">Edit advanced config? (y/n)&gt;</code>: <code class="language-plaintext highlighter-rouge">n</code></li>
      <li><code class="language-plaintext highlighter-rouge">Use auto config? (y/n)&gt;</code>: <code class="language-plaintext highlighter-rouge">y</code>
        <ul>
          <li>This will attempt to open a browser for authentication. If on a headless server, copy the URL it provides into a browser on another machine, authenticate, and then copy the verification code back to <code class="language-plaintext highlighter-rouge">rclone</code>.</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">Configure this as a Shared Drive (Team Drive)? (y/n)&gt;</code>: <code class="language-plaintext highlighter-rouge">n</code> (unless you are using a Shared Drive).</li>
      <li>Review the summary and if OK, choose <code class="language-plaintext highlighter-rouge">y</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">q</code> (Quit config).</li>
    </ul>
  </li>
</ol>

<p><strong>Step 3: Prepare for the Long Transfer (Using <code class="language-plaintext highlighter-rouge">screen</code> or <code class="language-plaintext highlighter-rouge">tmux</code>)</strong></p>

<p>A 500GiB upload can take many hours or even days. If your SSH session disconnects, the <code class="language-plaintext highlighter-rouge">rclone</code> process will terminate. Use a terminal multiplexer like <code class="language-plaintext highlighter-rouge">screen</code> or <code class="language-plaintext highlighter-rouge">tmux</code> to prevent this.</p>

<ul>
  <li><strong>Using <code class="language-plaintext highlighter-rouge">screen</code> (simpler for beginners):</strong>
    <ol>
      <li>Install if needed: <code class="language-plaintext highlighter-rouge">sudo apt install screen</code></li>
      <li>Start a new screen session: <code class="language-plaintext highlighter-rouge">screen -S rclone_upload_session</code></li>
      <li>You are now “inside” the screen session. Run your <code class="language-plaintext highlighter-rouge">rclone</code> command here.</li>
      <li>To detach (leave it running in the background): Press <code class="language-plaintext highlighter-rouge">Ctrl+A</code>, then <code class="language-plaintext highlighter-rouge">d</code>.</li>
      <li>To reattach later: <code class="language-plaintext highlighter-rouge">screen -r rclone_upload_session</code></li>
    </ol>
  </li>
</ul>

<p><strong>Step 4: Perform a Dry Run (Crucial Safety Check)</strong></p>

<p>Before transferring any data, simulate the process to see what <code class="language-plaintext highlighter-rouge">rclone</code> <em>would</em> do.
Let’s assume you want to copy everything to a folder named <code class="language-plaintext highlighter-rouge">My500GB_External_Backup</code> on your Google Drive.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure you are in your screen/tmux session</span>
rclone copy /mnt/my_external_hdd gdrive_backup:My500GB_External_Backup <span class="nt">--dry-run</span> <span class="nt">-P</span> <span class="nt">--check-first</span> <span class="nt">--checksum</span> <span class="nt">--skip-links</span> <span class="nt">--verbose</span> <span class="nt">--log-file</span><span class="o">=</span>rclone_dry_run_<span class="si">$(</span><span class="nb">date</span> +%Y%m%d_%H%M%S<span class="si">)</span>.log
</code></pre></div></div>
<ul>
  <li><code class="language-plaintext highlighter-rouge">gdrive_backup:My500GB_External_Backup</code>: Replace <code class="language-plaintext highlighter-rouge">gdrive_backup</code> with your rclone remote name.</li>
  <li><code class="language-plaintext highlighter-rouge">--dry-run</code>: Simulates the copy.</li>
  <li><code class="language-plaintext highlighter-rouge">-P</code> (or <code class="language-plaintext highlighter-rouge">--progress</code>): Shows progress.</li>
  <li><code class="language-plaintext highlighter-rouge">--check-first</code>: Checks all source/destination files before starting.</li>
  <li><code class="language-plaintext highlighter-rouge">--checksum</code>: Uses checksums for comparison (more reliable than just size/modtime).</li>
  <li><code class="language-plaintext highlighter-rouge">--skip-links</code>: Ignores symbolic links.</li>
  <li><code class="language-plaintext highlighter-rouge">--verbose</code>: More detailed output.</li>
  <li><code class="language-plaintext highlighter-rouge">--log-file</code>: Logs all output. <strong>Review this log carefully.</strong></li>
</ul>

<p><strong>If the dry run output looks correct and shows no errors, proceed.</strong></p>

<p><strong>Step 5: Execute the Full Data Transfer</strong></p>

<p>Remove <code class="language-plaintext highlighter-rouge">--dry-run</code> and add more robustness flags:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure you are in your screen/tmux session</span>
rclone copy /mnt/my_external_hdd gdrive_backup:My500GB_External_Backup <span class="se">\</span>
    <span class="nt">-P</span> <span class="se">\</span>
    <span class="nt">--check-first</span> <span class="se">\</span>
    <span class="nt">--checksum</span> <span class="se">\</span>
    <span class="nt">--skip-links</span> <span class="se">\</span>
    <span class="nt">--verbose</span> <span class="se">\</span>
    <span class="nt">--log-file</span><span class="o">=</span>rclone_upload_<span class="si">$(</span><span class="nb">date</span> +%Y%m%d_%H%M%S<span class="si">)</span>.log <span class="se">\</span>
    <span class="nt">--stats</span> 1m <span class="se">\</span>
    <span class="nt">--retries</span> 5 <span class="se">\</span>
    <span class="nt">--low-level-retries</span> 10 <span class="se">\</span>
    <span class="nt">--buffer-size</span> 64M <span class="se">\</span>
    <span class="nt">--drive-chunk-size</span> 64M <span class="se">\</span>
    <span class="nt">--transfers</span> 4
</code></pre></div></div>
<ul>
  <li><strong>New/Important Flags:</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">--log-file</code>: <strong>Essential for diagnosing any issues during the long transfer.</strong></li>
      <li><code class="language-plaintext highlighter-rouge">--stats 1m</code>: Prints transfer stats every minute.</li>
      <li><code class="language-plaintext highlighter-rouge">--retries 5</code>: Retries failed file transfers up to 5 times.</li>
      <li><code class="language-plaintext highlighter-rouge">--low-level-retries 10</code>: Retries low-level operations (like single HTTP requests).</li>
      <li><code class="language-plaintext highlighter-rouge">--buffer-size 64M</code>: In-memory buffer per transfer. Adjust based on your RAM (e.g., 32M, 128M).</li>
      <li><code class="language-plaintext highlighter-rouge">--drive-chunk-size 64M</code>: Uploads large files to Google Drive in 64MB chunks. Can significantly improve speed for large files (default is 8M). Max is 256M.</li>
      <li><code class="language-plaintext highlighter-rouge">--transfers 4</code>: Number of files to transfer in parallel. Default is 4. Adjust based on your internet upload speed and CPU (e.g., 2-8).</li>
    </ul>
  </li>
</ul>

<p>Monitor progress via the terminal and the log file (<code class="language-plaintext highlighter-rouge">tail -f rclone_upload_...log</code>). Be patient.</p>

<p><strong>Step 6: Verify the Upload (Critical!)</strong></p>

<p>After <code class="language-plaintext highlighter-rouge">rclone copy</code> finishes, you <strong>must</strong> verify that all data was transferred correctly.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure you are in your screen/tmux session</span>
rclone check /mnt/my_external_hdd gdrive_backup:My500GB_External_Backup <span class="se">\</span>
    <span class="nt">-P</span> <span class="se">\</span>
    <span class="nt">--checksum</span> <span class="se">\</span>
    <span class="nt">--one-way</span> <span class="se">\</span>
    <span class="nt">--log-file</span><span class="o">=</span>rclone_check_<span class="si">$(</span><span class="nb">date</span> +%Y%m%d_%H%M%S<span class="si">)</span>.log <span class="se">\</span>
    <span class="nt">--verbose</span>
</code></pre></div></div>
<ul>
  <li><code class="language-plaintext highlighter-rouge">rclone check</code>: Compares source and destination.</li>
  <li><code class="language-plaintext highlighter-rouge">--checksum</code>: <strong>Crucial for verifying data integrity.</strong> Compares files based on content hashes.</li>
  <li><code class="language-plaintext highlighter-rouge">--one-way</code>: Checks that every file in the source exists and is identical in the destination. It won’t report extra files in the destination (which is fine after a <code class="language-plaintext highlighter-rouge">copy</code>).</li>
  <li><code class="language-plaintext highlighter-rouge">--log-file</code>: Logs the verification process.</li>
</ul>

<p>Review the <code class="language-plaintext highlighter-rouge">rclone_check</code> log. Ideally, it should report 0 differences or only differences that are explainable (e.g., files skipped by <code class="language-plaintext highlighter-rouge">--skip-links</code>). Any unexpected “missing on destination” or “files differ” entries need investigation.</p>

<hr />

<p><strong>Phase 3: Safely Unmounting the External Hard Drive</strong></p>

<p><strong>Step 1: Flush Disk Caches</strong></p>

<p>Before unmounting, ensure all cached data is written to the disk:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sync
sync</span>
</code></pre></div></div>
<p>Running <code class="language-plaintext highlighter-rouge">sync</code> (some do it twice for good measure) flushes filesystem buffers.</p>

<p><strong>Step 2: Unmount the Drive</strong></p>

<ol>
  <li>Ensure your terminal is not currently in the mount point directory:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~
</code></pre></div>    </div>
  </li>
  <li>Unmount:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>umount /mnt/my_external_hdd
</code></pre></div>    </div>
    <p>Or by device/UUID: <code class="language-plaintext highlighter-rouge">sudo umount UUID="69AF-5F99"</code></p>
  </li>
</ol>

<p><strong>Step 3: Troubleshooting Unmount Issues (“target is busy”)</strong></p>

<p>If you get a “target is busy” error:</p>
<ul>
  <li>Make sure no terminal or application is using <code class="language-plaintext highlighter-rouge">/mnt/my_external_hdd</code>.</li>
  <li>Use these commands to find the culprit process(es):
```bash
sudo lsof +D /mnt/my_external_hdd
    <h1 id="or">OR</h1>
    <p>sudo fuser -vmM /mnt/my_external_hdd
```    Close the identified applications or (carefully) kill the processes.</p>
  </li>
  <li>As a last resort, a “lazy unmount” can be used, but ensure <code class="language-plaintext highlighter-rouge">sync</code> was run:
<code class="language-plaintext highlighter-rouge">sudo umount -l /mnt/my_external_hdd</code></li>
</ul>

<p>Once successfully unmounted, you can safely disconnect the USB drive.</p>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">Running LightRAG on Baremetal Linux (ARM64)</title><link href="https://ib.bsb.br/lightrag-arm64/" rel="alternate" type="text/html" title="Running LightRAG on Baremetal Linux (ARM64)" /><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T04:02:53+00:00</updated><id>https://ib.bsb.br/lightrag-arm64</id><content type="html" xml:base="https://ib.bsb.br/lightrag-arm64/"><![CDATA[<p>This guide provides instructions and considerations for running LightRAG on a baremetal Linux machine with an ARM64 architecture.</p>

<h2 id="1-system-prerequisites">1. System Prerequisites</h2>

<p>Before installing LightRAG, ensure your ARM64 Linux system meets the following prerequisites:</p>

<ul>
  <li><strong>Python:</strong> Python 3.10 or newer is required. You can check your Python version with <code class="language-plaintext highlighter-rouge">python3 --version</code>. If you need to install or upgrade Python, consult your Linux distribution’s package manager (e.g., <code class="language-plaintext highlighter-rouge">apt</code> for Debian/Ubuntu, <code class="language-plaintext highlighter-rouge">yum</code> for CentOS/RHEL, <code class="language-plaintext highlighter-rouge">dnf</code> for Fedora).
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Example for Debian/Ubuntu</span>
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>python3 python3-pip python3-venv
</code></pre></div>    </div>
  </li>
  <li><strong>Build Tools:</strong> Since some Python packages may need to be compiled from source on ARM64 (if pre-built wheels are not available), you’ll need standard build tools.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Example for Debian/Ubuntu</span>
<span class="nb">sudo </span>apt <span class="nb">install </span>build-essential python3-dev
<span class="c"># For other distributions, you might need packages like 'gcc', 'g++', 'make'</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Pip:</strong> Ensure <code class="language-plaintext highlighter-rouge">pip</code> for Python 3 is installed. It’s usually included with Python or can be installed separately.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> ensurepip <span class="nt">--upgrade</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Git:</strong> You’ll need Git to clone the repository.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Example for Debian/Ubuntu</span>
<span class="nb">sudo </span>apt <span class="nb">install </span>git
</code></pre></div>    </div>
  </li>
  <li><strong>System Dependencies for <code class="language-plaintext highlighter-rouge">textract</code> (Optional but Recommended for Full File Support):</strong>
LightRAG uses the <code class="language-plaintext highlighter-rouge">textract</code> library to extract text from various file types (PDF, DOCX, etc.). To enable support for these formats, you’ll need to install their underlying system dependencies. The specific packages can vary slightly by distribution, but the following are common for Debian-based systems. Adapt them for your specific Linux distribution.
    <ul>
      <li>For <strong>.docx</strong> files: <code class="language-plaintext highlighter-rouge">libxml2-dev</code>, <code class="language-plaintext highlighter-rouge">libxslt1-dev</code></li>
      <li>For <strong>.doc</strong> files: <code class="language-plaintext highlighter-rouge">antiword</code></li>
      <li>For <strong>.rtf</strong> files: <code class="language-plaintext highlighter-rouge">unrtf</code></li>
      <li>For <strong>.pdf</strong> files: <code class="language-plaintext highlighter-rouge">poppler-utils</code> (provides <code class="language-plaintext highlighter-rouge">pdftotext</code>)</li>
      <li>For <strong>.ps</strong> files: <code class="language-plaintext highlighter-rouge">pstotext</code> (may require manual installation or be part of a larger PostScript handling package)</li>
      <li>For image-based text extraction (OCR for <strong>.jpg, .png, .gif</strong>): <code class="language-plaintext highlighter-rouge">tesseract-ocr</code> and its language data packs (e.g., <code class="language-plaintext highlighter-rouge">tesseract-ocr-eng</code> for English).</li>
      <li>For audio files (<strong>.mp3, .ogg, .wav</strong>): <code class="language-plaintext highlighter-rouge">sox</code>, <code class="language-plaintext highlighter-rouge">libsox-fmt-all</code>, <code class="language-plaintext highlighter-rouge">ffmpeg</code>, <code class="language-plaintext highlighter-rouge">lame</code>, <code class="language-plaintext highlighter-rouge">libmad0</code></li>
      <li>Other potentially useful packages mentioned by <code class="language-plaintext highlighter-rouge">textract</code> documentation: <code class="language-plaintext highlighter-rouge">libjpeg-dev</code>, <code class="language-plaintext highlighter-rouge">swig</code>, <code class="language-plaintext highlighter-rouge">flac</code>.</li>
    </ul>

    <p>A comprehensive command for Debian/Ubuntu to install most <code class="language-plaintext highlighter-rouge">textract</code> dependencies would be:</p>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr tesseract-ocr-eng sox libsox-fmt-all ffmpeg lame libmad0 libjpeg-dev swig flac
</code></pre></div>    </div>
    <p><strong>Note:</strong> Some dependencies like <code class="language-plaintext highlighter-rouge">pstotext</code> might be harder to find in all distributions. <code class="language-plaintext highlighter-rouge">textract</code> has fallbacks for some formats (e.g., a pure Python PDF parser if <code class="language-plaintext highlighter-rouge">pdftotext</code> is missing), but functionality might be limited. Refer to your distribution’s package repositories and the <code class="language-plaintext highlighter-rouge">textract</code> documentation for the most accurate package names.</p>
  </li>
</ul>

<h2 id="2-installation-from-source">2. Installation from Source</h2>

<p>Installing from source is recommended for a baremetal setup, as it gives you the most control and ensures compatibility with your ARM64 architecture.</p>

<ol>
  <li><strong>Clone the Repository:</strong>
Open your terminal and clone the LightRAG repository from GitHub:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/HKUDS/LightRAG.git
<span class="nb">cd </span>LightRAG
</code></pre></div>    </div>
  </li>
  <li><strong>Create and Activate a Python Virtual Environment:</strong>
It’s highly recommended to use a virtual environment to manage project dependencies and avoid conflicts with system-wide packages.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> venv venv
<span class="nb">source </span>venv/bin/activate
</code></pre></div>    </div>
    <p><em>(To deactivate the virtual environment later, simply type <code class="language-plaintext highlighter-rouge">deactivate</code>)</em></p>
  </li>
  <li><strong>Install LightRAG and its Dependencies:</strong>
LightRAG uses <code class="language-plaintext highlighter-rouge">pip</code> for installation. You have two main options:
    <ul>
      <li><strong>To install the core LightRAG engine along with the API server and web UI components:</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-e</span> <span class="s2">".[api]"</span>
</code></pre></div>        </div>
      </li>
      <li><strong>To install only the core LightRAG engine (if you don’t need the API server or web UI):</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-e</span> <span class="nb">.</span>
</code></pre></div>        </div>
      </li>
    </ul>

    <p><strong>Note on Compilation:</strong> This step might take a significant amount of time, especially on ARM64 devices, as some dependencies may need to be compiled from source. Ensure your system has a stable internet connection and sufficient resources (RAM, CPU).</p>
  </li>
  <li><strong>Troubleshooting Compilation Issues:</strong>
If you encounter errors during the <code class="language-plaintext highlighter-rouge">pip install</code> step, they are often due to missing development libraries for a particular package.
    <ul>
      <li>Carefully read the error messages. They usually indicate which library is missing.</li>
      <li>Use your system’s package manager to search for and install the required development package. For example, if an error mentions something related to <code class="language-plaintext highlighter-rouge">xyz</code>, you might need to install <code class="language-plaintext highlighter-rouge">libxyz-dev</code> (on Debian/Ubuntu) or a similarly named package.</li>
      <li>Ensure your build tools (<code class="language-plaintext highlighter-rouge">gcc</code>, <code class="language-plaintext highlighter-rouge">python3-dev</code>, etc.) are correctly installed.</li>
    </ul>
  </li>
</ol>

<h2 id="3-configuration-for-baremetal-deployment">3. Configuration for Baremetal Deployment</h2>

<p>After successful installation, you need to configure LightRAG for your baremetal environment. This is primarily done through an <code class="language-plaintext highlighter-rouge">.env</code> file.</p>

<ol>
  <li><strong>Create the <code class="language-plaintext highlighter-rouge">.env</code> File:</strong>
Navigate to the root directory of your cloned LightRAG project (if you’re not already there) and copy the example environment file:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cp </span>env.example .env
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Edit the <code class="language-plaintext highlighter-rouge">.env</code> File:</strong>
Open the <code class="language-plaintext highlighter-rouge">.env</code> file with a text editor. Here are some key configurations to consider for a baremetal ARM64 setup:</p>

    <ul>
      <li><strong>LLM Configuration:</strong>
You’ll likely want to use LLMs that can run locally on your ARM64 machine.
        <ul>
          <li><strong>Using Ollama (Recommended for local models):</strong>
If you have Ollama installed and serving a model:
            <pre><code class="language-env">LLM_BINDING=ollama
LLM_BINDING_HOST=http://localhost:11434 # Or your Ollama server address
LLM_MODEL=your_ollama_model_name # e.g., llama3, gemma2
</code></pre>
            <p>Ensure Ollama is running and the specified model is pulled (<code class="language-plaintext highlighter-rouge">ollama pull your_ollama_model_name</code>).
The <code class="language-plaintext highlighter-rouge">README.md</code> has specific instructions for increasing Ollama’s context window (<code class="language-plaintext highlighter-rouge">num_ctx</code>), which is important for LightRAG.</p>
          </li>
          <li><strong>Using Hugging Face Models (Directly or via a local inference server):</strong>
The <code class="language-plaintext highlighter-rouge">README.md</code> provides examples for using Hugging Face models. This might involve more manual setup to ensure the model runs efficiently on your ARM64 hardware.
            <pre><code class="language-env"># Example (refer to LightRAG docs for specific HuggingFace setup)
# LLM_BINDING=hf 
# LLM_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct 
</code></pre>
          </li>
          <li><strong>Using OpenAI (If you have internet access and an API key):</strong>
While this is a baremetal guide, you can still use OpenAI if desired:
            <pre><code class="language-env">LLM_BINDING=openai
OPENAI_API_KEY=your_openai_api_key
LLM_MODEL=gpt-4o-mini # Or other model
</code></pre>
          </li>
        </ul>
      </li>
      <li><strong>Embedding Model Configuration:</strong>
Similar to LLMs, you’ll need to configure embedding models.
        <ul>
          <li><strong>Using Ollama:</strong>
            <pre><code class="language-env">EMBEDDING_BINDING=ollama
EMBEDDING_BINDING_HOST=http://localhost:11434 # Or your Ollama server address
EMBEDDING_MODEL=nomic-embed-text # Or another Ollama embedding model
</code></pre>
            <p>Ensure the embedding model is pulled in Ollama (<code class="language-plaintext highlighter-rouge">ollama pull nomic-embed-text</code>).</p>
          </li>
          <li><strong>Using Hugging Face Models:</strong>
Refer to the main <code class="language-plaintext highlighter-rouge">README.md</code> for Hugging Face embedding examples.</li>
          <li><strong>Using OpenAI:</strong>
            <pre><code class="language-env">EMBEDDING_BINDING=openai
# OPENAI_API_KEY should already be set if using OpenAI LLM
EMBEDDING_MODEL=text-embedding-3-small # Or other model
</code></pre>
          </li>
        </ul>
      </li>
      <li><strong>Storage Configuration:</strong>
LightRAG supports various storage backends. For a simple baremetal setup, the defaults (using local JSON files) are often sufficient to get started.
        <ul>
          <li><strong>Default (JSON-based):</strong> No specific <code class="language-plaintext highlighter-rouge">.env</code> changes are typically needed for the default storage, as data will be stored in the <code class="language-plaintext highlighter-rouge">working_dir</code> (defaults to <code class="language-plaintext highlighter-rouge">lightrag_cache_&lt;timestamp&gt;</code> or as specified in your scripts).</li>
          <li><strong>Using PostgreSQL or Neo4j (Advanced):</strong>
If you prefer a more robust local database, you can set up PostgreSQL (with pgvector and Apache AGE extensions) or Neo4j on your ARM64 machine.
The main <code class="language-plaintext highlighter-rouge">README.md</code> provides guidance on configuring LightRAG to use these:
            <ul>
              <li>Set <code class="language-plaintext highlighter-rouge">KV_STORAGE</code>, <code class="language-plaintext highlighter-rouge">VECTOR_STORAGE</code>, <code class="language-plaintext highlighter-rouge">GRAPH_STORAGE</code>, <code class="language-plaintext highlighter-rouge">DOC_STATUS_STORAGE</code> variables in the <code class="language-plaintext highlighter-rouge">.env</code> file or directly in your Python scripts when initializing <code class="language-plaintext highlighter-rouge">LightRAG</code>.</li>
              <li>Example for Neo4j (ensure Neo4j server is running and configured):
                <pre><code class="language-env">GRAPH_STORAGE=Neo4JStorage
NEO4J_URI=neo4j://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password
</code></pre>
              </li>
              <li>Example for PostgreSQL (ensure PostgreSQL server is running with necessary extensions):
                <pre><code class="language-env"># In your Python script or set as environment variables
# os.environ["DB_USER"] = "your_postgres_user"
# os.environ["DB_PASSWORD"] = "your_postgres_password"
# os.environ["DB_HOST"] = "localhost"
# os.environ["DB_PORT"] = "5432"
# os.environ["DB_NAME"] = "your_database_name"
# KV_STORAGE=PGKVStorage
# VECTOR_STORAGE=PGVectorStorage
# GRAPH_STORAGE=AGEStorage 
</code></pre>
                <p>Refer to the “Storage” section in the main <code class="language-plaintext highlighter-rouge">README.md</code> and the example <code class="language-plaintext highlighter-rouge">examples/lightrag_zhipu_postgres_demo.py</code> for more details.</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>API Server Configuration (if using <code class="language-plaintext highlighter-rouge">.[api]</code> installation):</strong>
        <ul>
          <li><code class="language-plaintext highlighter-rouge">HOST</code>: Server host (default: <code class="language-plaintext highlighter-rouge">0.0.0.0</code> to listen on all interfaces)</li>
          <li><code class="language-plaintext highlighter-rouge">PORT</code>: Server port (default: <code class="language-plaintext highlighter-rouge">9621</code>)</li>
          <li><code class="language-plaintext highlighter-rouge">LIGHTRAG_API_KEY</code>: Set a secure API key if you plan to expose the API.</li>
        </ul>
      </li>
      <li><strong>Other Parameters:</strong>
        <ul>
          <li><code class="language-plaintext highlighter-rouge">MAX_ASYNC</code>: Maximum async operations.</li>
          <li><code class="language-plaintext highlighter-rouge">MAX_TOKENS</code>: Maximum token size for LLM.</li>
          <li><code class="language-plaintext highlighter-rouge">WORKING_DIR</code>: Default directory for storing data if not overridden in scripts. Can be set in <code class="language-plaintext highlighter-rouge">.env</code> as <code class="language-plaintext highlighter-rouge">LIGHTRAG_WORKING_DIR</code>.
            <pre><code class="language-env">  # LIGHTRAG_WORKING_DIR=./my_lightrag_data 
</code></pre>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Save the <code class="language-plaintext highlighter-rouge">.env</code> File:</strong>
After making your changes, save the file. LightRAG will load these settings when it starts.</li>
</ol>

<h2 id="4-running-lightrag">4. Running LightRAG</h2>

<p>Once LightRAG is installed and configured, you can start using it.</p>

<h3 id="running-the-lightrag-server-optional">Running the LightRAG Server (Optional)</h3>

<p>If you installed LightRAG with the API extras (<code class="language-plaintext highlighter-rouge">pip install -e ".[api]"</code>) and want to use the Web UI or API:</p>

<ol>
  <li><strong>Ensure your <code class="language-plaintext highlighter-rouge">.env</code> file is configured</strong>, especially <code class="language-plaintext highlighter-rouge">HOST</code>, <code class="language-plaintext highlighter-rouge">PORT</code>, <code class="language-plaintext highlighter-rouge">LIGHTRAG_API_KEY</code>, and your LLM/embedding model settings.</li>
  <li><strong>Activate your virtual environment</strong> (if not already active):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>venv/bin/activate
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Start the server:</strong>
The main <code class="language-plaintext highlighter-rouge">README.md</code> mentions running the server. Typically, this involves a command like <code class="language-plaintext highlighter-rouge">python -m lightrag.api.lightrag_server</code> or a specific script if provided. Refer to the main <code class="language-plaintext highlighter-rouge">README.md</code> or <code class="language-plaintext highlighter-rouge">./lightrag/api/README.md</code> for the precise command to start the server.
You might also use <code class="language-plaintext highlighter-rouge">docker compose up</code> if you later decide to use Docker and have configured <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> appropriately for arm64.</p>

    <p>Once started, the API should be accessible at <code class="language-plaintext highlighter-rouge">http://&lt;your_host&gt;:&lt;your_port&gt;</code> and the Web UI (if included) at a similar address.</p>
  </li>
</ol>

<h3 id="running-example-scripts-core-engine">Running Example Scripts (Core Engine)</h3>

<p>The <code class="language-plaintext highlighter-rouge">examples/</code> directory contains various scripts demonstrating how to use the LightRAG core engine.</p>

<ol>
  <li><strong>Activate your virtual environment:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source </span>venv/bin/activate
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Ensure your <code class="language-plaintext highlighter-rouge">.env</code> file is configured</strong> with your chosen LLM and embedding models (e.g., local Ollama models). The example scripts often default to OpenAI, so you’ll need to modify them or ensure your LightRAG initialization in the script picks up the <code class="language-plaintext highlighter-rouge">.env</code> settings or is explicitly set to your local models.</p>
  </li>
  <li><strong>Prepare a test document (Optional, for some demos):</strong>
Some demos, like <code class="language-plaintext highlighter-rouge">lightrag_openai_demo.py</code>, use a sample text file.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># From the LightRAG root directory</span>
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt <span class="o">&gt;</span> ./book.txt
</code></pre></div>    </div>
  </li>
  <li><strong>Run an example script:</strong>
Navigate to the LightRAG root directory. Let’s take <code class="language-plaintext highlighter-rouge">examples/lightrag_openai_demo.py</code> as a base.
    <ul>
      <li><strong>If you configured OpenAI in <code class="language-plaintext highlighter-rouge">.env</code>:</strong>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure OPENAI_API_KEY is in your .env or exported</span>
python examples/lightrag_openai_demo.py
</code></pre></div>        </div>
      </li>
      <li>
        <p><strong>If you configured a local model (e.g., Ollama) in <code class="language-plaintext highlighter-rouge">.env</code> and the script is set up to use it, or if you modify the script:</strong>
Many examples in the <code class="language-plaintext highlighter-rouge">examples</code> directory show how to initialize <code class="language-plaintext highlighter-rouge">LightRAG</code> with specific model functions (e.g., <code class="language-plaintext highlighter-rouge">ollama_model_complete</code>, <code class="language-plaintext highlighter-rouge">hf_model_complete</code>). You might need to adapt <code class="language-plaintext highlighter-rouge">lightrag_openai_demo.py</code> or use a different example that’s closer to your setup (like <code class="language-plaintext highlighter-rouge">examples/lightrag_ollama_demo.py</code>).</p>

        <p>For <code class="language-plaintext highlighter-rouge">lightrag_ollama_demo.py</code>:</p>
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inside lightrag_ollama_demo.py, you'd typically see:
# from lightrag.llm.ollama import ollama_model_complete, ollama_embed
# ...
# rag = LightRAG(
#     llm_model_func=ollama_model_complete,
#     llm_model_name="your_ollama_model_from_env_or_hardcoded",
#     embedding_func=EmbeddingFunc(
#         embedding_dim=..., # set based on your ollama embedding model
#         max_token_size=...,
#         func=lambda texts: ollama_embed(texts, embed_model="your_ollama_embedding_model")
#     ),
#     ...
# )
</span></code></pre></div>        </div>
        <p>To run such a script:</p>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python examples/lightrag_ollama_demo.py
</code></pre></div>        </div>
        <p><strong>Important:</strong> Review the script you choose. Ensure the <code class="language-plaintext highlighter-rouge">LightRAG</code> initialization parameters (like <code class="language-plaintext highlighter-rouge">llm_model_func</code>, <code class="language-plaintext highlighter-rouge">embedding_func</code>, model names, dimensions) match your ARM64 setup and the models you have available. The <code class="language-plaintext highlighter-rouge">.env</code> file settings are used by default by the server, but scripts can override these if they explicitly pass parameters to <code class="language-plaintext highlighter-rouge">LightRAG()</code>.</p>
      </li>
    </ul>

    <p><strong>Note on <code class="language-plaintext highlighter-rouge">WORKING_DIR</code>:</strong> LightRAG will create a directory (e.g., <code class="language-plaintext highlighter-rouge">rag_storage</code> or <code class="language-plaintext highlighter-rouge">lightrag_cache_&lt;timestamp&gt;</code>) to store data, indexes, and caches. Make sure you have write permissions in the location where the script is run or where <code class="language-plaintext highlighter-rouge">WORKING_DIR</code> points. If you switch embedding models, you might need to clear this directory as per the main README’s advice.</p>
  </li>
</ol>

<h2 id="5-alternative-using-docker-on-arm64">5. Alternative: Using Docker on ARM64</h2>

<p>While this guide focuses on baremetal installation, you can also run LightRAG using Docker on your ARM64 Linux machine, provided Docker is installed.</p>

<ul>
  <li>
    <p><strong>Dockerfiles Provided:</strong> The repository includes a <code class="language-plaintext highlighter-rouge">Dockerfile</code> and a <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file, which are the starting points for a Docker-based deployment.</p>
  </li>
  <li><strong>ARM64 Docker Image:</strong>
    <ul>
      <li>Check if the project provides official multi-arch Docker images that support <code class="language-plaintext highlighter-rouge">linux/arm64</code>. You can find information on this in the main <code class="language-plaintext highlighter-rouge">README.md</code> or on the project’s container registry (e.g., Docker Hub, GitHub Packages).</li>
      <li>If an official arm64 image is not available, you may need to build the Docker image directly on your ARM64 machine. This can be done using <code class="language-plaintext highlighter-rouge">docker build</code> or <code class="language-plaintext highlighter-rouge">docker compose build</code>. Ensure the <code class="language-plaintext highlighter-rouge">Dockerfile</code> is compatible with ARM64 (e.g., base images are available for ARM64, and any compiled dependencies can be built for ARM64).</li>
    </ul>
  </li>
  <li>
    <p><strong>Configuration:</strong> You would still use an <code class="language-plaintext highlighter-rouge">.env</code> file (or Docker Compose environment variables) to configure LightRAG, similar to the baremetal setup, paying attention to aspects like <code class="language-plaintext highlighter-rouge">LLM_BINDING_HOST</code> (which might need to be <code class="language-plaintext highlighter-rouge">host.docker.internal</code> or a specific container network IP if Ollama or other services are also running in Docker).</p>
  </li>
  <li><strong>Further Docker Instructions:</strong> For more detailed information on Docker deployment, refer to the <a href="./DockerDeployment.md">DockerDeployment.md</a> file in the <code class="language-plaintext highlighter-rouge">docs/</code> directory.</li>
</ul>

<p>Using Docker can simplify dependency management but adds a layer of abstraction. Choose the method that best suits your comfort level and technical requirements.</p>

<h2 id="6-performance-considerations-for-arm64">6. Performance Considerations for ARM64</h2>

<p>Running Large Language Models (LLMs) and associated processes (like embeddings and graph analysis) can be resource-intensive. When deploying LightRAG on a baremetal ARM64 machine, keep the following performance considerations in mind:</p>

<ul>
  <li><strong>Hardware Limitations:</strong> The performance of LightRAG will heavily depend on the capabilities of your ARM64 hardware:
    <ul>
      <li><strong>CPU:</strong> A powerful multi-core ARM64 CPU will significantly speed up processing.</li>
      <li><strong>RAM:</strong> LLMs, especially larger ones, require a substantial amount of RAM. Insufficient RAM can lead to slow performance or out-of-memory errors. Monitor your RAM usage closely.</li>
      <li><strong>Storage Speed:</strong> Fast storage (e.g., NVMe SSD) can improve loading times for models and data.</li>
      <li><strong>Accelerators:</strong> While many ARM64 SoCs include AI/ML accelerators, the ability to leverage them depends on the specific LLM serving framework (e.g., Ollama, llama.cpp) and model compatibility with those accelerators on Linux.</li>
    </ul>
  </li>
  <li><strong>Model Choice:</strong> The size and type of the LLM and embedding models you choose will be the primary determinant of performance and resource consumption.
    <ul>
      <li><strong>Start Small:</strong> If you are unsure about your hardware’s capacity or if you have limited resources (e.g., on a Raspberry Pi or similar single-board computer), start with the smallest available models (e.g., 2B or 3B parameter models if using Ollama).</li>
      <li><strong>Quantization:</strong> Using quantized versions of models can significantly reduce their size and computational requirements, often with a manageable impact on performance. Check if your chosen LLM framework supports quantized models (e.g., GGUF for llama.cpp-based backends like Ollama).</li>
    </ul>
  </li>
  <li><strong>Batch Sizes and Concurrency:</strong>
    <ul>
      <li>Parameters like <code class="language-plaintext highlighter-rouge">MAX_ASYNC</code> in the <code class="language-plaintext highlighter-rouge">.env</code> file, <code class="language-plaintext highlighter-rouge">embedding_batch_num</code>, and <code class="language-plaintext highlighter-rouge">llm_model_max_async</code> in the <code class="language-plaintext highlighter-rouge">LightRAG</code> initialization can be tuned. However, on resource-constrained ARM64 devices, increasing concurrency too much might lead to thrashing rather than improved performance. Start with conservative values.</li>
    </ul>
  </li>
  <li><strong>System Optimization:</strong>
    <ul>
      <li>Ensure your Linux system is optimized. Minimize background processes to free up resources.</li>
      <li>Consider performance governors for your CPU if applicable (e.g., setting to <code class="language-plaintext highlighter-rouge">performance</code> mode if thermal headroom allows, though be mindful of heat on passively cooled devices).</li>
    </ul>
  </li>
  <li><strong>Monitoring:</strong>
    <ul>
      <li>Use system monitoring tools (<code class="language-plaintext highlighter-rouge">htop</code>, <code class="language-plaintext highlighter-rouge">vmstat</code>, <code class="language-plaintext highlighter-rouge">iotop</code>) to observe CPU, RAM, and disk I/O usage while LightRAG is processing data or handling queries. This can help you identify bottlenecks.</li>
    </ul>
  </li>
</ul>

<p>Running complex RAG pipelines on ARM64 is feasible, especially with newer, more powerful ARM64 processors. However, managing expectations and carefully selecting models appropriate for your hardware are key to a successful deployment.</p>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">LightRAG - extracting relevant information from mixed data</title><link href="https://ib.bsb.br/lightrag-extracting-relevant-information-from-mixed-data/" rel="alternate" type="text/html" title="LightRAG - extracting relevant information from mixed data" /><published>2025-05-21T00:00:00+00:00</published><updated>2025-05-21T17:13:45+00:00</updated><id>https://ib.bsb.br/lightrag-extracting-relevant-information-from-mixed-data</id><content type="html" xml:base="https://ib.bsb.br/lightrag-extracting-relevant-information-from-mixed-data/"><![CDATA[<h2 id="introduction-can-lightrag-solve-your-problem">Introduction: Can LightRAG Solve Your Problem?</h2>

<p>Based on the provided <code class="language-plaintext highlighter-rouge">LightRAG</code> documentation, your assessment of its capabilities is largely correct—and, in some respects, the tool is even more powerful than you might expect. <code class="language-plaintext highlighter-rouge">LightRAG</code> is designed as a lightweight, high-performance Retrieval-Augmented Generation (RAG) system with a strong focus on knowledge graph construction, semantic search, and efficient document management. It supports a range of file formats, offers robust document ingestion and indexing pipelines, and provides advanced querying and referencing features. Its architecture is optimized for handling large, disorganized collections, making it well-suited to your scenario: quickly filtering, searching, and referencing only the relevant content from a “sea of mixed files.”</p>

<p>While <code class="language-plaintext highlighter-rouge">LightRAG</code> has specific requirements for certain file types (e.g., legacy <code class="language-plaintext highlighter-rouge">.doc</code> files), it covers many common research file formats. The system is also extensible, supporting multiple storage backends and LLM/embedding providers, and can be operated entirely via its user-friendly web interface or API.</p>

<p><strong>Please note:</strong> This tutorial is based <em>exclusively</em> on the <code class="language-plaintext highlighter-rouge">LightRAG</code> documentation provided. Features, UI elements, and behavior might differ with other <code class="language-plaintext highlighter-rouge">LightRAG</code> versions or if the documentation isn’t fully comprehensive for your setup.</p>

<p>This tutorial will guide you step-by-step from the <code class="language-plaintext highlighter-rouge">LightRAG</code> homepage, focusing on how to leverage its features to tackle your specific file organization and information retrieval challenges.</p>

<hr />

<h2 id="part-1-getting-started-with-lightrags-web-interface">Part 1: Getting Started with LightRAG’s Web Interface</h2>

<p>This part guides you through accessing the <code class="language-plaintext highlighter-rouge">LightRAG</code> WebUI and understanding its main layout, assuming you have successfully installed <code class="language-plaintext highlighter-rouge">LightRAG</code> via <code class="language-plaintext highlighter-rouge">Docker</code> and created an account.</p>

<h3 id="step-1-access-the-lightrag-webui">Step 1: Access the LightRAG WebUI</h3>

<ol>
  <li><strong>Open</strong> your web browser.</li>
  <li><strong>Navigate</strong> to <code class="language-plaintext highlighter-rouge">http://localhost:9621/webui</code>. (This is the default URL; if you configured a different port during <code class="language-plaintext highlighter-rouge">Docker</code> setup, use that port number instead of <code class="language-plaintext highlighter-rouge">9621</code>.)</li>
  <li>You should be greeted by the <code class="language-plaintext highlighter-rouge">LightRAG</code> homepage. If authentication is enabled (as configured in your <code class="language-plaintext highlighter-rouge">.env</code> file or server arguments), you will be prompted to <strong>log in</strong>.</li>
</ol>

<h3 id="step-2-overview-of-the-homepage-and-main-sections">Step 2: Overview of the Homepage and Main Sections</h3>

<p>The <code class="language-plaintext highlighter-rouge">LightRAG</code> web interface, as suggested by its source files (e.g., <code class="language-plaintext highlighter-rouge">lightrag_webui/src/features/SiteHeader.tsx</code>) and UI text definitions (<code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>), typically presents a top navigation bar. This bar provides access to the main sections of the application:</p>

<ul>
  <li><strong>Documents:</strong> Your primary workspace for uploading, managing, and monitoring the processing status of your research files. (Tab label: <strong>“Documents”</strong>)</li>
  <li><strong>Knowledge Graph:</strong> Allows you to visualize and interactively explore the entities (e.g., concepts, people, organizations) and relationships that <code class="language-plaintext highlighter-rouge">LightRAG</code> automatically extracts from your documents. (Tab label: <strong>“Knowledge Graph”</strong>)</li>
  <li><strong>Retrieval:</strong> The main interface for querying your indexed documents to find specific information and get answers. (Tab label: <strong>“Retrieval”</strong>)</li>
  <li><strong>API:</strong> Provides access to <code class="language-plaintext highlighter-rouge">LightRAG</code>’s API documentation (usually Swagger UI or ReDoc), which is useful for developers or for understanding the underlying API endpoints. (Tab label: <strong>“API”</strong>)</li>
  <li><strong>Project Repository:</strong> A direct link to the <code class="language-plaintext highlighter-rouge">LightRAG</code> GitHub project page.</li>
  <li><strong>Logout:</strong> Allows you to <strong>log out</strong> of your <code class="language-plaintext highlighter-rouge">LightRAG</code> session.</li>
  <li><strong>Settings (Gear Icon):</strong> Access application-level settings, such as theme preferences and language selection.</li>
  <li><strong>Theme Toggle (Sun/Moon Icon):</strong> Quickly <strong>switch</strong> between light and dark visual themes for the interface.</li>
</ul>

<p>For your immediate goal of organizing files and extracting information, this tutorial will focus primarily on the <strong>Documents</strong> and <strong>Retrieval</strong> sections.</p>

<hr />

<h2 id="part-2-ingesting-your-research-files-into-lightrag">Part 2: Ingesting Your Research Files into LightRAG</h2>

<p>This crucial step involves preparing your files and getting them into <code class="language-plaintext highlighter-rouge">LightRAG</code> for processing.</p>

<h3 id="step-1-understand-supported-file-formats-and-important-notes">Step 1: Understand Supported File Formats and Important Notes</h3>

<p><code class="language-plaintext highlighter-rouge">LightRAG</code>’s ability to process your files effectively depends on their format. Based on the <code class="language-plaintext highlighter-rouge">Dockerfile</code> and the backend code (<code class="language-plaintext highlighter-rouge">lightrag/api/routers/document_routes.py</code>):</p>

<ul>
  <li><strong>Well-Supported by Default <code class="language-plaintext highlighter-rouge">Docker</code> Installation:</strong>
    <ul>
      <li><strong>PDF (<code class="language-plaintext highlighter-rouge">.pdf</code>):</strong> Processed using <code class="language-plaintext highlighter-rouge">PyPDF2</code>. <strong>Crucial:</strong> Ensure your PDFs are text-searchable (contain actual selectable text, not just scanned images).</li>
      <li><strong>Microsoft Word (<code class="language-plaintext highlighter-rouge">.docx</code>):</strong> Processed using <code class="language-plaintext highlighter-rouge">python-docx</code>.</li>
      <li><strong>Text Files (<code class="language-plaintext highlighter-rouge">.txt</code>, <code class="language-plaintext highlighter-rouge">.md</code>):</strong> Read directly. Markdown (<code class="language-plaintext highlighter-rouge">.md</code>) is also supported.</li>
      <li><strong>Microsoft PowerPoint (<code class="language-plaintext highlighter-rouge">.pptx</code>):</strong> Processed using <code class="language-plaintext highlighter-rouge">python-pptx</code>.</li>
      <li><strong>Microsoft Excel (<code class="language-plaintext highlighter-rouge">.xlsx</code>):</strong> Processed using <code class="language-plaintext highlighter-rouge">openpyxl</code>.</li>
      <li><strong>Common Text-Based Formats:</strong> Many other formats listed in <code class="language-plaintext highlighter-rouge">DocumentManager</code>’s <code class="language-plaintext highlighter-rouge">SUPPORTED_EXTENSIONS</code> (e.g., <code class="language-plaintext highlighter-rouge">.csv</code>, <code class="language-plaintext highlighter-rouge">.json</code>, <code class="language-plaintext highlighter-rouge">.xml</code>, <code class="language-plaintext highlighter-rouge">.html</code>, <code class="language-plaintext highlighter-rouge">.py</code>, <code class="language-plaintext highlighter-rouge">.java</code>, <code class="language-plaintext highlighter-rouge">.css</code>) are generally processed by attempting a UTF-8 decode.</li>
    </ul>
  </li>
  <li><strong>Critical Note on Legacy <code class="language-plaintext highlighter-rouge">.doc</code> files (Microsoft Word 97-2003):</strong>
    <ul>
      <li>The <code class="language-plaintext highlighter-rouge">SUPPORTED_EXTENSIONS</code> list in <code class="language-plaintext highlighter-rouge">DocumentManager</code> (<code class="language-plaintext highlighter-rouge">document_routes.py</code>) does <strong>not</strong> include <code class="language-plaintext highlighter-rouge">.doc</code>.</li>
      <li>The <code class="language-plaintext highlighter-rouge">Dockerfile</code> for <code class="language-plaintext highlighter-rouge">LightRAG</code> <strong>does not install</strong> the <code class="language-plaintext highlighter-rouge">docling</code> library, which the backend code (<code class="language-plaintext highlighter-rouge">pipeline_enqueue_file</code> function in <code class="language-plaintext highlighter-rouge">document_routes.py</code>) would conditionally attempt to use for converting some other formats (like RTF, ODT).</li>
      <li><strong>Conclusion:</strong> <code class="language-plaintext highlighter-rouge">LightRAG</code>, with its default <code class="language-plaintext highlighter-rouge">Docker</code> setup, will <strong>fail to process <code class="language-plaintext highlighter-rouge">.doc</code> files</strong>.</li>
      <li><strong>Action Required:</strong> You <strong>must convert</strong> your <code class="language-plaintext highlighter-rouge">.doc</code> files to a supported format like <code class="language-plaintext highlighter-rouge">.docx</code>, text-searchable <code class="language-plaintext highlighter-rouge">.pdf</code>, or <code class="language-plaintext highlighter-rouge">.txt</code> <em>before</em> uploading them to <code class="language-plaintext highlighter-rouge">LightRAG</code>.</li>
    </ul>
  </li>
  <li><strong>Other Listed Formats (e.g., <code class="language-plaintext highlighter-rouge">.rtf</code>, <code class="language-plaintext highlighter-rouge">.odt</code>, <code class="language-plaintext highlighter-rouge">.epub</code>, <code class="language-plaintext highlighter-rouge">.tex</code>, <code class="language-plaintext highlighter-rouge">.htm</code>):</strong>
    <ul>
      <li>While these extensions are listed in <code class="language-plaintext highlighter-rouge">SUPPORTED_EXTENSIONS</code> (<code class="language-plaintext highlighter-rouge">document_routes.py</code>), the <code class="language-plaintext highlighter-rouge">pipeline_enqueue_file</code> function attempts to process them using <code class="language-plaintext highlighter-rouge">docling</code> if available.</li>
      <li>Since <code class="language-plaintext highlighter-rouge">docling</code> is <strong>not installed by default</strong> in the <code class="language-plaintext highlighter-rouge">Docker</code> image, these formats will also <strong>likely fail to process</strong>.</li>
      <li><strong>Recommendation:</strong> For critical research files in these formats, it’s safest to <strong>convert</strong> them to <code class="language-plaintext highlighter-rouge">PDF</code> (text-searchable), <code class="language-plaintext highlighter-rouge">DOCX</code>, or <code class="language-plaintext highlighter-rouge">TXT</code> if you encounter processing issues.</li>
    </ul>
  </li>
</ul>

<h3 id="step-2-choose-your-file-ingestion-method">Step 2: Choose Your File Ingestion Method</h3>

<p><code class="language-plaintext highlighter-rouge">LightRAG</code> offers two main ways to ingest your documents:</p>

<ol>
  <li><strong>Direct Upload via the Web UI:</strong> Select files from your computer and upload them through the interface.</li>
  <li><strong>Input Directory Scan:</strong> Place files into a specific directory that <code class="language-plaintext highlighter-rouge">LightRAG</code> monitors, then trigger a scan.</li>
</ol>

<p>For your initial large, disorganized collection, using the <strong>“Upload”</strong> feature via the UI (Method B below) might be more straightforward as you can directly <strong>select</strong> files. If you later establish a workflow where new research files are regularly saved to a specific folder, setting up that folder as <code class="language-plaintext highlighter-rouge">LightRAG</code>’s <code class="language-plaintext highlighter-rouge">INPUT_DIR</code> and using the <strong>“Scan”</strong> feature (Method A) can be very efficient for ongoing updates.</p>

<h4 id="method-a-placing-files-directly-in-the-input-directory-and-scanning">Method A: Placing Files Directly in the Input Directory and Scanning</h4>

<p>This method is efficient for batch processing if you can easily copy files into <code class="language-plaintext highlighter-rouge">LightRAG</code>’s monitored folder.</p>

<ol>
  <li><strong>Locate and Prepare Your Input Directory:</strong>
    <ul>
      <li>Your <code class="language-plaintext highlighter-rouge">LightRAG</code> <code class="language-plaintext highlighter-rouge">Docker</code> setup (as per <code class="language-plaintext highlighter-rouge">docker-compose.yml</code>) maps a directory on your host machine to its internal input directory. By default, this is usually a folder named <code class="language-plaintext highlighter-rouge">./data/inputs</code> located in the same directory where you run <code class="language-plaintext highlighter-rouge">docker-compose up</code>. The <code class="language-plaintext highlighter-rouge">Dockerfile</code> and <code class="language-plaintext highlighter-rouge">lightrag/api/config.py</code> reference this as <code class="language-plaintext highlighter-rouge">INPUT_DIR</code> (defaulting to <code class="language-plaintext highlighter-rouge">/app/data/inputs</code> inside the container).</li>
      <li><strong>Action:</strong> <strong>Copy</strong> your selected research files (ensuring all <code class="language-plaintext highlighter-rouge">.doc</code> files are converted to a supported format!) into this <code class="language-plaintext highlighter-rouge">./data/inputs</code> directory on your computer.</li>
    </ul>
  </li>
  <li><strong>Initiate a Scan from the Web UI:</strong>
    <ul>
      <li>In the <code class="language-plaintext highlighter-rouge">LightRAG</code> WebUI, <strong>navigate</strong> to the <strong>“Documents”</strong> tab.</li>
      <li>Look for a button labeled <strong>“Scan”</strong> (often accompanied by a refresh icon, as per <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>: <code class="language-plaintext highlighter-rouge">"documentPanel.documentManager.scanButton": "Scan"</code>).</li>
      <li><strong>Action:</strong> <strong>Click</strong> the <strong>“Scan”</strong> button.</li>
      <li><strong>Purpose:</strong> This tells <code class="language-plaintext highlighter-rouge">LightRAG</code> to check its <code class="language-plaintext highlighter-rouge">INPUT_DIR</code> for any new files it hasn’t processed yet and begin indexing them.</li>
      <li><strong>Expected Feedback:</strong> A notification might appear (e.g., “Scanning documents started.”). The document list in the UI should update as new files are discovered and their processing status changes.</li>
    </ul>
  </li>
</ol>

<h4 id="method-b-uploading-files-directly-via-the-web-ui">Method B: Uploading Files Directly via the Web UI</h4>

<p>This method allows you to select specific files from any location on your computer.</p>

<ol>
  <li><strong>Access the Upload Dialog:</strong>
    <ul>
      <li>In the <code class="language-plaintext highlighter-rouge">LightRAG</code> WebUI, <strong>navigate</strong> to the <strong>“Documents”</strong> tab.</li>
      <li>Look for a button labeled <strong>“Upload”</strong> (as per <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>: <code class="language-plaintext highlighter-rouge">"documentPanel.uploadDocuments.button": "Upload"</code>).</li>
      <li><strong>Action:</strong> <strong>Click</strong> the <strong>“Upload”</strong> button.</li>
      <li><strong>Expected Feedback:</strong> An <strong>“Upload Documents”</strong> dialog will appear (controlled by <code class="language-plaintext highlighter-rouge">lightrag_webui/src/components/documents/UploadDocumentsDialog.tsx</code>). It will likely prompt you to “Drag and drop your documents here or click to browse.” (from <code class="language-plaintext highlighter-rouge">"documentPanel.uploadDocuments.description"</code>).</li>
    </ul>
  </li>
  <li><strong>Select Your Research Files:</strong>
    <ul>
      <li><strong>Action:</strong> Inside the <strong>“Upload Documents”</strong> dialog:
        <ul>
          <li>Either <strong>drag and drop</strong> your selected research files (PDF, TXT, DOCX, etc., ensuring <code class="language-plaintext highlighter-rouge">.doc</code> files are converted!) onto the designated area.</li>
          <li>Or, <strong>click</strong> on the upload area to open your computer’s file selection window. <strong>Navigate</strong> to your disorganized folder and <strong>select</strong> the files you want to upload.</li>
        </ul>
      </li>
      <li>The documentation (<code class="language-plaintext highlighter-rouge">lightrag/api/routers/document_routes.py</code> for the <code class="language-plaintext highlighter-rouge">/documents/batch</code> endpoint, and <code class="language-plaintext highlighter-rouge">UploadDocumentsDialog.tsx</code>) confirms you can <strong>select</strong> and <strong>upload</strong> multiple files at once.</li>
    </ul>
  </li>
  <li><strong>Initiate Upload:</strong>
    <ul>
      <li>After <strong>selecting</strong> files, they will appear listed in the dialog.</li>
      <li><strong>Action:</strong> <strong>Click</strong> the primary confirmation button (likely labeled <strong>“Upload”</strong> or similar, based on the UI’s general “confirm” action text).</li>
      <li><strong>Expected Feedback:</strong> <code class="language-plaintext highlighter-rouge">LightRAG</code> will begin uploading and then processing your files. The dialog or the main <strong>“Documents”</strong> page should display progress. You might see messages like “Uploading : %” for individual files (from <code class="language-plaintext highlighter-rouge">locales/en.json</code>: <code class="language-plaintext highlighter-rouge">"documentPanel.uploadDocuments.single.uploading"</code>).</li>
      <li>This ingestion step might take some time, depending on the number and size of your files.</li>
    </ul>
  </li>
</ol>

<h3 id="step-3-understanding-what-happens-behind-the-scenes-conceptual-overview">Step 3: Understanding What Happens Behind the Scenes (Conceptual Overview)</h3>

<p>You don’t need to perform these actions directly, but understanding this background process helps in troubleshooting and using <code class="language-plaintext highlighter-rouge">LightRAG</code> effectively. After you upload or scan files, <code class="language-plaintext highlighter-rouge">LightRAG</code> (as inferred from <code class="language-plaintext highlighter-rouge">lightrag.py</code>, <code class="language-plaintext highlighter-rouge">operate.py</code>, and the <code class="language-plaintext highlighter-rouge">lightrag/core/</code> module structure):</p>

<ol>
  <li><strong>Parses Content:</strong> <strong>Reads</strong> and <strong>extracts</strong> text and structural information from your files.</li>
  <li><strong>Chunks Documents:</strong> <strong>Divides</strong> long documents into smaller, semantically coherent “chunks” (default is 1200 tokens per chunk, with some overlap to maintain context, configurable via <code class="language-plaintext highlighter-rouge">chunk_token_size</code> in the <code class="language-plaintext highlighter-rouge">LightRAG</code> class). This is crucial for efficient retrieval.</li>
  <li><strong>Extracts Entities &amp; Relationships:</strong> <strong>Identifies</strong> key entities (like people, organizations, specific topics) and the relationships between them within the text. This data forms the basis of the knowledge graph.</li>
  <li><strong>Generates Embeddings:</strong> <strong>Converts</strong> each text chunk, entity, and relationship into a numerical representation called an “embedding” using a sophisticated language model (as referenced in <code class="language-plaintext highlighter-rouge">lightrag/core/embedder.py</code>). Embeddings capture the semantic meaning, enabling searches based on concepts rather than just exact keywords.</li>
  <li><strong>Indexes Data:</strong> <strong>Stores</strong> these embeddings and their associated text/metadata in specialized databases (a vector store for similarity search and a graph store for relationship data). This allows for rapid retrieval.</li>
</ol>

<hr />

<h2 id="part-3-monitoring-document-processing--understanding-the-pipeline">Part 3: Monitoring Document Processing &amp; Understanding the Pipeline</h2>

<p>After initiating file ingestion, it’s important to monitor the progress and status of your documents.</p>

<h3 id="step-1-view-document-statuses-in-the-document-manager">Step 1: View Document Statuses in the Document Manager</h3>

<p>The <strong>“Documents”</strong> tab (<code class="language-plaintext highlighter-rouge">lightrag_webui/src/features/DocumentManager.tsx</code>) is your central dashboard for this.</p>

<ul>
  <li><strong>Document List:</strong> A table will display your ingested documents.</li>
  <li><strong>Key Columns to Observe</strong> (based on <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code> and the <code class="language-plaintext highlighter-rouge">DocStatusResponse</code> model in <code class="language-plaintext highlighter-rouge">lightrag/api/routers/document_routes.py</code>):
    <ul>
      <li><strong>ID</strong> / <strong>File Name</strong>: You can usually <strong>toggle</strong> between a system ID and the original file name (look for a <strong>“File Name”</strong> toggle or button, as suggested by <code class="language-plaintext highlighter-rouge">DocumentManager.tsx</code> and <code class="language-plaintext highlighter-rouge">locales/en.json</code>’s <code class="language-plaintext highlighter-rouge">"fileNameLabel"</code>). The <strong>File Path</strong> is also tracked internally and is crucial for referencing.</li>
      <li><strong>Summary</strong>: A brief preview of the document’s content.</li>
      <li><strong>Status</strong>: The current processing state of the document.</li>
      <li><strong>Length</strong>: The size or length of the document content.</li>
      <li><strong>Chunks</strong>: The number of chunks the document was divided into.</li>
      <li><strong>Created / Updated</strong>: Timestamps for document creation and last update.</li>
    </ul>
  </li>
  <li><strong>Document Status Categories</strong> (<code class="language-plaintext highlighter-rouge">DocStatus</code> enum in <code class="language-plaintext highlighter-rouge">lightrag/base.py</code> and <code class="language-plaintext highlighter-rouge">document_routes.py</code>):
    <ul>
      <li><strong>Pending</strong>: Queued for processing.</li>
      <li><strong>Processing</strong>: Actively being analyzed (chunking, embedding, entity extraction).</li>
      <li><strong>Processed</strong> (or <strong>Completed</strong>): Successfully indexed and ready for querying.</li>
      <li><strong>Failed</strong>: An error occurred during processing. The <code class="language-plaintext highlighter-rouge">error</code> field in the status might provide details.</li>
    </ul>
  </li>
  <li><strong>Filtering by Status:</strong> The UI typically allows you to <strong>filter</strong> the document list by these statuses (e.g., view only “Failed” documents to troubleshoot).</li>
</ul>

<h3 id="step-2-check-the-pipeline-status-dialog-for-detailed-progress">Step 2: Check the Pipeline Status Dialog for Detailed Progress</h3>

<p>For a more granular view of <code class="language-plaintext highlighter-rouge">LightRAG</code>’s background operations:</p>

<ol>
  <li><strong>Open Pipeline Status Dialog:</strong>
    <ul>
      <li>On the <strong>“Documents”</strong> page, look for and <strong>click</strong> the <strong>“Pipeline Status”</strong> button (as per <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>).</li>
      <li><strong>Expected Feedback:</strong> A dialog titled <strong>“Pipeline Status”</strong> should open (controlled by <code class="language-plaintext highlighter-rouge">lightrag_webui/src/components/documents/PipelineStatusDialog.tsx</code>).</li>
    </ul>
  </li>
  <li><strong>Interpret Pipeline Information</strong> (based on <code class="language-plaintext highlighter-rouge">PipelineStatusResponse</code> in <code class="language-plaintext highlighter-rouge">document_routes.py</code> and <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>):
    <ul>
      <li><strong>“Pipeline Busy”</strong>: Indicates if the system is actively processing documents.</li>
      <li><strong>“Request Pending”</strong>: Shows if there are more documents in the queue waiting to be processed.</li>
      <li><strong>“Job Name”</strong>: Describes the current high-level task (e.g., “indexing files”).</li>
      <li><strong>“Progress”</strong>: Displays batch processing information (e.g., “Current Batch: X / Y total documents”).</li>
      <li><strong>“Latest Message”</strong> and <strong>“History Messages”</strong>: Provide logs and specific updates from the processing pipeline, which can be helpful for diagnosing issues.</li>
    </ul>
  </li>
</ol>

<h3 id="step-3-handling-processing-errors">Step 3: Handling Processing Errors</h3>

<ul>
  <li>If a document’s <strong>Status</strong> is “Failed,” <strong>examine</strong> any error messages provided in the document list or pipeline status.</li>
  <li><strong>Common Causes for Failure:</strong>
    <ul>
      <li>Unsupported file format (e.g., an uncoverted <code class="language-plaintext highlighter-rouge">.doc</code> file).</li>
      <li>Corrupted or unreadable file.</li>
      <li>Password-protected documents that <code class="language-plaintext highlighter-rouge">LightRAG</code> cannot decrypt.</li>
    </ul>
  </li>
  <li><strong>Action:</strong>
    <ol>
      <li><strong>Identify</strong> the problematic file(s).</li>
      <li><strong>Address</strong> the issue (e.g., <strong>convert</strong> the file to a supported format like <code class="language-plaintext highlighter-rouge">.docx</code> or text-searchable PDF, <strong>ensure</strong> it’s not corrupted).</li>
      <li>You can then either <strong>re-upload</strong> the corrected file(s) or, if you placed them in the <code class="language-plaintext highlighter-rouge">INPUT_DIR</code>, <strong>trigger</strong> another <strong>“Scan”</strong>. The documentation (<code class="language-plaintext highlighter-rouge">lightrag/api/README.md</code>) notes: “Reprocessing of failed files can be initiated by pressing the ‘Scan’ button on the web UI.”</li>
    </ol>
  </li>
</ul>

<hr />

<h2 id="part-4-finding-relevant-information-querying-your-knowledge-base">Part 4: Finding Relevant Information: Querying Your Knowledge Base</h2>

<p>Once your documents show a “Processed” status, you can start extracting the information you need for your research paper.</p>

<h3 id="step-1-navigate-to-the-retrieval-interface">Step 1: Navigate to the Retrieval Interface</h3>

<ol>
  <li>In the main navigation bar of the <code class="language-plaintext highlighter-rouge">LightRAG</code> WebUI, <strong>click</strong> on the tab labeled <strong>“Retrieval”</strong> (as per <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>).</li>
  <li>This will <strong>open</strong> the query interface, which is likely a chat-style window (controlled by <code class="language-plaintext highlighter-rouge">lightrag_webui/src/features/RetrievalTesting.tsx</code>).</li>
</ol>

<h3 id="step-2-formulating-and-running-queries">Step 2: Formulating and Running Queries</h3>

<ol>
  <li><strong>Query Input:</strong>
    <ul>
      <li>You’ll see an input box, typically at the bottom of the chat interface. The placeholder text might be “Enter your query (Support prefix: /<Query Mode="">)" (from `lightrag_webui/src/locales/en.json`: `"retrievePanel.retrieval.placeholder"`).</Query></li>
      <li><strong>Action:</strong> <strong>Type</strong> your research question or keywords into this box. For example: “What are the main arguments against Theory X discussed in these papers?” or “key findings on renewable energy adoption in Brazil.”</li>
    </ul>
  </li>
  <li><strong>Understanding and Selecting Query Modes:</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">LightRAG</code> offers several query modes to tailor how it searches for information and synthesizes answers. These modes are defined in the <code class="language-plaintext highlighter-rouge">QueryRequest</code> model (<code class="language-plaintext highlighter-rouge">lightrag/api/routers/query_routes.py</code>) and are selectable in the UI (<code class="language-plaintext highlighter-rouge">lightrag_webui/src/components/retrieval/QuerySettings.tsx</code>).</li>
      <li><strong>Accessing Query Mode Settings:</strong> Look for a <strong>“Parameters”</strong> section or a settings icon on the <strong>“Retrieval”</strong> page. This panel will contain a <strong>“Query Mode”</strong> dropdown.</li>
      <li><strong>Available Modes:</strong>
        <ul>
          <li><strong><code class="language-plaintext highlighter-rouge">/naive</code></strong>: Performs a basic, straightforward search.</li>
          <li><strong><code class="language-plaintext highlighter-rouge">/local</code></strong>: Focuses on context-dependent information, likely retrieving specific text chunks and entities directly related to the query terms.</li>
          <li><strong><code class="language-plaintext highlighter-rouge">/global</code></strong>: Utilizes the broader knowledge graph, emphasizing relationships between entities across your entire document set.</li>
          <li><strong><code class="language-plaintext highlighter-rouge">/hybrid</code></strong>: Combines aspects of both local and global retrieval. <strong>This is the default mode if no prefix is specified</strong> (as stated in <code class="language-plaintext highlighter-rouge">lightrag/api/README.md</code>).</li>
          <li><strong><code class="language-plaintext highlighter-rouge">/mix</code></strong>: Integrates knowledge graph traversal with vector-based similarity search for comprehensive results.</li>
          <li><strong><code class="language-plaintext highlighter-rouge">/bypass</code></strong>: Sends the query directly to the underlying Large Language Model (LLM) without performing any retrieval from your documents. This is generally not what you want for finding information <em>within</em> your research files.</li>
        </ul>
      </li>
      <li><strong>Action:</strong> You can either <strong>select</strong> the desired mode from the <strong>“Query Mode”</strong> dropdown in the UI settings or <strong>type</strong> the mode prefix directly into the chat input before your query (e.g., <code class="language-plaintext highlighter-rouge">/mix your question here</code>).</li>
    </ul>
  </li>
  <li><strong>Adjusting Query Settings (Parameters Section):</strong>
    <ul>
      <li>The <strong>“Parameters”</strong> section (<code class="language-plaintext highlighter-rouge">QuerySettings.tsx</code>) allows you to fine-tune your queries:
        <ul>
          <li><strong>“Response Format”</strong>: <strong>Choose</strong> how the LLM should structure its answer (e.g., “Multiple Paragraphs,” “Single Paragraph,” “Bullet Points” - from <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>).</li>
          <li><strong>“Top K Results”</strong>: (Default: 60) <strong>Set</strong> the number of top relevant items (entities, relationships, or text chunks) <code class="language-plaintext highlighter-rouge">LightRAG</code> should retrieve to form the context for the LLM.</li>
          <li><strong>“Max Tokens for Text Unit / Global Context / Local Context”</strong>: These settings control the maximum length of different types of context provided to the LLM. Defaults are usually around 4000 tokens.</li>
          <li><strong>“History Turns”</strong>: (Default: 3) <strong>Set</strong> how many previous turns of your current conversation with <code class="language-plaintext highlighter-rouge">LightRAG</code> are included as context for the LLM, enabling follow-up questions.</li>
          <li><strong>“Stream Response”</strong>: If checked, the LLM’s response will appear token by token, which can feel more interactive for longer answers.</li>
          <li><strong>“User Prompt”</strong>: This is a powerful feature. You can <strong>enter</strong> specific instructions here to guide the LLM on <em>how to format its answer</em> or <em>what aspects to emphasize</em>, separate from your main query content. For example: “Please summarize the findings and list the source documents for each point.”</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Executing the Query:</strong>
    <ul>
      <li>After <strong>typing</strong> your query and <strong>adjusting</strong> any settings, <strong>click</strong> the <strong>“Send”</strong> button (usually an icon like a paper airplane, labeled “Send” as per <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>: <code class="language-plaintext highlighter-rouge">"retrievePanel.retrieval.send"</code>).</li>
      <li><strong>Expected Feedback:</strong> <code class="language-plaintext highlighter-rouge">LightRAG</code> will process your query. If streaming is enabled, the assistant’s response will appear incrementally in the chat window. Otherwise, you’ll wait a moment for the complete response to be generated.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="part-5-utilizing-query-results-for-your-paper--referencing">Part 5: Utilizing Query Results for Your Paper &amp; Referencing</h2>

<p>This part explains how to use the information <code class="language-plaintext highlighter-rouge">LightRAG</code> provides and how it supports your citation needs.</p>

<h3 id="step-1-reviewing-and-copying-responses">Step 1: Reviewing and Copying Responses</h3>

<ol>
  <li><strong>Response Display:</strong> The LLM-generated answer will appear in the chat area, rendered by the <code class="language-plaintext highlighter-rouge">ChatMessage.tsx</code> component. This component supports Markdown formatting, code blocks, and can even display Mermaid diagrams if the LLM generates graph descriptions in that format.</li>
  <li><strong>Copying Information:</strong>
    <ul>
      <li><strong>Action:</strong> Look for a <strong>Copy</strong> icon next to the assistant’s message. <strong>Click</strong> it to copy the response text to your clipboard.</li>
      <li><strong>Purpose:</strong> This allows you to easily transfer quotes, summaries, or key points into your research paper draft.</li>
    </ul>
  </li>
</ol>

<h3 id="step-2-referencing-your-sources-with-lightrag">Step 2: Referencing Your Sources with LightRAG</h3>

<p>Properly citing your sources is critical. Here’s how <code class="language-plaintext highlighter-rouge">LightRAG</code> helps:</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">LightRAG</code>’s Built-in Source Tracking:</strong>
    <ul>
      <li>The <code class="language-plaintext highlighter-rouge">lightrag/api/README.md</code> states: “<code class="language-plaintext highlighter-rouge">LightRAG</code> now supports citation functionality, enabling proper source attribution.”</li>
      <li>The backend system (<code class="language-plaintext highlighter-rouge">lightrag/operate.py</code>) includes the <code class="language-plaintext highlighter-rouge">file_path</code> (and sometimes <code class="language-plaintext highlighter-rouge">created_at</code> timestamps) of the original documents when it constructs the context for the LLM. This means the LLM <em>has access</em> to the source file information when generating its response.</li>
    </ul>
  </li>
  <li><strong>Obtaining Source Information for Your Citations:</strong>
    <ul>
      <li><strong>Direct UI Display of Sources:</strong> The <code class="language-plaintext highlighter-rouge">LightRAG</code> WebUI’s chat response area (<code class="language-plaintext highlighter-rouge">ChatMessage.tsx</code>) does <strong>not</strong> automatically display a list of source files or page numbers for every statement made by the LLM.</li>
      <li><strong>Strategy 1: Prompting the LLM for Sources:</strong>
        <ul>
          <li><strong>Action:</strong> When you formulate your query, or by using the <strong>“User Prompt”</strong> field in the Query Settings, explicitly <strong>ask</strong> the LLM to identify its sources from the context it was given.</li>
          <li><strong>Example Query Addition:</strong> “… For each point, please indicate the source document name.”</li>
          <li><strong>Example User Prompt:</strong> “Cite the source file for each key finding mentioned in your response.”</li>
          <li><strong>Expected Outcome:</strong> The LLM, having received <code class="language-plaintext highlighter-rouge">file_path</code> information in its context, <em>may</em> include these source file names in its generated answer. The success of this depends on the LLM’s ability to follow such instructions.</li>
        </ul>
      </li>
      <li><strong>Strategy 2: Using “Only Need Context” Mode (Most Reliable for Source Identification):</strong>
        <ul>
          <li><strong>Action:</strong> In the Query Settings panel, <strong>check</strong> the box for <strong>“Only Need Context”</strong>. Then, <strong>run</strong> your query as usual.</li>
          <li><strong>Expected Outcome:</strong> Instead of an LLM-generated summary or answer, <code class="language-plaintext highlighter-rouge">LightRAG</code> will display the raw retrieved context that <em>would have been sent</em> to the LLM. This raw context will include the text chunks, entities, and relationships, along with their associated metadata, which critically includes the <code class="language-plaintext highlighter-rouge">file_path</code>.</li>
          <li><strong>Purpose:</strong> You can then directly see which document(s) contributed to the relevant information for your query and use these <code class="language-plaintext highlighter-rouge">file_path</code> details for your citations.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Source Granularity:</strong>
    <ul>
      <li>The documentation confirms that <code class="language-plaintext highlighter-rouge">file_path</code> is tracked and available.</li>
      <li>While specific page numbers for PDFs or precise section headers within documents are not explicitly guaranteed to be part of the metadata for <em>every</em> retrieved chunk across all file types, the source <em>file</em> itself will be identifiable.</li>
    </ul>
  </li>
  <li><strong>Final Citation Formatting:</strong>
    <ul>
      <li>Once you have identified the relevant content and its source file path using <code class="language-plaintext highlighter-rouge">LightRAG</code>, you will still need to:
        <ol>
          <li><strong>Open</strong> the original document to verify the context and gather full bibliographic details (author, year, title, etc.).</li>
          <li>Manually <strong>format</strong> your citations according to your required academic style (e.g., APA, MLA, Chicago) in your word processor or using dedicated reference management software (like Zotero, Mendeley, EndNote).</li>
        </ol>
      </li>
      <li><code class="language-plaintext highlighter-rouge">LightRAG</code> excels at the <em>discovery</em> and <em>sourcing</em> of information from your vast collection, but it does not automate the final step of bibliographic formatting.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="part-6-advanced-exploration--workflow-enhancement">Part 6: Advanced Exploration &amp; Workflow Enhancement</h2>

<p>Beyond basic querying, <code class="language-plaintext highlighter-rouge">LightRAG</code> offers features for deeper analysis and customization.</p>

<h3 id="step-1-exploring-connections-with-the-knowledge-graph">Step 1: Exploring Connections with the Knowledge Graph</h3>

<p>The Knowledge Graph (KG) provides a visual representation of the entities and relationships extracted from your documents. This can help you discover connections you might not have noticed.</p>

<ol>
  <li><strong>Navigate to the Knowledge Graph Section:</strong>
    <ul>
      <li>In the main navigation bar, <strong>click</strong> on the <strong>“Knowledge Graph”</strong> tab. This will load the interactive 3D graph visualization interface (controlled by <code class="language-plaintext highlighter-rouge">lightrag_webui/src/features/GraphViewer.tsx</code>).</li>
    </ul>
  </li>
  <li><strong>Interacting with the Graph:</strong>
    <ul>
      <li><strong>Select Query Label:</strong> On the left sidebar, use the <strong>“Label”</strong> dropdown or search bar (<code class="language-plaintext highlighter-rouge">lightrag_webui/src/components/graph/GraphLabels.tsx</code>) to focus the graph. You can <strong>select</strong> an entity type (e.g., “person,” “organization”) or <strong>search</strong> for a specific entity name. Selecting <code class="language-plaintext highlighter-rouge">*</code> (asterisk) attempts to load all nodes (be mindful of the “Max Nodes” setting).</li>
      <li><strong>Refresh Graph Data vs. Layout:</strong>
        <ul>
          <li>To reload graph data after adding new files or making backend changes, <strong>click</strong> the <strong>“Refresh”</strong> button (circular arrow icon) next to the label selection in the <strong>“Label”</strong> section (<code class="language-plaintext highlighter-rouge">GraphLabels.tsx</code>).</li>
          <li>To visually re-arrange the currently displayed nodes, use the <strong>“Layout Graph”</strong> control (<code class="language-plaintext highlighter-rouge">lightrag_webui/src/components/graph/LayoutsControl.tsx</code>) to <strong>select</strong> and <strong>apply</strong> different layout algorithms (e.g., “Circular,” “Force Directed”).</li>
        </ul>
      </li>
      <li><strong>Node Interaction:</strong>
        <ul>
          <li><strong>Hover</strong> your mouse over nodes to highlight them.</li>
          <li><strong>Click</strong> on a node to select it. This opens the <strong>“Properties”</strong> panel on the right (<code class="language-plaintext highlighter-rouge">lightrag_webui/src/components/graph/PropertiesView.tsx</code>).</li>
        </ul>
      </li>
      <li><strong>Camera Controls:</strong> Use <strong>W, A, S, D</strong> keys for panning, <strong>Q</strong> and <strong>E</strong> for up/down movement. <strong>Hold</strong> the <strong>right mouse button</strong> and <strong>drag</strong> to rotate the view. Use <strong>“Zoom In”</strong> / <strong>“Zoom Out”</strong> buttons or your mouse scroll wheel. <strong>“Reset Zoom”</strong> returns to the default view.</li>
      <li><strong>Search within Graph:</strong> Use the graph-specific search bar (“Search nodes…” from <code class="language-plaintext highlighter-rouge">GraphSearch.tsx</code>) to find nodes in the current view.</li>
    </ul>
  </li>
  <li><strong>Viewing and Editing Node/Relationship Properties:</strong>
    <ul>
      <li>When a node or edge is selected, the <strong>“Properties”</strong> panel displays its details: “ID,” “Labels,” “Degree,” and other properties like “Description,” “Name,” “Type,” “Source ID,” “File Path,” “Keywords,” “Weight.”</li>
      <li>The documentation suggests that you can <strong>edit</strong> these properties directly from this panel (as supported by <code class="language-plaintext highlighter-rouge">lightrag_webui/src/components/graph/EditablePropertyRow.tsx</code> and backend routes in <code class="language-plaintext highlighter-rouge">lightrag/api/routers/graph_routes.py</code>). This is useful for refining your knowledge graph.</li>
      <li>Buttons like <strong>“Expand Node”</strong> and <strong>“Prune Node”</strong> in the properties panel allow you to dynamically add or remove connected nodes from the visualization, helping you focus on specific subgraphs.</li>
    </ul>
  </li>
</ol>

<h3 id="step-2-entity-merging-advanced-data-cleaning---conceptual">Step 2: Entity Merging (Advanced Data Cleaning - Conceptual)</h3>

<p><code class="language-plaintext highlighter-rouge">LightRAG</code>’s core library supports merging multiple entities into a single target entity, automatically handling relationships (<code class="language-plaintext highlighter-rouge">rag.merge_entities()</code> in <code class="language-plaintext highlighter-rouge">lightrag.py</code>). This is useful for de-duplicating concepts.</p>

<ul>
  <li><strong>From the Web UI:</strong> The provided documentation does <strong>not</strong> explicitly detail a direct “Merge Entities” button or feature within the Web UI. This functionality is primarily described as a Python function in the <code class="language-plaintext highlighter-rouge">LightRAG</code> Core.</li>
  <li><strong>Conceptual Use:</strong> If you identify duplicate entities, you would typically use the <code class="language-plaintext highlighter-rouge">merge_entities</code> function via the <code class="language-plaintext highlighter-rouge">LightRAG</code> Core API (e.g., in a Python script) or look for such features if they are added to the UI in future versions.</li>
</ul>

<hr />

<h2 id="part-7-maintaining-your-lightrag-instance--data">Part 7: Maintaining Your LightRAG Instance &amp; Data</h2>

<h3 id="step-1-clearing-documents-and-cache">Step 1: Clearing Documents and Cache</h3>

<p>For maintenance or to start fresh with a new set of documents:</p>

<ol>
  <li><strong>Clear All Documents:</strong>
    <ul>
      <li><strong>Navigate</strong> to the <strong>“Documents”</strong> tab.</li>
      <li>Look for the <strong>“Clear”</strong> button (often an eraser icon, as per <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>: <code class="language-plaintext highlighter-rouge">"documentPanel.clearDocuments.button": "Clear"</code>).</li>
      <li><strong>Action:</strong> Clicking this button opens a <strong>“Clear Documents”</strong> dialog (<code class="language-plaintext highlighter-rouge">lightrag_webui/src/components/documents/ClearDocumentsDialog.tsx</code>).</li>
      <li><strong>WARNING:</strong> This action, as described in the UI text (<code class="language-plaintext highlighter-rouge">"documentPanel.clearDocuments.warning"</code>), “will permanently delete all documents and cannot be undone!” It removes all documents, entities, relationships, and files from the system. You will need to <strong>type</strong> <code class="language-plaintext highlighter-rouge">yes</code> in a confirmation box to proceed.</li>
      <li><strong>Purpose:</strong> Use this if you want to completely reset your <code class="language-plaintext highlighter-rouge">LightRAG</code> instance and re-ingest a new set of documents.</li>
    </ul>
  </li>
  <li><strong>Clear LLM Cache:</strong>
    <ul>
      <li>Within the <strong>“Clear Documents”</strong> dialog, there’s also an option to <strong>“Clear LLM cache”</strong> (as per <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>).</li>
      <li><strong>Purpose:</strong> This clears <code class="language-plaintext highlighter-rouge">LightRAG</code>’s cache of responses from the Large Language Model (LLM) (e.g., from previous queries or entity extractions). This can be useful if you’ve changed LLM models or configurations and want to ensure fresh responses, without re-indexing all your documents. It does <em>not</em> delete your documents or the knowledge graph itself.</li>
    </ul>
  </li>
</ol>

<h3 id="step-2-exporting-your-knowledge-graph-data-conceptual">Step 2: Exporting Your Knowledge Graph Data (Conceptual)</h3>

<p>For your research paper, you might want to export the structured data from <code class="language-plaintext highlighter-rouge">LightRAG</code> for further analysis or to include as supplementary material.</p>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">lightrag/api/README.md</code> and <code class="language-plaintext highlighter-rouge">lightrag/lightrag.py</code> documentation mention an <code class="language-plaintext highlighter-rouge">export_data()</code> Python function in the <code class="language-plaintext highlighter-rouge">LightRAG</code> Core library, which supports exporting data to formats like CSV, Excel, Markdown, and plain text.</li>
  <li><strong>From the Web UI:</strong> The provided documentation for the WebUI components does <strong>not</strong> explicitly show a direct “Export Data” button or feature for the knowledge graph or document list. This functionality is primarily exposed via the <code class="language-plaintext highlighter-rouge">LightRAG</code> Core API.</li>
  <li><strong>Conceptual Export Process (if no direct UI button):</strong> If you need to export data and a direct UI button is not present, you would conceptually need to:
    <ol>
      <li><strong>Interact</strong> with the underlying API endpoints. You can explore these via the Swagger UI, typically accessible at <code class="language-plaintext highlighter-rouge">http://localhost:9621/docs</code>.</li>
      <li>Alternatively, <strong>use</strong> the <code class="language-plaintext highlighter-rouge">LightRAG</code> Core library programmatically in a Python script to call the <code class="language-plaintext highlighter-rouge">export_data</code> function.</li>
    </ol>
  </li>
</ul>

<hr />

<h2 id="part-8-limitations-and-important-notes">Part 8: Limitations and Important Notes</h2>

<ul>
  <li><strong>Legacy <code class="language-plaintext highlighter-rouge">.doc</code> Files:</strong> <code class="language-plaintext highlighter-rouge">LightRAG</code> (with default <code class="language-plaintext highlighter-rouge">Docker</code> setup) does <strong>not</strong> natively support legacy <code class="language-plaintext highlighter-rouge">.doc</code> files. You <strong>must convert</strong> these to <code class="language-plaintext highlighter-rouge">.docx</code>, text-searchable <code class="language-plaintext highlighter-rouge">.pdf</code>, or <code class="language-plaintext highlighter-rouge">.txt</code> before uploading.</li>
  <li><strong>Image-Only PDFs:</strong> For best results with PDF files, <strong>ensure</strong> they are text-searchable (i.e., contain actual selectable text, not just scanned images).</li>
  <li><strong>Other File Formats:</strong> While <code class="language-plaintext highlighter-rouge">LightRAG</code> lists many file extensions as supported (in <code class="language-plaintext highlighter-rouge">DocumentManager</code>), some (like RTF, ODT, EPUB, TEX) might require additional, non-default system libraries (e.g., <code class="language-plaintext highlighter-rouge">docling</code>) for full processing. If you encounter issues with these, <strong>convert</strong> them to more reliably supported formats like PDF, DOCX, or TXT.</li>
  <li><strong>No Manual Folder Organization Needed:</strong> <code class="language-plaintext highlighter-rouge">LightRAG</code> is designed to work with a flat collection of files in its input directory. You do not need to manually <strong>sort</strong> your files into subdirectories before uploading or scanning.</li>
  <li><strong>Final Judgment Rests with User:</strong> Always critically <strong>review</strong> the information retrieved by <code class="language-plaintext highlighter-rouge">LightRAG</code> and <strong>consult</strong> the original source document to ensure accuracy and proper context before using it in your research paper.</li>
</ul>

<hr />

<h2 id="conclusion-empowering-your-research">Conclusion: Empowering Your Research</h2>

<p><code class="language-plaintext highlighter-rouge">LightRAG</code>, based on its documented features, offers a robust and user-friendly way to tackle your disorganized research files. By following this tutorial, you can:</p>

<ul>
  <li>Successfully <strong>ingest and index</strong> your mixed-format documents (after necessary conversions, especially for <code class="language-plaintext highlighter-rouge">.doc</code> files).</li>
  <li><strong>Utilize</strong> a powerful query interface with various modes to efficiently find specific information.</li>
  <li><strong>Leverage</strong> <code class="language-plaintext highlighter-rouge">LightRAG</code>’s <strong>source tracking (<code class="language-plaintext highlighter-rouge">file_path</code>)</strong> to support your referencing needs.</li>
  <li>Optionally, <strong>explore</strong> your data visually through the <strong>knowledge graph</strong>.</li>
</ul>

<p>This system has the potential to significantly reduce the time you spend sifting through documents, allowing you to focus more on the critical tasks of analyzing information and writing your paper. Remember to <strong>consult</strong> the <code class="language-plaintext highlighter-rouge">lightrag/api/README.md</code> and the UI tooltips (many are defined in <code class="language-plaintext highlighter-rouge">lightrag_webui/src/locales/en.json</code>) for quick reminders as you use the tool.</p>

<p>Good luck with your research!</p>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">Maximizing VM Performance with Incus v1</title><link href="https://ib.bsb.br/maximizing-vm-performance-with-incus-v1/" rel="alternate" type="text/html" title="Maximizing VM Performance with Incus v1" /><published>2025-05-19T00:00:00+00:00</published><updated>2025-05-19T09:10:23+00:00</updated><id>https://ib.bsb.br/maximizing-vm-performance-with-incus-v1</id><content type="html" xml:base="https://ib.bsb.br/maximizing-vm-performance-with-incus-v1/"><![CDATA[<h2 id="quick-reference-guide">Quick Reference Guide</h2>

<p>This comprehensive guide helps you configure Ubuntu Server 25 and Incus to achieve near-bare-metal performance for a single virtual machine. Follow these steps systematically, testing after each major change.</p>

<p><strong>Key Performance Factors:</strong></p>
<ul>
  <li>CPU isolation and pinning</li>
  <li>Memory allocation and hugepages</li>
  <li>Storage optimization</li>
  <li>Network configuration</li>
  <li>Host OS minimization</li>
</ul>

<p><strong>Prerequisites:</strong></p>
<ul>
  <li>Ubuntu Server 25 installed</li>
  <li>Administrative (sudo) access</li>
  <li>Basic understanding of Linux system administration</li>
  <li>Hardware with virtualization support (Intel VT-x/AMD-V)</li>
  <li>Backup of any important data</li>
</ul>

<h2 id="i-pre-implementation-planning">I. Pre-Implementation Planning</h2>

<h3 id="1-hardware-assessment">1. Hardware Assessment</h3>

<p>Before beginning optimizations, assess your hardware capabilities:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check CPU information</span>
lscpu

<span class="c"># Check virtualization support</span>
egrep <span class="nt">-c</span> <span class="s1">'(vmx|svm)'</span> /proc/cpuinfo  <span class="c"># Should return &gt; 0</span>

<span class="c"># Check IOMMU support (for PCI passthrough)</span>
dmesg | <span class="nb">grep</span> <span class="nt">-e</span> DMAR <span class="nt">-e</span> IOMMU

<span class="c"># Check memory</span>
free <span class="nt">-h</span>

<span class="c"># Check storage devices</span>
lsblk <span class="nt">-o</span> NAME,SIZE,MODEL,ROTA
</code></pre></div></div>

<h3 id="2-workload-requirements-analysis">2. Workload Requirements Analysis</h3>

<p>Different workloads have different optimization priorities:</p>
<ul>
  <li><strong>CPU-intensive workloads</strong>: Focus on CPU isolation and frequency scaling</li>
  <li><strong>Memory-intensive workloads</strong>: Prioritize hugepages and NUMA optimization</li>
  <li><strong>I/O-intensive workloads</strong>: Focus on storage and network passthrough</li>
  <li><strong>Graphics-intensive workloads</strong>: Prioritize GPU passthrough</li>
</ul>

<h3 id="3-risk-assessment">3. Risk Assessment</h3>

<table>
  <thead>
    <tr>
      <th>Optimization</th>
      <th>Performance Benefit</th>
      <th>Security Impact</th>
      <th>Stability Risk</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CPU isolation</td>
      <td>High</td>
      <td>Low</td>
      <td>Low</td>
    </tr>
    <tr>
      <td>Disabling CPU mitigations</td>
      <td>High</td>
      <td>High</td>
      <td>Low</td>
    </tr>
    <tr>
      <td>Hugepages</td>
      <td>Medium</td>
      <td>Low</td>
      <td>Low</td>
    </tr>
    <tr>
      <td>PCI passthrough</td>
      <td>High</td>
      <td>Medium</td>
      <td>Medium</td>
    </tr>
    <tr>
      <td>Service disabling</td>
      <td>Low</td>
      <td>Varies</td>
      <td>Medium</td>
    </tr>
  </tbody>
</table>

<h2 id="ii-ubuntu-server-host-optimizations">II. Ubuntu Server Host Optimizations</h2>

<h3 id="1-minimal-installation-and-service-optimization">1. Minimal Installation and Service Optimization</h3>

<p>Start with the most minimal Ubuntu Server 25 installation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List all enabled services</span>
<span class="nb">sudo </span>systemctl list-unit-files <span class="nt">--type</span><span class="o">=</span>service <span class="nt">--state</span><span class="o">=</span>enabled

<span class="c"># Identify non-essential services (examples below)</span>
<span class="c"># - snapd (if not using snaps for Incus)</span>
<span class="c"># - apport (automated crash reporting)</span>
<span class="c"># - motd-news (message of the day updates)</span>
<span class="c"># - unattended-upgrades (if you prefer manual updates)</span>

<span class="c"># Disable non-essential services (example)</span>
<span class="nb">sudo </span>systemctl stop snapd.service
<span class="nb">sudo </span>systemctl disable snapd.service

<span class="c"># Verify service is disabled</span>
systemctl is-enabled snapd.service  <span class="c"># Should return "disabled"</span>
</code></pre></div></div>

<p><strong>CAUTION</strong>: Do not disable these essential services:</p>
<ul>
  <li>systemd-journald</li>
  <li>systemd-logind</li>
  <li>ssh (if you need remote access)</li>
  <li>networking/NetworkManager</li>
  <li>incus-related services</li>
</ul>

<h3 id="2-firmware-and-microcode-updates">2. Firmware and Microcode Updates</h3>

<p>Ensure your system firmware and CPU microcode are up-to-date:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install firmware update tools</span>
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> fwupd intel-microcode  <span class="c"># For Intel CPUs</span>
<span class="c"># OR</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> fwupd amd64-microcode  <span class="c"># For AMD CPUs</span>

<span class="c"># Check for and apply firmware updates</span>
<span class="nb">sudo </span>fwupdmgr refresh
<span class="nb">sudo </span>fwupdmgr get-updates
<span class="nb">sudo </span>fwupdmgr update

<span class="c"># Verify microcode is loaded</span>
dmesg | <span class="nb">grep </span>microcode
</code></pre></div></div>

<h3 id="3-biosuefi-configuration">3. BIOS/UEFI Configuration</h3>

<p>Access your system’s BIOS/UEFI settings and configure:</p>

<ol>
  <li>
    <p><strong>Enable virtualization technologies</strong>:
   - Intel: VT-x, VT-d
   - AMD: AMD-V, AMD-Vi/IOMMU</p>
  </li>
  <li>
    <p><strong>Disable power-saving features</strong>:
   - C-states beyond C1/C2
   - P-states (or set to OS control)
   - Set power profile to “Performance” or “High Performance”</p>
  </li>
  <li>
    <p><strong>Memory settings</strong>:
   - Disable memory power saving
   - For NUMA systems: Set appropriate memory interleaving mode</p>
  </li>
  <li>
    <p><strong>Disable unused devices</strong>:
   - Audio controllers (if not needed)
   - Serial/parallel ports
   - Unused SATA/PCIe controllers</p>
  </li>
</ol>

<h3 id="4-kernel-boot-parameters">4. Kernel Boot Parameters</h3>

<p>Modify GRUB configuration to optimize the kernel:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Edit GRUB configuration</span>
<span class="nb">sudo </span>nano /etc/default/grub

<span class="c"># Identify your CPU cores</span>
lscpu

<span class="c"># Determine cores to reserve for host vs. VM</span>
<span class="c"># Example: On an 8-core system (0-7), reserve cores 0-1 for host, 2-7 for VM</span>
</code></pre></div></div>

<p>Add these parameters to <code class="language-plaintext highlighter-rouge">GRUB_CMDLINE_LINUX_DEFAULT</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># For Intel CPUs:
isolcpus=2-7 nohz_full=2-7 rcu_nocbs=2-7 intel_iommu=on iommu=pt

# For AMD CPUs:
isolcpus=2-7 nohz_full=2-7 rcu_nocbs=2-7 amd_iommu=on iommu=pt
</code></pre></div></div>

<p><strong>Parameter Explanation</strong>:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">isolcpus=2-7</code>: Prevents the kernel scheduler from using these cores for general tasks</li>
  <li><code class="language-plaintext highlighter-rouge">nohz_full=2-7</code>: Reduces timer interrupts on isolated cores</li>
  <li><code class="language-plaintext highlighter-rouge">rcu_nocbs=2-7</code>: Offloads RCU callbacks from isolated cores</li>
  <li><code class="language-plaintext highlighter-rouge">intel_iommu=on</code>/<code class="language-plaintext highlighter-rouge">amd_iommu=on</code>: Enables IOMMU for device passthrough</li>
  <li><code class="language-plaintext highlighter-rouge">iommu=pt</code>: Optimizes IOMMU for passthrough performance</li>
</ul>

<p><strong>Optional Performance Parameters</strong> (with security implications):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># SECURITY RISK - only use in trusted environments:
mitigations=off
</code></pre></div></div>

<p>Apply changes and verify:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Update GRUB</span>
<span class="nb">sudo </span>update-grub

<span class="c"># Reboot</span>
<span class="nb">sudo </span>reboot

<span class="c"># Verify parameters are applied</span>
<span class="nb">cat</span> /proc/cmdline
</code></pre></div></div>

<h3 id="5-cpu-frequency-scaling">5. CPU Frequency Scaling</h3>

<p>Configure CPU governor for maximum performance:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install CPU frequency utilities</span>
<span class="nb">sudo </span>apt update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> cpufrequtils

<span class="c"># Set performance governor</span>
<span class="nb">echo</span> <span class="s1">'GOVERNOR="performance"'</span> | <span class="nb">sudo tee</span> /etc/default/cpufrequtils

<span class="c"># Disable ondemand service (conflicts with performance governor)</span>
<span class="nb">sudo </span>systemctl disable <span class="nt">--now</span> ondemand

<span class="c"># Enable cpufrequtils service</span>
<span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> cpufrequtils

<span class="c"># Verify settings (should show "performance" for all CPUs)</span>
<span class="nb">cat</span> /sys/devices/system/cpu/cpu<span class="k">*</span>/cpufreq/scaling_governor
</code></pre></div></div>

<p>For Intel CPUs with <code class="language-plaintext highlighter-rouge">intel_pstate</code> driver:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check if using intel_pstate</span>
<span class="nb">cat</span> /sys/devices/system/cpu/cpu0/cpufreq/scaling_driver

<span class="c"># If using intel_pstate, set performance mode</span>
<span class="nb">echo </span>performance | <span class="nb">sudo tee</span> /sys/devices/system/cpu/intel_pstate/status
</code></pre></div></div>

<h3 id="6-memory-optimization-swappiness-and-hugepages">6. Memory Optimization: Swappiness and Hugepages</h3>

<h4 id="61-reduce-swappiness">6.1 Reduce Swappiness</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set swappiness to minimum</span>
<span class="nb">sudo </span>sysctl vm.swappiness<span class="o">=</span>1

<span class="c"># Make permanent</span>
<span class="nb">echo</span> <span class="s1">'vm.swappiness=1'</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/sysctl.conf

<span class="c"># Verify setting</span>
<span class="nb">cat</span> /proc/sys/vm/swappiness  <span class="c"># Should return 1</span>
</code></pre></div></div>

<h4 id="62-configure-hugepages">6.2 Configure Hugepages</h4>

<p>First, determine your hugepage size and VM memory requirements:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check hugepage size</span>
<span class="nb">cat</span> /proc/meminfo | <span class="nb">grep </span>Hugepagesize  <span class="c"># Usually 2048 kB (2MB)</span>

<span class="c"># Calculate number of hugepages needed</span>
<span class="c"># Formula: (VM_RAM_in_GB * 1024 + overhead_MB) / hugepage_size_in_MB</span>
<span class="c"># Example for 60GB VM with 2MB pages: (60*1024 + 512) / 2 = 30976</span>
</code></pre></div></div>

<p>Configure hugepages:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># For 2MB pages (temporary setting)</span>
<span class="nb">echo </span>30976 | <span class="nb">sudo tee</span> /proc/sys/vm/nr_hugepages

<span class="c"># Make permanent</span>
<span class="nb">echo</span> <span class="s1">'vm.nr_hugepages = 30976'</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/sysctl.conf

<span class="c"># Alternative: Configure via GRUB for 1GB pages</span>
<span class="c"># Add to GRUB_CMDLINE_LINUX_DEFAULT: hugepagesz=1G hugepages=62</span>

<span class="c"># Verify hugepages allocation</span>
<span class="nb">cat</span> /proc/meminfo | <span class="nb">grep</span> <span class="nt">-i</span> huge
</code></pre></div></div>

<p><strong>Note</strong>: If hugepages allocation fails, try:</p>
<ol>
  <li>Allocating fewer pages</li>
  <li>Rebooting and allocating before memory fragmentation occurs</li>
  <li>Adding slightly more than needed to account for system usage</li>
</ol>

<h3 id="7-disk-io-optimization">7. Disk I/O Optimization</h3>

<h4 id="71-choose-optimal-io-scheduler">7.1 Choose Optimal I/O Scheduler</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Identify your storage devices</span>
lsblk <span class="nt">-d</span> <span class="nt">-o</span> NAME,ROTA,MODEL

<span class="c"># Check current scheduler</span>
<span class="nb">cat</span> /sys/block/nvme0n1/queue/scheduler  <span class="c"># For NVMe</span>
<span class="c"># OR</span>
<span class="nb">cat</span> /sys/block/sda/queue/scheduler  <span class="c"># For SATA</span>

<span class="c"># Set optimal scheduler</span>
<span class="c"># For NVMe:</span>
<span class="nb">echo </span>none | <span class="nb">sudo tee</span> /sys/block/nvme0n1/queue/scheduler

<span class="c"># For SATA SSD:</span>
<span class="nb">echo </span>mq-deadline | <span class="nb">sudo tee</span> /sys/block/sda/queue/scheduler

<span class="c"># For SATA HDD:</span>
<span class="nb">echo </span>bfq | <span class="nb">sudo tee</span> /sys/block/sda/queue/scheduler
</code></pre></div></div>

<p>Make changes persistent:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create udev rules file</span>
<span class="nb">sudo </span>nano /etc/udev/rules.d/60-scheduler.rules

<span class="c"># Add these lines (adjust device patterns as needed)</span>
<span class="nv">ACTION</span><span class="o">==</span><span class="s2">"add|change"</span>, <span class="nv">KERNEL</span><span class="o">==</span><span class="s2">"sd[a-z]"</span>, ATTR<span class="o">{</span>queue/rotational<span class="o">}==</span><span class="s2">"0"</span>, ATTR<span class="o">{</span>queue/scheduler<span class="o">}=</span><span class="s2">"mq-deadline"</span>
<span class="nv">ACTION</span><span class="o">==</span><span class="s2">"add|change"</span>, <span class="nv">KERNEL</span><span class="o">==</span><span class="s2">"sd[a-z]"</span>, ATTR<span class="o">{</span>queue/rotational<span class="o">}==</span><span class="s2">"1"</span>, ATTR<span class="o">{</span>queue/scheduler<span class="o">}=</span><span class="s2">"bfq"</span>
<span class="nv">ACTION</span><span class="o">==</span><span class="s2">"add|change"</span>, <span class="nv">KERNEL</span><span class="o">==</span><span class="s2">"nvme[0-9]*"</span>, ATTR<span class="o">{</span>queue/scheduler<span class="o">}=</span><span class="s2">"none"</span>
</code></pre></div></div>

<h4 id="72-additional-storage-optimizations">7.2 Additional Storage Optimizations</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Disable disk barriers if using battery-backed cache</span>
<span class="c"># CAUTION: Data loss risk if power failure occurs without proper battery backup</span>
<span class="nb">sudo </span>mount <span class="nt">-o</span> remount,nobarrier /mount/point  <span class="c"># Only for specific filesystems</span>

<span class="c"># Increase read-ahead for sequential workloads (example for nvme0n1)</span>
<span class="nb">sudo </span>blockdev <span class="nt">--setra</span> 4096 /dev/nvme0n1
</code></pre></div></div>

<h3 id="8-irq-affinity">8. IRQ Affinity</h3>

<p>Properly configure IRQ affinity to prevent interrupts from impacting VM performance:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install tools</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> irqbalance

<span class="c"># Disable irqbalance service (we'll manually configure IRQs)</span>
<span class="nb">sudo </span>systemctl stop irqbalance
<span class="nb">sudo </span>systemctl disable irqbalance

<span class="c"># Identify IRQs and their current affinity</span>
<span class="nb">cat</span> /proc/interrupts
</code></pre></div></div>

<p>Create an IRQ affinity script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>nano /usr/local/bin/set-irq-affinity.sh
</code></pre></div></div>

<p>Add this content (adjust CPU mask for your configuration):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c"># This script sets IRQ affinity to host-only CPUs (0-1 in this example)</span>
<span class="c"># CPU mask for cores 0-1: 3 (binary 11)</span>

<span class="c"># Set default affinity for all IRQs to cores 0-1</span>
<span class="k">for </span>irq <span class="k">in</span> <span class="si">$(</span><span class="nb">cat</span> /proc/interrupts | <span class="nb">grep</span> <span class="nt">-v</span> CPU | <span class="nb">awk</span> <span class="s1">'{print $1}'</span> | <span class="nb">sed </span>s/<span class="se">\:</span>//g<span class="si">)</span><span class="p">;</span> <span class="k">do</span>
  <span class="k">if</span> <span class="o">[</span> <span class="nt">-f</span> /proc/irq/<span class="nv">$irq</span>/smp_affinity <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nb">echo</span> <span class="s2">"Setting IRQ </span><span class="nv">$irq</span><span class="s2"> affinity to 3 (CPU 0-1)"</span>
    <span class="nb">echo </span>3 <span class="o">&gt;</span> /proc/irq/<span class="nv">$irq</span>/smp_affinity
  <span class="k">fi
done</span>

<span class="c"># For devices used by the VM via passthrough, you might want different settings</span>
<span class="c"># Example: If passing through a NIC with IRQ 40, you might want it on the VM's CPUs</span>
<span class="c"># echo 252 &gt; /proc/irq/40/smp_affinity  # Binary 11111100 = CPUs 2-7</span>
</code></pre></div></div>

<p>Make the script executable and run at boot:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chmod</span> +x /usr/local/bin/set-irq-affinity.sh

<span class="c"># Add to /etc/rc.local or create a systemd service</span>
<span class="nb">sudo </span>nano /etc/systemd/system/irqaffinity.service
</code></pre></div></div>

<p>Add this content:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Unit]
Description=Set IRQ Affinity
After=network.target

[Service]
Type=oneshot
ExecStart=/usr/local/bin/set-irq-affinity.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
</code></pre></div></div>

<p>Enable and start the service:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl <span class="nb">enable </span>irqaffinity.service
<span class="nb">sudo </span>systemctl start irqaffinity.service

<span class="c"># Verify IRQ affinity</span>
<span class="nb">cat</span> /proc/irq/<span class="k">*</span>/smp_affinity
</code></pre></div></div>

<h3 id="9-system-wide-tuning-with-tuned">9. System-wide Tuning with tuned</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install tuned</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> tuned

<span class="c"># Apply virtual-host profile</span>
<span class="nb">sudo </span>tuned-adm profile virtual-host

<span class="c"># Verify active profile</span>
<span class="nb">sudo </span>tuned-adm active  <span class="c"># Should show "Current active profile: virtual-host"</span>
</code></pre></div></div>

<p>For advanced users, create a custom tuned profile:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create custom profile directory</span>
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> /etc/tuned/vm-host-custom

<span class="c"># Create profile configuration</span>
<span class="nb">sudo </span>nano /etc/tuned/vm-host-custom/tuned.conf
</code></pre></div></div>

<p>Add this content (adjust as needed):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[main]
include=virtual-host

[cpu]
force_latency=1
governor=performance
energy_perf_bias=performance

[vm]
transparent_hugepages=never

[sysctl]
vm.swappiness=1
kernel.sched_min_granularity_ns=10000000
kernel.sched_wakeup_granularity_ns=15000000
</code></pre></div></div>

<p>Apply the custom profile:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>tuned-adm profile vm-host-custom

<span class="c"># Verify</span>
<span class="nb">sudo </span>tuned-adm active
</code></pre></div></div>

<h2 id="iii-incus-virtual-machine-configuration">III. Incus Virtual Machine Configuration</h2>

<h3 id="1-incus-installation-and-basic-setup">1. Incus Installation and Basic Setup</h3>

<p>If not already installed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add Incus repository</span>
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> curl gpg
curl <span class="nt">-fsSL</span> https://pkgs.zabbly.com/key.asc | <span class="nb">sudo </span>gpg <span class="nt">--dearmor</span> <span class="nt">-o</span> /etc/apt/keyrings/zabbly.gpg
<span class="nb">echo</span> <span class="s2">"deb [arch=amd64,arm64 signed-by=/etc/apt/keyrings/zabbly.gpg] https://pkgs.zabbly.com/incus/stable </span><span class="si">$(</span>lsb_release <span class="nt">-cs</span><span class="si">)</span><span class="s2"> main"</span> | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/incus-stable.list

<span class="c"># Install Incus</span>
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> incus

<span class="c"># Initialize Incus</span>
<span class="nb">sudo </span>incus admin init
</code></pre></div></div>

<h3 id="2-cpu-configuration">2. CPU Configuration</h3>

<p>Create or configure your VM with optimal CPU settings:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a new VM (example)</span>
incus init images:ubuntu/24.04 YOUR_VM_NAME <span class="nt">--vm</span> <span class="nt">-c</span> limits.cpu<span class="o">=</span>2-7

<span class="c"># Or configure existing VM</span>
incus config <span class="nb">set </span>YOUR_VM_NAME limits.cpu 2-7
incus config <span class="nb">set </span>YOUR_VM_NAME limits.cpu.allowance 100%
incus config <span class="nb">set </span>YOUR_VM_NAME limits.cpu.priority 10

<span class="c"># Enable CPU host passthrough for best performance</span>
incus config <span class="nb">set </span>YOUR_VM_NAME security.guest.features.cpu.host_passthrough <span class="nb">true</span>

<span class="c"># For NUMA systems, pin to specific node</span>
incus config <span class="nb">set </span>YOUR_VM_NAME limits.cpu.nodes 0
</code></pre></div></div>

<p>Verify CPU configuration:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>incus config show YOUR_VM_NAME | <span class="nb">grep</span> <span class="nt">-i</span> cpu
</code></pre></div></div>

<h3 id="3-memory-configuration">3. Memory Configuration</h3>

<p>Configure VM memory for optimal performance:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set memory limit (adjust based on your system)</span>
incus config <span class="nb">set </span>YOUR_VM_NAME limits.memory 60GiB

<span class="c"># Enforce hard memory limits</span>
incus config <span class="nb">set </span>YOUR_VM_NAME limits.memory.enforce hard

<span class="c"># Enable hugepages (after configuring on host)</span>
incus config <span class="nb">set </span>YOUR_VM_NAME limits.memory.hugepages <span class="nb">true</span>

<span class="c"># For NUMA systems, pin memory to same node as CPUs</span>
incus config <span class="nb">set </span>YOUR_VM_NAME limits.memory.nodes 0
</code></pre></div></div>

<p>Verify memory configuration:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>incus config show YOUR_VM_NAME | <span class="nb">grep</span> <span class="nt">-i</span> memory
</code></pre></div></div>

<h3 id="4-storage-configuration">4. Storage Configuration</h3>

<h4 id="41-direct-device-passthrough-best-performance">4.1 Direct Device Passthrough (Best Performance)</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Identify available block devices</span>
lsblk

<span class="c"># Pass through entire disk or partition</span>
incus config device add YOUR_VM_NAME root disk <span class="nb">source</span><span class="o">=</span>/dev/sdb <span class="nv">path</span><span class="o">=</span>/
</code></pre></div></div>

<h4 id="42-storage-pool-configuration">4.2 Storage Pool Configuration</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a storage pool (if not already created)</span>
incus storage create vm-storage zfs <span class="nb">source</span><span class="o">=</span>/dev/sdc

<span class="c"># Create a volume</span>
incus storage volume create vm-storage vm-disk <span class="nv">size</span><span class="o">=</span>100GiB

<span class="c"># Add to VM</span>
incus config device add YOUR_VM_NAME root disk <span class="nv">pool</span><span class="o">=</span>vm-storage <span class="nb">source</span><span class="o">=</span>vm-disk <span class="nv">path</span><span class="o">=</span>/

<span class="c"># Optimize disk settings</span>
incus config device <span class="nb">set </span>YOUR_VM_NAME root limits.read 0
incus config device <span class="nb">set </span>YOUR_VM_NAME root limits.write 0
incus config device <span class="nb">set </span>YOUR_VM_NAME root limits.disk.priority 10
</code></pre></div></div>

<p>For advanced disk performance:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Disable cache for direct I/O</span>
incus config device <span class="nb">set </span>YOUR_VM_NAME root cache none

<span class="c"># Set appropriate I/O mode</span>
incus config device <span class="nb">set </span>YOUR_VM_NAME root io native
</code></pre></div></div>

<h3 id="5-network-configuration">5. Network Configuration</h3>

<h4 id="51-pci-passthrough-best-performance">5.1 PCI Passthrough (Best Performance)</h4>

<p>First, identify the network device:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List PCI devices</span>
lspci <span class="nt">-nnk</span> | <span class="nb">grep</span> <span class="nt">-i</span> ethernet

<span class="c"># Identify the device name</span>
ip <span class="nb">link </span>show
</code></pre></div></div>

<p>Configure PCI passthrough:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add the device to the VM</span>
incus config device add YOUR_VM_NAME eth0 nic <span class="nv">nictype</span><span class="o">=</span>physical <span class="nv">parent</span><span class="o">=</span>enp3s0

<span class="c"># If using SR-IOV (for supported NICs)</span>
<span class="c"># First create virtual functions on host</span>
<span class="nb">echo </span>4 | <span class="nb">sudo tee</span> /sys/class/net/enp3s0/device/sriov_numvfs

<span class="c"># Then assign a VF to the VM</span>
incus config device add YOUR_VM_NAME eth0 nic <span class="nv">nictype</span><span class="o">=</span>sriov <span class="nv">parent</span><span class="o">=</span>enp3s0
</code></pre></div></div>

<h4 id="52-macvlan-configuration-good-performance">5.2 Macvlan Configuration (Good Performance)</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add macvlan interface</span>
incus config device add YOUR_VM_NAME eth0 nic <span class="nv">nictype</span><span class="o">=</span>macvlan <span class="nv">parent</span><span class="o">=</span>enp3s0

<span class="c"># Enable multi-queue for better performance</span>
incus config device <span class="nb">set </span>YOUR_VM_NAME eth0 queues 8
</code></pre></div></div>

<p>Verify network configuration:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>incus config show YOUR_VM_NAME | <span class="nb">grep</span> <span class="nt">-A</span> 10 <span class="s2">"devices:"</span>
</code></pre></div></div>

<h3 id="6-gpu-passthrough-for-graphics-workloads">6. GPU Passthrough (For Graphics Workloads)</h3>

<p>If you need GPU acceleration in your VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Identify GPU</span>
lspci <span class="nt">-nnk</span> | <span class="nb">grep</span> <span class="nt">-i</span> vga

<span class="c"># Ensure IOMMU is enabled and GPU is in its own IOMMU group</span>
find /sys/kernel/iommu_groups/ <span class="nt">-type</span> l | <span class="nb">sort</span> <span class="nt">-n</span>

<span class="c"># Add GPU to VM (example for NVIDIA GPU at 01:00.0)</span>
incus config device add YOUR_VM_NAME gpu gpu <span class="nv">pci</span><span class="o">=</span>0000:01:00.0
</code></pre></div></div>

<p>For NVIDIA GPUs, you may need to hide the virtualization:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Hide KVM from NVIDIA driver</span>
incus config <span class="nb">set </span>YOUR_VM_NAME raw.qemu <span class="s1">'-cpu host,kvm=off'</span>
</code></pre></div></div>

<h3 id="7-guest-os-optimization">7. Guest OS Optimization</h3>

<p>Start the VM and perform guest-side optimizations:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Start the VM</span>
incus start YOUR_VM_NAME

<span class="c"># Access VM console</span>
incus console YOUR_VM_NAME
</code></pre></div></div>

<p>Inside the VM, install and configure:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install QEMU guest agent</span>
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> qemu-guest-agent
<span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> qemu-guest-agent

<span class="c"># Install virtio drivers (usually pre-installed on Linux guests)</span>
lsmod | <span class="nb">grep </span>virtio  <span class="c"># Should show multiple virtio modules</span>

<span class="c"># Optimize I/O scheduler inside guest</span>
<span class="nb">echo </span>none | <span class="nb">sudo tee</span> /sys/block/vda/queue/scheduler

<span class="c"># Set CPU governor to performance</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> cpufrequtils
<span class="nb">echo</span> <span class="s1">'GOVERNOR="performance"'</span> | <span class="nb">sudo tee</span> /etc/default/cpufrequtils
<span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> cpufrequtils
</code></pre></div></div>

<h2 id="iv-verification-and-benchmarking">IV. Verification and Benchmarking</h2>

<h3 id="1-host-verification">1. Host Verification</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Verify kernel parameters</span>
<span class="nb">cat</span> /proc/cmdline

<span class="c"># Check CPU isolation</span>
ps <span class="nt">-eo</span> psr,pid,comm | <span class="nb">sort</span> <span class="nt">-n</span> | <span class="nb">head</span> <span class="nt">-20</span>  <span class="c"># Should show most processes on cores 0-1</span>

<span class="c"># Verify hugepages allocation</span>
<span class="nb">cat</span> /proc/meminfo | <span class="nb">grep</span> <span class="nt">-i</span> huge

<span class="c"># Check CPU governor</span>
<span class="nb">cat</span> /sys/devices/system/cpu/cpu<span class="k">*</span>/cpufreq/scaling_governor

<span class="c"># Verify QEMU process CPU affinity</span>
ps aux | <span class="nb">grep </span>qemu
taskset <span class="nt">-cp</span> <span class="si">$(</span>pgrep <span class="nt">-f</span> <span class="s2">"qemu.*YOUR_VM_NAME"</span><span class="si">)</span>  <span class="c"># Should show it's running on isolated cores</span>
</code></pre></div></div>

<h3 id="2-vm-verification">2. VM Verification</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check VM status and resource allocation</span>
incus info YOUR_VM_NAME

<span class="c"># Monitor VM resource usage</span>
incus top YOUR_VM_NAME
</code></pre></div></div>

<p>Inside the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check CPU information</span>
lscpu

<span class="c"># Verify memory allocation</span>
free <span class="nt">-h</span>

<span class="c"># Check for virtualization-specific hardware</span>
lspci

<span class="c"># Examine kernel messages for hardware detection</span>
dmesg | <span class="nb">grep</span> <span class="nt">-i</span> virt
</code></pre></div></div>

<h3 id="3-performance-benchmarking">3. Performance Benchmarking</h3>

<h4 id="31-cpu-benchmarks">3.1 CPU Benchmarks</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install benchmarking tools</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> sysbench

<span class="c"># CPU benchmark</span>
sysbench cpu <span class="nt">--cpu-max-prime</span><span class="o">=</span>20000 <span class="nt">--threads</span><span class="o">=</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> run

<span class="c"># Compare with host performance (run same test on host)</span>
</code></pre></div></div>

<h4 id="32-memory-benchmarks">3.2 Memory Benchmarks</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Memory benchmark</span>
sysbench memory <span class="nt">--memory-block-size</span><span class="o">=</span>1K <span class="nt">--memory-total-size</span><span class="o">=</span>100G <span class="nt">--threads</span><span class="o">=</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> run
</code></pre></div></div>

<h4 id="33-disk-io-benchmarks">3.3 Disk I/O Benchmarks</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install fio</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> fio

<span class="c"># Random read/write test</span>
fio <span class="nt">--name</span><span class="o">=</span>random-rw <span class="nt">--ioengine</span><span class="o">=</span>libaio <span class="nt">--direct</span><span class="o">=</span>1 <span class="nt">--bs</span><span class="o">=</span>4k <span class="nt">--iodepth</span><span class="o">=</span>64 <span class="nt">--size</span><span class="o">=</span>4G <span class="nt">--numjobs</span><span class="o">=</span>4 <span class="nt">--rw</span><span class="o">=</span>randrw <span class="nt">--group_reporting</span>

<span class="c"># Sequential read test</span>
fio <span class="nt">--name</span><span class="o">=</span>seq-read <span class="nt">--ioengine</span><span class="o">=</span>libaio <span class="nt">--direct</span><span class="o">=</span>1 <span class="nt">--bs</span><span class="o">=</span>1M <span class="nt">--iodepth</span><span class="o">=</span>16 <span class="nt">--size</span><span class="o">=</span>4G <span class="nt">--numjobs</span><span class="o">=</span>1 <span class="nt">--rw</span><span class="o">=</span><span class="nb">read</span> <span class="nt">--group_reporting</span>
</code></pre></div></div>

<h4 id="34-network-benchmarks">3.4 Network Benchmarks</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install iperf3</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> iperf3

<span class="c"># Run iperf3 server on one machine</span>
iperf3 <span class="nt">-s</span>

<span class="c"># Run client on the other machine</span>
iperf3 <span class="nt">-c</span> SERVER_IP <span class="nt">-P</span> 4 <span class="nt">-t</span> 30
</code></pre></div></div>

<h2 id="v-monitoring-and-maintenance">V. Monitoring and Maintenance</h2>

<h3 id="1-ongoing-performance-monitoring">1. Ongoing Performance Monitoring</h3>

<p>Set up basic monitoring:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install monitoring tools</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> sysstat htop iotop

<span class="c"># Configure sysstat collection</span>
<span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> sysstat
</code></pre></div></div>

<p>For advanced monitoring, consider:</p>
<ul>
  <li>Prometheus + Grafana</li>
  <li>Netdata</li>
  <li>Telegraf + InfluxDB + Grafana</li>
</ul>

<h3 id="2-update-management">2. Update Management</h3>

<p>Create a maintenance plan:</p>

<ol>
  <li>
    <p><strong>Kernel Updates</strong>: Test in a non-production environment first, as they may affect:
   - CPU isolation settings
   - IOMMU functionality
   - PCI passthrough behavior</p>
  </li>
  <li>
    <p><strong>Incus Updates</strong>: Check release notes for changes that might affect VM configuration</p>
  </li>
  <li>
    <p><strong>Firmware Updates</strong>: Periodically check for and apply firmware/microcode updates</p>
  </li>
</ol>

<h3 id="3-backup-strategy">3. Backup Strategy</h3>

<p>Implement a VM backup strategy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a snapshot</span>
incus snapshot YOUR_VM_NAME snapshot1

<span class="c"># Export the snapshot</span>
incus <span class="nb">export </span>YOUR_VM_NAME/snapshot1 /path/to/backup/location/vm-backup.tar.gz
</code></pre></div></div>

<p>Consider automating backups with a script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c"># VM backup script</span>
<span class="nv">DATE</span><span class="o">=</span><span class="si">$(</span><span class="nb">date</span> +%Y%m%d<span class="si">)</span>
<span class="nv">SNAPSHOT_NAME</span><span class="o">=</span><span class="s2">"backup-</span><span class="nv">$DATE</span><span class="s2">"</span>
<span class="nv">VM_NAME</span><span class="o">=</span><span class="s2">"YOUR_VM_NAME"</span>
<span class="nv">BACKUP_DIR</span><span class="o">=</span><span class="s2">"/path/to/backups"</span>

<span class="c"># Create snapshot</span>
incus snapshot <span class="nv">$VM_NAME</span> <span class="nv">$SNAPSHOT_NAME</span>

<span class="c"># Export snapshot</span>
incus <span class="nb">export</span> <span class="nv">$VM_NAME</span>/<span class="nv">$SNAPSHOT_NAME</span> <span class="nv">$BACKUP_DIR</span>/<span class="nv">$VM_NAME</span>-<span class="nv">$DATE</span>.tar.gz

<span class="c"># Clean up old snapshots (keep last 5)</span>
incus info <span class="nv">$VM_NAME</span> | <span class="nb">grep </span>snapshot | <span class="nb">sort</span> | <span class="nb">head</span> <span class="nt">-n</span> <span class="nt">-5</span> | <span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> line<span class="p">;</span> <span class="k">do</span>
  <span class="nv">SNAP</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="nv">$line</span> | <span class="nb">awk</span> <span class="s1">'{print $1}'</span><span class="si">)</span>
  incus delete <span class="nv">$VM_NAME</span>/<span class="nv">$SNAP</span>
<span class="k">done</span>
</code></pre></div></div>

<h2 id="vi-troubleshooting-common-issues">VI. Troubleshooting Common Issues</h2>

<h3 id="1-vm-performance-issues">1. VM Performance Issues</h3>

<p>If VM performance is not as expected:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check if CPU isolation is working</span>
ps <span class="nt">-eo</span> psr,pid,comm | <span class="nb">grep</span> <span class="nt">-v</span> <span class="s2">"^[0-1]"</span>  <span class="c"># Should show minimal processes on isolated cores</span>

<span class="c"># Check if VM processes are on isolated cores</span>
ps <span class="nt">-eo</span> psr,pid,comm | <span class="nb">grep </span>qemu  <span class="c"># Should show on isolated cores</span>

<span class="c"># Check for CPU throttling</span>
<span class="nb">grep</span> <span class="nt">-i</span> throttling /var/log/syslog

<span class="c"># Monitor for resource contention</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> perf
<span class="nb">sudo </span>perf top  <span class="c"># Look for unexpected system activity</span>
</code></pre></div></div>

<h3 id="2-pci-passthrough-issues">2. PCI Passthrough Issues</h3>

<p>If PCI passthrough doesn’t work:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Verify IOMMU is enabled</span>
dmesg | <span class="nb">grep</span> <span class="nt">-e</span> DMAR <span class="nt">-e</span> IOMMU

<span class="c"># Check IOMMU groups</span>
find /sys/kernel/iommu_groups/ <span class="nt">-type</span> l | <span class="nb">sort</span> <span class="nt">-n</span>

<span class="c"># Ensure device is properly bound to vfio-pci</span>
lspci <span class="nt">-nnk</span> | <span class="nb">grep</span> <span class="nt">-A3</span> <span class="s2">"PCI_ID_OF_DEVICE"</span>
</code></pre></div></div>

<h3 id="3-memory-allocation-issues">3. Memory Allocation Issues</h3>

<p>If hugepages allocation fails:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check memory fragmentation</span>
<span class="nb">cat</span> /proc/buddyinfo

<span class="c"># Try allocating hugepages early in boot process</span>
<span class="c"># Add to GRUB_CMDLINE_LINUX_DEFAULT: hugepagesz=2M hugepages=30976</span>

<span class="c"># Or try 1GB hugepages if supported</span>
<span class="c"># Add to GRUB_CMDLINE_LINUX_DEFAULT: hugepagesz=1G hugepages=62</span>
</code></pre></div></div>

<h2 id="vii-advanced-optimizations">VII. Advanced Optimizations</h2>

<h3 id="1-cpu-pinning-fine-tuning">1. CPU Pinning Fine-Tuning</h3>

<p>For multi-socket NUMA systems, further optimize CPU pinning:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Identify NUMA topology</span>
numactl <span class="nt">--hardware</span>

<span class="c"># Pin emulator threads to host CPUs</span>
incus config <span class="nb">set </span>YOUR_VM_NAME raw.qemu <span class="s1">'-object iothread,id=iothread0 -object iothread,id=iothread1'</span>

<span class="c"># For VMs with many vCPUs, consider topology awareness</span>
incus config <span class="nb">set </span>YOUR_VM_NAME raw.qemu <span class="s1">'-smp 6,sockets=1,cores=6,threads=1'</span>
</code></pre></div></div>

<h3 id="2-storage-io-fine-tuning">2. Storage I/O Fine-Tuning</h3>

<p>For storage-intensive workloads:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add dedicated I/O threads</span>
incus config device <span class="nb">set </span>YOUR_VM_NAME root io.threads 4

<span class="c"># For ZFS storage pools, optimize ARC cache</span>
<span class="nb">echo</span> <span class="s2">"options zfs zfs_arc_max=4294967296"</span> | <span class="nb">sudo tee</span> /etc/modprobe.d/zfs.conf  <span class="c"># Limit to 4GB</span>
<span class="nb">sudo </span>systemctl restart zfs-import-cache

<span class="c"># For multi-queue block device support</span>
incus config device <span class="nb">set </span>YOUR_VM_NAME root queues 4
</code></pre></div></div>

<h3 id="3-real-time-kernel-for-latency-sensitive-workloads">3. Real-Time Kernel (For Latency-Sensitive Workloads)</h3>

<p>For workloads requiring minimal latency:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install real-time kernel</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> linux-image-rt-amd64

<span class="c"># Update GRUB to boot with RT kernel</span>
<span class="nb">sudo </span>update-grub

<span class="c"># After reboot, verify RT kernel</span>
<span class="nb">uname</span> <span class="nt">-a</span>  <span class="c"># Should show "PREEMPT_RT"</span>

<span class="c"># Set RT priorities for VM</span>
<span class="nb">sudo </span>chrt <span class="nt">-f</span> <span class="nt">-p</span> 80 <span class="si">$(</span>pgrep <span class="nt">-f</span> <span class="s2">"qemu.*YOUR_VM_NAME"</span><span class="si">)</span>
</code></pre></div></div>

<h3 id="4-power-management-optimization">4. Power Management Optimization</h3>

<p>Balance performance and power consumption:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install power management tools</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> powertop

<span class="c"># Run powertop calibration</span>
<span class="nb">sudo </span>powertop <span class="nt">--calibrate</span>

<span class="c"># Generate optimized settings</span>
<span class="nb">sudo </span>powertop <span class="nt">--auto-tune</span>

<span class="c"># Apply only to non-isolated cores (advanced)</span>
<span class="k">for </span>cpu <span class="k">in </span>0 1<span class="p">;</span> <span class="k">do</span>
  <span class="nb">echo </span>performance | <span class="nb">sudo tee</span> /sys/devices/system/cpu/cpu<span class="nv">$cpu</span>/cpufreq/scaling_governor
<span class="k">done</span>
</code></pre></div></div>

<h2 id="viii-specialized-configurations">VIII. Specialized Configurations</h2>

<h3 id="1-high-performance-computing-hpc-vm">1. High-Performance Computing (HPC) VM</h3>

<p>For scientific computing workloads:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Enable CPU features like AVX, AVX2, AVX512</span>
incus config <span class="nb">set </span>YOUR_VM_NAME raw.qemu <span class="s1">'-cpu host,+avx,+avx2,+avx512f,+avx512dq,+avx512bw,+avx512vl'</span>

<span class="c"># Optimize memory access</span>
<span class="nb">echo </span>0 | <span class="nb">sudo tee</span> /proc/sys/kernel/numa_balancing  <span class="c"># Disable automatic NUMA balancing</span>
</code></pre></div></div>

<h3 id="2-gaming-vm-configuration">2. Gaming VM Configuration</h3>

<p>For gaming workloads:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Enable CPU features for gaming</span>
incus config <span class="nb">set </span>YOUR_VM_NAME raw.qemu <span class="s1">'-cpu host,kvm=off,hv_vendor_id=1234567890ab,hv_relaxed,hv_spinlocks=0x1fff,hv_vapic,hv_time'</span>

<span class="c"># Add gaming-optimized kernel parameters to host</span>
<span class="c"># Add to GRUB_CMDLINE_LINUX_DEFAULT: clocksource=tsc tsc=reliable</span>

<span class="c"># Inside VM, optimize for gaming</span>
<span class="nb">echo </span>1 | <span class="nb">sudo tee</span> /proc/sys/kernel/sched_child_runs_first
</code></pre></div></div>

<h3 id="3-database-vm-configuration">3. Database VM Configuration</h3>

<p>For database workloads:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Optimize I/O scheduler for database workloads</span>
<span class="nb">echo </span>kyber | <span class="nb">sudo tee</span> /sys/block/nvme0n1/queue/scheduler

<span class="c"># Set I/O priorities</span>
ionice <span class="nt">-c</span> 1 <span class="nt">-n</span> 0 <span class="nt">-p</span> <span class="si">$(</span>pgrep <span class="nt">-f</span> <span class="s2">"qemu.*YOUR_VM_NAME"</span><span class="si">)</span>

<span class="c"># Inside VM, optimize memory management</span>
<span class="nb">echo</span> <span class="s1">'vm.dirty_ratio = 10'</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/sysctl.conf
<span class="nb">echo</span> <span class="s1">'vm.dirty_background_ratio = 5'</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/sysctl.conf
<span class="nb">sudo </span>sysctl <span class="nt">-p</span>
</code></pre></div></div>

<h2 id="ix-security-considerations">IX. Security Considerations</h2>

<h3 id="1-mitigating-security-risks">1. Mitigating Security Risks</h3>

<p>When using performance-enhancing but security-reducing options:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># If using mitigations=off, implement network isolation</span>
<span class="nb">sudo </span>ufw <span class="nb">enable
sudo </span>ufw default deny incoming
<span class="nb">sudo </span>ufw allow ssh
<span class="nb">sudo </span>ufw allow from TRUSTED_IP_ADDRESS

<span class="c"># Consider running VM in isolated VLAN</span>
incus network create isolated-net <span class="nt">--type</span><span class="o">=</span>bridge
incus network attach isolated-net YOUR_VM_NAME eth0
</code></pre></div></div>

<h3 id="2-secure-pci-passthrough">2. Secure PCI Passthrough</h3>

<p>When passing through PCI devices:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure device firmware is up-to-date</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> pciutils
<span class="nb">sudo </span>update-pciids
lspci <span class="nt">-vvv</span> | <span class="nb">grep</span> <span class="nt">-i</span> <span class="s2">"firmware"</span>

<span class="c"># For GPU passthrough, consider firmware security</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> vgabios
</code></pre></div></div>

<h3 id="3-vm-isolation">3. VM Isolation</h3>

<p>Enhance VM isolation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Disable unnecessary VM features</span>
incus config <span class="nb">set </span>YOUR_VM_NAME security.guest.features.secureboot <span class="nb">false
</span>incus config <span class="nb">set </span>YOUR_VM_NAME security.guest.features.smm <span class="nb">false</span>

<span class="c"># Enable nested virtualization only if needed</span>
incus config <span class="nb">set </span>YOUR_VM_NAME security.guest.features.nested <span class="nb">false</span>
</code></pre></div></div>

<h2 id="x-migration-and-backup-strategies">X. Migration and Backup Strategies</h2>

<h3 id="1-vm-migration-preparation">1. VM Migration Preparation</h3>

<p>Prepare your VM for potential migration:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use consistent device naming</span>
incus config device <span class="nb">set </span>YOUR_VM_NAME root <span class="nv">path</span><span class="o">=</span>/dev/vda

<span class="c"># Document all passthrough devices</span>
incus config show YOUR_VM_NAME <span class="o">&gt;</span> vm_config_backup.txt

<span class="c"># Create a migration-ready snapshot</span>
incus snapshot YOUR_VM_NAME migration-ready
</code></pre></div></div>

<h3 id="2-comprehensive-backup-strategy">2. Comprehensive Backup Strategy</h3>

<p>Implement a comprehensive backup strategy:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create backup script with verification</span>
<span class="nb">cat</span> <span class="o">&gt;</span> /usr/local/bin/vm-backup.sh <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
#!/bin/bash
set -e

VM_NAME="YOUR_VM_NAME"
BACKUP_DIR="/path/to/backups"
DATE=</span><span class="si">$(</span><span class="nb">date</span> +%Y%m%d-%H%M%S<span class="si">)</span><span class="sh">
SNAPSHOT_NAME="backup-</span><span class="nv">$DATE</span><span class="sh">"
BACKUP_FILE="</span><span class="nv">$BACKUP_DIR</span><span class="sh">/</span><span class="nv">$VM_NAME</span><span class="sh">-</span><span class="nv">$DATE</span><span class="sh">.tar.gz"
LOG_FILE="</span><span class="nv">$BACKUP_DIR</span><span class="sh">/backup-</span><span class="nv">$DATE</span><span class="sh">.log"

echo "Starting backup of </span><span class="nv">$VM_NAME</span><span class="sh"> at </span><span class="si">$(</span><span class="nb">date</span><span class="si">)</span><span class="sh">" | tee -a "</span><span class="nv">$LOG_FILE</span><span class="sh">"

# Create snapshot
incus snapshot "</span><span class="nv">$VM_NAME</span><span class="sh">" "</span><span class="nv">$SNAPSHOT_NAME</span><span class="sh">" 2&gt;&amp;1 | tee -a "</span><span class="nv">$LOG_FILE</span><span class="sh">"

# Export snapshot
incus export "</span><span class="nv">$VM_NAME</span><span class="sh">/</span><span class="nv">$SNAPSHOT_NAME</span><span class="sh">" "</span><span class="nv">$BACKUP_FILE</span><span class="sh">" 2&gt;&amp;1 | tee -a "</span><span class="nv">$LOG_FILE</span><span class="sh">"

# Verify backup integrity
tar -tzf "</span><span class="nv">$BACKUP_FILE</span><span class="sh">" &gt; /dev/null 2&gt;&amp;1
if [ </span><span class="nv">$?</span><span class="sh"> -eq 0 ]; then
  echo "Backup verified successfully" | tee -a "</span><span class="nv">$LOG_FILE</span><span class="sh">"
else
  echo "ERROR: Backup verification failed!" | tee -a "</span><span class="nv">$LOG_FILE</span><span class="sh">"
  exit 1
fi

# Clean up old snapshots (keep last 5)
for SNAP in </span><span class="si">$(</span>incus info <span class="s2">"</span><span class="nv">$VM_NAME</span><span class="s2">"</span> | <span class="nb">grep</span> <span class="s2">"backup-"</span> | <span class="nb">sort</span> | <span class="nb">head</span> <span class="nt">-n</span> <span class="nt">-5</span> | <span class="nb">awk</span> <span class="s1">'{print $1}'</span><span class="si">)</span><span class="sh">; do
  echo "Removing old snapshot: </span><span class="nv">$SNAP</span><span class="sh">" | tee -a "</span><span class="nv">$LOG_FILE</span><span class="sh">"
  incus delete "</span><span class="nv">$VM_NAME</span><span class="sh">/</span><span class="nv">$SNAP</span><span class="sh">" 2&gt;&amp;1 | tee -a "</span><span class="nv">$LOG_FILE</span><span class="sh">"
done

echo "Backup completed successfully at </span><span class="si">$(</span><span class="nb">date</span><span class="si">)</span><span class="sh">" | tee -a "</span><span class="nv">$LOG_FILE</span><span class="sh">"
</span><span class="no">EOF

</span><span class="nb">chmod</span> +x /usr/local/bin/vm-backup.sh
</code></pre></div></div>

<p>Set up automated backups:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create systemd timer for regular backups</span>
<span class="nb">cat</span> <span class="o">&gt;</span> /etc/systemd/system/vm-backup.service <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
[Unit]
Description=Backup Incus VM
After=network.target

[Service]
Type=oneshot
ExecStart=/usr/local/bin/vm-backup.sh
User=root

[Install]
WantedBy=multi-user.target
</span><span class="no">EOF

</span><span class="nb">cat</span> <span class="o">&gt;</span> /etc/systemd/system/vm-backup.timer <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
[Unit]
Description=Run VM backup daily

[Timer]
OnCalendar=*-*-* 02:00:00
Persistent=true

[Install]
WantedBy=timers.target
</span><span class="no">EOF

</span><span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> vm-backup.timer
</code></pre></div></div>

<h2 id="xi-long-term-maintenance-and-evolution">XI. Long-term Maintenance and Evolution</h2>

<h3 id="1-performance-monitoring-dashboard">1. Performance Monitoring Dashboard</h3>

<p>Set up a comprehensive monitoring solution:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install Prometheus and node_exporter</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> prometheus prometheus-node-exporter

<span class="c"># Install Grafana</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> apt-transport-https software-properties-common
wget <span class="nt">-q</span> <span class="nt">-O</span> - https://packages.grafana.com/gpg.key | <span class="nb">sudo </span>apt-key add -
<span class="nb">echo</span> <span class="s2">"deb https://packages.grafana.com/oss/deb stable main"</span> | <span class="nb">sudo tee</span> /etc/apt/sources.list.d/grafana.list
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> grafana

<span class="c"># Enable and start services</span>
<span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> prometheus prometheus-node-exporter grafana-server

<span class="c"># Access Grafana at http://your-server-ip:3000 (default: admin/admin)</span>
<span class="c"># Add Prometheus as a data source and import VM monitoring dashboards</span>
</code></pre></div></div>

<h3 id="2-update-management-strategy">2. Update Management Strategy</h3>

<p>Create a systematic update process:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create update testing script</span>
<span class="nb">cat</span> <span class="o">&gt;</span> /usr/local/bin/test-updates.sh <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
#!/bin/bash
set -e

VM_NAME="YOUR_VM_NAME"
SNAPSHOT_NAME="pre-update-</span><span class="si">$(</span><span class="nb">date</span> +%Y%m%d<span class="si">)</span><span class="sh">"

# Create pre-update snapshot
incus snapshot "</span><span class="nv">$VM_NAME</span><span class="sh">" "</span><span class="nv">$SNAPSHOT_NAME</span><span class="sh">"

# Apply updates
sudo apt update
sudo apt upgrade -y

# Run performance tests
# Add your benchmarking commands here

# If tests fail, restore snapshot
# incus restore "</span><span class="nv">$VM_NAME</span><span class="sh">" "</span><span class="nv">$SNAPSHOT_NAME</span><span class="sh">"
</span><span class="no">EOF

</span><span class="nb">chmod</span> +x /usr/local/bin/test-updates.sh
</code></pre></div></div>

<h3 id="3-documentation-and-knowledge-base">3. Documentation and Knowledge Base</h3>

<p>Maintain comprehensive documentation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create documentation directory</span>
<span class="nb">mkdir</span> <span class="nt">-p</span> ~/vm-documentation

<span class="c"># Document current configuration</span>
incus config show YOUR_VM_NAME <span class="o">&gt;</span> ~/vm-documentation/vm-config.txt
<span class="nb">cat</span> /proc/cmdline <span class="o">&gt;</span> ~/vm-documentation/kernel-parameters.txt
lspci <span class="nt">-vvv</span> <span class="o">&gt;</span> ~/vm-documentation/pci-devices.txt
<span class="nb">cat</span> /etc/default/grub <span class="o">&gt;</span> ~/vm-documentation/grub-config.txt

<span class="c"># Document performance baseline</span>
sysbench cpu <span class="nt">--cpu-max-prime</span><span class="o">=</span>20000 <span class="nt">--threads</span><span class="o">=</span><span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> run <span class="o">&gt;</span> ~/vm-documentation/cpu-baseline.txt
fio <span class="nt">--name</span><span class="o">=</span>random-rw <span class="nt">--ioengine</span><span class="o">=</span>libaio <span class="nt">--direct</span><span class="o">=</span>1 <span class="nt">--bs</span><span class="o">=</span>4k <span class="nt">--iodepth</span><span class="o">=</span>64 <span class="nt">--size</span><span class="o">=</span>4G <span class="nt">--numjobs</span><span class="o">=</span>4 <span class="nt">--rw</span><span class="o">=</span>randrw <span class="nt">--group_reporting</span> <span class="o">&gt;</span> ~/vm-documentation/disk-baseline.txt
</code></pre></div></div>

<h2 id="xii-conclusion-and-final-checklist">XII. Conclusion and Final Checklist</h2>

<h3 id="1-performance-optimization-checklist">1. Performance Optimization Checklist</h3>

<p>Before considering your optimization complete, verify:</p>

<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />CPU isolation is working (processes stay on designated cores)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />VM is using hugepages (check with <code class="language-plaintext highlighter-rouge">incus info</code>)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Storage performance meets expectations (verify with <code class="language-plaintext highlighter-rouge">fio</code>)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Network performance meets expectations (verify with <code class="language-plaintext highlighter-rouge">iperf3</code>)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />VM startup and operation is stable</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Backup and recovery procedures are tested</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Monitoring is in place</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Documentation is complete</li>
</ul>

<h3 id="2-final-thoughts">2. Final Thoughts</h3>

<p>Remember that performance optimization is an ongoing process:</p>

<ol>
  <li><strong>Measure First</strong>: Always benchmark before and after changes</li>
  <li><strong>Incremental Changes</strong>: Make one change at a time and test</li>
  <li><strong>Document Everything</strong>: Keep detailed records of all configurations</li>
  <li><strong>Security Balance</strong>: Understand the security implications of performance optimizations</li>
  <li><strong>Workload Adaptation</strong>: Be prepared to adjust optimizations as workloads change</li>
</ol>

<p>By systematically applying these optimizations to both the Ubuntu Server host and Incus VM configuration, you can achieve virtual machine performance very close to bare-metal, with minimal overhead from the virtualization layer.</p>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">Maximizing VM Performance with Incus v2</title><link href="https://ib.bsb.br/maximizing-vm-performance-with-incus-v2/" rel="alternate" type="text/html" title="Maximizing VM Performance with Incus v2" /><published>2025-05-19T00:00:00+00:00</published><updated>2025-05-19T09:10:31+00:00</updated><id>https://ib.bsb.br/maximizing-vm-performance-with-incus-v2</id><content type="html" xml:base="https://ib.bsb.br/maximizing-vm-performance-with-incus-v2/"><![CDATA[<p>Incus for Maximum Single VM Performance</p>

<p><strong>IMPORTANT NOTE:</strong> This guide provides a comprehensive approach to dedicating maximum resources to a single Incus virtual machine. Many values (e.g., <code class="language-plaintext highlighter-rouge">performant-vm</code>, CPU core lists like <code class="language-plaintext highlighter-rouge">1-7</code>, memory allocations like <code class="language-plaintext highlighter-rouge">60GiB</code>, disk paths like <code class="language-plaintext highlighter-rouge">/dev/sdb1</code>, network interfaces like <code class="language-plaintext highlighter-rouge">enp3s0</code>) are <strong>examples</strong>. You <strong>MUST</strong> adapt these to your specific hardware, total resources, and virtual machine requirements. Incorrectly applying these settings, especially kernel parameters or PCI passthrough, can lead to system instability or data loss. Proceed with caution and ensure you understand each step.</p>

<p><strong>I. Ubuntu Server Host Optimizations (The “Lean Hypervisor” Layer)</strong></p>

<p>The goal is to minimize the host OS’s footprint and tune it for optimal resource allocation to the VM.</p>

<p>1.  <strong>Minimal Installation:</strong>
    *   Start with the most minimal Ubuntu Server 25 installation. Avoid desktop environments, graphical tools, or any server roles not strictly required for Incus or essential system management.
    *   During installation, if prompted, deselect optional software bundles. Consider a “netboot” or “minimal ISO” if available.</p>

<p>2.  <strong>Disable Unnecessary Services:</strong>
    *   Audit and disable services that don’t contribute to running Incus or the VM.
        <code class="language-plaintext highlighter-rouge">bash
        sudo systemctl list-unit-files --type=service --state=enabled
        # Example: Disable apport, snapd (if not used for Incus), motd-news
        sudo systemctl stop apport snapd motd-news.timer
        sudo systemctl disable apport snapd motd-news.timer
       </code>
    *   Be cautious and ensure you understand a service’s role before disabling it.</p>

<p>3.  <strong>Kernel Boot Parameters (GRUB Configuration):</strong>
    <em>   Modify <code class="language-plaintext highlighter-rouge">/etc/default/grub</code> and add parameters to <code class="language-plaintext highlighter-rouge">GRUB_CMDLINE_LINUX_DEFAULT</code>. These significantly impact performance and resource isolation.
        *   <strong>Example for an 8-core CPU (0-7), reserving cores 1-7 for the VM, core 0 for the host:</strong>
            <code class="language-plaintext highlighter-rouge">isolcpus=1-7 nohz_full=1-7 rcu_nocbs=1-7</code>
        *   <strong>IOMMU (Essential for PCI Passthrough):</strong>
            <code class="language-plaintext highlighter-rouge">intel_iommu=on</code> (for Intel) or <code class="language-plaintext highlighter-rouge">amd_iommu=on iommu=pt</code> (for AMD; <code class="language-plaintext highlighter-rouge">iommu=pt</code> can improve passthrough compatibility).
        *   <strong>CPU P-State Control (for finer frequency management):</strong>
            <code class="language-plaintext highlighter-rouge">intel_pstate=disable</code> (for Intel, to use <code class="language-plaintext highlighter-rouge">acpi-cpufreq</code>) or <code class="language-plaintext highlighter-rouge">amd_pstate=passive</code> (for AMD).
        *   <strong>HugePages (Pre-allocation at boot):</strong>
            <code class="language-plaintext highlighter-rouge">hugepagesz=1G hugepages=30</code> (Example for 30GB of 1GB pages) or <code class="language-plaintext highlighter-rouge">hugepagesz=2M hugepages=15360</code> (Example for 30GB of 2MB pages). Adjust <code class="language-plaintext highlighter-rouge">hugepages</code> count based on VM memory.
        *   <strong>Security vs. Performance (Use with Extreme Caution):</strong>
            <code class="language-plaintext highlighter-rouge">mitigations=off</code> <strong>SECURITY RISK!</strong> Disables CPU speculative execution mitigations (e.g., Spectre/Meltdown). This can significantly boost performance but makes your system vulnerable. *Only consider this if the machine is in a highly trusted, isolated environment and performance is the absolute priority over security.</em>
    *   <strong>Combined Example for <code class="language-plaintext highlighter-rouge">GRUB_CMDLINE_LINUX_DEFAULT</code>:</strong>
        <code class="language-plaintext highlighter-rouge">"quiet splash isolcpus=1-7 nohz_full=1-7 rcu_nocbs=1-7 intel_iommu=on intel_pstate=disable hugepagesz=1G hugepages=30"</code> (Adapt for your core count, CPU type, and HugePage needs).
    *   After editing <code class="language-plaintext highlighter-rouge">/etc/default/grub</code>:
        <code class="language-plaintext highlighter-rouge">bash
        sudo update-grub
        sudo reboot
       </code>
    *   <strong>NUMA Consideration:</strong> If your system has multiple NUMA nodes (check with <code class="language-plaintext highlighter-rouge">lscpu</code>), ensure the cores specified in <code class="language-plaintext highlighter-rouge">isolcpus</code> belong to the same NUMA node to which you’ll also pin the VM’s memory and, ideally, passthrough devices.</p>

<p>4.  <strong>CPU Governor and Frequency:</strong>
    *   Set the CPU frequency scaling governor to <code class="language-plaintext highlighter-rouge">performance</code> for all cores (as host activity on isolated cores will be minimal, and dedicated cores should run at max).
        <code class="language-plaintext highlighter-rouge">bash
        sudo apt update &amp;&amp; sudo apt install -y cpufrequtils
        echo 'GOVERNOR="performance"' | sudo tee /etc/default/cpufrequtils
        sudo systemctl disable ondemand # Or other conservative governors
        sudo systemctl enable --now cpufrequtils
        # Verify: cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
       </code></p>

<p>5.  <strong>Memory: Swappiness and HugePages (Runtime):</strong>
    *   <strong>Swappiness:</strong> Reduce the host’s tendency to swap.
        <code class="language-plaintext highlighter-rouge">bash
        sudo sysctl vm.swappiness=1
        # Make permanent: echo 'vm.swappiness=1' | sudo tee -a /etc/sysctl.conf
       </code>
    *   <strong>HugePages (Runtime allocation if not done via kernel params or for adjustment):</strong>
        1.  Determine hugepage size: <code class="language-plaintext highlighter-rouge">cat /proc/meminfo | grep Hugepagesize</code> (usually 2MB or 1GB)
        2.  Allocate (example for 15360 pages of 2MB size, totaling ~30GB):
            <code class="language-plaintext highlighter-rouge">bash
            echo 15360 | sudo tee /proc/sys/vm/nr_hugepages
            # Make permanent: echo 'vm.nr_hugepages=15360' | sudo tee -a /etc/sysctl.conf
            # Ensure hugetlbfs is mounted: sudo mount -t hugetlbfs none /dev/hugepages (if not already)
           </code></p>

<p>6.  <strong>Disk I/O Scheduler:</strong>
    *   For NVMe drives, <code class="language-plaintext highlighter-rouge">none</code> is often best. For SATA SSDs, <code class="language-plaintext highlighter-rouge">mq-deadline</code> or <code class="language-plaintext highlighter-rouge">kyber</code>.
        <code class="language-plaintext highlighter-rouge">bash
        # Check current scheduler for nvme0n1:
        cat /sys/block/nvme0n1/queue/scheduler
        # Set to none for nvme0n1:
        echo none | sudo tee /sys/block/nvme0n1/queue/scheduler
       </code>
    *   <strong>Make persistent via udev rules:</strong> Create <code class="language-plaintext highlighter-rouge">/etc/udev/rules.d/60-ioschedulers.rules</code>:
        <code class="language-plaintext highlighter-rouge">
        ACTION=="add|change", KERNEL=="nvme[0-9]*n[0-9]*", ATTR{queue/scheduler}="none"
        ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/scheduler}="mq-deadline"
       </code>
        Reload rules: <code class="language-plaintext highlighter-rouge">sudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger</code></p>

<p>7.  <strong>IRQ Affinity (Advanced):</strong>
    *   Pin interrupts from high-throughput devices (NICs, storage controllers for VM) to host-reserved CPU cores (e.g., core 0 if <code class="language-plaintext highlighter-rouge">isolcpus=1-7</code>).
        1.  Identify device IRQs: <code class="language-plaintext highlighter-rouge">cat /proc/interrupts</code>
        2.  Disable <code class="language-plaintext highlighter-rouge">irqbalance</code> service: <code class="language-plaintext highlighter-rouge">sudo systemctl disable --now irqbalance</code>
        3.  Manually set affinity (e.g., pin IRQ <code class="language-plaintext highlighter-rouge">&lt;irq_num&gt;</code> to CPU0, mask is <code class="language-plaintext highlighter-rouge">1</code>):
            <code class="language-plaintext highlighter-rouge">echo 1 | sudo tee /proc/irq/&lt;irq_num&gt;/smp_affinity</code> (Replace <code class="language-plaintext highlighter-rouge">&lt;irq_num&gt;</code>)</p>

<p>8.  <strong>BIOS/UEFI Settings:</strong>
    *   Enable virtualization technologies: Intel VT-x, AMD-V.
    *   Enable IOMMU: Intel VT-d, AMD IOMMU/Vi.
    *   Disable aggressive power-saving features (deep C-states beyond C1/C2, BIOS-controlled frequency scaling if it interferes with OS control). Set power profiles to “Maximum Performance” or “OS Controlled.”
    *   Disable unused integrated peripherals (serial ports, audio).
    *   For NUMA systems: Ensure settings promote locality (e.g., memory interleaving might be “NUMA-aware” or “channel specific”).</p>

<p>9.  <strong>Consider <code class="language-plaintext highlighter-rouge">tuned</code> Utility:</strong>
    *   <code class="language-plaintext highlighter-rouge">tuned</code> can apply system-wide tuning profiles.
        <code class="language-plaintext highlighter-rouge">bash
        sudo apt install -y tuned
        sudo tuned-adm profile virtual-host
       </code>
    *   Review the profile and customize further if needed. Manual settings above might override or complement <code class="language-plaintext highlighter-rouge">tuned</code>.</p>

<p><strong>II. Incus Virtual Machine Configuration (Maximizing Resource Allocation)</strong></p>

<p>Configure the Incus VM (<code class="language-plaintext highlighter-rouge">performant-vm</code> in examples) to claim and efficiently use resources.</p>

<p>1.  <strong>CPU Allocation, Pinning, and Model:</strong>
    *   <strong><code class="language-plaintext highlighter-rouge">limits.cpu</code></strong>: Assign specific isolated host physical cores (e.g., host cores 1-7).
        <code class="language-plaintext highlighter-rouge">bash
        incus config set performant-vm limits.cpu 1-7
       </code>
    *   <strong><code class="language-plaintext highlighter-rouge">limits.cpu.allowance</code></strong>: Ensure 100% usage of assigned cores.
        <code class="language-plaintext highlighter-rouge">bash
        incus config set performant-vm limits.cpu.allowance 100%
       </code>
    *   <strong><code class="language-plaintext highlighter-rouge">limits.cpu.priority</code></strong>: Higher priority for VM tasks.
        <code class="language-plaintext highlighter-rouge">bash
        incus config set performant-vm limits.cpu.priority 10
       </code>
    *   <strong>CPU Model Passthrough (Expose Host CPU Features):</strong>
        <code class="language-plaintext highlighter-rouge">bash
        incus config set performant-vm security.guest.features.cpu.host_passthrough=true
       </code>
    *   <strong>NUMA Pinning (If host cores 1-7 are on NUMA node 0):</strong>
        <code class="language-plaintext highlighter-rouge">bash
        incus config set performant-vm limits.cpu.nodes 0
       </code></p>

<p>2.  <strong>Memory Allocation:</strong>
    *   <strong><code class="language-plaintext highlighter-rouge">limits.memory</code></strong>: Allocate desired RAM (e.g., 60GiB for a 64GB system, leaving 4GB for host).
        <code class="language-plaintext highlighter-rouge">bash
        incus config set performant-vm limits.memory 60GiB
       </code>
    *   <strong><code class="language-plaintext highlighter-rouge">limits.memory.hugepages</code></strong>: Enable VM to use host-configured HugePages.
        <code class="language-plaintext highlighter-rouge">bash
        incus config set performant-vm limits.memory.hugepages true
       </code>
        Ensure <code class="language-plaintext highlighter-rouge">limits.memory</code> is a multiple of the hugepage size.
    *   <strong><code class="language-plaintext highlighter-rouge">limits.memory.enforce</code></strong>: Ensure VM gets its allocated memory.
        <code class="language-plaintext highlighter-rouge">bash
        incus config set performant-vm limits.memory.enforce hard
       </code>
    *   <strong>NUMA Pinning (Pin memory to the same NUMA node as CPUs, e.g., node 0):</strong>
        <code class="language-plaintext highlighter-rouge">bash
        incus config set performant-vm limits.memory.nodes 0
       </code></p>

<p>3.  <strong>Disk I/O Performance:</strong>
    *   <strong>Storage Backend:</strong>
        *   <strong>Direct Block Device Passthrough (Recommended for single disk performance):</strong>
            <code class="language-plaintext highlighter-rouge">bash
            # Example: Pass through /dev/sdb1 as the VM's root disk
            incus config device add performant-vm rootdisk disk source=/dev/sdb1 path=/
           </code>
        *   <strong>Or, LVM/ZFS Pool (Flexible, good for SSD/NVMe):</strong>
            <code class="language-plaintext highlighter-rouge">bash
            # Assuming 'mypool' is an existing LVM or ZFS storage pool on a fast device
            # incus storage volume create mypool vm_root_disk --type=block size=100GiB
            # incus config device add performant-vm rootdisk disk pool=mypool source=vm_root_disk path=/
            # For ZFS, consider pool/dataset tuning: atime=off, recordsize=128k (or match workload)
            # zfs set recordsize=128k mypool/virtual-machines/performant-vm.block # Example
            # zfs set atime=off mypool/virtual-machines/performant-vm.block # Example
           </code>
    *   <strong>Disk Cache Options for the device:</strong>
        <code class="language-plaintext highlighter-rouge">bash
        incus config device set performant-vm rootdisk cache none
       </code>
    *   <strong>I/O Mode:</strong>
        <code class="language-plaintext highlighter-rouge">bash
        incus config device set performant-vm rootdisk io writethrough
        # Modern QEMU often defaults well. `io_uring` is preferred if host kernel &amp; QEMU support it;
        # Incus may enable this automatically or via raw.qemu if needed for specific tuning.
       </code>
    *   Ensure the VM uses <code class="language-plaintext highlighter-rouge">virtio-blk</code> or <code class="language-plaintext highlighter-rouge">virtio-scsi</code> (usually default).</p>

<p>4.  <strong>Network I/O Performance:</strong>
    *   <strong>PCI Passthrough (SR-IOV or Full NIC for best performance):</strong>
        1.  Identify NIC PCI address: <code class="language-plaintext highlighter-rouge">lspci -nnk | grep -i ethernet</code>
        2.  (If SR-IOV) Create Virtual Function (VF) on the host.
        3.  Add to Incus VM (replace <code class="language-plaintext highlighter-rouge">enp3s0f0</code> with your host physical NIC or VF name):
            <code class="language-plaintext highlighter-rouge">bash
            incus config device add performant-vm eth0 nic nictype=physical parent=enp3s0f0
           </code>
    *   <strong><code class="language-plaintext highlighter-rouge">macvtap</code> (Good performance if PCI passthrough isn’t feasible):</strong>
        <code class="language-plaintext highlighter-rouge">bash
        # Replace enp2s0 with your host physical interface
        incus config device add performant-vm eth0 nic nictype=macvtap parent=enp2s0 mode=bridge
       </code>
        (<code class="language-plaintext highlighter-rouge">mode=bridge</code> allows host-guest communication. <code class="language-plaintext highlighter-rouge">private</code> or <code class="language-plaintext highlighter-rouge">vepa</code> modes restrict this).
    *   <strong>Multi-Queue <code class="language-plaintext highlighter-rouge">virtio-net</code> (N = number of vCPUs, or power of 2):</strong>
        <code class="language-plaintext highlighter-rouge">bash
        incus config device set performant-vm eth0 queues.rx 4
        incus config device set performant-vm eth0 queues.tx 4
        # Or if your Incus version supports a combined key:
        # incus config device set performant-vm eth0 queues 4
       </code></p>

<p>5.  <strong>Guest OS Considerations:</strong>
    *   Install <code class="language-plaintext highlighter-rouge">qemu-guest-agent</code> inside the VM for better integration (shutdown, time sync, etc.).
        *   Debian/Ubuntu guests: <code class="language-plaintext highlighter-rouge">sudo apt install qemu-guest-agent</code>
        *   RHEL/CentOS guests: <code class="language-plaintext highlighter-rouge">sudo yum install qemu-guest-agent</code>
    *   Ensure the guest OS uses virtio drivers for disk, network, etc. (standard for modern Linux).</p>

<p>6.  <strong>Disable Unused Virtual Hardware:</strong>
    *   Review <code class="language-plaintext highlighter-rouge">incus config show performant-vm --expanded</code>.
    *   If a USB tablet or other unneeded devices are present by default, consider removing them if not used:
        <code class="language-plaintext highlighter-rouge">bash
        # incus config device remove performant-vm &lt;device_name_from_expanded_config&gt;
       </code></p>

<p>7.  <strong><code class="language-plaintext highlighter-rouge">raw.qemu</code> (Use Sparingly for Advanced QEMU Options):</strong>
    *   For QEMU options not directly exposed by Incus. This is for advanced users.
        <code class="language-plaintext highlighter-rouge">bash
        # Example: incus config set performant-vm raw.qemu "-some-qemu-option value"
       </code></p>

<p><strong>III. Verification and Monitoring</strong></p>

<p>1.  <strong>Host Verification:</strong>
    *   Kernel parameters applied: <code class="language-plaintext highlighter-rouge">cat /proc/cmdline</code>
    *   CPU topology/NUMA: <code class="language-plaintext highlighter-rouge">lscpu</code>
    *   HugePages status: <code class="language-plaintext highlighter-rouge">cat /proc/meminfo | grep -i huge</code>
    *   QEMU process affinity (find PID of <code class="language-plaintext highlighter-rouge">performant-vm</code>’s QEMU process with <code class="language-plaintext highlighter-rouge">ps aux | grep qemu</code> or <code class="language-plaintext highlighter-rouge">incus info performant-vm --resources</code>): <code class="language-plaintext highlighter-rouge">taskset -cp &lt;qemu_pid&gt;</code> (should show affinity to isolated cores).
    *   I/O scheduler for devices: <code class="language-plaintext highlighter-rouge">cat /sys/block/nvme0n1/queue/scheduler</code></p>

<p>2.  <strong>Incus Verification:</strong>
    *   Full VM configuration: <code class="language-plaintext highlighter-rouge">incus config show performant-vm --expanded</code>
    *   VM status and resources: <code class="language-plaintext highlighter-rouge">incus info performant-vm</code> and <code class="language-plaintext highlighter-rouge">incus info performant-vm --resources</code></p>

<p>3.  <strong>Guest Verification (Inside the VM):</strong>
    *   CPU info: <code class="language-plaintext highlighter-rouge">lscpu</code>
    *   Memory: <code class="language-plaintext highlighter-rouge">free -h</code>, <code class="language-plaintext highlighter-rouge">cat /proc/meminfo | grep -i huge</code> (to see if guest is using hugepages from host)
    *   Kernel messages for hardware/driver info: <code class="language-plaintext highlighter-rouge">dmesg</code>
    *   Run relevant benchmarks (disk: <code class="language-plaintext highlighter-rouge">fio</code>; network: <code class="language-plaintext highlighter-rouge">iperf3</code>; CPU: <code class="language-plaintext highlighter-rouge">sysbench</code>, Phoronix Test Suite).</p>

<p>4.  <strong>Continuous Monitoring:</strong>
    *   <strong>Host:</strong> <code class="language-plaintext highlighter-rouge">htop</code> (observe CPU usage on isolated vs. host cores), <code class="language-plaintext highlighter-rouge">vmstat</code>, <code class="language-plaintext highlighter-rouge">iostat</code>, <code class="language-plaintext highlighter-rouge">perf top</code> (for deep dives).
    *   <strong>Incus:</strong> <code class="language-plaintext highlighter-rouge">incus top</code> (for Incus-specific metrics).</p>

<p><strong>IV. General Workflow &amp; Considerations</strong></p>

<p>*   <strong>Iterate and Benchmark:</strong> Performance tuning is often iterative. Change one major setting at a time and benchmark to observe its impact. What works best can be workload-dependent.
*   <strong>Stability is Paramount:</strong> While aiming for maximum performance, ensure the host and VM remain stable. An unstable host means an unstable VM.
*   <strong>Documentation:</strong> Document your specific changes and system configuration.
*   <strong>Recovery Plan:</strong> Before major GRUB or system config changes, ensure you have a way to recover (e.g., bootable USB, familiarity with recovery mode).
*   <strong>Incus Documentation:</strong> Specific commands and configuration keys can evolve. Always refer to the official Incus documentation for your version if you encounter discrepancies or need more detail.</p>

<p>By systematically applying these optimizations, you can significantly reduce overhead from both Ubuntu Server and Incus, allowing your primary virtual machine (<code class="language-plaintext highlighter-rouge">performant-vm</code>) to utilize the maximum available hardware resources and achieve performance much closer to bare-metal.</p>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">messy folder reorganizer documentation</title><link href="https://ib.bsb.br/messy-folder-reorganizer-documentation/" rel="alternate" type="text/html" title="messy folder reorganizer documentation" /><published>2025-05-19T00:00:00+00:00</published><updated>2025-05-19T08:54:40+00:00</updated><id>https://ib.bsb.br/messy-folder-reorganizer-documentation</id><content type="html" xml:base="https://ib.bsb.br/messy-folder-reorganizer-documentation/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;!-- [![codecov](/gh/PerminovEugene/messy-folder-reorganizer-ai/branch/main/graph/badge.svg)](/gh/PerminovEugene/messy-folder-reorganizer-ai) --&gt;

![Build](/github/actions/workflow/status/PerminovEugene/messy-folder-reorganizer-ai/ci.yml?branch=main)
![License](/github/license/PerminovEugene/messy-folder-reorganizer-ai)
![Language](/github/languages/top/PerminovEugene/messy-folder-reorganizer-ai)
![Local AI](/badge/AI-local--only-green?logo=ai)

## messy-folder-reorganizer-ai - 🤖 AI-powered CLI for file reorganization.

### Runs fully locally — no data leaves your machine.

### How It Works

CLI supports multiple commands:

#### Process

1. **User Input** – The user runs the app and provides:

   - a **source folder** path containing the files to organize
   - a **destination folder** path where organized files will be placed
   - an **AI model name** (loaded in Ollama) used to generate folder names
   - an **embedding model name** (also loaded in Ollama) used to generate vector embeddings

2. **Destination Folder Scan**

   - The app scans the destination folder and generates embeddings for each folder name.
   - These embeddings are stored in a **Qdrant** vector database.

3. **Source Folder Scan**

   - The app scans the source folder and generates embeddings for each file name.
   - It compares each file’s embedding to existing folder embeddings in the database.
   - Files without a sufficiently close match are marked for further processing.

4. **Clustering &amp; AI Folder Naming**

   - Unmatched file embeddings are grouped using **agglomerative hierarchical clustering**.
   - Each cluster is sent to the LLM to generate a suggested folder name.

5. **Preview Results**

   - A table is displayed showing the proposed destination for each file.

6. **User Decision**
   - The user reviews the suggested structure and decides whether to apply the changes.

#### Apply

If you decided to not apply changes after `process`, you can apply changes later with `apply` command. It expects that you didn't change files locations. This command applied migrations from the latest succesfull `process` launch.

#### Rollback

For the case if after files migrations you are changed your mind and want to return everything back.

&gt; ⚠️ **Warning:** Do not use `messy-folder-reorganizer-ai` on important files such as passwords, confidential documents, or critical system files.  
&gt; In the event of a bug or interruption, the app may irreversibly modify or delete files. Always create backups before using it on valuable data.  
&gt; The author assumes no responsibility for data loss or misplaced files caused by this application.

## Small articles for the curious minds

📌 [Adding RAG &amp; ML to the CLI](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3)

📌 [How cosine similarity helped files find their place](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3)

📌 [Teaching embeddings to understand folders](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg)

📌 [Hierarchical clustering for file grouping](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k)

## Setup

1. Install core developer tools

- macOS

  ```
  Install or update **Xcode**
  ```

- Linux x86_64

  ```sh
  sudo apt update
  sudo apt install -y build-essential
  ```

2. Install **Ollama** and start the service.
3. Download the required LLM via Ollama:

   ```sh
   ollama pull deepseek-r1:latest
   ```

   &gt; Recommended: Use models with a higher number of parameters for better accuracy.  
   &gt; This project has been tested with `deepseek-r1:latest` (4.7 GB, 7.6B params).

4. Download the embedding model:

   ```sh
   ollama pull mxbai-embed-large:latest
   ```

5. Launch Qdrant vector database (easiest via Docker):

   ```sh
   docker pull qdrant/qdrant
   docker run -p 6333:6333 \
     -v $(pwd)/path/to/data:/qdrant/storage \
     qdrant/qdrant
   ```

6. Download the latest app release:

- Apple Silicon (macOS ARM64):

  ```sh
  curl -s https://api.github.com/repos/PerminovEugene/messy-folder-reorganizer-ai/releases/tags/v0.2.0 | \
    grep "browser_download_url.*messy-folder-reorganizer-ai-v0.2.0-aarch64-apple-darwin.tar.gz" | \
    cut -d '"' -f 4 | \
    xargs curl -L -o messy-folder-reorganizer-ai-macos-arm64.tar.gz
  ```

- Intel Mac (macOS x86_64):

  ```sh
  curl -s https://api.github.com/repos/PerminovEugene/messy-folder-reorganizer-ai/releases/tags/v0.2.0 | \
    grep "browser_download_url.*messy-folder-reorganizer-ai-v0.2.0-x86_64-apple-darwin.tar.gz" | \
    cut -d '"' -f 4 | \
    xargs curl -L -o messy-folder-reorganizer-ai-macos-x64.tar.gz
  ```

- Linux x86_64:

  ```sh
  curl -s https://api.github.com/repos/PerminovEugene/messy-folder-reorganizer-ai/releases/tags/v0.2.0 | \
    grep "browser_download_url.*messy-folder-reorganizer-ai-v0.2.0-x86_64-unknown-linux-gnu.tar.gz" | \
    cut -d '"' -f 4 | \
    xargs curl -L -o messy-folder-reorganizer-ai-linux-x64.tar.gz
  ```

7. Extract and install:

- Apple Silicon (macOS ARM64):

  ```sh
  tar -xvzf messy-folder-reorganizer-ai-macos-arm64.tar.gz
  sudo mv messy-folder-reorganizer-ai /usr/local/bin/messy-folder-reorganizer-ai
  ```

- Intel Mac (macOS x86_64):

  ```sh
  tar -xvzf messy-folder-reorganizer-ai-macos-x64.tar.gz
  sudo mv messy-folder-reorganizer-ai /usr/local/bin/messy-folder-reorganizer-ai
  ```

- Linux x86_64:

  ```sh
  tar -xvzf messy-folder-reorganizer-ai-linux-x64.tar.gz
  sudo mv messy-folder-reorganizer-ai /usr/local/bin/messy-folder-reorganizer-ai
  ```

8. Verify the installation:

   ```sh
   messy-folder-reorganizer-ai --help
   ```

## Build from Source

1. Clone the repository:

   ```sh
   git clone git@github.com:PerminovEugene/messy-folder-reorganizer-ai.git
   ```

2. Build the project:

   ```sh
   cargo build --release
   ```

3. Run it:

   ```sh
   cargo run -- \
     -E mxbai-embed-large \
     -L deepseek-r1:latest \
     -S ./test_cases/clustering/messy-folder \
     -D ./test_cases/clustering/structured-folder
   ```

## Usage

### Run the App

```sh
messy-folder-reorganizer-ai process \
  -E &lt;EMBEDDING_MODEL_NAME&gt; \
  -L &lt;LLM_MODEL_NAME&gt; \
  -S &lt;SOURCE_FOLDER_PATH&gt; \
  -D &lt;DESTINATION_FOLDER_PATH&gt;
```

```sh
messy-folder-reorganizer-ai apply \
  -i &lt;SESSION_ID&gt;
```

```sh
messy-folder-reorganizer-ai rollback \
 -i &lt;SESSION_ID&gt;
```

## Command-Line Arguments

The CLI supports the following subcommands:

---

### `process`

Processes source files, finds best-matching destination folders using embeddings, and generates a migration plan.

| Argument                  | Short | Default                  | Description                                                                          |
| ------------------------- | ----- | ------------------------ | ------------------------------------------------------------------------------------ |
| `--language-model`        | `-L`  | _required_               | Ollama LLM model name used to generate semantic folder names.                        |
| `--embedding-model`       | `-E`  | _required_               | Embedding model used for representing folder and file names as vectors.              |
| `--source`                | `-S`  | _required_               | Path to the folder with unorganized files.                                           |
| `--destination`           | `-D`  | `home`                   | Path to the folder where organized files should go.                                  |
| `--recursive`             | `-R`  | `false`                  | Whether to scan subfolders of the source folder recursively.                         |
| `--force-apply`           | `-F`  | `false`                  | Automatically apply changes after processing without showing preview.                |
| `--continue-on-fs-errors` | `-C`  | `false`                  | Allow skipping files/folders that throw filesystem errors (e.g., permission denied). |
| `--llm-address`           | `-n`  | `http://localhost:11434` | Address of the local or remote Ollama LLM server.                                    |
| `--qdrant-address`        | `-q`  | `http://localhost:6334`  | Address of the Qdrant vector database instance.                                      |

---

### `apply`

Applies a previously saved migration plan using the session ID.
Session Id will be printed during `process` execution.

| Argument       | Short | Description                                        |
| -------------- | ----- | -------------------------------------------------- |
| `--session-id` | `-i`  | The session ID generated by the `process` command. |

---

### 🔙 `rollback`

Rolls back a previously applied migration using the session ID.
Session Id will be printed during `process` execution.

| Argument       | Short | Description                                              |
| -------------- | ----- | -------------------------------------------------------- |
| `--session-id` | `-i`  | The session ID used to identify which migration to undo. |

## Configuration

### Model &amp; ML Configuration

On the first run, the app creates a `.messy-folder-reorganizer-ai/` directory in your home folder containing:

- llm_config.toml – LLM model request configuration options
- embeddings_config.toml – Embedding model request configuration options
- rag_ml_config.toml – RAG and ML behavior settings

Model request configurations are commented out by default and will fall back to built-in values unless edited.

More information about LLM and Embedding model configuration options can be found [https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values](here).

RAG and ML configuration parameters are required and should always be present in rag_ml_config.toml.
You also can set up ignore lists for destionation and source pathes in that config file.

You can change the path where `.messy-folder-reorganizer-ai` will be created. Simply add `MESSY_FOLDER_REORGANIZER_AI_PATH` environment variable with path with desired location.

### Prompt Customization

Prompts are stored in:

```sh
~/.messy-folder-reorganizer-ai/prompts/
```

You can edit these to experiment with different phrasing.  
The source file list will be appended automatically, so **do not** use `{}` or other placeholders in the prompt.

Feel free to contribute improved prompts via PR!

### Auto-Recovery

If you break or delete any config/prompt files, simply re-run the app — missing files will be regenerated with default values.

### Additional help

- [Ollama GitHub](https://github.com/ollama/ollama)
- [Embedding Models with Ollama](https://ollama.com/blog/embedding-models)
- [Qdrant Docs](https://qdrant.tech/documentation/guides/installation/)

## Contributing

1. Run the setup script before contributing:

   ```sh
   bash setup-hooks.sh
   ```

2. Lint &amp; format code:

   ```sh
   cargo clippy
   cargo fmt
   ```

3. Check for unused dependencies:

   ```sh
   cargo +nightly udeps
   ```

### Running tests:

To run all tests

```sh
cargo test
```

To run integration tests

```sh
cargo test --test '*' -- --nocapture
```

To run specific integration test (file_collision for example)

```sh
cargo test file_collision -- --nocapture
```

## Uninstall &amp; Purge

```sh
rm -f /usr/local/bin/messy-folder-reorganizer-ai
rm -rf ~/.messy-folder-reorganizer-ai
```

## License

This project is dual-licensed under either:

- [MIT License](./LICENSE-MIT)
- [Apache License, Version 2.0](./LICENSE-APACHE)

at your option.

It interacts with external services including:

- [Ollama](https://github.com/ollama/ollama) – MIT License
- [Qdrant](https://github.com/qdrant/qdrant) – Apache 2.0 License
</code></pre></div></div>

<p>:::</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Title: How I Built a Local LLM-Powered File Reorganizer with Rust

URL Source: /evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip

Published Time: 2025-02-19T15:20:29Z

Markdown Content:
[[1: Cover image for How I Built a Local LLM-Powered File Reorganizer with Rust](width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0x60v2k0khd050bbcyfn.png)](width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0x60v2k0khd050bbcyfn.png)

[](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip#introduction-diving-back-into-rust) Introduction: Diving (Back) Into Rust
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Some time ago, I decided to dive into Rust **once again**—this must be my _nth_ attempt. I’d tried learning it before, but each time I either got swamped by the borrow checker or got sidetracked by other projects. This time, I wanted a small, _practical_ project to force myself to stick with Rust. The result is [messy-folder-reorganizer-ai](https://github.com/PerminovEugene/messy-folder-reorganizer-ai/tree/main), a command-line tool for file organization powered by a local LLM.

* * *

[](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip#the-inspiration-a-bloated-downloads-folder) The Inspiration: A Bloated Downloads Folder
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The main motivation was my messy **Downloads** folder, which often ballooned to hundreds of files—images, documents, installers—essentially chaos. Instead of manually sorting through them, I thought, “Why not let an AI propose a structure?”

* * *

[](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip#discovering-local-llms) Discovering Local LLMs
----------------------------------------------------------------------------------------------------------------------------------------------

While brainstorming, I stumbled upon the possibility of running LLMs **locally**, like Ollama or other self-hosted frameworks. I loved the idea of **not sending** my data to some cloud service. So I decided to build a Rust-based CLI that **queries** a local LLM server for suggestions on how to reorganize my folders.

* * *

[](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip#challenges-llm-amp-large-folders) Challenges: LLM &amp; Large Folders
-----------------------------------------------------------------------------------------------------------------------------------------------------------------

*   **Initial Model:** I started using `llama3.2:1b`, but the responses didn’t follow prompt instructions well, so I switched to **deepseek-r1**, which performed much better. 
*   **Context Limits:** When testing on folders with many files, the model began forgetting the beginning of the prompt and stopped following instructions properly. Increasing `num_ctx` (which defines the model’s context size) helped partially, but the model still struggles with **100+ files**. 
*   **Possible Solutions:**
    *   **Batching Requests:** Split the file list into smaller chunks and send multiple prompts. 
    *   **Other Ideas?:** If you’re an LLM expert—especially with local models like Ollama—I’d love advice on how to handle larger sets without hitting memory or context limits.

* * *

[](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip#cli-features) CLI Features
--------------------------------------------------------------------------------------------------------------------------

*   **Configurable Model:** Specify the local LLM endpoint, model name, or other model options. 
*   **Customizable Prompts:** Tweak the AI prompt to fine-tune how the model interprets your folder’s contents. 
*   **Confirmation Prompt:** The tool shows you the proposed structure and asks for confirmation before reorganizing any files.

* * *

[](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip#looking-for-feedback) Looking for Feedback
------------------------------------------------------------------------------------------------------------------------------------------

*   **Rust Community:** I’d love code feedback — best practices, performance tips, or suggestions on how to structure the CLI. 
*   **LLM Gurus:** Any advice on optimizing local model inference for large file sets or advanced chunking strategies would be invaluable.

* * *

[](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip#conclusion) Conclusion
----------------------------------------------------------------------------------------------------------------------

This project has been a great way to re-learn some Rust features and experiment with local AI solutions. While it works decently for medium-sized folders, there’s plenty of room to grow. If this concept resonates with you—maybe your Downloads folder is as messy as mine—give it a try, open an issue, or contribute a pull request.

**Thanks for reading!**

Feel free to reach out on the [GitHub issues page](https://github.com/PerminovEugene/messy-folder-reorganizer-ai/issues), or drop me a note if you have any thoughts, suggestions, or just want to talk about Rust and AI!

[[2: Heroku](width=775%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fi.imgur.com%2FEtkoO96.png)](https://www.heroku.com/?utm_source=devto&amp;utm_medium=paid&amp;utm_campaign=heroku_2025&amp;bb=217501)

[](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip#built-for-developers-by-developers)[Built for developers, by developers.](https://www.heroku.com/?utm_source=devto&amp;utm_medium=paid&amp;utm_campaign=heroku_2025&amp;bb=217501)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Whether you're building a simple prototype or a business-critical product, Heroku's fully-managed platform gives you the simplest path to delivering apps quickly — using the tools and languages you already love!

[Learn More](https://www.heroku.com/?utm_source=devto&amp;utm_medium=paid&amp;utm_campaign=heroku_2025&amp;bb=217501)
</code></pre></div></div>

<p>:::</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Title: Adding RAG and ML to AI files reorganization CLI (messy-folder-reorganizer-ai)

URL Source: /evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3

Published Time: 2025-03-28T15:41:36Z

Markdown Content:
Adding RAG and ML to AI files reorganization CLI (messy-folder-reorganizer-ai) - DEV Community
===============
[Skip to content](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#main-content)

[[1: DEV Community](quality=100/https://dev-to-uploads.s3.amazonaws.com/uploads/logos/resized_logo_UQww2soKuUsjaOGNB38o.png)](/)

[Powered by Algolia](https://www.algolia.com/developers/?utm_source=devto&amp;utm_medium=referral)

[Log in](/enter)[Create account](/enter?state=new-user)

DEV Community
-------------

[2](heart-plus-active-9ea3b22f2bc311281db911d416166c5f430636e76b15cd5df6b3b841d830eefa.svg)1 Add reaction 

[3](sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)1 Like [4](multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)0 Unicorn [5](exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)0 Exploding Head [6](raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)0 Raised Hands [7](fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)0 Fire 

0 Jump to Comments 0 Save  Boost 

 Moderate 

Copy link

Copied to Clipboard

[Share to X](https://twitter.com/intent/tweet?text=%22Adding%20RAG%20and%20ML%20to%20AI%20files%20reorganization%20CLI%20%28messy-folder-reorganizer-ai%29%22%20by%20Evgenii%20Perminov%20%23DEVCommunity%20https%3A%2F%2Fdev.to%2Fevgeniiperminov%2Fadding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3)[Share to LinkedIn](https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fdev.to%2Fevgeniiperminov%2Fadding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3&amp;title=Adding%20RAG%20and%20ML%20to%20AI%20files%20reorganization%20CLI%20%28messy-folder-reorganizer-ai%29&amp;summary=A%20month%20ago%2C%20I%20created%20the%20first%20naive%20version%20of%20a%20CLI%20tool%20for%20AI-powered%20file%20reorganization%20in...&amp;source=DEV%20Community)[Share to Facebook](https://www.facebook.com/sharer.php?u=https%3A%2F%2Fdev.to%2Fevgeniiperminov%2Fadding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3)[Share to Mastodon](https://toot.kytta.dev/?text=https%3A%2F%2Fdev.to%2Fevgeniiperminov%2Fadding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3)

[Share Post via...](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#)[Report Abuse](/report-abuse)

[[8: Cover image for Adding RAG and ML to AI files reorganization CLI (messy-folder-reorganizer-ai)](width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzgjimz33n68u6crsuf89.png)](width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzgjimz33n68u6crsuf89.png)

[[9: Evgenii Perminov](width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1973401%2F3bc0834c-aae8-4342-9fbb-14588e5533f9.jpg)](/evgeniiperminov)

[Evgenii Perminov](/evgeniiperminov)
Posted on Mar 28

[10](sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)1[11](multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)[12](exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)[13](raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)[14](fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)

Adding RAG and ML to AI files reorganization CLI (messy-folder-reorganizer-ai)
==============================================================================

[#llm](/t/llm)[#cli](/t/cli)[#opensource](/t/opensource)[#rag](/t/rag)

[messy-folder-reorganizer-ai (4 Part Series)](/evgeniiperminov/series/30981)
------------------------------------------------------------------------------------------

[1 Adding RAG and ML to AI files reorganization CLI (messy-folder-reorganizer-ai)](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3 "Published Mar 28")[2 How Cosine Similarity Helped My CLI Decide Where Files Belong (messy-folder-reorganizer-ai)](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3 "Published Mar 28")[3 Making Embeddings Understand Files and Folders with Simple Sentences (messy-folder-reorganizer-ai)](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg "Published Mar 28")[4 Embeddings clustering with Agglomerative Hierarchical Clustering (messy-folder-reorganizer-ai)](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k "Published Mar 28")

A month ago, I created the first naive version of a CLI tool for AI-powered file reorganization in Rust — [messy-folder-reorganizer-ai](https://github.com/PerminovEugene/messy-folder-reorganizer-ai). It sent file names and paths to Ollama and asked the LLM to generate new paths for each file. This worked fine for a small number of files, but once the count exceeded around 50, the LLM context filled up quickly.

So, I decided to improve the entire workflow by integrating RAG (Retrieval-Augmented Generation).

* * *

[](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#version-02-workflow-updates) Version 0.2 Workflow Updates
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Here’s how adding RAG and a bit of ML helped improve the file reorganization flow in the CLI:

### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#1-custom-source-and-destination-paths) 1. Custom Source and Destination Paths

First, I allowed users to specify different paths:

*   A **source path** where files are located.
*   A **destination path** where files will be moved.

### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#2-adding-rag-with-qdrant) 2. Adding RAG with Qdrant

Next, I introduced RAG into the system. As a vector database, I chose [Qdrant](https://qdrant.tech/) — an open-source, easy-to-run local vector store.

&gt; _Currently, users need to manually download and launch Qdrant. Automatic setup is planned for future versions._

The core of RAG is generating embeddings from text. Here's the step-by-step:

### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#3-embedding-folder-and-file-names) 3. Embedding Folder and File Names

The CLI sends destination folder names and source file names to an Ollama embedding model. The model returns an embedding (vector) for each name.

#### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#contextualizing-the-input) Contextualizing the Input

Instead of sending raw names, I added context like:

`"This is a folder name: {folder_name}"`

&gt; _A more detailed explanation will be in the next article._

#### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#embedding-model-selection) Embedding Model Selection

Different models return vectors of different dimensions. I used the **mxbai-embed-large:latest** model from Ollama, which produces 1024-dimensional vectors. It performed well for most use cases.

### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#4-storing-folder-embeddings-in-qdrant) 4. Storing Folder Embeddings in Qdrant

Each destination folder's embedding is stored in Qdrant, with the original folder name included as payload metadata.

### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#5-matching-files-to-closest-folders) 5. Matching Files to Closest Folders

For each source file embedding, the CLI searches Qdrant for the closest destination folder vector.

 Qdrant returns the most similar match along with a similarity score.

&gt; _More about similarity measures and why I picked a particular one will be covered in the third article._

### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#6-thresholdbased-filtering) 6. Threshold-Based Filtering

The CLI compares each similarity score to a configurable threshold (set via config files). If no suitable match is found, the file is filtered out and sent to an additional step — **clustering** and **folder name generation via LLM**.

### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#7-clustering-unmatched-files) 7. Clustering Unmatched Files

Since LLMs struggle with large input contexts, we split unmatched files into clusters using machine learning — specifically **agglomerative hierarchical clustering**.

&gt; _More details about clustering are in the fourth article in this series._

### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#8-naming-clusters-via-llm) 8. Naming Clusters via LLM

Once clustering is complete, we end up with small, manageable groups of files. For each cluster, we send a prompt to the LLM to generate a suitable folder name.

After some LLM thinking time, we receive the missing folder names and can show the user a preview of the proposed file reorganization.

### [](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#9-applying-the-changes) 9. Applying the Changes

If the user is happy with the proposed structure, they can confirm it. The CLI will then move the files to their new paths accordingly.

* * *

[](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#conclusion) Conclusion
-----------------------------------------------------------------------------------------------------------------------------------------

In the upcoming articles, I’ll dive into some of the more technical and interesting parts of the project:

*   How to choose a similarity search method.
*   Ways to improve embeddings for files and folders.
*   Selecting and preparing data for clustering.

* * *

[](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#looking-for-feedback) Looking for Feedback
-------------------------------------------------------------------------------------------------------------------------------------------------------------

I’d really appreciate any feedback — positive or critical — on the project, the codebase, the article series, or the general approach used in the CLI.

* * *

[](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#thanks-for-reading)**Thanks for Reading!**
-------------------------------------------------------------------------------------------------------------------------------------------------------------

Feel free to reach out here or connect with me on:

*   [GitHub](https://github.com/PerminovEugene)
*   [LinkedIn](https://www.linkedin.com/in/eugene-perminov/)

Or just drop me a note if you want to chat about Rust, AI, or creative ways to clean up messy folders!

[messy-folder-reorganizer-ai (4 Part Series)](/evgeniiperminov/series/30981)
------------------------------------------------------------------------------------------

[1 Adding RAG and ML to AI files reorganization CLI (messy-folder-reorganizer-ai)](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3 "Published Mar 28")[2 How Cosine Similarity Helped My CLI Decide Where Files Belong (messy-folder-reorganizer-ai)](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3 "Published Mar 28")[3 Making Embeddings Understand Files and Folders with Simple Sentences (messy-folder-reorganizer-ai)](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg "Published Mar 28")[4 Embeddings clustering with Agglomerative Hierarchical Clustering (messy-folder-reorganizer-ai)](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k "Published Mar 28")

[[15: profile](width=64,height=64,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Forganization%2Fprofile_image%2F123%2F38b10714-65da-4f1d-88ae-e9b28c1d7a5e.png) Heroku](/heroku)Promoted

*   [What's a billboard?](/billboards)
*   [Manage preferences](/settings/customization#sponsors)

* * *

*   [Report billboard](/report-abuse?billboard=217501)

[[16: Heroku](width=775%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fi.imgur.com%2FEtkoO96.png)](https://www.heroku.com/?utm_source=devto&amp;utm_medium=paid&amp;utm_campaign=heroku_2025&amp;bb=217501)

[](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#built-for-developers-by-developers)[Built for developers, by developers.](https://www.heroku.com/?utm_source=devto&amp;utm_medium=paid&amp;utm_campaign=heroku_2025&amp;bb=217501)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Whether you're building a simple prototype or a business-critical product, Heroku's fully-managed platform gives you the simplest path to delivering apps quickly — using the tools and languages you already love!

[Learn More](https://www.heroku.com/?utm_source=devto&amp;utm_medium=paid&amp;utm_campaign=heroku_2025&amp;bb=217501)

 Read More 

Top comments (0)
----------------

Subscribe

[17: pic](width=256,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)

Personal Trusted User[Create template](/settings/response-templates)
Templates let you quickly answer FAQs or store snippets for re-use.

Submit Preview[Dismiss](/404.html)

[Code of Conduct](/code-of-conduct)•[Report abuse](/report-abuse)

Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's [permalink](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#).

- [x] 
Hide child comments as well

 
Confirm

For further actions, you may consider blocking this person and/or [reporting abuse](/report-abuse)

[[18: profile](width=64,height=64,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Forganization%2Fprofile_image%2F1726%2Fe01690b9-c8bd-4eb9-bbe2-a4db25a702a9.png) AWS](/aws)Promoted

*   [What's a billboard?](/billboards)
*   [Manage preferences](/settings/customization#sponsors)

* * *

*   [Report billboard](/report-abuse?billboard=228259)

[[19: AWS Security LIVE! Stream](width=775%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fi.imgur.com%2F9Zvgtm6.png)](https://aws.bpc.digital/3Cxb0RL?bb=228259)

[](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#stream-aws-security-live)[Stream AWS Security LIVE!](https://aws.bpc.digital/3Cxb0RL?bb=228259)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

See how AWS is redefining security by design with simple, seamless solutions on Security LIVE!

[Learn More](https://aws.bpc.digital/3Cxb0RL?bb=228259)

Read next
---------

[[20: fallon_jimmy profile image](width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2191871%2F01077452-5ffd-4131-9dde-caaffa7e2af8.jpg) ### Suna AI: Open-Source General Software: Cost and Deployment Tutorial🔥 Fallon Jimmy - May 6](/fallon_jimmy/suna-ai-open-source-general-software-cost-and-deployment-tutorial-3bk9)[[21: aairom profile image](width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F430591%2F85dbea8d-e5cb-47db-a4a7-ddb98f739bb5.jpeg) ### Using Docling’s OCR features with RapidOCR Alain Airom - Apr 3](/aairom/using-doclings-ocr-features-with-rapidocr-29hd)[[22: deepanshup04 profile image](width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1404950%2Fd5728a67-8686-40aa-8e4a-e5ace47349ef.jpg) ### Unlocking Efficiency: The Power of Kong AI Gateway Deepanshu Pandey - Apr 26](/deepanshup04/unlocking-efficiency-the-power-of-kong-ai-gateway-4f9m)[[23: seuros profile image](width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F766706%2F3d9f585d-7da2-4e5f-a994-9e0c8b4db410.png) ### Smarter RAG Systems with Graphs Abdelkader Boudih - May 6](/seuros/smarter-rag-systems-with-graphs-4bg)

[[24](width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1973401%2F3bc0834c-aae8-4342-9fbb-14588e5533f9.jpg) Evgenii Perminov](/evgeniiperminov)

Follow

 Hey! My name is Evgenii and I am software engineer with 10 years of experience. Currently working on Rust+AI based CLI for files reorganization. 

*    Location   Estonia  
*    Joined  Aug 24, 2024 

### More from [Evgenii Perminov](/evgeniiperminov)

[Embeddings clustering with Agglomerative Hierarchical Clustering (messy-folder-reorganizer-ai) #vectordatabase#ai#machinelearning#cli](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k)[How Cosine Similarity Helped My CLI Decide Where Files Belong (messy-folder-reorganizer-ai) #llm#rust#cli#opensource](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3)[How I Built a Local LLM-Powered File Reorganizer with Rust #llm#rust#cli#opensource](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip)

[[25: profile](width=64,height=64,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Forganization%2Fprofile_image%2F5369%2Fbf0b17ac-3757-4494-ae6d-69f47c5be2c2.png) Stellar Development Foundation](/stellar)Promoted

*   [What's a billboard?](/billboards)
*   [Manage preferences](/settings/customization#sponsors)

* * *

*   [Report billboard](/report-abuse?billboard=225974)

[[26: Image of Stellar post](width=350%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fi.imgur.com%2FFHXRlQs.png)](https://www.youtube.com/watch?v=FInE2PSx1es&amp;t=1s&amp;bb=225974)

[](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#how-a-hackathon-win-led-to-my-startup-getting-funded)[How a Hackathon Win Led to My Startup Getting Funded](https://www.youtube.com/watch?v=FInE2PSx1es&amp;t=1s&amp;bb=225974)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In this episode, you'll see:

*   The hackathon wins that sparked the journey.
*   The moment José and Joseph decided to go all-in.
*   Building a working prototype on Stellar.
*   Using the PassKeys feature of Soroban.
*   Getting funded via the Stellar Community Fund.

[Watch the video 🎥](https://www.youtube.com/watch?v=FInE2PSx1es&amp;t=1s&amp;bb=225974)

👋 Kindness is contagious

*   [What's a billboard?](/billboards)
*   [Manage preferences](/settings/customization#sponsors)

* * *

*   [Report billboard](/report-abuse?billboard=225483)

Dive into this thoughtful article, cherished within the supportive DEV Community. **Coders of every background** are encouraged to share and grow our collective expertise.

A genuine "thank you" can brighten someone’s day—drop your appreciation in the comments below!

On DEV, **sharing knowledge smooths our journey** and strengthens our community bonds. Found value here? A quick thank you to the author makes a big difference.

[](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3#-cta-httpsdevtoenterstatenewuser-)[Okay](/enter?state=new-user&amp;bb=225483)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

💎 DEV Diamond Sponsors

Thank you to our Diamond Sponsors for supporting the DEV Community

[[27: Neon - Official Database Partner](width=880%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbnl88cil6afxzmgwrgtt.png)](https://neon.tech/?ref=devto&amp;bb=146443)
Neon is the official database partner of DEV

[[28: Algolia - Official Search Partner](width=880%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fv30ephnolfvnlwgwm0yz.png)](https://www.algolia.com/developers/?utm_source=devto&amp;utm_medium=referral&amp;bb=146443)
Algolia is the official search partner of DEV

[DEV Community](/) — A space to discuss and keep up software development and manage your software career

*   [Home](/)
*   [DEV++](/++)
*   [Podcasts](/pod)
*   [Videos](/videos)
*   [Tags](/tags)
*   [DEV Help](/help)
*   [Forem Shop](https://shop.forem.com/)
*   [Advertise on DEV](/advertise)
*   [DEV Challenges](/challenges)
*   [DEV Showcase](/showcase)
*   [About](/about)
*   [Contact](/contact)
*   [Free Postgres Database](/free-postgres-database-tier)
*   [Software comparisons](/software-comparisons)

*   [Code of Conduct](/code-of-conduct)
*   [Privacy Policy](/privacy)
*   [Terms of use](/terms)

Built on [Forem](https://www.forem.com/) — the [open source](/t/opensource) software that powers [DEV](/) and other inclusive communities.

Made with love and [Ruby on Rails](/t/rails). DEV Community © 2016 - 2025.

[29: DEV Community](width=190,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)

We're a place where coders share, stay up-to-date and grow their careers.

[Log in](/enter)[Create account](/enter?state=new-user)

[30](sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)[31](multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)[32](exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)[33](raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)[34](fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)
</code></pre></div></div>

<p>:::</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Title: How Cosine Similarity Helped My CLI Decide Where Files Belong (messy-folder-reorganizer-ai)

URL Source: /evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3

Published Time: 2025-03-28T15:41:57Z

Markdown Content:
[](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#introduction) Introduction
----------------------------------------------------------------------------------------------------------------------------------------------------------

In version 0.2 of [messy-folder-reorganizer-ai](https://github.com/PerminovEugene/messy-folder-reorganizer-ai), I used the Qdrant vector database to search for similar vectors. This was necessary to determine which folder a file should go into based on its embedding. Because of this, I needed to revisit different distance/similarity metrics and choose the most appropriate one.

* * *

[](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#choosing-the-right-vector-similarity-metric-in-qdrant) Choosing the Right Vector Similarity Metric in Qdrant
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Qdrant supports the following distance/similarity metrics:

*   **Dot Product**
*   **Cosine Similarity**
*   **Euclidean Distance**
*   **Manhattan Distance**

### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#distancesimilarity-formulas) Distance/Similarity Formulas

Let **x** and **y** be two vectors of dimensionality _n_.

#### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#cosine-similarity) Cosine Similarity

cosine(x, y) = (x · y) / (‖x‖ · ‖y‖)

#### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#dot-product) Dot Product

dot(x, y) = Σ (xᵢ * yᵢ)

&gt; ⚠️ If vectors are normalized to unit length, then: `cosine(x, y) = dot(x, y)`

#### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#euclidean-distance) Euclidean Distance

euclidean(x, y) = sqrt(Σ (xᵢ - yᵢ)²)

#### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#manhattan-distance-l1) Manhattan Distance (L1)

manhattan(x, y) = Σ |xᵢ - yᵢ|

When working with high-dimensional vectors (e.g., 1024 dimensions, as in the **mxbai-embed-large:latest** Ollama model) that have **small magnitudes**, **Cosine Similarity** is often the best choice — especially for embeddings.

* * *

[](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#why-cosine-similarity-is-a-good-choice) Why Cosine Similarity is a Good Choice
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#focuses-on-orientation-not-magnitude) Focuses on orientation, not magnitude

Cosine similarity measures the angle between vectors. It tells you 

 how similar the directions are*, regardless of vector length. This 

 is useful when comparing embeddings, where absolute length may not 

 be meaningful.

### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#builtin-normalization) Built-in normalization

Cosine similarity is equivalent to the dot product of **L2-

 normalized vectors**, which helps reduce the effect of the "curse 

 of dimensionality."

### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#great-for-semantic-embeddings) Great for semantic embeddings

Works very well when vectors represent meaning or context. Many models (e.g., OpenAI, BERT, Sentence Transformers) are trained 

 with cosine similarity in mind.

### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#efficient) Efficient

Can be computed quickly even in high dimensions.

* * *

[](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#cosine-similarity-in-detail) Cosine Similarity in Detail
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Imagine two arrows (vectors) starting from the origin in a multi-dimensional space. Cosine similarity measures the **angle between them**:

*   If they point in **exactly the same direction**, similarity = `1.0`
*   If they are **completely opposite**, similarity = `-1.0`
*   If they are **orthogonal** (90° apart), similarity = `0.0`

The closer the angle is to zero, the more similar the vectors are.

* * *

### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#formula) Formula

Given two vectors **A** and **B**, cosine similarity is calculated as:

cos(θ) = (A · B) / (||A|| * ||B||)

*   `A · B` is the dot product of the vectors 
*   `||A||` and `||B||` are the magnitudes (lengths) of the vectors

* * *

### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#example) Example

Let's take two simple 2D vectors:

A = [1, 2] B = [2, 3]

#### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#1-dot-product) 1. Dot Product:

A · B = (1 * 2) + (2 * 3) = 2 + 6 = 8

#### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#2-magnitudes) 2. Magnitudes:

||A|| = √(1² + 2²) = √5 ≈ 2.236 ||B|| = √(2² + 3²) = √13 ≈ 3.606

#### [](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#3-cosine-similarity) 3. Cosine Similarity:

cos(θ) = 8 / (2.236 * 3.606) ≈ 8 / 8.062 ≈ 0.993

**Result: 0.993** — Very high similarity!

* * *

[](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#in-the-context-of-the-cli) In the Context of the CLI
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In `messy-folder-reorganizer-ai`, embeddings represent file and folder names. Cosine similarity allows the CLI to:

*   Find files with similar meaning or content 
*   Group files together 
*   Match files to folder "themes" based on vector similarity

* * *

[](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#looking-for-feedback) Looking for Feedback
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

I’d really appreciate any feedback — positive or critical — on the project, the codebase, the article series, or the general approach used in the CLI.

* * *

[](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3#thanks-for-reading)**Thanks for Reading!**
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Feel free to reach out here or connect with me on:

*   [GitHub](https://github.com/PerminovEugene)
*   [LinkedIn](https://www.linkedin.com/in/eugene-perminov/)

Or just drop me a note if you want to chat about Rust, AI, or creative ways to clean up messy folders!
</code></pre></div></div>

<p>:::</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Title: Making Embeddings Understand Files and Folders with Simple Sentences (messy-folder-reorganizer-ai)

URL Source: /evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg

Published Time: 2025-03-28T15:42:08Z

Markdown Content:
[](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#do-embeddings-need-context-a-practical-look-at-filetofolder-matching) Do Embeddings Need Context? A Practical Look at File-to-Folder Matching
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

When building smart systems that classify or match content — such as automatically sorting files into folders — embeddings are a powerful tool. But how well do they work with minimal input? And does adding natural language context make a difference?

During development [messy-folder-reorganizer-ai](https://github.com/PerminovEugene/messy-folder-reorganizer-ai) I found how adding **contextual phrasing** to file and folder names significantly improved the performance of embedding models and in this article I will share it with the reader.

* * *

[](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#test-case-matching-files-to-valid-folder-names) Test Case: Matching Files to Valid Folder Names
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### [](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#test-a-using-only-file-and-folder-names) Test A: Using Only File and Folder Names

```
| File Name               | Folder Name | Score     |
|-------------------------|-------------|-----------|
| crack.exe               | apps        | 0.5147713 |
| lovecraft novels.txt    | books       | 0.5832841 |
| police report.docx      | docs        | 0.6303186 |
| database admin.pkg      | docs        | 0.5538312 |
| invoice from google.pdf | docs        | 0.5381457 |
| meme.png                | images      | 0.6993392 |
| funny cat.jpg           | images      | 0.5511819 |
| lord of the ring.avi    | movies      | 0.5454072 |
| harry potter.mpeg4      | movies      | 0.5410566 |
```

### [](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#test-b-adding-natural-language-context) Test B: Adding Natural Language Context

Each string was framed like:

*   `"This is a file name: {file_name}"`
*   `"This is a folder name: {folder_name}"`

```
| File Name                       | Folder Name | Score    |
|--------------------------------|-------------|-----------|
| crack.exe                      | apps        | 0.6714907 |
| lovecraft novels.txt           | books       | 0.7517922 |
| database admin.pkg             | dest        | 0.7194574 |
| police report.docx             | docs        | 0.7456068 |
| invoice from google.pdf        | docs        | 0.7141885 |
| meme.png                       | images      | 0.7737676 |
| funny cat.jpg                  | images      | 0.7438067 |
| harry potter.mpeg4             | movies      | 0.7156760 |
| lord of the ring.avi           | movies      | 0.6718528 |
```

#### [](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#observations) Observations:

*   **Scores were consistently higher** across the board when context was added.
*   The model **made more accurate matches**, such as correctly associating `database admin.pkg` with `dest` instead of `books`.
*   This suggests that **embeddings perform better with structured, semantic context**, not just bare tokens.

* * *

[](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#test-case-only-some-files-have-valid-matches) Test Case: Only Some Files Have Valid Matches
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Now let's delete the movies and images folders and observe how the matching behavior changes:

### [](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#test-a-using-only-file-and-folder-names) Test A: Using Only File and Folder Names

```
| File Name               | Folder Name | Score      |
|-------------------------|-------------|------------|
| hobbit.fb2              | apps        | 0.55056566 |
| crack.exe               | apps        | 0.5147713  |
| lovecraft novels.txt    | books       | 0.57081085 |
| police report.docx      | docs        | 0.6303186  |
| meme.png                | docs        | 0.58589196 |
| database admin.pkg      | docs        | 0.5538312  |
| invoice from google.pdf | docs        | 0.5381457  |
| lord of the ring.avi    | docs        | 0.492918   |
| funny cat.jpg           | docs        | 0.45956808 |
| harry potter.mpeg4      | docs        | 0.45733657 |
```

### [](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#test-b-adding-natural-language-context) Test B: Adding Natural Language Context

Same context generation pattern as in previous test case

```
| File Name               | Folder Name | Score      |
|-------------------------|-------------|------------|
| crack.exe               | apps        | 0.6714907  |
| lovecraft novels.txt    | books       | 0.72899115 |
| database admin.pkg      | dest        | 0.7194574  |
| meme.png                | dest        | 0.68507683 |
| funny cat.jpg           | dest        | 0.6797525  |
| lord of the ring.avi    | dest        | 0.5323342  |
| police report.docx      | docs        | 0.7456068  |
| invoice from google.pdf | docs        | 0.71418846 |
| hobbit.fb2              | docs        | 0.6780642  |
| harry potter.mpeg4      | docs        | 0.5984984  |
```

#### [](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#observations) Observations:

*   In Test A, files like meme.png, funny cat.jpg, and lord of the ring.avi were incorrectly matched to the docs folder. In Test B, they appeared in the more appropriate dest folder.

*   There are still some mismatches — for example, hobbit.fb2 was matched with docs instead of books, likely due to the less common .fb2 format. harry potter.mpeg4 also matched with docs, though with a relatively low score.

* * *

[](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#why-does-this-happen) Why Does This Happen?
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### [](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#1-context-gives-structure) 1. **Context Gives Structure**

Embedding models are trained on natural language. So when we provide structured inputs like:

&gt; “This is a file name: invoice from google.pdf”
&gt; 
&gt; 
&gt; “This is a folder name: docs”

...the model better understands the **semantic role** of each string. It knows these aren't just tokens — they are _types of things_, which makes embeddings more aligned.

* * *

### [](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#2-its-not-just-word-overlap) 2. **It’s Not Just Word Overlap**

Yes, phrases like `"this is a file name"` and `"this is a folder name"` are similar. But if word overlap were the only reason for higher scores, all scores would rise evenly — regardless of actual content.

Instead, we're seeing better matching. That means the model is using **true context** to judge compatibility — a sign that semantic meaning is being used, not just lexical similarity.

* * *

### [](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#3-raw-strings-without-context-can-be-misleading) 3. **Raw Strings Without Context Can Be Misleading**

A folder named `docs` or `my-pc` is vague. A file named `database admin.pkg` is even more so. Embeddings of such raw strings might be overly similar due to lack of semantic separation.

Adding even a light wrapper like `"This is a file name..."` or `"This is a folder name..."` gives the model **clearer context and role assignment**, helping it avoid false positives and improve semantic accuracy.

* * *

[](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#conclusion) Conclusion
-------------------------------------------------------------------------------------------------------------------------------------------------------------

*   **Embeddings require context to be effective**, especially for classification or matching tasks.
*   Providing **natural-language-like structure** (even just a short prefix) significantly improves performance.
*   It’s not just about higher scores — it’s about **better semantics and more accurate results**.

If you're building tools that rely on embeddings, especially for classification, recommendation, or clustering — **don't be afraid to add a little helpful context.** It goes a long way.

* * *

[](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#looking-for-feedback) Looking for Feedback
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

I’d really appreciate any feedback — positive or critical — on the project, the codebase, the article series, or the general approach used in the CLI.

* * *

[](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg#thanks-for-reading)**Thanks for Reading!**
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Feel free to reach out here or connect with me on:

*   [GitHub](https://github.com/PerminovEugene)
*   [LinkedIn](https://www.linkedin.com/in/eugene-perminov/)

Or just drop me a note if you want to chat about Rust, AI, or creative ways to clean up messy folders!
</code></pre></div></div>

<p>:::</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Title: Embeddings clustering with Agglomerative Hierarchical Clustering (messy-folder-reorganizer-ai)

URL Source: /evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k

Published Time: 2025-03-28T15:42:16Z

Markdown Content:
Embeddings clustering with Agglomerative Hierarchical Clustering (messy-folder-reorganizer-ai) - DEV Community
===============
[Skip to content](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#main-content)

[[1: DEV Community](quality=100/https://dev-to-uploads.s3.amazonaws.com/uploads/logos/resized_logo_UQww2soKuUsjaOGNB38o.png)](/)

[Powered by Algolia](https://www.algolia.com/developers/?utm_source=devto&amp;utm_medium=referral)

[Log in](/enter)[Create account](/enter?state=new-user)

DEV Community
-------------

[2](heart-plus-active-9ea3b22f2bc311281db911d416166c5f430636e76b15cd5df6b3b841d830eefa.svg)1 Add reaction 

[3](sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)1 Like [4](multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)0 Unicorn [5](exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)0 Exploding Head [6](raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)0 Raised Hands [7](fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)0 Fire 

0 Jump to Comments 0 Save  Boost 

 Moderate 

Copy link

Copied to Clipboard

[Share to X](https://twitter.com/intent/tweet?text=%22Embeddings%20clustering%20with%20Agglomerative%20Hierarchical%20Clustering%20%28messy-folder-reorganizer-ai%29%22%20by%20Evgenii%20Perminov%20%23DEVCommunity%20https%3A%2F%2Fdev.to%2Fevgeniiperminov%2Fembeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k)[Share to LinkedIn](https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fdev.to%2Fevgeniiperminov%2Fembeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k&amp;title=Embeddings%20clustering%20with%20Agglomerative%20Hierarchical%20Clustering%20%28messy-folder-reorganizer-ai%29&amp;summary=Adding%20RAG%20and%20ML%20to%20Messy-Folder-Reorganizer-AI%20%20%20%20%20%20%20%20%20%20%20%20Why%20ML%20Methods%20for...&amp;source=DEV%20Community)[Share to Facebook](https://www.facebook.com/sharer.php?u=https%3A%2F%2Fdev.to%2Fevgeniiperminov%2Fembeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k)[Share to Mastodon](https://toot.kytta.dev/?text=https%3A%2F%2Fdev.to%2Fevgeniiperminov%2Fembeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k)

[Share Post via...](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#)[Report Abuse](/report-abuse)

[[8: Cover image for Embeddings clustering with Agglomerative Hierarchical Clustering (messy-folder-reorganizer-ai)](width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fpvnzlztc87l4nn2wa19d.png)](width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fpvnzlztc87l4nn2wa19d.png)

[[9: Evgenii Perminov](width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1973401%2F3bc0834c-aae8-4342-9fbb-14588e5533f9.jpg)](/evgeniiperminov)

[Evgenii Perminov](/evgeniiperminov)
Posted on Mar 28

[10](sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)1[11](multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)[12](exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)[13](raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)[14](fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)

Embeddings clustering with Agglomerative Hierarchical Clustering (messy-folder-reorganizer-ai)
==============================================================================================

[#vectordatabase](/t/vectordatabase)[#ai](/t/ai)[#machinelearning](/t/machinelearning)[#cli](/t/cli)

[messy-folder-reorganizer-ai (4 Part Series)](/evgeniiperminov/series/30981)
------------------------------------------------------------------------------------------

[1 Adding RAG and ML to AI files reorganization CLI (messy-folder-reorganizer-ai)](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3 "Published Mar 28")[2 How Cosine Similarity Helped My CLI Decide Where Files Belong (messy-folder-reorganizer-ai)](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3 "Published Mar 28")[3 Making Embeddings Understand Files and Folders with Simple Sentences (messy-folder-reorganizer-ai)](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg "Published Mar 28")[4 Embeddings clustering with Agglomerative Hierarchical Clustering (messy-folder-reorganizer-ai)](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k "Published Mar 28")

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#adding-rag-and-ml-to-messyfolderreorganizerai) Adding RAG and ML to Messy-Folder-Reorganizer-AI
===================================================================================================================================================================================================================================

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#why-ml-methods-for-clustering) Why ML Methods for Clustering
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

As we discovered in previous articles, all LLMs have context restrictions, so we cannot send hundreds of file names to an LLM and ask it to create folder names for all of them. On the other hand, sending a request for each file separately is not only inefficient and redundant—it also breaks the global context.

For example, if you have files like `bill_for_electricity.pdf` and `bill_for_leasing.docx`, you don’t want to end up with folder names like `bills` for the first and `documents` for the second. These results are technically valid, but they’re disconnected. **We need to group related files together first**, and the best way to do that is by clustering their embeddings.

 For [messy-folder-reorganizer-ai](https://github.com/PerminovEugene/messy-folder-reorganizer-ai) I picked agglomerative hierarchical clustering and I will try to explain my choice to the reader.

* * *

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#selecting-a-clustering-method) Selecting a Clustering Method
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

There are many clustering algorithms out there, but not all are suitable for the nature of embeddings. We're working with:

*   **High-dimensional vectors** (e.g., 384, 768, or more dimensions).
*   **Relatively small datasets** (e.g., a few hundred or thousand files).

Here's a comparison of a few clustering options:

| Algorithm | Pros | Cons |
| --- | --- | --- |
| **K-Means** | Fast, simple, widely used | Requires choosing `k`, assumes spherical clusters |
| **DBSCAN** | Detects arbitrary shapes, noise handling | Sensitive to parameters, poor with high dimensions |
| **HDBSCAN** | Improved DBSCAN, handles hierarchy | Slower, more complex |
| **Agglomerative** | No need for `k`, builds hierarchy, flexible distances | Slower, high memory use |

**Agglomerative hierarchical clustering** is a strong fit because it:

*   Doesn’t require you to predefine the number of clusters.
*   Works well with custom distance metrics (like cosine).
*   Builds a dendrogram that can be explored at different levels of granularity.

* * *

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#agglomerative-clustering-preparations) Agglomerative Clustering Preparations
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### [](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#input-embedding-matrix) Input: Embedding Matrix

We assume an input matrix of shape **M x N**:

*   `M`: Number of files (embeddings).
*   `N`: Dimensionality of the embeddings (depends on the model used).

### [](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#building-a-normalized-matrix) Building a Normalized Matrix

**What is normalization?**

 Normalization ensures that all vectors are of unit length, which is especially important when using cosine distance.

**Why normalize?**

*   Prevents length from affecting similarity.
*   Ensures cosine distance reflects angular difference only.

**Formula:**

 For vector (x), normalize it as:

x̂ = x / ||x||

Where ||x|| is the Euclidean norm (i.e., the square root of the sum of squares of the elements of x).

### [](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#building-the-distance-matrix-using-cosine-distance) Building the Distance Matrix Using Cosine Distance

**Why cosine distance?**

*   It captures **semantic similarity** better in high-dimensional embedding spaces.
*   More stable than Euclidean in high dimensions.

**Does it help with the curse of dimensionality?**

 To some extent, yes. While no method fully escapes the curse, **cosine similarity** is more robust than Euclidean for textual or semantic data.

**Formula:**

 Given two normalized vectors (x) and (y):

cosine_similarity(x, y) = (x · y) / (‖x‖ · ‖y‖)

cosine_distance(x, y) = 1 - cosine_similarity(x, y)

* * *

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#agglomerative-clustering-algorithm) Agglomerative Clustering Algorithm
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Once we have the distance matrix, the agglomerative process begins:

1.   **Start**: Treat each embedding as its own cluster.
2.   **Merge**: Find the two closest clusters using the selected linkage method: 
    *   **Single**: Minimum distance between points across clusters.
    *   **Complete**: Maximum distance.
    *   **Average**: Mean distance.
    *   **Ward**: Minimizes variance (works only with Euclidean distance).

3.   **Repeat**: Merge the next closest pair until one cluster remains or a distance threshold is reached.
4.   **Cut the dendrogram**: Decide how many clusters to extract based on height (distance) or desired granularity.

This method gives you **interpretable, connected groupings**—a critical step before folder naming or generating structured representations.

* * *

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#implementation) Implementation
------------------------------------------------------------------------------------------------------------------------------------------------------------------

If you are interested, you can check out implementation on Rust

[here](https://github.com/PerminovEugene/messy-folder-reorganizer-ai/blob/main/src/ml/agglomerative_clustering.rs)

* * *

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#looking-for-feedback) Looking for Feedback
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

I’d really appreciate any feedback — positive or critical — on the project, the codebase, the article series, or the general approach used in the CLI.

* * *

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#thanks-for-reading)**Thanks for Reading!**
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Feel free to reach out here or connect with me on:

*   [GitHub](https://github.com/PerminovEugene)
*   [LinkedIn](https://www.linkedin.com/in/eugene-perminov/)

Or just drop me a note if you want to chat about Rust, AI, or creative ways to clean up messy folders!

[messy-folder-reorganizer-ai (4 Part Series)](/evgeniiperminov/series/30981)
------------------------------------------------------------------------------------------

[1 Adding RAG and ML to AI files reorganization CLI (messy-folder-reorganizer-ai)](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3 "Published Mar 28")[2 How Cosine Similarity Helped My CLI Decide Where Files Belong (messy-folder-reorganizer-ai)](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3 "Published Mar 28")[3 Making Embeddings Understand Files and Folders with Simple Sentences (messy-folder-reorganizer-ai)](/evgeniiperminov/making-embeddings-understand-files-and-folders-with-simple-sentences-messy-folder-reorganizer-ai-mjg "Published Mar 28")[4 Embeddings clustering with Agglomerative Hierarchical Clustering (messy-folder-reorganizer-ai)](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k "Published Mar 28")

[[15: profile](width=64,height=64,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Forganization%2Fprofile_image%2F123%2F38b10714-65da-4f1d-88ae-e9b28c1d7a5e.png) Heroku](/heroku)Promoted

*   [What's a billboard?](/billboards)
*   [Manage preferences](/settings/customization#sponsors)

* * *

*   [Report billboard](/report-abuse?billboard=217501)

[[16: Heroku](width=775%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fi.imgur.com%2FEtkoO96.png)](https://www.heroku.com/?utm_source=devto&amp;utm_medium=paid&amp;utm_campaign=heroku_2025&amp;bb=217501)

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#built-for-developers-by-developers)[Built for developers, by developers.](https://www.heroku.com/?utm_source=devto&amp;utm_medium=paid&amp;utm_campaign=heroku_2025&amp;bb=217501)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Whether you're building a simple prototype or a business-critical product, Heroku's fully-managed platform gives you the simplest path to delivering apps quickly — using the tools and languages you already love!

[Learn More](https://www.heroku.com/?utm_source=devto&amp;utm_medium=paid&amp;utm_campaign=heroku_2025&amp;bb=217501)

 Read More 

Top comments (0)
----------------

Subscribe

[17: pic](width=256,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)

Personal Trusted User[Create template](/settings/response-templates)
Templates let you quickly answer FAQs or store snippets for re-use.

Submit Preview[Dismiss](/404.html)

[Code of Conduct](/code-of-conduct)•[Report abuse](/report-abuse)

Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's [permalink](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#).

- [x] 
Hide child comments as well

 
Confirm

For further actions, you may consider blocking this person and/or [reporting abuse](/report-abuse)

[[18: profile](width=64,height=64,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Forganization%2Fprofile_image%2F10846%2Fb8131f88-3d8a-476d-bcf0-2e0fb946e4d5.png) ACI.dev](/acidev)Promoted

*   [What's a billboard?](/billboards)
*   [Manage preferences](/settings/customization#sponsors)

* * *

*   [Report billboard](/report-abuse?billboard=231138)

[[19: ACI image](width=775%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fi.imgur.com%2FJdwzkK1.jpeg)](https://bit.ly/4mdlYOl?bb=231138)

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#acidev-the-only-mcp-server-your-ai-agents-need)[ACI.dev: The Only MCP Server Your AI Agents Need](https://bit.ly/4mdlYOl?bb=231138)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

ACI.dev’s open-source tool-use platform and Unified MCP Server turns 600+ functions into two simple MCP tools on one server—search and execute. Comes with multi-tenant auth and natural-language permission scopes. 100% open-source under Apache 2.0.

[Star our GitHub!](https://bit.ly/4mdlYOl?bb=231138)

Read next
---------

[[20: yasiga_3 profile image](width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F3080683%2F63757d20-cd24-413e-9287-f5c41bb27471.jpg) ### Docker Image creation and pushing to DockerHub yasiga - Apr 28](/yasiga_3/docker-image-creation-and-pushing-to-dockerhub-42eh)[[21: dev_kumar_9a1db98e34077b6 profile image](width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2556866%2F5b2f1e16-9acb-4454-9d17-afd33e81fd60.png) ### The Impact of AI on Retail Shopping Experiences Dev Kumar - Apr 28](/dev_kumar_9a1db98e34077b6/the-impact-of-ai-on-retail-shopping-experiences-35jf)[[22: koolkamalkishor profile image](width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1372052%2Fcc05edd7-e6c0-42f5-b52c-8dd7a6f826e7.webp) ### How to Upload Your Project to Hugging Face Spaces: A Beginner's Step-by-Step Guide KAMAL KISHOR - May 2](/koolkamalkishor/how-to-upload-your-project-to-hugging-face-spaces-a-beginners-step-by-step-guide-1pkn)[[23: reachyugesh profile image](width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F3095083%2Fa0a1c7b8-9e2d-4650-b86a-d21083028be4.jpg) ### Embedded Real-Time Systems: The Engine Behind Precision Healthcare Robotics Yugesh Anne - Apr 27](/reachyugesh/embedded-real-time-systems-the-engine-behind-precision-healthcare-robotics-j14)

[[24](width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1973401%2F3bc0834c-aae8-4342-9fbb-14588e5533f9.jpg) Evgenii Perminov](/evgeniiperminov)

Follow

 Hey! My name is Evgenii and I am software engineer with 10 years of experience. Currently working on Rust+AI based CLI for files reorganization. 

*    Location   Estonia  
*    Joined  Aug 24, 2024 

### More from [Evgenii Perminov](/evgeniiperminov)

[How Cosine Similarity Helped My CLI Decide Where Files Belong (messy-folder-reorganizer-ai) #llm#rust#cli#opensource](/evgeniiperminov/how-cosine-similarity-helped-my-cli-decide-where-files-belong-messy-folder-reorganizer-ai-fm3)[Adding RAG and ML to AI files reorganization CLI (messy-folder-reorganizer-ai) #llm#cli#opensource#rag](/evgeniiperminov/adding-rag-and-ml-to-ai-files-reorganization-cli-messy-folder-reorganizer-ai-1d3)[How I Built a Local LLM-Powered File Reorganizer with Rust #llm#rust#cli#opensource](/evgeniiperminov/how-i-built-a-local-llm-powered-file-reorganizer-in-rust-1bip)

[[25: profile](width=64,height=64,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Forganization%2Fprofile_image%2F5369%2Fbf0b17ac-3757-4494-ae6d-69f47c5be2c2.png) Stellar Development Foundation](/stellar)Promoted

*   [What's a billboard?](/billboards)
*   [Manage preferences](/settings/customization#sponsors)

* * *

*   [Report billboard](/report-abuse?billboard=225974)

[[26: Image of Stellar post](width=350%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fi.imgur.com%2FFHXRlQs.png)](https://www.youtube.com/watch?v=FInE2PSx1es&amp;t=1s&amp;bb=225974)

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#how-a-hackathon-win-led-to-my-startup-getting-funded)[How a Hackathon Win Led to My Startup Getting Funded](https://www.youtube.com/watch?v=FInE2PSx1es&amp;t=1s&amp;bb=225974)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In this episode, you'll see:

*   The hackathon wins that sparked the journey.
*   The moment José and Joseph decided to go all-in.
*   Building a working prototype on Stellar.
*   Using the PassKeys feature of Soroban.
*   Getting funded via the Stellar Community Fund.

[Watch the video 🎥](https://www.youtube.com/watch?v=FInE2PSx1es&amp;t=1s&amp;bb=225974)

👋 Kindness is contagious

*   [What's a billboard?](/billboards)
*   [Manage preferences](/settings/customization#sponsors)

* * *

*   [Report billboard](/report-abuse?billboard=225474)

### [](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#dive-into-this-informative-piece-backed-by-our-vibrant-dev-community) Dive into this informative piece, backed by our vibrant DEV Community

**Whether you’re a novice or a pro**, your perspective enriches our collective insight.

A simple “thank you” can lift someone’s spirits—share your gratitude in the comments!

On DEV, **the power of shared knowledge paves a smoother path** and tightens our community ties. Found value here? A quick thanks to the author makes a big impact.

[](/evgeniiperminov/embeddings-clustering-with-agglomerative-hierarchical-clustering-messy-folder-reorganizer-ai-520k#-cta-httpsdevtoenterstatenewuser-)[Okay](/enter?state=new-user&amp;bb=225474)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

💎 DEV Diamond Sponsors

Thank you to our Diamond Sponsors for supporting the DEV Community

[[27: Neon - Official Database Partner](width=880%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbnl88cil6afxzmgwrgtt.png)](https://neon.tech/?ref=devto&amp;bb=146443)
Neon is the official database partner of DEV

[[28: Algolia - Official Search Partner](width=880%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fv30ephnolfvnlwgwm0yz.png)](https://www.algolia.com/developers/?utm_source=devto&amp;utm_medium=referral&amp;bb=146443)
Algolia is the official search partner of DEV

[DEV Community](/) — A space to discuss and keep up software development and manage your software career

*   [Home](/)
*   [DEV++](/++)
*   [Podcasts](/pod)
*   [Videos](/videos)
*   [Tags](/tags)
*   [DEV Help](/help)
*   [Forem Shop](https://shop.forem.com/)
*   [Advertise on DEV](/advertise)
*   [DEV Challenges](/challenges)
*   [DEV Showcase](/showcase)
*   [About](/about)
*   [Contact](/contact)
*   [Free Postgres Database](/free-postgres-database-tier)
*   [Software comparisons](/software-comparisons)

*   [Code of Conduct](/code-of-conduct)
*   [Privacy Policy](/privacy)
*   [Terms of use](/terms)

Built on [Forem](https://www.forem.com/) — the [open source](/t/opensource) software that powers [DEV](/) and other inclusive communities.

Made with love and [Ruby on Rails](/t/rails). DEV Community © 2016 - 2025.

[29: DEV Community](width=190,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)

We're a place where coders share, stay up-to-date and grow their careers.

[Log in](/enter)[Create account](/enter?state=new-user)

[30](sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)[31](multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)[32](exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)[33](raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)[34](fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)
</code></pre></div></div>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">sharing files with `miniserve` HTTP server and `netbird`</title><link href="https://ib.bsb.br/sharing-files-with-miniserve-http-server-and-netbird/" rel="alternate" type="text/html" title="sharing files with `miniserve` HTTP server and `netbird`" /><published>2025-05-18T00:00:00+00:00</published><updated>2025-05-18T13:08:07+00:00</updated><id>https://ib.bsb.br/sharing-files-with-miniserve-http-server-and-netbird</id><content type="html" xml:base="https://ib.bsb.br/sharing-files-with-miniserve-http-server-and-netbird/"><![CDATA[<h3 id="1-introduction-to-miniserve">1. Introduction to <code class="language-plaintext highlighter-rouge">miniserve</code></h3>

<p>As detailed in its <code class="language-plaintext highlighter-rouge">README.md</code>, <code class="language-plaintext highlighter-rouge">miniserve</code> is a “CLI tool to serve files and dirs over HTTP.” It’s designed to be small, self-contained, and cross-platform, making it ideal for quickly sharing files. Built in Rust, it offers good performance. Key features include serving single files or entire directories, MIME type handling, authentication, folder downloads, file uploading, directory creation, themes, QR code support, TLS, and read-only WebDAV support.</p>

<h3 id="2-prerequisites">2. Prerequisites</h3>

<ul>
  <li><strong>RK3588 Device</strong>: An ARM64 device (e.g., Orange Pi 5, Rock 5B).</li>
  <li><strong>Operating System</strong>: Debian Bullseye (or compatible) installed.</li>
  <li><strong>Basic Linux Knowledge</strong>: Command line, package installation, file editing.</li>
  <li><strong>Root/Sudo Access</strong>.</li>
  <li><strong>Internet Connectivity</strong>.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">netbird</code> Account</strong>: You’ll need a <code class="language-plaintext highlighter-rouge">netbird.io</code> account.</li>
</ul>

<hr />

<h3 id="3-phase-1-rk3588-debian-bullseye-preparation">3. Phase 1: RK3588 Debian Bullseye Preparation</h3>

<p>Ensure your Debian Bullseye system is ready.</p>

<h4 id="system-update">System Update</h4>
<p>Log in via SSH and run:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt full-upgrade <span class="nt">-y</span>
<span class="nb">sudo </span>apt autoremove <span class="nt">-y</span>
<span class="nb">sudo </span>apt clean
<span class="c"># Consider a reboot if kernel updates occurred: sudo reboot</span>
</code></pre></div></div>

<h4 id="essential-tools">Essential Tools</h4>
<p>Install common utilities if not already present:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> curl wget git ca-certificates gnupg
</code></pre></div></div>

<hr />

<h3 id="4-phase-2-netbird-installation-and-configuration">4. Phase 2: <code class="language-plaintext highlighter-rouge">netbird</code> Installation and Configuration</h3>

<p><code class="language-plaintext highlighter-rouge">netbird</code> creates a secure peer-to-peer VPN.</p>

<h4 id="install-netbird-client">Install <code class="language-plaintext highlighter-rouge">netbird</code> Client</h4>
<p>The recommended way to install <code class="language-plaintext highlighter-rouge">netbird</code> on Linux is using their installation script:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-fsSL</span> https://pkgs.netbird.io/install.sh | <span class="nb">sudo </span>bash
</code></pre></div></div>
<p>This script will typically detect your OS (Debian), add the necessary repository and GPG key, and install the <code class="language-plaintext highlighter-rouge">netbird</code> package.</p>

<p>Alternatively, for manual installation, refer to the <a href="https://netbird.io/docs/installation/overview">official Netbird documentation</a>, as steps can change.</p>

<h4 id="log-in-to-netbird">Log in to <code class="language-plaintext highlighter-rouge">netbird</code></h4>
<p>Once installed, connect your RK3588 to your <code class="language-plaintext highlighter-rouge">netbird</code> network:</p>

<ul>
  <li><strong>Method 1: Interactive Login (if you have easy browser access):</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>netbird up
</code></pre></div>    </div>
    <p>This will provide a URL. Open it in a browser on any device, log in to your <code class="language-plaintext highlighter-rouge">netbird</code> account, and authorize the RK3588.</p>
  </li>
  <li><strong>Method 2: Setup Key (Recommended for headless servers):</strong>
    <ol>
      <li>In your <code class="language-plaintext highlighter-rouge">netbird</code> admin dashboard (app.netbird.io), go to “Setup Keys.”</li>
      <li>Create a new key (reusable or one-time, as needed). Copy the generated key.</li>
      <li>On your RK3588, run:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>netbird up <span class="nt">--setup-key</span> YOUR_COPIED_SETUP_KEY
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ul>

<h4 id="verify-netbird-connection--identify-ip">Verify <code class="language-plaintext highlighter-rouge">netbird</code> Connection &amp; Identify IP</h4>
<p>Check the <code class="language-plaintext highlighter-rouge">netbird</code> status on your RK3588:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>netbird status
</code></pre></div></div>
<p>This command will show if the client is connected and will display its <code class="language-plaintext highlighter-rouge">netbird</code> IP address (e.g., <code class="language-plaintext highlighter-rouge">100.x.y.z</code>). Note this IP.
You can also use:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip addr show netbird0 <span class="c"># Or the interface name shown by 'netbird status'</span>
</code></pre></div></div>

<h4 id="enable-netbird-service">Enable <code class="language-plaintext highlighter-rouge">netbird</code> Service</h4>
<p>Ensure <code class="language-plaintext highlighter-rouge">netbird</code> starts on boot:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> netbird
</code></pre></div></div>

<hr />

<h3 id="5-phase-3-miniserve-installation-on-arm64">5. Phase 3: <code class="language-plaintext highlighter-rouge">miniserve</code> Installation on ARM64</h3>

<h4 id="option-a-using-pre-compiled-binaries-recommended">Option A: Using Pre-compiled Binaries (Recommended)</h4>
<p><code class="language-plaintext highlighter-rouge">miniserve</code> provides pre-built <code class="language-plaintext highlighter-rouge">aarch64-unknown-linux-musl</code> (statically linked, good portability) or <code class="language-plaintext highlighter-rouge">aarch64-unknown-linux-gnu</code> binaries.</p>

<ol>
  <li><strong>Find the Latest Release:</strong> Go to <code class="language-plaintext highlighter-rouge">https://github.com/svenstaro/miniserve/releases</code>.</li>
  <li><strong>Download the ARM64 Binary:</strong>
On your RK3588 (replace <code class="language-plaintext highlighter-rouge">&lt;VERSION&gt;</code> and adjust binary name if needed):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">VERSION</span><span class="o">=</span><span class="s2">"0.29.0"</span> <span class="c"># Check for the actual latest version!</span>
<span class="c"># Choose musl for static linking or gnu</span>
<span class="nv">BINARY_FILENAME</span><span class="o">=</span><span class="s2">"miniserve-</span><span class="k">${</span><span class="nv">VERSION</span><span class="k">}</span><span class="s2">-aarch64-unknown-linux-musl"</span>
<span class="c"># BINARY_FILENAME="miniserve-${VERSION}-aarch64-unknown-linux-gnu"</span>

<span class="nb">cd</span> /tmp
wget <span class="s2">"https://github.com/svenstaro/miniserve/releases/download/v</span><span class="k">${</span><span class="nv">VERSION</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">BINARY_FILENAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">-O</span> miniserve-arm64
</code></pre></div>    </div>
  </li>
  <li><strong>Make Executable and Install:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod</span> +x miniserve-arm64
<span class="nb">sudo mv </span>miniserve-arm64 /usr/local/bin/miniserve
</code></pre></div>    </div>
  </li>
  <li><strong>Verify:</strong> <code class="language-plaintext highlighter-rouge">miniserve --version</code></li>
</ol>

<h4 id="option-b-building-from-source-with-cargo">Option B: Building from Source with <code class="language-plaintext highlighter-rouge">cargo</code></h4>
<ol>
  <li><strong>Install Rust &amp; Build Tools:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--proto</span> <span class="s1">'=https'</span> <span class="nt">--tlsv1</span>.2 <span class="nt">-sSf</span> https://sh.rustup.rs | sh
<span class="nb">source</span> <span class="s2">"</span><span class="nv">$HOME</span><span class="s2">/.cargo/env"</span> <span class="c"># Or re-login/open new terminal</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> build-essential pkg-config libssl-dev <span class="c"># For GNU target</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Install <code class="language-plaintext highlighter-rouge">miniserve</code>:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cargo <span class="nb">install</span> <span class="nt">--locked</span> miniserve
</code></pre></div>    </div>
    <p>The binary will be in <code class="language-plaintext highlighter-rouge">$HOME/.cargo/bin/miniserve</code>. Ensure this is in your <code class="language-plaintext highlighter-rouge">PATH</code> or move the binary to <code class="language-plaintext highlighter-rouge">/usr/local/bin/</code>.</p>
  </li>
  <li><strong>Verify:</strong> <code class="language-plaintext highlighter-rouge">$HOME/.cargo/bin/miniserve --version</code> (or <code class="language-plaintext highlighter-rouge">miniserve --version</code> if in PATH).</li>
</ol>

<h4 id="option-c-using-docker">Option C: Using Docker</h4>
<p><code class="language-plaintext highlighter-rouge">miniserve</code> offers multi-arch Docker images.</p>
<ol>
  <li><strong>Install Docker:</strong>
The simplest way is often the convenience script:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-fsSL</span> https://get.docker.com <span class="nt">-o</span> get-docker.sh
<span class="nb">sudo </span>sh get-docker.sh
<span class="nb">sudo </span>usermod <span class="nt">-aG</span> docker your_user <span class="c"># Add your user to docker group, re-login after</span>
</code></pre></div>    </div>
    <p>Alternatively, follow manual instructions from the <a href="https://docs.docker.com/engine/install/debian/">Docker Debian documentation</a>.</p>
  </li>
  <li><strong>Run <code class="language-plaintext highlighter-rouge">miniserve</code>:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Replace /host/path/to/share with the actual path on your RK3588</span>
docker run <span class="nt">-d</span> <span class="nt">--name</span> my-miniserve <span class="se">\</span>
  <span class="nt">-v</span> /host/path/to/share:/data:ro <span class="se">\</span>
  <span class="nt">-p</span> &lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;:8080 <span class="se">\</span>
  <span class="nt">--restart</span> unless-stopped <span class="se">\</span>
  docker.io/svenstaro/miniserve /data <span class="nt">-p</span> 8080
</code></pre></div>    </div>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">-d</code>: Run detached.</li>
      <li><code class="language-plaintext highlighter-rouge">--name my-miniserve</code>: Name the container.</li>
      <li><code class="language-plaintext highlighter-rouge">-v /host/path/to/share:/data:ro</code>: Mounts your host directory read-only (<code class="language-plaintext highlighter-rouge">:ro</code>) into the container. Remove <code class="language-plaintext highlighter-rouge">:ro</code> for uploads.</li>
      <li><code class="language-plaintext highlighter-rouge">-p &lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;:8080</code>: Binds <code class="language-plaintext highlighter-rouge">miniserve</code>’s port <code class="language-plaintext highlighter-rouge">8080</code> inside the container to a specific <code class="language-plaintext highlighter-rouge">&lt;PORT&gt;</code> on your RK3588’s <code class="language-plaintext highlighter-rouge">&lt;YOUR_NETBIRD_IP&gt;</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">--restart unless-stopped</code>: For persistence.</li>
      <li><code class="language-plaintext highlighter-rouge">/data -p 8080</code>: Tells <code class="language-plaintext highlighter-rouge">miniserve</code> inside the container to serve <code class="language-plaintext highlighter-rouge">/data</code> on port <code class="language-plaintext highlighter-rouge">8080</code>.</li>
      <li><strong>Note on Volume Permissions:</strong> If <code class="language-plaintext highlighter-rouge">miniserve</code> inside Docker (often runs as non-root) can’t read <code class="language-plaintext highlighter-rouge">/host/path/to/share</code>, ensure the host path has appropriate read permissions for the user/group ID <code class="language-plaintext highlighter-rouge">miniserve</code> runs as in the container, or explore Docker’s user mapping options.</li>
    </ul>
  </li>
</ol>

<p><em>(The rest of this tutorial focuses on non-Docker systemd setup for <code class="language-plaintext highlighter-rouge">miniserve</code>)</em></p>

<hr />

<h3 id="6-phase-4-running-miniserve">6. Phase 4: Running <code class="language-plaintext highlighter-rouge">miniserve</code></h3>

<h4 id="create-a-directory-to-serve">Create a Directory to Serve</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /srv/miniserve_data <span class="c"># Or your preferred location</span>
<span class="nb">echo</span> <span class="s2">"Hello from miniserve on RK3588 via Netbird!"</span> <span class="o">&gt;</span> /srv/miniserve_data/index.html
<span class="nb">sudo chown</span> <span class="nt">-R</span> your_user:your_user /srv/miniserve_data <span class="c"># Change 'your_user' if needed</span>
</code></pre></div></div>

<h4 id="manual-test-run-binding-to-netbird-ip">Manual Test Run (Binding to <code class="language-plaintext highlighter-rouge">netbird</code> IP)</h4>
<p>Replace <code class="language-plaintext highlighter-rouge">&lt;YOUR_NETBIRD_IP&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;PORT&gt;</code> (e.g., 8080).</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>miniserve <span class="nt">-i</span> &lt;YOUR_NETBIRD_IP&gt; <span class="nt">-p</span> &lt;PORT&gt; /srv/miniserve_data
</code></pre></div></div>
<p>Test access from another <code class="language-plaintext highlighter-rouge">netbird</code> device: <code class="language-plaintext highlighter-rouge">http://&lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;</code>. Press <code class="language-plaintext highlighter-rouge">CTRL+C</code> to stop.</p>

<h4 id="setting-up-miniserve-as-a-systemd-service">Setting up <code class="language-plaintext highlighter-rouge">miniserve</code> as a <code class="language-plaintext highlighter-rouge">systemd</code> Service</h4>

<ol>
  <li><strong>Obtain and Place the <code class="language-plaintext highlighter-rouge">systemd</code> Unit File:</strong>
Create <code class="language-plaintext highlighter-rouge">/etc/systemd/system/miniserve@.service</code> (ensure <code class="language-plaintext highlighter-rouge">miniserve</code> binary path is correct):
    <div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span>
<span class="py">Description</span><span class="p">=</span><span class="s">miniserve for %i</span>
<span class="py">After</span><span class="p">=</span><span class="s">network-online.target netbird.service</span>
<span class="py">Wants</span><span class="p">=</span><span class="s">network-online.target netbird.service</span>

<span class="nn">[Service]</span>
<span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/local/bin/miniserve -- %I # Verify this path to miniserve</span>

<span class="c"># Security Hardening
</span><span class="py">IPAccounting</span><span class="p">=</span><span class="s">yes</span>
<span class="c"># IPAddressAllow/Deny are removed here as miniserve will bind to a specific IP.
# If miniserve were to bind 0.0.0.0, you'd use IPAddressAllow for Netbird subnet.
</span><span class="py">DynamicUser</span><span class="p">=</span><span class="s">yes</span>
<span class="py">PrivateTmp</span><span class="p">=</span><span class="s">yes</span>
<span class="py">PrivateUsers</span><span class="p">=</span><span class="s">yes</span>
<span class="py">PrivateDevices</span><span class="p">=</span><span class="s">yes</span>
<span class="py">NoNewPrivileges</span><span class="p">=</span><span class="s">true</span>
<span class="py">ProtectSystem</span><span class="p">=</span><span class="s">strict</span>
<span class="py">ProtectHome</span><span class="p">=</span><span class="s">read-only # Change to 'no' or use a dedicated user if serving from /home</span>
<span class="py">ProtectClock</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectControlGroups</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectKernelLogs</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectKernelModules</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectKernelTunables</span><span class="p">=</span><span class="s">yes</span>
<span class="py">ProtectProc</span><span class="p">=</span><span class="s">invisible</span>
<span class="py">CapabilityBoundingSet</span><span class="p">=</span><span class="s">CAP_NET_BIND_SERVICE CAP_DAC_READ_SEARCH</span>

<span class="nn">[Install]</span>
<span class="py">WantedBy</span><span class="p">=</span><span class="s">multi-user.target</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Dedicated User for <code class="language-plaintext highlighter-rouge">miniserve</code> (Optional but Recommended):</strong>
If <code class="language-plaintext highlighter-rouge">DynamicUser=yes</code> is problematic for permissions, or you prefer explicit control:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>groupadd <span class="nt">--system</span> miniserve-runner
<span class="nb">sudo </span>useradd <span class="nt">--system</span> <span class="nt">-g</span> miniserve-runner <span class="nt">-d</span> /var/empty <span class="nt">-s</span> /bin/false miniserve-runner
</code></pre></div>    </div>
    <p>Then, in your systemd override (next step), you’ll add <code class="language-plaintext highlighter-rouge">User=miniserve-runner</code> and <code class="language-plaintext highlighter-rouge">Group=miniserve-runner</code>.</p>
  </li>
  <li><strong>Determine and Escape the Path to Serve:</strong>
Example: For <code class="language-plaintext highlighter-rouge">/srv/miniserve_data</code>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemd-escape /srv/miniserve_data
</code></pre></div>    </div>
    <p>This might output <code class="language-plaintext highlighter-rouge">srv-miniserve_data</code>. This is your <code class="language-plaintext highlighter-rouge">&lt;escaped-path&gt;</code>.</p>
  </li>
  <li><strong>Create <code class="language-plaintext highlighter-rouge">systemd</code> Override File for Custom Options:</strong>
Use <code class="language-plaintext highlighter-rouge">sudo systemctl edit miniserve@&lt;escaped-path&gt;.service</code>. Add:
    <div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Service]</span>
<span class="c"># If using a dedicated user:
# User=miniserve-runner
# Group=miniserve-runner
</span>
<span class="c"># Clear default ExecStart
</span><span class="py">ExecStart</span><span class="p">=</span>
<span class="c"># Set our custom ExecStart. Replace &lt;YOUR_NETBIRD_IP&gt; and &lt;PORT&gt;.
# Add other miniserve flags as needed (e.g., --auth, -u, --tls-cert).
</span><span class="py">ExecStart</span><span class="p">=</span><span class="s">/usr/local/bin/miniserve -i &lt;YOUR_NETBIRD_IP&gt; -p &lt;PORT&gt; --title "RK3588 Files" -- %I</span>
</code></pre></div>    </div>
    <p>Ensure <code class="language-plaintext highlighter-rouge">/usr/local/bin/miniserve</code> is the correct absolute path to your <code class="language-plaintext highlighter-rouge">miniserve</code> binary.</p>
  </li>
  <li><strong>File System Permissions for the Served Directory:</strong>
The user <code class="language-plaintext highlighter-rouge">miniserve</code> runs as (either dynamic or <code class="language-plaintext highlighter-rouge">miniserve-runner</code>) needs read access to <code class="language-plaintext highlighter-rouge">/srv/miniserve_data</code> and its contents.
If using <code class="language-plaintext highlighter-rouge">miniserve-runner</code>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chown</span> <span class="nt">-R</span> miniserve-runner:miniserve-runner /srv/miniserve_data
<span class="nb">sudo chmod</span> <span class="nt">-R</span> <span class="nv">u</span><span class="o">=</span>rX,g<span class="o">=</span>rX,o-rwx /srv/miniserve_data <span class="c"># Read/execute for user/group</span>
</code></pre></div>    </div>
    <p>If <code class="language-plaintext highlighter-rouge">DynamicUser=yes</code> and serving from outside standard system paths (like <code class="language-plaintext highlighter-rouge">/srv</code>), you might need <code class="language-plaintext highlighter-rouge">setfacl</code> for more granular permissions if <code class="language-plaintext highlighter-rouge">chown</code> is not desired, or ensure the path is world-readable (less secure).</p>
  </li>
  <li><strong>Reload <code class="language-plaintext highlighter-rouge">systemd</code>, Enable, and Start the Service:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl daemon-reload
<span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> miniserve@&lt;escaped-path&gt;.service
</code></pre></div>    </div>
  </li>
  <li><strong>Check Service Status:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl status miniserve@&lt;escaped-path&gt;.service
<span class="nb">sudo </span>journalctl <span class="nt">-u</span> miniserve@&lt;escaped-path&gt;.service <span class="nt">-f</span>
</code></pre></div>    </div>
  </li>
</ol>

<hr />

<h3 id="7-phase-5-accessing-miniserve-and-other-services-via-netbird">7. Phase 5: Accessing <code class="language-plaintext highlighter-rouge">miniserve</code> and Other Services via <code class="language-plaintext highlighter-rouge">netbird</code></h3>

<h4 id="accessing-miniserve">Accessing <code class="language-plaintext highlighter-rouge">miniserve</code></h4>
<p>From another device on your <code class="language-plaintext highlighter-rouge">netbird</code> network, use your browser:
<code class="language-plaintext highlighter-rouge">http://&lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;</code> (or <code class="language-plaintext highlighter-rouge">https://</code> if you configured TLS).</p>

<h4 id="accessing-ssh-ftp-etc">Accessing SSH, FTP, etc.</h4>
<p>Services like SSH on your RK3588 are now accessible via its <code class="language-plaintext highlighter-rouge">netbird</code> IP from other peers in your <code class="language-plaintext highlighter-rouge">netbird</code> network:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh your_rk3588_user@&lt;YOUR_NETBIRD_IP&gt;
</code></pre></div></div>
<p>Similarly for FTP or other services configured to listen on the <code class="language-plaintext highlighter-rouge">netbird</code> IP or <code class="language-plaintext highlighter-rouge">0.0.0.0</code>.</p>

<hr />

<h3 id="8-phase-6-advanced-miniserve-configuration--security">8. Phase 6: Advanced <code class="language-plaintext highlighter-rouge">miniserve</code> Configuration &amp; Security</h3>

<h4 id="key-miniserve-features">Key <code class="language-plaintext highlighter-rouge">miniserve</code> Features</h4>
<p>Consult <code class="language-plaintext highlighter-rouge">miniserve --help</code> and the <code class="language-plaintext highlighter-rouge">README.md</code>.</p>
<ul>
  <li><strong>Authentication:</strong> <code class="language-plaintext highlighter-rouge">--auth username:password</code> or <code class="language-plaintext highlighter-rouge">--auth-file /path/auth.txt</code>. Highly recommended.
Example for systemd override: <code class="language-plaintext highlighter-rouge">ExecStart=... --auth "admin:$(openssl passwd -1 'securepass')" -- %I</code></li>
  <li><strong>TLS (HTTPS):</strong> <code class="language-plaintext highlighter-rouge">--tls-cert /path/cert.pem --tls-key /path/key.pem</code>.
See “TLS Certificate Generation with SAN” below.</li>
  <li><strong>File Uploads:</strong> <code class="language-plaintext highlighter-rouge">-u</code> or <code class="language-plaintext highlighter-rouge">-u /allowed/subdir</code>. Use with caution regarding permissions.</li>
  <li><strong>Directory Creation:</strong> <code class="language-plaintext highlighter-rouge">--mkdir</code> (requires uploads).</li>
  <li><strong>WebDAV:</strong> <code class="language-plaintext highlighter-rouge">--enable-webdav</code> for read-only WebDAV access. This can be a good alternative to FTP for file management over HTTP.</li>
</ul>

<h4 id="tls-certificate-generation-with-san">TLS Certificate Generation with SAN</h4>
<p>For self-signed certificates to work well with modern browsers (even over <code class="language-plaintext highlighter-rouge">netbird</code>), include a Subject Alternative Name (SAN) for the IP.</p>
<ol>
  <li>Create a directory for certs: <code class="language-plaintext highlighter-rouge">sudo mkdir -p /etc/miniserve/tls; cd /etc/miniserve/tls</code></li>
  <li>Generate key and cert (replace <code class="language-plaintext highlighter-rouge">&lt;YOUR_NETBIRD_IP&gt;</code>):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>openssl req <span class="nt">-x509</span> <span class="nt">-newkey</span> rsa:4096 <span class="nt">-keyout</span> key.pem <span class="nt">-out</span> cert.pem <span class="se">\</span>
  <span class="nt">-sha256</span> <span class="nt">-days</span> 3650 <span class="nt">-nodes</span> <span class="se">\</span>
  <span class="nt">-subj</span> <span class="s2">"/CN=&lt;YOUR_NETBIRD_IP&gt;"</span> <span class="se">\</span>
  <span class="nt">-addext</span> <span class="s2">"subjectAltName = IP:&lt;YOUR_NETBIRD_IP&gt;"</span>
</code></pre></div>    </div>
  </li>
  <li>Set permissions (if <code class="language-plaintext highlighter-rouge">miniserve</code> runs as <code class="language-plaintext highlighter-rouge">miniserve-runner</code>):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chown</span> <span class="nt">-R</span> miniserve-runner:miniserve-runner /etc/miniserve/tls
<span class="nb">sudo chmod </span>600 /etc/miniserve/tls/key.pem
<span class="nb">sudo chmod </span>644 /etc/miniserve/tls/cert.pem
</code></pre></div>    </div>
  </li>
  <li>Update your systemd override <code class="language-plaintext highlighter-rouge">ExecStart</code> with:
<code class="language-plaintext highlighter-rouge">--tls-cert /etc/miniserve/tls/cert.pem --tls-key /etc/miniserve/tls/key.pem</code></li>
  <li>Access via <code class="language-plaintext highlighter-rouge">https://&lt;YOUR_NETBIRD_IP&gt;:&lt;PORT&gt;</code>. You’ll need to accept the self-signed certificate warning in your browser.</li>
</ol>

<h4 id="firewall-ufw-on-rk3588">Firewall (UFW on RK3588)</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> ufw
<span class="nb">sudo </span>ufw default deny incoming
<span class="nb">sudo </span>ufw default allow outgoing
<span class="nb">sudo </span>ufw allow ssh <span class="c"># Essential for remote access</span>
<span class="c"># Allow miniserve port ONLY from Netbird interface (e.g., netbird0)</span>
<span class="nb">sudo </span>ufw allow <span class="k">in </span>on netbird0 to any port &lt;PORT&gt; proto tcp 
<span class="nb">sudo </span>ufw <span class="nb">enable
sudo </span>ufw status verbose
</code></pre></div></div>

<h4 id="netbird-access-controls"><code class="language-plaintext highlighter-rouge">netbird</code> Access Controls</h4>
<p>Utilize <code class="language-plaintext highlighter-rouge">netbird</code>’s dashboard to create access policies, restricting which peers can connect to your RK3588 and on which ports/protocols for fine-grained security.</p>

<h4 id="regular-updates">Regular Updates</h4>
<p>Keep Debian, <code class="language-plaintext highlighter-rouge">netbird</code>, and <code class="language-plaintext highlighter-rouge">miniserve</code> updated.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt full-upgrade <span class="nt">-y</span>
<span class="c"># For miniserve from cargo: cargo install --locked miniserve</span>
<span class="c"># For Netbird: usually updates via apt if repo was added.</span>
</code></pre></div></div>

<hr />

<h3 id="9-phase-7-troubleshooting">9. Phase 7: Troubleshooting</h3>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">miniserve</code> Service Issues:</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">sudo systemctl status miniserve@&lt;escaped-path&gt;.service</code></li>
      <li><code class="language-plaintext highlighter-rouge">sudo journalctl -u miniserve@&lt;escaped-path&gt;.service -n 100 --no-pager --follow</code></li>
      <li>Check <code class="language-plaintext highlighter-rouge">miniserve</code> binary path and permissions.</li>
      <li>Manually run the <code class="language-plaintext highlighter-rouge">ExecStart</code> command from the systemd unit (as the correct user if specified) to see direct errors.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">netbird</code> Connectivity:</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">sudo netbird status</code> on all relevant peers.</li>
      <li>Ping <code class="language-plaintext highlighter-rouge">netbird</code> IPs between peers.</li>
      <li>Check <code class="language-plaintext highlighter-rouge">netbird</code> admin dashboard for peer status and access rules.</li>
    </ul>
  </li>
  <li><strong>Firewall:</strong>
    <ul>
      <li>Temporarily <code class="language-plaintext highlighter-rouge">sudo ufw disable</code> to isolate firewall issues. If it works, your UFW rules need adjustment. Re-enable UFW.</li>
      <li>Check UFW logs: <code class="language-plaintext highlighter-rouge">sudo less /var/log/ufw.log</code>.</li>
    </ul>
  </li>
  <li><strong>Permissions:</strong> Double-check that the user <code class="language-plaintext highlighter-rouge">miniserve</code> runs as has read (and write, if uploads enabled) permissions for the target directories.</li>
</ul>]]></content><author><name></name></author><category term="scratchpad" /></entry><entry><title type="html">bash script to setup ‘wifish’ on Linux</title><link href="https://ib.bsb.br/bash-script-to-setup-wifish-on-linux/" rel="alternate" type="text/html" title="bash script to setup ‘wifish’ on Linux" /><published>2025-05-17T00:00:00+00:00</published><updated>2025-05-17T22:44:46+00:00</updated><id>https://ib.bsb.br/bash-script-to-setup-wifish-on-linux</id><content type="html" xml:base="https://ib.bsb.br/bash-script-to-setup-wifish-on-linux/"><![CDATA[<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="#!/bin/bash
# This script automates the setup of &#39;wifish&#39; on a Debian Bullseye system.
# Version 2: Improved backup, idempotency, and interface detection.
#
# IMPORTANT: This script makes certain assumptions:
# 1. It should be run as root.
# 2. It attempts to auto-detect the Wi-Fi interface. If detection fails or is
#    incorrect, the default &quot;wlan0&quot; is used, or you may need to edit the script
#    or use wifish with the -i flag.
# 3. The active wpa_supplicant configuration file is assumed to be
#    &#39;/etc/wpa_supplicant/wpa_supplicant.conf&#39;. If different, this needs to be changed.
# 4. It will attempt to identify the primary non-root user (UID 1000 or SUDO_USER)
#    to add to the &#39;netdev&#39; group.
#
# Review these assumptions and the script content before execution.

set -e # Exit immediately if a command exits with a non-zero status.

# --- Configuration ---
DEFAULT_WIFI_INTERFACE=&quot;wlan0&quot;
WPA_SUPPLICANT_CONF_FILE=&quot;/etc/wpa_supplicant/wpa_supplicant.conf&quot;
WIFI_INTERFACE=&quot;&quot; # Will be auto-detected or fall back to DEFAULT_WIFI_INTERFACE

# Determine the primary non-root user to add to the &#39;netdev&#39; group.
PRIMARY_USER=&quot;&quot;
if [ -n &quot;$SUDO_USER&quot; ] &amp;&amp; [ &quot;$SUDO_USER&quot; != &quot;root&quot; ]; then # Prefer SUDO_USER if available and not root
    PRIMARY_USER=&quot;$SUDO_USER&quot;
elif command -v id &gt;/dev/null &amp;&amp; id -u 1000 &amp;&gt;/dev/null; then # Try UID 1000
    PRIMARY_USER_UID1000=$(id -un 1000)
    if [ &quot;$PRIMARY_USER_UID1000&quot; != &quot;root&quot; ]; then # Ensure user 1000 is not root
        PRIMARY_USER=&quot;$PRIMARY_USER_UID1000&quot;
    fi
fi
# Fallback to parsing /etc/passwd if PRIMARY_USER is still not set
if [ -z &quot;$PRIMARY_USER&quot; ]; then
    PRIMARY_USER=$(awk -F: &#39;($3&gt;=1000 &amp;&amp; $1 != &quot;root&quot; &amp;&amp; $1 != &quot;&quot; &amp;&amp; $1 != &quot;nobody&quot;){print $1; exit}&#39; /etc/passwd)
fi


# --- Helper Functions ---
log_action() {
    echo &quot;[INFO] $1&quot;
}

log_warning() {
    echo &quot;[WARNING] $1&quot;
}

log_error() {
    echo &quot;[ERROR] $1&quot; &gt;&amp;2
}

# --- Wi-Fi Interface Detection ---
detect_wifi_interface() {
    log_action &quot;Attempting to auto-detect Wi-Fi interface...&quot;
    local detected_iface=&quot;&quot;
    # Try with &#39;iw dev&#39; first, common for Wi-Fi specific tools
    if command -v iw &gt;/dev/null; then
        detected_iface=$(iw dev | awk &#39;$1==&quot;Interface&quot;{print $2; exit}&#39;)
    fi

    # If &#39;iw dev&#39; didn&#39;t yield a result, try with &#39;ip link&#39; for wlan*
    if [ -z &quot;$detected_iface&quot; ] &amp;&amp; command -v ip &gt;/dev/null; then
        detected_iface=$(ip -o link show type wlan | awk -F&#39;: &#39; &#39;{print $2; exit}&#39; | awk &#39;{print $1}&#39;)
    fi

    # If still no result, try &#39;ip link&#39; for wlp* (common alternative naming)
    if [ -z &quot;$detected_iface&quot; ] &amp;&amp; command -v ip &gt;/dev/null; then
        detected_iface=$(ip -o link show | grep -Eo &#39;wlp[0-9]+s[0-9]+&#39; | head -n1)
    fi

    if [ -n &quot;$detected_iface&quot; ]; then
        WIFI_INTERFACE=&quot;$detected_iface&quot;
        log_action &quot;Detected Wi-Fi interface: $WIFI_INTERFACE. Using this.&quot;
    else
        WIFI_INTERFACE=&quot;$DEFAULT_WIFI_INTERFACE&quot;
        log_warning &quot;Could not auto-detect Wi-Fi interface. Using default: $WIFI_INTERFACE.&quot;
        log_warning &quot;If this is incorrect, review script or use wifish with the &#39;-i &lt;your_interface&gt;&#39; option.&quot;
    fi
}


# --- Main Script ---

# Check if running as root
if [ &quot;$(id -u)&quot; -ne 0 ]; then
  log_error &quot;This script must be run as root. Please use &#39;sudo $0&#39; or run as the root user.&quot;
  exit 1
fi

detect_wifi_interface # Call detection function

log_action &quot;Starting wifish setup script (v2)...&quot;
log_action &quot;Using Wi-Fi Interface: ${WIFI_INTERFACE}&quot;
log_action &quot;Targeting wpa_supplicant config: ${WPA_SUPPLICANT_CONF_FILE}&quot;

if [ -n &quot;$PRIMARY_USER&quot; ]; then
    log_action &quot;Primary non-root user identified for &#39;netdev&#39; group: ${PRIMARY_USER}&quot;
else
    log_warning &quot;Could not automatically detect a primary non-root user. Manual &#39;usermod -a -G netdev youruser&#39; might be required.&quot;
fi
echo &quot;---------------------------------------------------------------------&quot;

# Phase 1: Prerequisites and Getting wifish
log_action &quot;Phase 1: Installing prerequisites and getting wifish...&quot;
log_action &quot;Updating package lists (apt update)...&quot;
if ! apt update -qq; then
    log_error &quot;apt update failed. Please check your network connection and apt sources.&quot;
    exit 1
fi

log_action &quot;Installing gawk, dialog, git...&quot;
if ! apt install -y gawk dialog git; then
    log_error &quot;Failed to install required packages. Please check apt output for errors.&quot;
    exit 1
fi

TEMP_WIFISH_DIR=&quot;/tmp/bougyman-wifish_install_temp_$(date +%Y%m%d%H%M%S)&quot;
log_action &quot;Cloning wifish repository to $TEMP_WIFISH_DIR...&quot;
rm -rf &quot;$TEMP_WIFISH_DIR&quot; # Clean up if exists
if ! git clone https://github.com/bougyman/wifish.git &quot;$TEMP_WIFISH_DIR&quot;; then
    log_error &quot;Failed to clone wifish repository. Check internet connection and git.&quot;
    exit 1
fi

log_action &quot;Setting execute permissions for scripts in repository...&quot;
(
    cd &quot;$TEMP_WIFISH_DIR&quot;
    chmod +x wifish install.sh test/test.sh
    chmod +x sv/wpa_supplicant/run sv/wpa_supplicant/log/run
)
log_action &quot;Phase 1 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 2: Configuring wpa_supplicant
log_action &quot;Phase 2: Configuring wpa_supplicant...&quot;
log_action &quot;Ensuring wpa_supplicant configuration file directory exists: $(dirname &quot;$WPA_SUPPLICANT_CONF_FILE&quot;)&quot;
mkdir -p &quot;$(dirname &quot;$WPA_SUPPLICANT_CONF_FILE&quot;)&quot;

if [ ! -f &quot;$WPA_SUPPLICANT_CONF_FILE&quot; ]; then
    log_action &quot;Creating empty wpa_supplicant configuration file: $WPA_SUPPLICANT_CONF_FILE&quot;
    touch &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
fi

TIMESTAMP=$(date +%Y%m%d%H%M%S)
BACKUP_FILE=&quot;${WPA_SUPPLICANT_CONF_FILE}.${TIMESTAMP}.bak&quot;
log_action &quot;Backing up current $WPA_SUPPLICANT_CONF_FILE to $BACKUP_FILE...&quot;
if cp &quot;$WPA_SUPPLICANT_CONF_FILE&quot; &quot;$BACKUP_FILE&quot;; then
    log_action &quot;Backup successful: $BACKUP_FILE&quot;
else
    log_warning &quot;Failed to create backup of $WPA_SUPPLICANT_CONF_FILE. Proceeding with caution.&quot;
fi

log_action &quot;Configuring &#39;ctrl_interface&#39; and &#39;update_config&#39; in $WPA_SUPPLICANT_CONF_FILE for idempotency...&quot;

# Comment out any existing ctrl_interface lines to avoid conflicts
sed -i -E &#39;s/^[[:space:]]*ctrl_interface=.*$/#&amp; (old_ctrl_interface, commented by wifish_setup.sh)/&#39; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
# Add the correct ctrl_interface line if it&#39;s not already present (uncommented)
if ! grep -qFx &quot;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&quot; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;; then
    log_action &quot;Adding &#39;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&#39; to $WPA_SUPPLICANT_CONF_FILE.&quot;
    echo &quot;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&quot; &gt;&gt; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
else
    log_action &quot;&#39;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&#39; already present or re-added.&quot;
fi

# Comment out any existing update_config lines
sed -i -E &#39;s/^[[:space:]]*update_config=.*$/#&amp; (old_update_config, commented by wifish_setup.sh)/&#39; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
# Add the correct update_config line if it&#39;s not already present (uncommented)
if ! grep -qFx &quot;update_config=1&quot; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;; then
    log_action &quot;Adding &#39;update_config=1&#39; to $WPA_SUPPLICANT_CONF_FILE.&quot;
    echo &quot;update_config=1&quot; &gt;&gt; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
else
    log_action &quot;&#39;update_config=1&#39; already present or re-added.&quot;
fi

log_action &quot;Securing $WPA_SUPPLICANT_CONF_FILE permissions (chmod 600)...&quot;
chmod 600 &quot;$WPA_SUPPLICANT_CONF_FILE&quot;

USER_MODIFIED_LOGIN_MESSAGE=&quot;&quot;
if [ -n &quot;$PRIMARY_USER&quot; ]; then
    log_action &quot;Ensuring &#39;netdev&#39; group exists and adding user &#39;$PRIMARY_USER&#39; to it...&quot;
    if ! getent group netdev &gt;/dev/null; then
        log_action &quot;Group &#39;netdev&#39; does not exist. Creating it...&quot;
        if ! groupadd --system netdev; then # Use --system for system groups if appropriate
            log_warning &quot;Could not create group &#39;netdev&#39;. Manual creation (groupadd netdev) might be needed.&quot;
        else
            log_action &quot;Group &#39;netdev&#39; created.&quot;
        fi
    fi
    # Check if user is already in group to avoid unnecessary usermod message
    if ! groups &quot;$PRIMARY_USER&quot; | grep -q &#39;\bnetdev\b&#39;; then
        if ! usermod -a -G netdev &quot;$PRIMARY_USER&quot;; then
             log_warning &quot;Failed to add user &#39;$PRIMARY_USER&#39; to &#39;netdev&#39; group. Check permissions or do it manually.&quot;
        else
            USER_MODIFIED_LOGIN_MESSAGE=&quot;User &#39;$PRIMARY_USER&#39; has been added to the &#39;netdev&#39; group. IMPORTANT: &#39;$PRIMARY_USER&#39; MUST log out and log back in for this change to take effect.&quot;
            log_action &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
        fi
    else
        log_action &quot;User &#39;$PRIMARY_USER&#39; is already a member of the &#39;netdev&#39; group.&quot;
        USER_MODIFIED_LOGIN_MESSAGE=&quot;User &#39;$PRIMARY_USER&#39; is a member of &#39;netdev&#39;. If this membership is recent, a logout/login might still be needed for all services to recognize it.&quot;

    fi
else
    USER_MODIFIED_LOGIN_MESSAGE=&quot;IMPORTANT: Could not automatically determine a primary non-root user. Please manually add your regular user to the &#39;netdev&#39; group (e.g., &#39;sudo usermod -a -G netdev yourusername&#39;) and then log out and log back in.&quot;
    log_warning &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
fi

log_action &quot;Attempting to restart wpa_supplicant for interface &#39;$WIFI_INTERFACE&#39;...&quot;
WPA_RESTARTED_MANUALLY_MSG=&quot;If wpa_supplicant was restarted, it should pick up the new configuration. If not, a manual restart of wpa_supplicant or a system reboot might be necessary.&quot;

SERVICE_NAME=&quot;wpa_supplicant@${WIFI_INTERFACE}.service&quot;
SERVICE_EXISTS=false
if systemctl list-unit-files | grep -q &quot;^${SERVICE_NAME}&quot;; then # More precise grep
    SERVICE_EXISTS=true
fi

if [ &quot;$SERVICE_EXISTS&quot; = true ]; then
    log_action &quot;${SERVICE_NAME} found.&quot;
    if systemctl is-active --quiet &quot;${SERVICE_NAME}&quot;; then
        log_action &quot;Restarting ${SERVICE_NAME} via systemd...&quot;
        if ! systemctl restart &quot;${SERVICE_NAME}&quot;; then
            log_warning &quot;Failed to restart ${SERVICE_NAME}. Check &#39;systemctl status ${SERVICE_NAME}&#39; and &#39;journalctl -u ${SERVICE_NAME}&#39;.&quot;
        fi
    else
        log_action &quot;${SERVICE_NAME} exists but is not active. Attempting to enable and start it...&quot;
        if ! systemctl enable &quot;${SERVICE_NAME}&quot; --now; then
             log_warning &quot;Failed to enable and start ${SERVICE_NAME}. Check status and journal.&quot;
        fi
    fi
elif [ -f &quot;/etc/network/interfaces&quot; ] &amp;&amp; grep -q &quot;iface ${WIFI_INTERFACE} inet&quot; /etc/network/interfaces; then # More specific grep
    log_action &quot;Attempting to restart network interface ${WIFI_INTERFACE} via ifdown/ifup...&quot;
    ifdown &quot;${WIFI_INTERFACE}&quot; &gt;/dev/null 2&gt;&amp;1 || true # Ignore errors if already down
    if ! ifup &quot;${WIFI_INTERFACE}&quot;; then
        log_warning &quot;ifup ${WIFI_INTERFACE} failed. Manual network reconfiguration might be needed. Check /etc/network/interfaces configuration.&quot;
    fi
else
    WPA_RESTARTED_MANUALLY_MSG=&quot;Could not determine how wpa_supplicant is managed for &#39;${WIFI_INTERFACE}&#39; (no specific systemd service or /etc/network/interfaces entry found). A manual restart of wpa_supplicant or a system reboot might be necessary to apply configuration changes.&quot;
    log_warning &quot;$WPA_RESTARTED_MANUALLY_MSG&quot;
fi
log_action &quot;Phase 2 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 3: Implementing wifish (System-Wide Installation)
log_action &quot;Phase 3: Installing wifish system-wide...&quot;
(
    cd &quot;$TEMP_WIFISH_DIR&quot;
    log_action &quot;Running install.sh from wifish repository (current directory: $(pwd))...&quot;
    if ! ./install.sh; then
        log_error &quot;wifish install.sh script failed. Please check output for errors.&quot;
        log_action &quot;Cleaning up temporary directory $TEMP_WIFISH_DIR...&quot;
        rm -rf &quot;$TEMP_WIFISH_DIR&quot;
        exit 1
    fi
)
log_action &quot;wifish&#39;s install.sh completed.&quot;
if [ -f &quot;$TEMP_WIFISH_DIR/test/test.sh&quot; ]; then
    log_action &quot;The wifish repository includes a test script: $TEMP_WIFISH_DIR/test/test.sh&quot;
    log_action &quot;You can explore running it manually from the &#39;$TEMP_WIFISH_DIR&#39; directory if desired (e.g., &#39;./test/test.sh&#39;).&quot;
    log_action &quot;Note: This test script typically mocks wpa_cli and runs its own checks.&quot;
fi
log_action &quot;Phase 3 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 4: Using wifish (Information for the user)
log_action &quot;Phase 4: Information on using wifish...&quot;
log_action &quot;wifish should now be installed and available as the &#39;wifish&#39; command.&quot;
log_action &quot;Example usage (as the non-root user, AFTER they have logged back in if their group membership was changed):&quot;
log_action &quot;  wifish list&quot;
log_action &quot;  wifish menu&quot;
log_action &quot;  wifish connect \&quot;Your_Network_SSID\&quot;&quot;
log_action &quot; &quot;
log_action &quot;If wifish needs to target a specific interface (detected/defaulted to &#39;${WIFI_INTERFACE}&#39;), use:&quot;
log_action &quot;  wifish -i ${WIFI_INTERFACE} menu&quot;
log_action &quot;or set the environment variable for the session:&quot;
log_action &quot;  export WPA_CLI_INTERFACE=${WIFI_INTERFACE}  (unset with &#39;unset WPA_CLI_INTERFACE&#39;)&quot;
log_action &quot; &quot;
log_action &quot;IMPORTANT NOTE ON IP ADDRESS:&quot;
log_action &quot;wifish handles the Wi-Fi connection (association). After connecting,&quot;
log_action &quot;your system still needs an IP address to access the internet.&quot;
log_action &quot;If not configured automatically, run as root (or with sudo):&quot;
log_action &quot;  dhclient ${WIFI_INTERFACE}&quot;
log_action &quot;(Or use another DHCP client like &#39;dhcpcd5&#39; if installed and preferred, or check &#39;systemd-networkd&#39; if active).&quot;
log_action &quot;Phase 4 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Final messages
log_action &quot;wifish setup script finished.&quot;
if [ -n &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot; ]; then
    log_action &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
fi
log_action &quot;$WPA_RESTARTED_MANUALLY_MSG&quot;
log_action &quot; &quot;
log_action &quot;To test, AFTER the designated user (&#39;${PRIMARY_USER:-your regular user}&#39;) has potentially logged out and back in:&quot;
log_action &quot;1. Open a new terminal as that user.&quot;
log_action &quot;2. Run &#39;wpa_cli status&#39;. It should show connection details without needing sudo.&quot;
log_action &quot;3. Run &#39;wifish list&#39; or &#39;wifish menu&#39;.&quot;
log_action &quot; &quot;
log_action &quot;The script used Wi-Fi interface &#39;${WIFI_INTERFACE}&#39;. If this was incorrect,&quot;
log_action &quot;use wifish with the &#39;-i &lt;your_actual_interface&gt;&#39; flag.&quot;

log_action &quot;Cleaning up temporary directory $TEMP_WIFISH_DIR...&quot;
rm -rf &quot;$TEMP_WIFISH_DIR&quot;

log_action &quot;Setup complete. Backup of wpa_supplicant config (if it existed) is at: $BACKUP_FILE&quot;
echo &quot;---------------------------------------------------------------------&quot;

exit 0" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">#!/bin/bash
# This script automates the setup of &#39;wifish&#39; on a Debian Bullseye system.
# Version 2: Improved backup, idempotency, and interface detection.
#
# IMPORTANT: This script makes certain assumptions:
# 1. It should be run as root.
# 2. It attempts to auto-detect the Wi-Fi interface. If detection fails or is
#    incorrect, the default &quot;wlan0&quot; is used, or you may need to edit the script
#    or use wifish with the -i flag.
# 3. The active wpa_supplicant configuration file is assumed to be
#    &#39;/etc/wpa_supplicant/wpa_supplicant.conf&#39;. If different, this needs to be changed.
# 4. It will attempt to identify the primary non-root user (UID 1000 or SUDO_USER)
#    to add to the &#39;netdev&#39; group.
#
# Review these assumptions and the script content before execution.

set -e # Exit immediately if a command exits with a non-zero status.

# --- Configuration ---
DEFAULT_WIFI_INTERFACE=&quot;wlan0&quot;
WPA_SUPPLICANT_CONF_FILE=&quot;/etc/wpa_supplicant/wpa_supplicant.conf&quot;
WIFI_INTERFACE=&quot;&quot; # Will be auto-detected or fall back to DEFAULT_WIFI_INTERFACE

# Determine the primary non-root user to add to the &#39;netdev&#39; group.
PRIMARY_USER=&quot;&quot;
if [ -n &quot;$SUDO_USER&quot; ] &amp;&amp; [ &quot;$SUDO_USER&quot; != &quot;root&quot; ]; then # Prefer SUDO_USER if available and not root
    PRIMARY_USER=&quot;$SUDO_USER&quot;
elif command -v id &gt;/dev/null &amp;&amp; id -u 1000 &amp;&gt;/dev/null; then # Try UID 1000
    PRIMARY_USER_UID1000=$(id -un 1000)
    if [ &quot;$PRIMARY_USER_UID1000&quot; != &quot;root&quot; ]; then # Ensure user 1000 is not root
        PRIMARY_USER=&quot;$PRIMARY_USER_UID1000&quot;
    fi
fi
# Fallback to parsing /etc/passwd if PRIMARY_USER is still not set
if [ -z &quot;$PRIMARY_USER&quot; ]; then
    PRIMARY_USER=$(awk -F: &#39;($3&gt;=1000 &amp;&amp; $1 != &quot;root&quot; &amp;&amp; $1 != &quot;&quot; &amp;&amp; $1 != &quot;nobody&quot;){print $1; exit}&#39; /etc/passwd)
fi


# --- Helper Functions ---
log_action() {
    echo &quot;[INFO] $1&quot;
}

log_warning() {
    echo &quot;[WARNING] $1&quot;
}

log_error() {
    echo &quot;[ERROR] $1&quot; &gt;&amp;2
}

# --- Wi-Fi Interface Detection ---
detect_wifi_interface() {
    log_action &quot;Attempting to auto-detect Wi-Fi interface...&quot;
    local detected_iface=&quot;&quot;
    # Try with &#39;iw dev&#39; first, common for Wi-Fi specific tools
    if command -v iw &gt;/dev/null; then
        detected_iface=$(iw dev | awk &#39;$1==&quot;Interface&quot;{print $2; exit}&#39;)
    fi

    # If &#39;iw dev&#39; didn&#39;t yield a result, try with &#39;ip link&#39; for wlan*
    if [ -z &quot;$detected_iface&quot; ] &amp;&amp; command -v ip &gt;/dev/null; then
        detected_iface=$(ip -o link show type wlan | awk -F&#39;: &#39; &#39;{print $2; exit}&#39; | awk &#39;{print $1}&#39;)
    fi

    # If still no result, try &#39;ip link&#39; for wlp* (common alternative naming)
    if [ -z &quot;$detected_iface&quot; ] &amp;&amp; command -v ip &gt;/dev/null; then
        detected_iface=$(ip -o link show | grep -Eo &#39;wlp[0-9]+s[0-9]+&#39; | head -n1)
    fi

    if [ -n &quot;$detected_iface&quot; ]; then
        WIFI_INTERFACE=&quot;$detected_iface&quot;
        log_action &quot;Detected Wi-Fi interface: $WIFI_INTERFACE. Using this.&quot;
    else
        WIFI_INTERFACE=&quot;$DEFAULT_WIFI_INTERFACE&quot;
        log_warning &quot;Could not auto-detect Wi-Fi interface. Using default: $WIFI_INTERFACE.&quot;
        log_warning &quot;If this is incorrect, review script or use wifish with the &#39;-i &lt;your_interface&gt;&#39; option.&quot;
    fi
}


# --- Main Script ---

# Check if running as root
if [ &quot;$(id -u)&quot; -ne 0 ]; then
  log_error &quot;This script must be run as root. Please use &#39;sudo $0&#39; or run as the root user.&quot;
  exit 1
fi

detect_wifi_interface # Call detection function

log_action &quot;Starting wifish setup script (v2)...&quot;
log_action &quot;Using Wi-Fi Interface: ${WIFI_INTERFACE}&quot;
log_action &quot;Targeting wpa_supplicant config: ${WPA_SUPPLICANT_CONF_FILE}&quot;

if [ -n &quot;$PRIMARY_USER&quot; ]; then
    log_action &quot;Primary non-root user identified for &#39;netdev&#39; group: ${PRIMARY_USER}&quot;
else
    log_warning &quot;Could not automatically detect a primary non-root user. Manual &#39;usermod -a -G netdev youruser&#39; might be required.&quot;
fi
echo &quot;---------------------------------------------------------------------&quot;

# Phase 1: Prerequisites and Getting wifish
log_action &quot;Phase 1: Installing prerequisites and getting wifish...&quot;
log_action &quot;Updating package lists (apt update)...&quot;
if ! apt update -qq; then
    log_error &quot;apt update failed. Please check your network connection and apt sources.&quot;
    exit 1
fi

log_action &quot;Installing gawk, dialog, git...&quot;
if ! apt install -y gawk dialog git; then
    log_error &quot;Failed to install required packages. Please check apt output for errors.&quot;
    exit 1
fi

TEMP_WIFISH_DIR=&quot;/tmp/bougyman-wifish_install_temp_$(date +%Y%m%d%H%M%S)&quot;
log_action &quot;Cloning wifish repository to $TEMP_WIFISH_DIR...&quot;
rm -rf &quot;$TEMP_WIFISH_DIR&quot; # Clean up if exists
if ! git clone https://github.com/bougyman/wifish.git &quot;$TEMP_WIFISH_DIR&quot;; then
    log_error &quot;Failed to clone wifish repository. Check internet connection and git.&quot;
    exit 1
fi

log_action &quot;Setting execute permissions for scripts in repository...&quot;
(
    cd &quot;$TEMP_WIFISH_DIR&quot;
    chmod +x wifish install.sh test/test.sh
    chmod +x sv/wpa_supplicant/run sv/wpa_supplicant/log/run
)
log_action &quot;Phase 1 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 2: Configuring wpa_supplicant
log_action &quot;Phase 2: Configuring wpa_supplicant...&quot;
log_action &quot;Ensuring wpa_supplicant configuration file directory exists: $(dirname &quot;$WPA_SUPPLICANT_CONF_FILE&quot;)&quot;
mkdir -p &quot;$(dirname &quot;$WPA_SUPPLICANT_CONF_FILE&quot;)&quot;

if [ ! -f &quot;$WPA_SUPPLICANT_CONF_FILE&quot; ]; then
    log_action &quot;Creating empty wpa_supplicant configuration file: $WPA_SUPPLICANT_CONF_FILE&quot;
    touch &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
fi

TIMESTAMP=$(date +%Y%m%d%H%M%S)
BACKUP_FILE=&quot;${WPA_SUPPLICANT_CONF_FILE}.${TIMESTAMP}.bak&quot;
log_action &quot;Backing up current $WPA_SUPPLICANT_CONF_FILE to $BACKUP_FILE...&quot;
if cp &quot;$WPA_SUPPLICANT_CONF_FILE&quot; &quot;$BACKUP_FILE&quot;; then
    log_action &quot;Backup successful: $BACKUP_FILE&quot;
else
    log_warning &quot;Failed to create backup of $WPA_SUPPLICANT_CONF_FILE. Proceeding with caution.&quot;
fi

log_action &quot;Configuring &#39;ctrl_interface&#39; and &#39;update_config&#39; in $WPA_SUPPLICANT_CONF_FILE for idempotency...&quot;

# Comment out any existing ctrl_interface lines to avoid conflicts
sed -i -E &#39;s/^[[:space:]]*ctrl_interface=.*$/#&amp; (old_ctrl_interface, commented by wifish_setup.sh)/&#39; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
# Add the correct ctrl_interface line if it&#39;s not already present (uncommented)
if ! grep -qFx &quot;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&quot; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;; then
    log_action &quot;Adding &#39;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&#39; to $WPA_SUPPLICANT_CONF_FILE.&quot;
    echo &quot;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&quot; &gt;&gt; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
else
    log_action &quot;&#39;ctrl_interface=DIR=/run/wpa_supplicant GROUP=netdev&#39; already present or re-added.&quot;
fi

# Comment out any existing update_config lines
sed -i -E &#39;s/^[[:space:]]*update_config=.*$/#&amp; (old_update_config, commented by wifish_setup.sh)/&#39; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
# Add the correct update_config line if it&#39;s not already present (uncommented)
if ! grep -qFx &quot;update_config=1&quot; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;; then
    log_action &quot;Adding &#39;update_config=1&#39; to $WPA_SUPPLICANT_CONF_FILE.&quot;
    echo &quot;update_config=1&quot; &gt;&gt; &quot;$WPA_SUPPLICANT_CONF_FILE&quot;
else
    log_action &quot;&#39;update_config=1&#39; already present or re-added.&quot;
fi

log_action &quot;Securing $WPA_SUPPLICANT_CONF_FILE permissions (chmod 600)...&quot;
chmod 600 &quot;$WPA_SUPPLICANT_CONF_FILE&quot;

USER_MODIFIED_LOGIN_MESSAGE=&quot;&quot;
if [ -n &quot;$PRIMARY_USER&quot; ]; then
    log_action &quot;Ensuring &#39;netdev&#39; group exists and adding user &#39;$PRIMARY_USER&#39; to it...&quot;
    if ! getent group netdev &gt;/dev/null; then
        log_action &quot;Group &#39;netdev&#39; does not exist. Creating it...&quot;
        if ! groupadd --system netdev; then # Use --system for system groups if appropriate
            log_warning &quot;Could not create group &#39;netdev&#39;. Manual creation (groupadd netdev) might be needed.&quot;
        else
            log_action &quot;Group &#39;netdev&#39; created.&quot;
        fi
    fi
    # Check if user is already in group to avoid unnecessary usermod message
    if ! groups &quot;$PRIMARY_USER&quot; | grep -q &#39;\bnetdev\b&#39;; then
        if ! usermod -a -G netdev &quot;$PRIMARY_USER&quot;; then
             log_warning &quot;Failed to add user &#39;$PRIMARY_USER&#39; to &#39;netdev&#39; group. Check permissions or do it manually.&quot;
        else
            USER_MODIFIED_LOGIN_MESSAGE=&quot;User &#39;$PRIMARY_USER&#39; has been added to the &#39;netdev&#39; group. IMPORTANT: &#39;$PRIMARY_USER&#39; MUST log out and log back in for this change to take effect.&quot;
            log_action &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
        fi
    else
        log_action &quot;User &#39;$PRIMARY_USER&#39; is already a member of the &#39;netdev&#39; group.&quot;
        USER_MODIFIED_LOGIN_MESSAGE=&quot;User &#39;$PRIMARY_USER&#39; is a member of &#39;netdev&#39;. If this membership is recent, a logout/login might still be needed for all services to recognize it.&quot;

    fi
else
    USER_MODIFIED_LOGIN_MESSAGE=&quot;IMPORTANT: Could not automatically determine a primary non-root user. Please manually add your regular user to the &#39;netdev&#39; group (e.g., &#39;sudo usermod -a -G netdev yourusername&#39;) and then log out and log back in.&quot;
    log_warning &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
fi

log_action &quot;Attempting to restart wpa_supplicant for interface &#39;$WIFI_INTERFACE&#39;...&quot;
WPA_RESTARTED_MANUALLY_MSG=&quot;If wpa_supplicant was restarted, it should pick up the new configuration. If not, a manual restart of wpa_supplicant or a system reboot might be necessary.&quot;

SERVICE_NAME=&quot;wpa_supplicant@${WIFI_INTERFACE}.service&quot;
SERVICE_EXISTS=false
if systemctl list-unit-files | grep -q &quot;^${SERVICE_NAME}&quot;; then # More precise grep
    SERVICE_EXISTS=true
fi

if [ &quot;$SERVICE_EXISTS&quot; = true ]; then
    log_action &quot;${SERVICE_NAME} found.&quot;
    if systemctl is-active --quiet &quot;${SERVICE_NAME}&quot;; then
        log_action &quot;Restarting ${SERVICE_NAME} via systemd...&quot;
        if ! systemctl restart &quot;${SERVICE_NAME}&quot;; then
            log_warning &quot;Failed to restart ${SERVICE_NAME}. Check &#39;systemctl status ${SERVICE_NAME}&#39; and &#39;journalctl -u ${SERVICE_NAME}&#39;.&quot;
        fi
    else
        log_action &quot;${SERVICE_NAME} exists but is not active. Attempting to enable and start it...&quot;
        if ! systemctl enable &quot;${SERVICE_NAME}&quot; --now; then
             log_warning &quot;Failed to enable and start ${SERVICE_NAME}. Check status and journal.&quot;
        fi
    fi
elif [ -f &quot;/etc/network/interfaces&quot; ] &amp;&amp; grep -q &quot;iface ${WIFI_INTERFACE} inet&quot; /etc/network/interfaces; then # More specific grep
    log_action &quot;Attempting to restart network interface ${WIFI_INTERFACE} via ifdown/ifup...&quot;
    ifdown &quot;${WIFI_INTERFACE}&quot; &gt;/dev/null 2&gt;&amp;1 || true # Ignore errors if already down
    if ! ifup &quot;${WIFI_INTERFACE}&quot;; then
        log_warning &quot;ifup ${WIFI_INTERFACE} failed. Manual network reconfiguration might be needed. Check /etc/network/interfaces configuration.&quot;
    fi
else
    WPA_RESTARTED_MANUALLY_MSG=&quot;Could not determine how wpa_supplicant is managed for &#39;${WIFI_INTERFACE}&#39; (no specific systemd service or /etc/network/interfaces entry found). A manual restart of wpa_supplicant or a system reboot might be necessary to apply configuration changes.&quot;
    log_warning &quot;$WPA_RESTARTED_MANUALLY_MSG&quot;
fi
log_action &quot;Phase 2 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 3: Implementing wifish (System-Wide Installation)
log_action &quot;Phase 3: Installing wifish system-wide...&quot;
(
    cd &quot;$TEMP_WIFISH_DIR&quot;
    log_action &quot;Running install.sh from wifish repository (current directory: $(pwd))...&quot;
    if ! ./install.sh; then
        log_error &quot;wifish install.sh script failed. Please check output for errors.&quot;
        log_action &quot;Cleaning up temporary directory $TEMP_WIFISH_DIR...&quot;
        rm -rf &quot;$TEMP_WIFISH_DIR&quot;
        exit 1
    fi
)
log_action &quot;wifish&#39;s install.sh completed.&quot;
if [ -f &quot;$TEMP_WIFISH_DIR/test/test.sh&quot; ]; then
    log_action &quot;The wifish repository includes a test script: $TEMP_WIFISH_DIR/test/test.sh&quot;
    log_action &quot;You can explore running it manually from the &#39;$TEMP_WIFISH_DIR&#39; directory if desired (e.g., &#39;./test/test.sh&#39;).&quot;
    log_action &quot;Note: This test script typically mocks wpa_cli and runs its own checks.&quot;
fi
log_action &quot;Phase 3 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Phase 4: Using wifish (Information for the user)
log_action &quot;Phase 4: Information on using wifish...&quot;
log_action &quot;wifish should now be installed and available as the &#39;wifish&#39; command.&quot;
log_action &quot;Example usage (as the non-root user, AFTER they have logged back in if their group membership was changed):&quot;
log_action &quot;  wifish list&quot;
log_action &quot;  wifish menu&quot;
log_action &quot;  wifish connect \&quot;Your_Network_SSID\&quot;&quot;
log_action &quot; &quot;
log_action &quot;If wifish needs to target a specific interface (detected/defaulted to &#39;${WIFI_INTERFACE}&#39;), use:&quot;
log_action &quot;  wifish -i ${WIFI_INTERFACE} menu&quot;
log_action &quot;or set the environment variable for the session:&quot;
log_action &quot;  export WPA_CLI_INTERFACE=${WIFI_INTERFACE}  (unset with &#39;unset WPA_CLI_INTERFACE&#39;)&quot;
log_action &quot; &quot;
log_action &quot;IMPORTANT NOTE ON IP ADDRESS:&quot;
log_action &quot;wifish handles the Wi-Fi connection (association). After connecting,&quot;
log_action &quot;your system still needs an IP address to access the internet.&quot;
log_action &quot;If not configured automatically, run as root (or with sudo):&quot;
log_action &quot;  dhclient ${WIFI_INTERFACE}&quot;
log_action &quot;(Or use another DHCP client like &#39;dhcpcd5&#39; if installed and preferred, or check &#39;systemd-networkd&#39; if active).&quot;
log_action &quot;Phase 4 complete.&quot;
echo &quot;---------------------------------------------------------------------&quot;

# Final messages
log_action &quot;wifish setup script finished.&quot;
if [ -n &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot; ]; then
    log_action &quot;$USER_MODIFIED_LOGIN_MESSAGE&quot;
fi
log_action &quot;$WPA_RESTARTED_MANUALLY_MSG&quot;
log_action &quot; &quot;
log_action &quot;To test, AFTER the designated user (&#39;${PRIMARY_USER:-your regular user}&#39;) has potentially logged out and back in:&quot;
log_action &quot;1. Open a new terminal as that user.&quot;
log_action &quot;2. Run &#39;wpa_cli status&#39;. It should show connection details without needing sudo.&quot;
log_action &quot;3. Run &#39;wifish list&#39; or &#39;wifish menu&#39;.&quot;
log_action &quot; &quot;
log_action &quot;The script used Wi-Fi interface &#39;${WIFI_INTERFACE}&#39;. If this was incorrect,&quot;
log_action &quot;use wifish with the &#39;-i &lt;your_actual_interface&gt;&#39; flag.&quot;

log_action &quot;Cleaning up temporary directory $TEMP_WIFISH_DIR...&quot;
rm -rf &quot;$TEMP_WIFISH_DIR&quot;

log_action &quot;Setup complete. Backup of wpa_supplicant config (if it existed) is at: $BACKUP_FILE&quot;
echo &quot;---------------------------------------------------------------------&quot;

exit 0</code>
</section>]]></content><author><name></name></author><category term="scripts&gt;bash" /><category term="software&gt;linux" /></entry></feed>