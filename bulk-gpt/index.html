<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>
    
      bulkGPT: multithreading for OpenAI's output limits — infoBAG
    
  </title>
  <meta name="title" content="bulkGPT: multithreading for OpenAI's output limits">
  <meta name="description" content="can't steer unless already moving">
  <meta property="og:title" content="bulkGPT: multithreading for OpenAI's output limits — infoBAG">
  <meta property="og:description" content="can't steer unless already moving">
  <meta property="og:url" content="https://ib.bsb.br/bulk-gpt/">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="infoBAG">
  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="bulkGPT: multithreading for OpenAI's output limits — infoBAG">
  <meta name="twitter:description" content="can't steer unless already moving">
  

  <link rel="canonical" href="https://ib.bsb.br/bulk-gpt/">
  <link rel="alternate" type="application/rss+xml" title="infoBAG" href="https://ib.bsb.br/rss.xml">

  
    <meta name="keywords" content="linux,scripts">
    
      <meta property="article:tag" content="linux">
    
      <meta property="article:tag" content="scripts">
    
  
  <link rel="icon" href="/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  

  <link href="/style.css" rel="stylesheet">
</head>
<body class="post-content-body">
  <header class="header-container">
    <nav aria-label="Main navigation" class="header-content">
      <a href="/" aria-label="Home">
        <img src="/favicon.ico" alt="Home" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/tags" aria-label="Tags">
        <img src="/assets/Label.gif" alt="Tags" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/archive" aria-label="Archive">
        <img src="/assets/Loose_Stone_Pile.gif" alt="Archive" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/events" aria-label="Events">
        <img src="/assets/Paralyse_Rune.gif" alt="Events" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/archive-created" aria-label="Archive created">
          <img src="/assets/Hole_(Rock).gif" alt="Archive created" class="favicon search-link" width="32" height="32" loading="lazy">
      </a>
      <a href="/rot64" aria-label="rot">
          <img src="/assets/rot.gif" alt="rot" class="favicon search-link" width="32" height="32" loading="lazy">
      </a>
    </nav>
    <h5 class="post-title">
      <a href="#bottom-of-page" aria-label="Go to bottom">
        bulkGPT: multithreading for OpenAI's output limits
      </a>
    </h5>
    <div class="post-meta">
      <time datetime="2024-06-13T03:00:00+00:00" class="post-date">
        13 Jun 2024
      </time>
      
        <span class="post-updated">
          ↣
          <time datetime="2024-09-20T02:37:06+00:00">20 Sep 2024</time>
        </span>
      
      
        <p class="post-slug">
          Slug: <a href="https://ib.bsb.br/bulk-gpt" class="tag">bulk-gpt</a>
        </p>
      
      
        <p class="post-tags">
          Tags:
          
            <a href="https://ib.bsb.br/tags/#linux" class="tag">linux</a>
          
            <a href="https://ib.bsb.br/tags/#scripts" class="tag">scripts</a>
          
        </p>
      
    </div>
    <div class="post-actions">
      <div class="page-stats mt-3" role="status" aria-label="Page statistics">
        <span class="badge bg-primary">
          6537 characters
        </span>
        <span class="separator mx-2" aria-hidden="true">•</span>
        <span class="badge bg-primary">
          706 words
        </span>
      </div>
      <div class="action-buttons d-flex flex-wrap gap-2">
        
          
            <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/edit/main/_posts/2024-06-13-bulk-gpt.md"
                  method="GET"
                  target="_blank"
                  rel="noopener noreferrer"
                  class="d-inline-block">
              <button type="submit" class="btn btn-danger" aria-label="Edit page content">
                <span class="button-text">Improve this page?</span>
                <span class="info-text">aberto.</span>
              </button>
            </form>
          
          <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/commits/main/_posts/2024-06-13-bulk-gpt.md"
                method="GET"
                target="_blank"
                rel="noopener noreferrer"
                class="d-inline-block">
            <button type="submit" class="btn btn-danger" aria-label="View page revision history">
              View revision history
            </button>
          </form>
        
      </div>
    </div>
  </header>
  <main class="content">
    <article class="post-wrapper">
      <div class="post-content-body">
        

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>import os
import openai
from concurrent.futures import ThreadPoolExecutor, as_completed
import tiktoken
import logging
from tqdm import tqdm
import time
import random

# Setup the logging system
logging.basicConfig(level=logging.INFO)

# Initialize OpenAI client with the API key
api_key = os.getenv('OPENAI_KEY')
if not api_key:
    raise ValueError("API key not found. Please set the OPENAI_KEY environment variable.")
client = openai.OpenAI(api_key=api_key)

def load_text(file_path):
    """Load text from a specified file."""
    try:
        with open(file_path, 'r') as file:
            return file.read()
    except Exception as e:
        logging.error(f'Failed to load file {file_path}: {str(e)}')
        raise

def initialize_files(output_file, log_file):
    """Initialize the output and log files by creating empty files."""
    try:
        open(output_file, 'w').close()
        open(log_file, 'w').close()
    except Exception as e:
        logging.error(f'Failed to initialize files {output_file}, {log_file}: {str(e)}')
        raise

def save_to_file(responses, output_file):
    """Save API responses to an output file."""
    try:
        with open(output_file, 'w') as file:
            for response in responses:
                file.write(response + '\n')
    except Exception as e:
        logging.error(f'Failed to save to file {output_file}: {str(e)}')
        raise

def log_to_file(log_file, message):
    """Log messages to a log file."""
    try:
        with open(log_file, 'a') as file:
            file.write(message + '\n')
    except Exception as e:
        logging.error(f'Failed to log to file {log_file}: {str(e)}')
        raise

def call_openai_api(chunk, model, max_tokens, temperature, prompt):
    """Call the OpenAI API with retries on rate limits."""
    for i in range(3):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": chunk},
                ],
                max_tokens=max_tokens,
                n=1,
                temperature=temperature,
            )
            return response.choices[0].message.content.strip()
        except openai.OpenAIError as e:
            if 'Rate limit' in str(e):
                wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter
                logging.warning(f'Rate limit exceeded. Retrying after {wait_time} seconds.')
                time.sleep(wait_time)
            else:
                logging.error(f'API call failed: {str(e)}')
                return None
    logging.error('Failed to call OpenAI API after multiple retries due to rate limiting.')
    return None

def split_into_chunks(text, model, tokens=3500):
    """Split the text into smaller chunks based on token limits."""
    encoding = tiktoken.encoding_for_model(model)
    words = encoding.encode(text)
    chunks = []
    for i in range(0, len(words), tokens):
        chunks.append(''.join(encoding.decode(words[i:i + tokens])))
    return chunks

def process_chunks(input_file, output_file, log_file, model, chunksize, max_tokens, temperature, prompt):
    """Process text chunks and call OpenAI API for each chunk."""
    initialize_files(output_file, log_file)
    text = load_text(input_file)
    chunks = split_into_chunks(text, model, tokens=chunksize)
    nCh = len(chunks)
    print(f'{nCh} chunks.')
    log_to_file(log_file, f'Number of chunks: {nCh}')
    with ThreadPoolExecutor() as executor:
        futures = {executor.submit(call_openai_api, chunk, model, max_tokens, temperature, prompt): chunk for chunk in chunks}
        responses = []
        for future in tqdm(as_completed(futures), total=len(futures), desc='Processing chunks'):
            response = future.result()
            if response is None:
                log_to_file(log_file, f'Failed to process chunk {futures[future]}')
            else:
                responses.append(response)
                log_to_file(log_file, 'Successfully processed chunk!')
    save_to_file(responses, output_file)

if __name__ == "__main__":
    input_file = 'input.txt'
    output_file = 'output.txt'
    log_file = 'log.txt'
    model = 'gpt-3.5-turbo'
    chunksize = 3500
    max_tokens = 4000
    temperature = 0.01
    prompt = '''You will be presented with a scrambled, poorly formatted BibTeX entry. 
    Your task is to refactor the entry to fix all syntax problems and ensure it adheres to the standard BibTeX format. 
    Here is an example of a properly formatted BibTeX entry: 
    @article{Smith2023, author = {Smith, John and Doe, Jane}, title = {The Impact of Artificial Intelligence on Society}, journal = {Journal of Artificial Intelligence Research}, year = {2023}, volume = {10}, number = {2}, pages = {123--145}, doi = {10.1234/jair.2023.10.2.123}, abstract = {This paper explores the profound impact of artificial intelligence (AI) on various aspects of society. We discuss the ethical implications, economic consequences, and potential societal benefits of AI. Our analysis highlights the need for responsible AI development and deployment to mitigate risks and maximize its positive impact.}, keywords = {Artificial Intelligence, Society, Ethics, Economics, Impact} } 
    Here is the scrambled BibTeX data you need to refactor: &lt;scrambled_bibtex&gt;  &lt;/scrambled_bibtex&gt; 
    To refactor the scrambled BibTeX data, follow these steps: 
    1. Check for missing or incorrect delimiters, such as curly braces, commas, and equals signs. Ensure that each field is properly enclosed in curly braces and that key-value pairs are separated by commas. 
    2. Ensure proper indentation and line breaks. Each field should start on a new line and be indented with two spaces. 
    3. Verify the presence and correct order of required fields, such as author, title, and year. Optional fields can be included as needed. 
    4. Check for proper capitalization and punctuation within fields. Titles should be capitalized appropriately, and punctuation should be consistent. 
    5. If any information is missing or unclear, note this in your response. 
    Please provide the refactored BibTeX entry inside &lt;refactored_bibtex&gt; tags.'''
    process_chunks(input_file, output_file, log_file, model, chunksize, max_tokens, temperature, prompt)
</code></section></div></div>

      </div>
      
        URL: https://ib.bsb.br/bulk-gpt
      
    </article>
        <nav class="post-navigation same-tag-navigation" aria-label="Posts sharing the same tag">
          
          
        </nav>
    
    
      
      
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          
      
    
    
    <div class="comment-box">
      Ref.
      <a href="github.com/andrewgcodes/lightspeedGPT" title="github.com/andrewgcodes/lightspeedGPT">github.com/andrewgcodes/lightspeedGPT</a>
    </div>
    
  </main>
  <footer id="bottom-of-page" class="site-footer">
    <p>
      <a
        href="#"
        aria-label="Back to top"
        class="back-to-top-link">
        <span class="sr-only">Back to top</span>
      </a>
      
      
      
      <a href="https://ib.bsb.br/404" aria-label="404">
        2025-01-28 21:27:44
      </a>
      &hArr;
      <a href="https://github.com/ib-bsb-br/ib-bsb-br.github.io" aria-label="GitHub">&#8505;</a>
      <a href="/" aria-label="Homepage">
        infoBAG
      </a>
      <button id="copyAllButton" aria-label="Copy all code">
        &copy;
      </button>
    </p>
  </footer>
  <style>
  .back-to-top-link {
    display: inline-block;
    width: 32px;
    height: 32px;
    background: url("/assets/Rope_(Old).gif") center center no-repeat;
    background-size: contain; /* or cover */
    text-decoration: none;
    vertical-align: middle;
  }
  .sr-only {
    /* Utility class to hide text visually but keep it accessible */
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    white-space: nowrap;
    border: 0;
  }
  </style>
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://ib.bsb.br/bulk-gpt/"
    },
    "headline": "bulkGPT: multithreading for OpenAI's output limits",
    "description": "",
    "datePublished": "2024-06-13T03:00:00+00:00",
    "dateModified": "2024-09-20T02:37:06+00:00",
    "author": {
      "@type": "Person",
      "name": "Author"
    },
    "publisher": {
      "@type": "Organization",
      "name": "infoBAG"
      
    }
    
  }
  </script>
  <script src="/assets/js/prism.js" defer></script>
  <script src="/assets/js/copy-all-code.js"></script>
</body>
</html>
