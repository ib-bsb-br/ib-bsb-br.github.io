<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>
    
      Meta-Prompt for Better API Integration ‚Äî infoBAG
    
  </title>
  <meta name="title" content="Meta-Prompt for Better API Integration">
  <meta name="description" content="can't steer unless already moving">

  <!-- Open Graph Meta Tags -->
  <meta property="og:title" content="Meta-Prompt for Better API Integration ‚Äî infoBAG">
  <meta property="og:description" content="can't steer unless already moving">
  <meta property="og:url" content="https://ib.bsb.br/meta-prompt-for-better-api-integration/">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="infoBAG">
  

  <!-- Twitter Card Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Meta-Prompt for Better API Integration ‚Äî infoBAG">
  <meta name="twitter:description" content="can't steer unless already moving">
  

  <link rel="canonical" href="https://ib.bsb.br/meta-prompt-for-better-api-integration/">
  <link rel="alternate" type="application/rss+xml" title="infoBAG" href="https://ib.bsb.br/rss.xml">

  
    <meta name="keywords" content="AI>LLM">
    
      <meta property="article:tag" content="AI>LLM">
    
  

  <!-- Favicons and Icons -->
  <link rel="icon" href="/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  

  <link href="/style.css" rel="stylesheet">
</head>
  <body class="post-content-body">
    
    
    
    
        
        
        "<p>URL Source: <code class="language-plaintext highlighter-rouge">https://jina.ai/news/meta-prompt-for-better-jina-api-integration-and-codegen/</code></p>\n    
        
        
        "\n    
        
        
        "<p>We recently published <a href="https://docs.jina.ai/">Meta-Prompt</a>, a single text file that outlines all of our API specifications. You can think of it as documentation for LLMs, and use it to automatically generate integrations of our APIs including Reader, Embeddings, Reranker, and more.</p>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/1:44</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/meta-prompt-square-VEED_thumb.jpg" alt="Image 1" /></p>\n    
        
        
        "\n    
        
        
        "<p>It‚Äôs as simple as copying and pasting our prompt into ChatGPT/Claude, or piping it into the <a href="https://github.com/simonw/llm"><code class="language-plaintext highlighter-rouge">llm</code></a> command as a system prompt, then adding your own prompt to specify what you want to build (which we do below). It‚Äôs great if you want to use LLMs to quickly build apps that scrape the web, work with embeddings, or even full-blown RAG systems. All that with minimal hallucinations.</p>\n    
        
        
        "\n    
        
        
        "<p>Let‚Äôs say you want to use an LLM to generate code that uses Jina‚Äôs APIs. Let‚Äôs ask GPT-4o to do just that:</p>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/0:27</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/output_thumb.jpg" alt="Image 2" /></p>\n    
        
        
        "\n    
        
        
        "<p>Looks good, right? It‚Äôs got the <code class="language-plaintext highlighter-rouge">from jina import Client</code> and everything.</p>\n    
        
        
        "\n    
        
        
        "<p>One small problem: The Jina package is in maintenance mode, and it is <em>not</em> the way to access our APIs. Even if you <em>do</em> install the Jina package, the generated program will crash when you try to run it:</p>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/0:21</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/Screencast-from-2024-11-11-14-43-51_thumb.jpg" alt="Image 3" /></p>\n    
        
        
        "\n    
        
        
        "<p>So what? We can just ask GPT to search the web for Jina‚Äôs APIs, right? Here‚Äôs what we get:</p>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/1:14</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/Screencast-from-2024-11-11-14-45-33_thumb.jpg" alt="Image 4" /></p>\n    
        
        
        "\n    
        
        
        "<p>However, if you look at the code it <em>doesn‚Äôt</em> use all of the relevant Jina APIs. It very clearly didn‚Äôt find out that Reader is a thing, instead making us install <a href="https://pypi.org/project/beautifulsoup4/">BeautifulSoup</a> to do the scraping. And, even when it <em>could</em> (supposedly) do the scraping with BeautifulSoup, it didn‚Äôt accurately parse the response format for Jina Embeddings, leading to a crash:</p>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/0:16</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/Screencast-from-2024-11-11-14-50-35--1-_thumb.jpg" alt="Image 5" /></p>\n    
        
        
        "\n    
        
        
        "<p>Yet, even if ChatGPT <em>could</em> do it properly by searching, many other LLMs (like Claude) don‚Äôt currently support web search, severely limiting your options.</p>\n    
        
        
        "\n    
        
        
        "<p>This is where Meta-Prompt shines. With Meta-Prompt, you can load all the context and specifications of Jina‚Äôs APIs into the LLM. This means the LLM can generate code that leverages Jina‚Äôs APIs directly, without hallucinations or unnecessary workarounds, giving you code that works <em>the first time</em>.</p>\n    
        
        
        "\n    
        
        
        "<p>üí°</p>\n    
        
        
        "\n    
        
        
        "<p>Okay, <strong>usually</strong> the first time. LLMs can be unpredictable, but as you can see below, things went well in our experiments.</p>\n    
        
        
        "\n    
        
        
        "<p>To put the Meta-Prompt through its paces, we ran a few experiments and evaluated the results. Unless otherwise specified, we used <a href="https://www.anthropic.com/news/claude-3-5-sonnet">Claude-3.5-Sonnet</a> as the LLM.</p>\n    
        
        
        "\n    
        
        
        "<p>For all experiments, we specified relevant API keys (like <code class="language-plaintext highlighter-rouge">JINA_API_KEY</code> and <code class="language-plaintext highlighter-rouge">ANTHROPIC_API_KEY</code>) as environment variables before running the generated code.</p>\n    
        
        
        "\n    
        
        
        "<h3 id="experiment-1-verifying-statements-using-meta-prompt-in-chatgpt"><a href="https://jina.ai/news/meta-prompt-for-better-jina-api-integration-and-codegen/#experiment-1-verifying-statements-using-meta-prompt-in-chatgpt" title="Experiment 1: Verifying Statements Using Meta-Prompt in ChatGPT"></a>Experiment 1: Verifying Statements Using Meta-Prompt in ChatGPT</h3>\n    
        
        
        "\n    
        
        
        "<p>We‚Äôre writing this just after the US elections, where more disinformation than ever was flying around. How can we separate the signal from the noise in our feeds, and get just the good stuff with none of the lies?</p>\n    
        
        
        "\n    
        
        
        "<p>Let‚Äôs say we want to check whether a new UK law is accurately reported on <a href="http://bbc.com/">BBC.com</a>, specifically the claim:</p>\n    
        
        
        "\n    
        
        
        "<blockquote>\n    
        
        
        "  <p>‚ÄúThe UK government has announced a new law that will require social media companies to verify the age of their users.‚Äù</p>\n    
        
        
        "</blockquote>\n    
        
        
        "\n    
        
        
        "<p>We can copy-paste the Meta-Prompt into ChatGPT, then type our own prompt to generate the code to do that, like:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Write the JavaScript code to check the validity\n    
        
        
        "of the following statement on bbc.com: \n    
        
        
        "\n    
        
        
        ""The UK government has announced a new law \n    
        
        
        "that will require social media companies to \n    
        
        
        "verify the age of their users."\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/0:35</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/grounding-chatgpt_thumb.jpg" alt="Image 6" /></p>\n    
        
        
        "\n    
        
        
        "<p>We can then run that with <code class="language-plaintext highlighter-rouge">node grounding.js</code> (after installing any prerequisite packages like <a href="https://www.npmjs.com/package/axios">axios</a>). We get output like this, showing that the claim is true, along with sources:</p>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/0:04</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/grounding-run-1_thumb.jpg" alt="Image 7" /></p>\n    
        
        
        "\n    
        
        
        "<h3 id="experiment-2-visualizing-hacker-news-from-the-cli"><a href="https://jina.ai/news/meta-prompt-for-better-jina-api-integration-and-codegen/#experiment-2-visualizing-hacker-news-from-the-cli" title="Experiment 2: Visualizing Hacker News from the CLI"></a>Experiment 2: Visualizing Hacker News from the CLI</h3>\n    
        
        
        "\n    
        
        
        "<p>If you‚Äôre more of a command line warrior, you can use Meta-Prompt from the CLI via cURL. First, you‚Äôll need to install the <code class="language-plaintext highlighter-rouge">llm</code> Python package:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install llm\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>And then the Claude-3 plugin:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llm install llm-claude-3\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>For the last stage of setup, specify your Anthropic API key:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export ANTHROPIC_API_KEY=&lt;your key&gt;\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>Now, let‚Äôs write a prompt to visualize every sentence from the Hacker News front page:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>grab every sentence from hackernews frontpage and \n    
        
        
        "visualize them in a 2d umap using matplotlib\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>We can <a href="https://wizardzines.com/comics/bash-pipes/">pipe</a> this into the <code class="language-plaintext highlighter-rouge">llm</code> command with:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl docs.jina.ai | llm -s "grab every sentence from hackernews frontpage and visualize them in a 2d umap using matplotlib" -m claude-3.5-sonnet\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/0:24</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/Screencast-from-2024-11-11-11-23-03_thumb.jpg" alt="Image 8" /></p>\n    
        
        
        "\n    
        
        
        "<p>If we extract and and run the generated code, we get something like this:</p>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/0:38</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/Screencast-from-2024-11-11-11-28-43_thumb.jpg" alt="Image 9" /></p>\n    
        
        
        "\n    
        
        
        "<p>üí°</p>\n    
        
        
        "\n    
        
        
        "<p>One current limitation (though I‚Äôm sure with some extra coding from the user there‚Äôs a way around it) is that you‚Äôll need to install requirements manually. No <code class="language-plaintext highlighter-rouge">requirements.txt</code> is generated. In this case we needed <a href="https://umap-learn.readthedocs.io/en/latest/">UMAP</a> and <a href="https://matplotlib.org/">Matplotlib</a>, though your mileage may vary.</p>\n    
        
        
        "\n    
        
        
        "<h3 id="experiment-3-building-a-simple-rag-system-with-json-storage"><a href="https://jina.ai/news/meta-prompt-for-better-jina-api-integration-and-codegen/#experiment-3-building-a-simple-rag-system-with-json-storage" title="Experiment 3: Building a Simple RAG System with JSON Storage"></a>Experiment 3: Building a Simple RAG System with JSON Storage</h3>\n    
        
        
        "\n    
        
        
        "<p>To push things even farther, let‚Äôs create a simple RAG system. In my spare time I‚Äôm learning <a href="https://github.com/jeff-dh/SolidPython">SolidPython</a> so we‚Äôll use the repo and wiki as a knowledge base. To keep things simple, we won‚Äôt use a database, but rather just store the data in a JSON file.</p>\n    
        
        
        "\n    
        
        
        "<p>Here‚Äôs the prompt, stored in the file <code class="language-plaintext highlighter-rouge">prompt.txt</code>:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Create a simple RAG system using pages from these sources:\n    
        
        
        "\n    
        
        
        "- repo: &lt;https://github.com/jeff-dh/SolidPython&gt;\n    
        
        
        "- wiki: &lt;https://github.com/jeff-dh/SolidPython/wiki&gt; (and all the subpages)\n    
        
        
        "\n    
        
        
        "Scrape no other pages.\n    
        
        
        "\n    
        
        
        "Instead of using vector database, use JSON file\n    
        
        
        "\n    
        
        
        "You can access an LLM with the CLI command: llm 'your prompt' -m claude-3.5-sonnet\n    
        
        
        "\n    
        
        
        "After segmenting and indexing all the pages, present a prompt for the user to ask a\n    
        
        
        "question. To answer the question, find the top three segments and pass them to the LLM\n    
        
        
        "with the prompt:\n    
        
        
        "\n    
        
        
        "--- prompt start ---\n    
        
        
        "Based on these segments:\n    
        
        
        "\n    
        
        
        "- {segment 1}\n    
        
        
        "- {segment 2}\n    
        
        
        "- {segment 3}\n    
        
        
        "\n    
        
        
        "Answer the question: {question}\n    
        
        
        "--- prompt end ---\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>As you can see, we can give the LLM additional tools by specifying them in the prompt. Without this, Claude often hallucinates a less optimal (or even broken) way to add the LLM to the RAG system.</p>\n    
        
        
        "\n    
        
        
        "<p>Since this is a very long prompt (with plenty of punctuation that may break any pipe we run it in), we‚Äôll use the text <code class="language-plaintext highlighter-rouge">$(cat prompt.txt)</code> rather than the prompt itself when we run our command:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl docs.jina.ai/v4 | llm -s "$(cat prompt.txt)" -m claude-3.5-sonnet\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/0:34</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/docsqa-claude_thumb.jpg" alt="Image 10" /></p>\n    
        
        
        "\n    
        
        
        "<p>Phew! That‚Äôs a lot of output. But (like with the Hacker News example) it‚Äôs a pain in the neck to extract and run the code from that big blob of text. Of course, there‚Äôs no problem that can‚Äôt be solved by just throwing more LLM at it, right? So let‚Äôs add another prompt to ‚Äúde-blob‚Äù the original output:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>leave just the code in this file, remove all surrounding explanatory text. \n    
        
        
        "do not wrap code in backticks, just return "pure code"\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>Now we add that to our command pipeline and run it:</p>\n    
        
        
        "\n    
        
        
        "<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl docs.jina.ai/v4 | llm -s "$(cat prompt.txt)" -m claude-3.5-sonnet | llm -s 'leave just the code in this file, remove all surrounding explanatory text. do not wrap code in backticks, just return "pure code"' -m claude-3.5-sonnet &gt; app.py\n    
        
        
        "</code></pre></div></div>\n    
        
        
        "\n    
        
        
        "<p>üí°</p>\n    
        
        
        "\n    
        
        
        "<p>Since we‚Äôre using <code class="language-plaintext highlighter-rouge">&gt; app.py</code> at the end of our command to direct all output into a file, there‚Äôs nothing to show in a video.</p>\n    
        
        
        "\n    
        
        
        "<p>We can then run that app with <code class="language-plaintext highlighter-rouge">python app.py</code> and we get our RAG program. As you can see, it can answer questions and maintain a working memory:</p>\n    
        
        
        "\n    
        
        
        "<p>0:00</p>\n    
        
        
        "\n    
        
        
        "<p>/0:34</p>\n    
        
        
        "\n    
        
        
        "<p><img src="https://jina-ai-gmbh.ghost.io/content/media/2024/11/docsqa-run_thumb.jpg" alt="Image 11" /></p>\n    
        
        
        "\n    
        
        
        "<p>üí°</p>\n    
        
        
        "\n    
        
        
        "<p>The first run of this took a little longer, since it had to segment and encode all the data. For subsequent runs it loaded that from a JSON file to save time and cost.</p>\n    
        
        
        "\n    
        
        
        "<h3 id="experiment-4-building-an-app-factory-with-meta-prompt"><a href="https://jina.ai/news/meta-prompt-for-better-jina-api-integration-and-codegen/#experiment-4-building-an-app-factory-with-meta-prompt" title="Experiment 4: Building an App Factory with Meta-Prompt"></a>Experiment 4: Building an App Factory with Meta-Prompt</h3>\n    
        
        
        "\n    
        
        
        "<p>Now that we can generate scripts and apps non-interactively, we can easily automate an ‚Äúapp factory‚Äù - a script that iterates over prompts and produces Python scripts as output. You can get the app factory script in a <a href="https://gist.github.com/alexcg1/4150f2e7dfe0d635260c71d59324172b">GitHub gist</a> for now:</p>\n    
        
        
        "\n    
        
        
        "<p><a href="https://gist.github.com/alexcg1/4150f2e7dfe0d635260c71d59324172b">App Factory with Jina AI Meta-Prompt App Factory with Jina AI Meta-Prompt. GitHub Gist: instantly share code, notes, and snippets. <img src="https://jina-ai-gmbh.ghost.io/content/images/icon/pinned-octocat-093da3e6fa40.svg" alt="Image 12" />262588213843476 <img src="https://jina-ai-gmbh.ghost.io/content/images/thumbnail/gist-og-image-54fd7dc0713e.png" alt="Image 13" /></a></p>\n    
        
        
        "\n    
        
        
        "<p>What it does, in short, is:</p>\n    
        
        
        "\n    
        
        
        "<ul>\n    
        
        
        "  <li>Iterate through the <code class="language-plaintext highlighter-rouge">prompts</code> directory which contains (you guessed it) prompt files.</li>\n    
        
        
        "  <li>Pass the Meta-Prompt and each prompt text to Claude-3.5-Sonnet (via <code class="language-plaintext highlighter-rouge">llm</code>).</li>\n    
        
        
        "  <li>Take the output and pass that to Claude <em>again</em>, this time with the prompt telling it to just leave the code.</li>\n    
        
        
        "  <li>Write that to a file in the <code class="language-plaintext highlighter-rouge">apps</code> directory.</li>\n    
        
        
        "</ul>\n    
        
        
        "\n    
        
        
        "<p>We‚Äôd show a demo, but there‚Äôs not much to see. It just logs which prompt filename it‚Äôs working on, and otherwise operates silently with no interesting output to the screen.</p>\n    
        
        
        "\n    
        
        
        "<p>üí°</p>\n    
        
        
        "\n    
        
        
        "<p><strong>Testing</strong> the apps it generates is another matter, one that I can‚Äôt solve off the top of my head. In our experience, we often specify the data we want to use in our prompts, usually by passing an external URL to download with Reader. Yet sometimes the LLM hallucinates mock data, and the script runs without obvious issues ‚Äî it just ‚Äúlies‚Äù about what it‚Äôs doing.</p>\n    
        
        
        "\n    
        
        
        "<p>To take the app factory to the next level, you could go full <a href="https://www.notion.so/Meta-Prompt-LLM-Generated-Code-without-The-Hallucinations-333ad1ddc735470e83f987d7dd6a644f?pvs=21">Factorio</a> and write <em>another</em> script to generate app ideas and from there generate prompts to feed into the factory. We haven‚Äôt done that yet, but we leave it as an exercise for you, the reader.</p>\n    
        
        
        "\n    
        
        
        "<p>We learned a lot from using Meta-Prompt, both about what to put in our own prompts and how different LLMs generate different output.</p>\n    
        
        
        "\n    
        
        
        "<h3 id="general-observations"><a href="https://jina.ai/news/meta-prompt-for-better-jina-api-integration-and-codegen/#general-observations" title="General Observations"></a>General Observations</h3>\n    
        
        
        "\n    
        
        
        "<ul>\n    
        
        
        "  <li><strong>API Specialization</strong>: Using task-specific APIs (e.g., <a href="https://developers.google.com/books">Google Books</a> for book-related queries) ensures more consistent results than general-purpose search APIs, which can reduce token usage and improve reliability.</li>\n    
        
        
        "  <li><strong>Custom Prompts for Reusability</strong>: For non-interactive setups, saving prompts as <code class="language-plaintext highlighter-rouge">.txt</code> files and piping them into the CLI enables efficient code-only outputs without extra explanatory text cluttering things up.</li>\n    
        
        
        "  <li><strong>Structured Output</strong>: Storing outputs (usually in JSON format) and reloading them as needed saves tokens and streamlines tasks like generating embeddings, where token usage can be expensive.</li>\n    
        
        
        "</ul>\n    
        
        
        "\n    
        
        
        "<h3 id="insights-from-using-different-llms"><a href="https://jina.ai/news/meta-prompt-for-better-jina-api-integration-and-codegen/#insights-from-using-different-llms" title="Insights from Using Different LLMs"></a>Insights from Using Different LLMs</h3>\n    
        
        
        "\n    
        
        
        "<p><strong>GPT</strong></p>\n    
        
        
        "\n    
        
        
        "<ul>\n    
        
        
        "  <li><strong>Prompt Retention Issues</strong>: GPT-4o sometimes loses details with lengthy instructions, leading to issues when it ‚Äúforgets‚Äù key elements mid-discussion. This leads to a <em>lot</em> of frustration when you have to remind it of simple things.</li>\n    
        
        
        "  <li><strong>API Integration Challenges</strong>: In cases like integrating <a href="https://milvus.io/docs/milvus_lite.md">Milvus Lite</a> with <code class="language-plaintext highlighter-rouge">jina-embeddings-v3</code>, even when we provide the Milvus Lite API instructions, GPT-4o fails completely and repeatedly, generating code that creates databases that lack the embeddings that the code just generated, making semantic search applications impossible.</li>\n    
        
        
        "</ul>\n    
        
        
        "\n    
        
        
        "<p><strong>Claude</strong></p>\n    
        
        
        "\n    
        
        
        "<ul>\n    
        
        
        "  <li><strong>Code Output Limitations</strong>: Claude-3.5 often produces scripts that appear complete but contain silent issues, like missing error handling or failing to account for missing API keys. Additionally, it sometimes falls back on pre-set examples rather than generating responses tailored to specific instructions.</li>\n    
        
        
        "  <li><strong>Silent Output</strong>: With LLM-generated code it <em>really</em> helps to have some logging of what‚Äôs happening behind the scenes when you run the program, just to make sure the model didn‚Äôt mess things up. Unless you directly specify to do so, apps created with Claude will often run silently, leaving you with no clue what‚Äôs happening.</li>\n    
        
        
        "  <li><strong>Interaction with CLI</strong>: You need to clearly specify that CLI commands are <em>CLI</em> commands. If you tell Claude it can use the <code class="language-plaintext highlighter-rouge">llm</code> command, often it will try to call a Python <code class="language-plaintext highlighter-rouge">llm()</code> function which doesn‚Äôt exist.</li>\n    
        
        
        "  <li><strong>Claude 3.5-Sonnet Is the Way to Go:</strong> Claude-3.5-Haiku also seemed to work okay in initial tests, but Opus and Sonnet-3 just summarize the Jina API instructions, without taking into account the user prompt.</li>\n    
        
        
        "</ul>\n    
        
        
        "\n    
        
        
        "<h2 id="conclusion"><a href="https://jina.ai/news/meta-prompt-for-better-jina-api-integration-and-codegen/#conclusion" title="Conclusion"></a>Conclusion</h2>\n    
        
        
        "\n    
        
        
        "<p>Using Meta-Prompt provides new ways to integrate Jina‚Äôs APIs with LLMs, allowing you to run experiments and build apps that work on the first try. No more crashes, missed API connections, or hallucinated functions ‚Äî Meta-Prompt ensures the code generated is accurate and functional right out of the gate. Whether you‚Äôre verifying statements, generating embeddings, building a lightweight RAG system, or automating app creation, Meta-Prompt transforms natural language instructions into actionable, correct code, bypassing the typical back and forth with an LLM to get things that actually work.</p>\n    
        
        
        "\n    
        
        
        "<p>Whether you‚Äôre copying Meta-Prompt into ChatGPT or using it with a custom LLM command, it offers a straightforward, reliable way to leverage Jina‚Äôs capabilities. Our experiments and insights show Meta-Prompt as a solid tool for robust integration into your projects.</p>\n    
        
        
        "\n    
        
        
        "<p>If you‚Äôre ready to explore what Meta-Prompt can do, head to <a href="http://docs.jina.ai/">docs.jina.ai</a> for the latest documentation and resources.</p>\n    "
  <header class="header-container">
    <nav aria-label="Main navigation" class="header-content">
      <a href="/" aria-label="Home">
        <img src="/favicon.ico" alt="Home" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/tags" aria-label="Tags">
        <img src="/assets/Label.gif" alt="Tags" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/events" aria-label="Events">
        <img src="/assets/Paralyse_Rune.gif" alt="Events" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/archive" aria-label="Archive">
        <img src="/assets/Loose_Stone_Pile.gif" alt="Archive" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
    </nav>
    <h1 class="post-title">Meta-Prompt for Better API Integration</h1>
    <div class="post-meta">
      <time datetime="2024-11-20T00:00:00+00:00" class="post-date">
        20 Nov 2024
      </time>
      
        <span class="post-updated">
          ‚Ü£
          <time datetime="2024-11-20T15:13:57+00:00">20 Nov 2024</time>
        </span>
      
      
        <p class="post-slug">
          Slug: <a href="https://ib.bsb.br/meta-prompt-for-better-api-integration" class="tag">meta-prompt-for-better-api-integration</a>
        </p>
      
      
        <p class="post-tags">
          Tags:
          
            <a href="https://ib.bsb.br/tags/#ai-llm" class="tag">AI>LLM</a>
          
        </p>
      
    </div>
    <div class="post-actions">
      <div class="page-stats mt-3" role="status" aria-label="Page statistics">
        <span class="badge bg-primary">
          20037 characters
        </span>
        <span class="separator mx-2" aria-hidden="true">‚Ä¢</span>
        <span class="badge bg-primary">
          2262 words
        </span>
      </div>
      <div class="action-buttons d-flex flex-wrap gap-2">
        
          
            <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/edit/main/_posts/2024-11-20-meta-prompt-for-better-api-integration.md"
                  method="GET"
                  target="_blank"
                  rel="noopener noreferrer"
                  class="d-inline-block">
              <button type="submit" class="btn btn-danger" aria-label="Edit page content">
                <span class="button-text">Improve this page?</span>
                <span class="info-text">aberto.</span>
              </button>
            </form>
          
          <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/commits/main/_posts/2024-11-20-meta-prompt-for-better-api-integration.md"
                method="GET"
                target="_blank"
                rel="noopener noreferrer"
                class="d-inline-block">
            <button type="submit" class="btn btn-danger" aria-label="View page revision history">
              View revision history
            </button>
          </form>
        
      </div>
    </div>
  </header>
  <main class="content">
    <article class="post-wrapper">
      <div class="post-content-body">
        

        

      </div>
      
        URL: https://ib.bsb.br/meta-prompt-for-better-api-integration
      
    </article>
    <nav class="post-navigation" aria-label="Post navigation">
      
        <div class="nav-arrow prev">
          <a href="/troubleshooting-debian-11/" title="Troubleshooting Debian 11" rel="prev">
            <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ/lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="Previous article" width="45" height="45" loading="lazy">
          </a>
        </div>
      
      
        <div class="nav-arrow next">
          <a href="/ftpd/" title="Setup FTP Server on Debian and Connect a Windows Machine" rel="next">
            <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAwdJREFUaEPtl01rG1cUht9z7p070ijKWLVS3I9YULtNMBRn2UIXxZtuvCjedNWfoKVXBvsHeOUfoIUx5AeY0pVXXSR052UxBJSi0kDiRplII83HvadI9CMtOFjCmBjubC7cc8+Zc555eYdLuIEP3cCe4Zu+rq/mSXvSbyHg5eHl4eVxXRrwpD3p+Qh4n56P2+xZnvTszC7MCAAUF0XfOdJa62+IaCeO40GSJA/zPP8RwB9vDjBtWmv9JRE5ImpZawdVpbo58CkAIyJnIrLAzEvOuXNm/h3AfQDOAGdj5z5m5gXn3G9ENCKilckaiDwpiFZEpCoiTyYrM3/knOtXmHs58BkABvCLc+4DZl6cxIjoh4ODA728vIzt7W2sra09PT4+fgTgQES4LMvHZIy5LyI/f/dJvfK6FKMJqAdqmBS2ZgWINY9zJyZ1whUiRJrTfmGjybSxUemwcFEmghqT1UxlUrpQEXD7jRq3NWelEz10okIi1AJOX+U2EgALgUrT0kVnmcKvQ+uazSZvbW2hWq2i3++j3W5jf38f6+vrebvdHovI5pT09+vN7teLYev990K8GpQYjS1u1TQqhnHez6EU4cM7FTw7z5AXDo16ACcyPRsaxp2FEM9ejKd7zUaIYVpiOCpRq2rUIo0XLzMwEZaaFTzvZ8hyh/iWnu69fF1AAoOfFr/A5w8eYDQaIY5jJEmC09NTNBoNdLtd7O3t4ejoCIeHh8k/pL8KUSkCZcg6hCLDjLk2ufYGpRuXAuOMYiosQiDNNEcQILQuzYgi0QxVWKuAMtcqJPdXDaKaMMOUNrOAtoFSVE5jaaY4mtQPS5dmQPS8WkdfGReYgDe/3cTdpbvo9XrY2NhAp9NBq9XKO53Ov6T/r2mlVBeX0DSAMzeHppm5h7doemdvR99buYfd3V2srq4+PTk5+a+mr9CmrqTU3+5Rr9cHg8HgYve4krddbZGb5dOXmf2d+7n4pi9D4LrOeHl40v42fl0a8KQ96fkIeJ+ej9vsWZ707Mzmy/Ck5+M2e9afhmuAz1BhwewAAAAASUVORK5CYII=" alt="Next article" width="45" height="45" loading="lazy">
          </a>
        </div>
      
    </nav>
    
  </main>
  <footer class="site-footer">
    <p>
      <a href="#" aria-label="Back to top">
        <img src="/assets/Rope_(Old).gif" alt="Back to top" width="32" height="32" loading="lazy">
      </a><a href="https://ib.bsb.br/404" aria-label="404">2024-12-28 18:00:44</a>‚Äé ‚áî‚Äé <a href="https://github.com/ib-bsb-br/ib-bsb-br.github.io" aria-label="GitHub">‚Ñπ</a>‚Äé <a href="/" aria-label="Homepage">infoBAG</a>
    </p>
  </footer>
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://ib.bsb.br/meta-prompt-for-better-api-integration/"
    },
    "headline": "Meta-Prompt for Better API Integration",
    "description": "",
    "datePublished": "2024-11-20T00:00:00+00:00",
    "dateModified": "2024-11-20T15:13:57+00:00",
    "author": {
      "@type": "Person",
      "name": "Author"
    },
    "publisher": {
      "@type": "Organization",
      "name": "infoBAG"
      
    }
    
  }
  </script>
  <script src="/assets/js/prism.js" defer></script>
</body>
</html>
