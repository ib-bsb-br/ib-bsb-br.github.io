<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
      
        Content-Based Retrieval System - infoBAG
      
    </title>
    <meta name="title" content="Content-Based Retrieval System - infoBAG" />
    <meta name="description" content="">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://ib.bsb.br/content-based-retrieval-system/">
    <meta property="og:title" content="Content-Based Retrieval System - infoBAG">
    <meta property="og:description" content="">
    <meta property="og:image" content="/favicon.ico">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://ib.bsb.br/content-based-retrieval-system/">
    <meta name="twitter:title" content="Content-Based Retrieval System - infoBAG">
    <meta name="twitter:description" content="">
    <meta name="twitter:image" content="/favicon.ico">
    <link rel="canonical" href="https://ib.bsb.br/content-based-retrieval-system/">
    <link rel="alternate" type="application/rss+xml" title="infoBAG" href="https://ib.bsb.br/rss.xml">
    
      <meta name="keywords" content="scratchpad">
      
        <meta property="article:tag" content="scratchpad">
      
    
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    
    <link rel="stylesheet" href="/style.css">
  </head>
  <body class="post-content-body">
    <header class="header-container">
      <nav aria-label="Main navigation" class="header-content">
        <a href="/" aria-label="Home">
          <img src="/favicon.ico" alt="Home" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/tags" aria-label="Tags">
          <img src="/assets/Label.gif" alt="Tags" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/send" aria-label="send">
          <img src="/assets/rot.gif" alt="send" class="favicon search-link" width="32" height="32" loading="lazy">
        </a> 
        <a href="/created" aria-label="archive created">
          <img src="/assets/Loose_Stone_Pile.gif" alt="archive created" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/events" aria-label="Events">
          <img src="/assets/Paralyse_Rune.gif" alt="Events" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/modified" aria-label="archive modified">
          <img src="/assets/Hole_(Rock).gif" alt="archive modified" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
    </nav>
      <h5 class="post-title">
        <a href="#bottom-of-page" aria-label="Go to bottom">
          Content-Based Retrieval System
        </a>
      </h5>
      <div class="post-meta">
        <time datetime="2025-05-22T00:00:00+00:00" class="post-date">
          22 May 2025
        </time>
        
          <span class="post-updated">
            ↣
            <time datetime="2025-05-22T19:34:45+00:00">
              22 May 2025
            </time>
          </span>
        
        
          <p class="post-slug">
            Slug: <a href="https://ib.bsb.br/content-based-retrieval-system" class="tag">content-based-retrieval-system</a>
          </p>
        
        
          <p class="post-tags">
            Tags:
            
              <a href="https://ib.bsb.br/tags/#scratchpad" class="tag">scratchpad</a>
            
          </p>
        
      </div>
      <div class="post-actions">
        <div class="page-stats mt-3" role="status" aria-label="Page statistics">
      
      <span class="badge bg-primary">
        105644 characters
      </span>
        <span class="separator mx-2" aria-hidden="true">•</span>
        <span class="badge bg-primary">
        12538 words
      </span>
      </div>
        <div class="action-buttons d-flex flex-wrap gap-2">
          
            
              <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/edit/main/_posts/2025-05-22-content-based-retrieval-system.md"
                    method="GET"
                    target="_blank"
                    rel="noopener noreferrer"
                    class="d-inline-block">
                <button type="submit" class="btn btn-danger" aria-label="Edit page content">
                  <span class="button-text">Improve this page?</span>
                  <span class="info-text">aberto.</span>
                </button>
              </form>
            
            <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/commits/main/_posts/2025-05-22-content-based-retrieval-system.md"
                  method="GET"
                  target="_blank"
                  rel="noopener noreferrer"
                  class="d-inline-block">
              <button type="submit" class="btn btn-danger" aria-label="View page revision history">
                View revision history
              </button>
            </form>
          
        </div>
      </div>
    </header>
    <main class="content">
      <article class="post-wrapper">
        <div class="post-content-body">
          
<ul><li><a href="#1-introduction"><strong>1. Introduction</strong></a><ul><li><a href="#11-acknowledging-the-challenge-uninformative-filenames-and-distributed-academic-archives"><strong>1.1. Acknowledging the Challenge: Uninformative Filenames and Distributed Academic Archives</strong></a></li><li><a href="#12-objective-building-an-efficient-content-based-retrieval-system"><strong>1.2. Objective: Building an Efficient, Content-Based Retrieval System</strong></a></li><li><a href="#13-high-level-strategy-overview"><strong>1.3. High-Level Strategy Overview</strong></a></li></ul></li><li><a href="#2-prerequisites-and-development-environment-setup"><strong>2. Prerequisites and Development Environment Setup</strong></a><ul><li><a href="#21-recommended-core-software-python-rust-docker-git"><strong>2.1. Recommended Core Software: Python, Rust, Docker, Git</strong></a></li><li><a href="#22-python-environment-management-eg-poetry-or-venv"><strong>2.2. Python Environment Management (e.g., Poetry or venv)</strong></a></li><li><a href="#23-rust-environment-management-cargo"><strong>2.3. Rust Environment Management (Cargo)</strong></a></li><li><a href="#24-essential-api-keys-and-sdks-google-drive-llm-providers---if-chosen"><strong>2.4. Essential API Keys and SDKs (Google Drive, LLM Providers - if chosen)</strong></a></li><li><a href="#25-setting-up-a-project-structure-and-version-control-git"><strong>2.5. Setting up a Project Structure and Version Control (Git)</strong></a></li></ul></li><li><a href="#3-phase-1-unified-file-ingestion-and-initial-processing"><strong>3. Phase 1: Unified File Ingestion and Initial Processing</strong></a><ul><li><a href="#31-aggregating-file-paths"><strong>3.1. Aggregating File Paths</strong></a><ul><li><a href="#311-accessing-local-external-hdd-files-linux-rustpython"><strong>3.1.1. Accessing Local External HDD Files (Linux, Rust/Python)</strong></a></li><li><a href="#312-accessing-google-drive-files-api-integration-rustpython"><strong>3.1.2. Accessing Google Drive Files (API integration, Rust/Python)</strong></a></li></ul></li><li><a href="#32-robust-file-type-identification-magic-numbers-libraries"><strong>3.2. Robust File Type Identification (Magic Numbers, Libraries)</strong></a></li><li><a href="#33-handling-archive-files-zip-rar-tar"><strong>3.3. Handling Archive Files (ZIP, RAR, TAR)</strong></a></li><li><a href="#34-core-content-extraction"><strong>3.4. Core Content Extraction</strong></a></li><li><a href="#35-ocr-for-image-based-pdfs-and-scanned-documents"><strong>3.5. OCR for Image-Based PDFs and Scanned Documents</strong></a></li></ul></li><li><a href="#4-phase-2-text-preparation-for-semantic-understanding"><strong>4. Phase 2: Text Preparation for Semantic Understanding</strong></a><ul><li><a href="#41-text-cleaning-and-normalization"><strong>4.1. Text Cleaning and Normalization</strong></a></li><li><a href="#42-document-chunking-strategies"><strong>4.2. Document Chunking Strategies</strong></a></li><li><a href="#43-embedding-model-selection"><strong>4.3. Embedding Model Selection</strong></a></li><li><a href="#44-generating-text-embeddings"><strong>4.4. Generating Text Embeddings</strong></a></li></ul></li><li><a href="#5-phase-3-vector-storage-and-indexing"><strong>5. Phase 3: Vector Storage and Indexing</strong></a><ul><li><a href="#51-vector-database-selection"><strong>5.1. Vector Database Selection</strong></a></li><li><a href="#52-setting-up-and-configuring-the-chosen-vector-database"><strong>5.2. Setting Up and Configuring the Chosen Vector Database</strong></a></li><li><a href="#53-indexing-strategies-for-efficient-search"><strong>5.3. Indexing Strategies for Efficient Search</strong></a></li></ul></li><li><a href="#6-phase-4-implementing-the-search-and-retrieval-interface"><strong>6. Phase 4: Implementing the Search and Retrieval Interface</strong></a><ul><li><a href="#61-query-processing"><strong>6.1. Query Processing</strong></a></li><li><a href="#62-performing-similarity-search"><strong>6.2. Performing Similarity Search</strong></a></li><li><a href="#63-presenting-search-results"><strong>6.3. Presenting Search Results</strong></a></li><li><a href="#64-optional-advanced-reranking-for-improved-precision"><strong>6.4. (Optional) Advanced Reranking for Improved Precision</strong></a></li><li><a href="#65-optional-enhancing-discoverability-with-result-diversification"><strong>6.5. (Optional) Enhancing Discoverability with Result Diversification</strong></a></li></ul></li><li><a href="#7-implementation-details-tools-libraries-and-code"><strong>7. Implementation Details: Tools, Libraries, and Code</strong></a><ul><li><a href="#71-table-recommended-python-libraries"><strong>7.1. Table: Recommended Python Libraries</strong></a></li><li><a href="#72-table-recommended-rust-crates"><strong>7.2. Table: Recommended Rust Crates</strong></a></li><li><a href="#73-conceptual-code-snippets"><strong>7.3. Conceptual Code Snippets</strong></a><ul><li><a href="#731-recursive-file-discovery-python-local--gdrive-placeholder"><strong>7.3.1. Recursive File Discovery (Python, Local + GDrive Placeholder)</strong></a></li><li><a href="#732-archive-extraction-loop-python-using-extractcode"><strong>7.3.2. Archive Extraction Loop (Python, using extractcode)</strong></a></li><li><a href="#733-content-extraction-and-ocr-python-conceptual"><strong>7.3.3. Content Extraction and OCR (Python, Conceptual)</strong></a></li><li><a href="#734-text-chunking-python-langchain-style-recursive"><strong>7.3.4. Text Chunking (Python, LangChain Style Recursive)</strong></a></li><li><a href="#735-embedding-generation-python-sentence-transformers-local--openai-api"><strong>7.3.5. Embedding Generation (Python, Sentence Transformers Local &amp; OpenAI API)</strong></a></li><li><a href="#736-vector-db-indexing-python-qdrant-client"><strong>7.3.6. Vector DB Indexing (Python, Qdrant Client)</strong></a></li><li><a href="#737-vector-db-querying-python-qdrant-client"><strong>7.3.7. Vector DB Querying (Python, Qdrant Client)</strong></a></li><li><a href="#works-cited"><strong>Works cited</strong></a></li></ul></li></ul></li></ul>
          <h2 id="1-introduction">
    
    
     <a href="#1-introduction">#</a><a href="#" aria-label="Back to top"><strong>1. Introduction</strong></a>
        
    
  </h2>
      

<p>The task of locating specific academic research documents within a vast and unorganized collection presents a significant challenge, particularly when compounded by uninformative filenames and distributed storage. This document outlines a comprehensive, step-by-step technical strategy to develop an efficient, content-based file retrieval system tailored to address these complexities. The strategy leverages advanced AI techniques, robust data processing pipelines, and a combination of local and cloud resources to transform a cumbersome manual search into an automated and precise information discovery process.</p>
  <h3 id="11-acknowledging-the-challenge-uninformative-filenames-and-distributed-academic-archives">
    
    
     <a href="#11-acknowledging-the-challenge-uninformative-filenames-and-distributed-academic-archives">#</a><a href="#" aria-label="Back to top"><strong>1.1. Acknowledging the Challenge: Uninformative Filenames and Distributed Academic Archives</strong></a>
        
    
  </h3>
      

<p>The primary impediment to efficient document retrieval in the described scenario is the prevalence of encoded or non-descriptive filenames, which render traditional filename-based search methods ineffective ([[user_query_for_strategy_generation]]). This lack of meaningful metadata necessitates a shift towards content-centric analysis. Compounding this issue is the distributed nature of the document archive, with files scattered across a local external hard drive and Google Drive. Manually opening and inspecting each file from these disparate sources is an exceedingly time-consuming and impractical endeavor, especially when dealing with thousands of documents. This situation is a classic information retrieval problem where the surface-level attributes of the files offer no clues to their content, demanding a deeper, content-based approach.<br />
The core problem is that without examining the actual content of each file, its relevance to specific academic topics like “sociology of quantification” or “jurimetrics” cannot be determined. This immediately signals the need for a system capable of ingesting files, extracting their textual content, and then making that content searchable.</p>
  <h3 id="12-objective-building-an-efficient-content-based-retrieval-system">
    
    
     <a href="#12-objective-building-an-efficient-content-based-retrieval-system">#</a><a href="#" aria-label="Back to top"><strong>1.2. Objective: Building an Efficient, Content-Based Retrieval System</strong></a>
        
    
  </h3>
      

<p>The principal objective of this strategy is to architect and implement a robust system that enables the user to perform content-based searches across their entire collection of academic documents ([[user_query_for_strategy_generation]]). This system will allow queries using natural language or specific academic keywords, retrieving relevant files regardless of their original names, formats, or storage locations. The aim is to move beyond simple keyword matching towards a more nuanced understanding of document content, aligning with the user’s familiarity with concepts like embeddings and cosine similarity ([[user_query_for_strategy_generation]]). Academic research often employs specialized terminology and explores complex interrelations between concepts; therefore, a system that can grasp semantic relationships will be significantly more effective than one relying solely on lexical matches. This points towards leveraging semantic search technologies, where documents are understood based on their meaning rather than just the presence or absence of specific words.</p>
  <h3 id="13-high-level-strategy-overview">
    
    
     <a href="#13-high-level-strategy-overview">#</a><a href="#" aria-label="Back to top"><strong>1.3. High-Level Strategy Overview</strong></a>
        
    
  </h3>
      

<p>The proposed solution involves a multi-phase approach, characteristic of sophisticated content-based retrieval systems. This modular design facilitates development, testing, and potential optimization of individual components:</p>

<ol>
  <li><strong>Unified File Ingestion:</strong> Systematically gathering file information and accessing file content from both the local external hard drive and Google Drive.</li>
  <li><strong>Content Extraction &amp; Preparation:</strong> Converting various file formats (PDF, DOCX, TXT, and contents of ZIP, RAR, TAR archives) into raw text. This stage includes Optical Character Recognition (OCR) for image-based documents or scanned PDFs.</li>
  <li><strong>Semantic Processing &amp; Embedding Generation:</strong> Transforming the cleaned textual content into dense vector representations (embeddings) that capture semantic meaning.</li>
  <li><strong>Vector Indexing:</strong> Storing these embeddings in a specialized vector database, optimized for fast similarity searches.</li>
  <li><strong>Search &amp; Retrieval Interface:</strong> Developing a mechanism to accept user queries, convert them into embeddings, search the vector database, and present relevant documents.</li>
</ol>

<p>This phased architecture not only organizes the development process but also allows for an incremental build-out, starting with core functionalities and progressively adding more advanced features. Each phase can be independently developed and tested, ensuring robustness before integration into the larger system, aligning with the “incremental approach” instructional guideline.</p>
  <h2 id="2-prerequisites-and-development-environment-setup">
    
    
     <a href="#2-prerequisites-and-development-environment-setup">#</a><a href="#" aria-label="Back to top"><strong>2. Prerequisites and Development Environment Setup</strong></a>
        
    
  </h2>
      

<p>A well-structured development environment is foundational for a project of this complexity. This section details the recommended software, tools, and initial setup steps.</p>
  <h3 id="21-recommended-core-software-python-rust-docker-git">
    
    
     <a href="#21-recommended-core-software-python-rust-docker-git">#</a><a href="#" aria-label="Back to top"><strong>2.1. Recommended Core Software: Python, Rust, Docker, Git</strong></a>
        
    
  </h3>
      

<p>The nature of the tasks involved—ranging from API interactions and data processing to performance-critical computations—suggests a hybrid approach leveraging the strengths of different languages and tools:</p>

<ul>
  <li><strong>Python:</strong> Its extensive ecosystem of libraries for data science, Natural Language Processing (NLP), machine learning model interaction (e.g., Hugging Face Transformers, Sentence Transformers), and API clients (e.g., Google Drive, OpenAI) makes it indispensable for rapid development and integration.</li>
  <li><strong>Rust:</strong> Given the user’s preference and its performance characteristics (speed and memory safety), Rust is highly recommended for computationally intensive tasks such as high-speed file parsing, local embedding generation (if custom models or optimized ONNX runtimes are used), and building custom command-line utilities.</li>
  <li><strong>Docker:</strong> Essential for containerizing services like vector databases (e.g., Qdrant, Weaviate), OCR engines, or even the entire processing pipeline. Docker ensures environment consistency, simplifies dependency management for complex tools, and facilitates deployment across different systems (including the user’s RK3588 and Intel N97 machines if needed).</li>
  <li><strong>Git:</strong> Non-negotiable for version control. A project of this scope requires robust tracking of code changes, branching for feature development, and the ability to revert to stable states.</li>
</ul>

<p>This combination allows for leveraging Python’s rich AI/ML ecosystem for tasks like interacting with embedding models or Google Drive APIs, while Rust can be employed for performance-critical components like file system traversal or custom parsing logic where efficiency is paramount. Docker will abstract away underlying OS-level dependencies, which is particularly useful for deploying third-party tools like vector databases that may have specific system library requirements.</p>
  <h3 id="22-python-environment-management-eg-poetry-or-venv">
    
    
     <a href="#22-python-environment-management-eg-poetry-or-venv">#</a><a href="#" aria-label="Back to top"><strong>2.2. Python Environment Management (e.g., Poetry or venv)</strong></a>
        
    
  </h3>
      

<p>To avoid dependency conflicts and ensure project reproducibility, a dedicated Python virtual environment is crucial.</p>

<ul>
  <li><strong>Poetry:</strong> Recommended for its robust dependency management, packaging capabilities, and deterministic builds via poetry.lock and pyproject.toml. It simplifies managing complex projects with numerous dependencies.</li>
  <li><strong>venv:</strong> Python’s built-in module for creating lightweight virtual environments. It can be used with a requirements.txt file, but dependency resolution is less sophisticated than Poetry’s.</li>
  <li><strong>Conda:</strong> Alternatively, Conda is another popular environment manager, particularly useful if the project expands to include complex data science libraries with non-Python dependencies, though Poetry/venv is likely sufficient here.</li>
</ul>

<p>Isolating project dependencies within a virtual environment prevents conflicts with system-wide Python packages or other projects, which is critical when integrating diverse libraries for file parsing, AI model interaction, and cloud services.</p>
  <h3 id="23-rust-environment-management-cargo">
    
    
     <a href="#23-rust-environment-management-cargo">#</a><a href="#" aria-label="Back to top"><strong>2.3. Rust Environment Management (Cargo)</strong></a>
        
    
  </h3>
      

<p>Rust’s build system and package manager, <strong>Cargo</strong>, will be used for managing Rust components of the project.</p>

<ul>
  <li>Dependencies (crates) are declared in the Cargo.toml file.</li>
  <li>Cargo handles fetching, compiling, and linking dependencies.</li>
  <li>Standard commands like cargo build, cargo run, and cargo test will be used. For larger Rust projects that might evolve into multiple interconnected components, Cargo Workspaces can be utilized to manage them collectively.</li>
</ul>
  <h3 id="24-essential-api-keys-and-sdks-google-drive-llm-providers---if-chosen">
    
    
     <a href="#24-essential-api-keys-and-sdks-google-drive-llm-providers---if-chosen">#</a><a href="#" aria-label="Back to top"><strong>2.4. Essential API Keys and SDKs (Google Drive, LLM Providers - if chosen)</strong></a>
        
    
  </h3>
      

<p>Programmatic access to services like Google Drive and potentially commercial LLM providers requires authentication credentials and Software Development Kits (SDKs).</p>

<ul>
  <li><strong>Google Drive API:</strong>
    <ul>
      <li>Credentials: An OAuth 2.0 client ID and secret must be obtained from the Google Cloud Console. The Drive API needs to be enabled for the project.</li>
      <li>Python SDK: google-api-python-client along with google-auth-oauthlib for authentication and interaction.</li>
      <li>Rust SDK: The drive-v3 crate provides a convenient wrapper around the Google Drive API v3.</li>
    </ul>
  </li>
  <li><strong>LLM Embedding Providers (Optional, if not using local models):</strong>
    <ul>
      <li>OpenAI: API key from the OpenAI platform. Python SDK: openai. Rust: Direct HTTP requests or a community-maintained client.</li>
      <li>Cohere: API key from Cohere. Python SDK: cohere. Rust: Direct HTTP requests or a community-maintained client.</li>
      <li>Jina AI: API key from Jina AI. Python SDK: jina-client.</li>
    </ul>
  </li>
</ul>

<p>API keys should be managed securely, for instance, using environment variables or a .env file (loaded by libraries like python-dotenv in Python or dotenv crate in Rust), rather than hardcoding them into scripts.</p>
  <h3 id="25-setting-up-a-project-structure-and-version-control-git">
    
    
     <a href="#25-setting-up-a-project-structure-and-version-control-git">#</a><a href="#" aria-label="Back to top"><strong>2.5. Setting up a Project Structure and Version Control (Git)</strong></a>
        
    
  </h3>
      

<p>A well-organized project structure is vital for maintainability and scalability. A suggested structure:<br />
<code class="language-plaintext highlighter-rouge">academic_search_project/</code><br />
<code class="language-plaintext highlighter-rouge">├──.git/</code><br />
<code class="language-plaintext highlighter-rouge">├──.gitignore</code><br />
<code class="language-plaintext highlighter-rouge">├── Cargo.toml         # For main Rust workspace or binary</code><br />
<code class="language-plaintext highlighter-rouge">├── pyproject.toml     # For Poetry (Python dependencies)</code><br />
<code class="language-plaintext highlighter-rouge">├── poetry.lock        # For Poetry</code><br />
<code class="language-plaintext highlighter-rouge">├── config/            # Configuration files (e.g., API endpoints, model names)</code><br />
<code class="language-plaintext highlighter-rouge">│   └── settings.yaml</code><br />
<code class="language-plaintext highlighter-rouge">├── data_raw/          # Temporary storage for downloaded/extracted raw files (add to.gitignore if large)</code><br />
<code class="language-plaintext highlighter-rouge">├── data_processed/    # Temporary storage for cleaned text, chunks (add to.gitignore if large)</code><br />
<code class="language-plaintext highlighter-rouge">├── logs/              # Application logs</code><br />
<code class="language-plaintext highlighter-rouge">├── scripts/           # Utility scripts (e.g., setup, batch processing triggers)</code><br />
<code class="language-plaintext highlighter-rouge">├── src/</code><br />
<code class="language-plaintext highlighter-rouge">│   ├── main.rs        # Main Rust application logic (if applicable)</code><br />
<code class="language-plaintext highlighter-rouge">│   ├── lib.rs         # Rust library code (if applicable)</code><br />
<code class="language-plaintext highlighter-rouge">│   └── python_pipeline/ # Python modules</code><br />
<code class="language-plaintext highlighter-rouge">│       ├── __init__.py</code><br />
<code class="language-plaintext highlighter-rouge">│       ├── ingestion.py</code><br />
<code class="language-plaintext highlighter-rouge">│       ├── parsing.py</code><br />
<code class="language-plaintext highlighter-rouge">│       ├── embedding.py</code><br />
<code class="language-plaintext highlighter-rouge">│       ├── indexing.py</code><br />
<code class="language-plaintext highlighter-rouge">│       └── search.py</code><br />
<code class="language-plaintext highlighter-rouge">├── tests/             # Unit and integration tests</code><br />
<code class="language-plaintext highlighter-rouge">│   ├── rust/</code><br />
<code class="language-plaintext highlighter-rouge">│   └── python/</code><br />
<code class="language-plaintext highlighter-rouge">└── README.md</code></p>

<p>Initialize a Git repository at the project’s inception:<br />
<code class="language-plaintext highlighter-rouge">git init</code></p>

<p>Commit frequently with descriptive messages to track development progress.</p>
  <h2 id="3-phase-1-unified-file-ingestion-and-initial-processing">
    
    
     <a href="#3-phase-1-unified-file-ingestion-and-initial-processing">#</a><a href="#" aria-label="Back to top"><strong>3. Phase 1: Unified File Ingestion and Initial Processing</strong></a>
        
    
  </h2>
      

<p>This phase focuses on systematically discovering, accessing, and preparing all relevant files from their diverse storage locations and formats.</p>
  <h3 id="31-aggregating-file-paths">
    
    
     <a href="#31-aggregating-file-paths">#</a><a href="#" aria-label="Back to top"><strong>3.1. Aggregating File Paths</strong></a>
        
    
  </h3>
      

<p>The first step is to create a comprehensive inventory of all target files.</p>
  <h4 id="311-accessing-local-external-hdd-files-linux-rustpython">
    
    
     <a href="#311-accessing-local-external-hdd-files-linux-rustpython">#</a><a href="#" aria-label="Back to top"><strong>3.1.1. Accessing Local External HDD Files (Linux, Rust/Python)</strong></a>
        
    
  </h4>
      

<p>The external HDD connected to the Debian Linux RK3588 machine needs to be mounted to make its file system accessible. Standard Linux mount procedures apply. Once mounted, file paths can be enumerated.</p>

<ul>
  <li>
    <p><strong>Python:</strong> The os.walk() function or the more modern pathlib.Path.rglob() method can be used to recursively traverse directories and list all files. os.scandir() is noted as a faster alternative to os.listdir() for Python &gt;= 3.5, and os.walk() uses os.scandir() internally since Python 3.5, offering good performance.<br />
<code class="language-plaintext highlighter-rouge"># Conceptual Python snippet for local file discovery</code><br />
<code class="language-plaintext highlighter-rouge">import os</code></p>

    <p><code class="language-plaintext highlighter-rouge">def find_local_files(root_dir):</code><br />
    <code class="language-plaintext highlighter-rouge">file_paths =</code><br />
    <code class="language-plaintext highlighter-rouge">for dirpath, _, filenames in os.walk(root_dir):</code><br />
        <code class="language-plaintext highlighter-rouge">for filename in filenames:</code><br />
            <code class="language-plaintext highlighter-rouge">file_paths.append(os.path.join(dirpath, filename))</code><br />
    <code class="language-plaintext highlighter-rouge">return file_paths</code></p>

    <p><code class="language-plaintext highlighter-rouge"># Example: local_files = find_local_files("/mnt/external_hdd")</code></p>
  </li>
  <li>
    <p><strong>Rust:</strong> The std::fs::read_dir function can be used for basic directory listing, but for recursive traversal, the walkdir crate is highly recommended for its efficiency and ease of use.<br />
<code class="language-plaintext highlighter-rouge">// Conceptual Rust snippet for local file discovery (using walkdir crate)</code><br />
<code class="language-plaintext highlighter-rouge">// Add `walkdir = "2"` to Cargo.toml</code><br />
<code class="language-plaintext highlighter-rouge">// use walkdir::WalkDir;</code><br />
<code class="language-plaintext highlighter-rouge">//</code><br />
<code class="language-plaintext highlighter-rouge">// fn find_local_files_rust(root_dir: &amp;str) -&gt; Vec&lt;String&gt; {</code><br />
<code class="language-plaintext highlighter-rouge">//     let mut file_paths = Vec::new();</code><br />
<code class="language-plaintext highlighter-rouge">//     for entry in WalkDir::new(root_dir).into_iter().filter_map(Result::ok) {</code><br />
<code class="language-plaintext highlighter-rouge">//         if entry.file_type().is_file() {</code><br />
<code class="language-plaintext highlighter-rouge">//             if let Some(path_str) = entry.path().to_str() {</code><br />
<code class="language-plaintext highlighter-rouge">//                 file_paths.push(path_str.to_string());</code><br />
<code class="language-plaintext highlighter-rouge">//             }</code><br />
<code class="language-plaintext highlighter-rouge">//         }</code><br />
<code class="language-plaintext highlighter-rouge">//     }</code><br />
<code class="language-plaintext highlighter-rouge">//     file_paths</code><br />
<code class="language-plaintext highlighter-rouge">// }</code><br />
<code class="language-plaintext highlighter-rouge">// Example: let local_files = find_local_files_rust("/mnt/external_hdd");</code></p>
  </li>
</ul>

<p>The collected paths, along with their source (“local_hdd”), should be stored, for example, in a simple database (SQLite) or a structured file (CSV, JSON Lines) for tracking and subsequent processing. The RK3588 machine, with its direct access to the HDD and potential for efficient Rust execution, is the ideal candidate for this task.</p>
  <h4 id="312-accessing-google-drive-files-api-integration-rustpython">
    
    
     <a href="#312-accessing-google-drive-files-api-integration-rustpython">#</a><a href="#" aria-label="Back to top"><strong>3.1.2. Accessing Google Drive Files (API integration, Rust/Python)</strong></a>
        
    
  </h4>
      

<p>Files stored on Google Drive require interaction with the Google Drive API. The 500 Mbps internet connection will be beneficial for downloading these files. This task can be run on either the RK3588 or the Intel N97 machine.</p>

<ul>
  <li><strong>Python:</strong>
    <ul>
      <li>Authentication: Use google-auth-oauthlib to handle the OAuth 2.0 flow.</li>
      <li>File Listing: Employ googleapiclient.discovery.build to create a service object. Use service.files().list() with parameters like q for filtering (e.g., by MIME type, parent folder), fields to specify returned data, and handle nextPageToken for pagination.</li>
      <li>File Download: Use service.files().get(fileId=file_id, alt=’media’) to download file content. For large files, implement resumable downloads.</li>
    </ul>
  </li>
</ul>

<p><code class="language-plaintext highlighter-rouge"># Conceptual Python snippet for Google Drive file listing and download</code><br />
<code class="language-plaintext highlighter-rouge"># from googleapiclient.discovery import build</code><br />
<code class="language-plaintext highlighter-rouge"># from googleapiclient.http import MediaIoBaseDownload</code><br />
<code class="language-plaintext highlighter-rouge"># from google.oauth2.credentials import Credentials # and auth flow</code><br />
<code class="language-plaintext highlighter-rouge"># import io</code></p>

<p><code class="language-plaintext highlighter-rouge"># Assume 'creds' is an authenticated Credentials object</code><br />
<code class="language-plaintext highlighter-rouge"># service = build('drive', 'v3', credentials=creds)</code></p>

<p><code class="language-plaintext highlighter-rouge"># def list_gdrive_files(folder_id=None):</code><br />
<code class="language-plaintext highlighter-rouge">#     gdrive_files =</code><br />
<code class="language-plaintext highlighter-rouge">#     page_token = None</code><br />
<code class="language-plaintext highlighter-rouge">#     query = f"'{folder_id}' in parents" if folder_id else None # Example query</code><br />
<code class="language-plaintext highlighter-rouge">#     while True:</code><br />
<code class="language-plaintext highlighter-rouge">#         response = service.files().list(q=query,</code><br />
<code class="language-plaintext highlighter-rouge">#                                         spaces='drive',</code><br />
<code class="language-plaintext highlighter-rouge">#                                         fields='nextPageToken, files(id, name, mimeType, parents)',</code><br />
<code class="language-plaintext highlighter-rouge">#                                         pageToken=page_token).execute()</code><br />
<code class="language-plaintext highlighter-rouge">#         for file_info in response.get('files',):</code><br />
<code class="language-plaintext highlighter-rouge">#             # Filter out folders, process actual files</code><br />
<code class="language-plaintext highlighter-rouge">#             if file_info.get('mimeType')!= 'application/vnd.google-apps.folder':</code><br />
<code class="language-plaintext highlighter-rouge">#                 gdrive_files.append(file_info)</code><br />
<code class="language-plaintext highlighter-rouge">#             else:</code><br />
<code class="language-plaintext highlighter-rouge">#                 # Recursively list files in subfolders if needed</code><br />
<code class="language-plaintext highlighter-rouge">#                 gdrive_files.extend(list_gdrive_files(folder_id=file_info.get('id')))</code><br />
<code class="language-plaintext highlighter-rouge">#         page_token = response.get('nextPageToken', None)</code><br />
<code class="language-plaintext highlighter-rouge">#         if page_token is None:</code><br />
<code class="language-plaintext highlighter-rouge">#             break</code><br />
<code class="language-plaintext highlighter-rouge">#     return gdrive_files</code></p>

<p><code class="language-plaintext highlighter-rouge"># def download_gdrive_file(service, file_id, local_download_path): # Added service parameter</code><br />
<code class="language-plaintext highlighter-rouge">#     request = service.files().get_media(fileId=file_id)</code><br />
<code class="language-plaintext highlighter-rouge">#     fh = io.FileIO(local_download_path, 'wb')</code><br />
<code class="language-plaintext highlighter-rouge">#     downloader = MediaIoBaseDownload(fh, request)</code><br />
<code class="language-plaintext highlighter-rouge">#     done = False</code><br />
<code class="language-plaintext highlighter-rouge">#     while done is False:</code><br />
<code class="language-plaintext highlighter-rouge">#         status, done = downloader.next_chunk()</code><br />
<code class="language-plaintext highlighter-rouge">#         # print(F'Download {int(status.progress() * 100)}.')</code></p>

<ul>
  <li><strong>Rust:</strong>
    <ul>
      <li>The drive-v3 crate simplifies Google Drive API interactions.</li>
      <li>Authentication: The crate provides mechanisms to use client_secrets.json.</li>
      <li>File Listing: Use drive.files.list().q(“mimeType!= ‘application/vnd.google-apps.folder’”).execute()?. Recursive listing would require iterating through folders similarly to the Python example.</li>
      <li>File Download: Use drive.files.get_media(\&amp;file_id).execute()?.</li>
    </ul>
  </li>
</ul>

<p><code class="language-plaintext highlighter-rouge">// Conceptual Rust snippet for Google Drive (using drive-v3 crate)</code><br />
<code class="language-plaintext highlighter-rouge">// Add `drive-v3 = "0.6"` and `tokio = { version = "1", features = ["full"] }` to Cargo.toml</code><br />
<code class="language-plaintext highlighter-rouge">// use drive_v3::{Drive, Credentials};</code><br />
<code class="language-plaintext highlighter-rouge">// use drive_v3::objects::Scope;</code><br />
<code class="language-plaintext highlighter-rouge">//</code><br />
<code class="language-plaintext highlighter-rouge">// async fn list_and_download_gdrive_files_rust(client_secrets_path: &amp;str, token_storage_path: &amp;str) -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {</code><br />
<code class="language-plaintext highlighter-rouge">//     let scopes = vec!; // Or Scope::DriveFile for downloads</code><br />
<code class="language-plaintext highlighter-rouge">//     let creds = Credentials::from_client_secrets_file(client_secrets_path, scopes, token_storage_path).await?;</code><br />
<code class="language-plaintext highlighter-rouge">//     let drive = Drive::new(creds);</code><br />
<code class="language-plaintext highlighter-rouge">//</code><br />
<code class="language-plaintext highlighter-rouge">//     let file_list = drive.files</code><br />
<code class="language-plaintext highlighter-rouge">//       .list()</code><br />
<code class="language-plaintext highlighter-rouge">//       .q("mimeType!= 'application/vnd.google-apps.folder' and 'root' in parents") // Example: files in root</code><br />
<code class="language-plaintext highlighter-rouge">//       .fields("files(id, name, mimeType)")</code><br />
<code class="language-plaintext highlighter-rouge">//       .execute()</code><br />
<code class="language-plaintext highlighter-rouge">//       .await?;</code><br />
<code class="language-plaintext highlighter-rouge">//</code><br />
<code class="language-plaintext highlighter-rouge">//     if let Some(files) = file_list.files {</code><br />
<code class="language-plaintext highlighter-rouge">//         for file_info in files {</code><br />
<code class="language-plaintext highlighter-rouge">//             if let (Some(id), Some(name)) = (file_info.id, file_info.name) {</code><br />
<code class="language-plaintext highlighter-rouge">//                 println!("Found GDrive file: {} (ID: {})", name, id);</code><br />
<code class="language-plaintext highlighter-rouge">//                 // Conceptual download</code><br />
<code class="language-plaintext highlighter-rouge">//                 // let file_bytes = drive.files.get_media(&amp;id).execute().await?;</code><br />
<code class="language-plaintext highlighter-rouge">//                 // std::fs::write(format!("./gdrive_downloads/{}", name), file_bytes)?;</code><br />
<code class="language-plaintext highlighter-rouge">//             }</code><br />
<code class="language-plaintext highlighter-rouge">//         }</code><br />
<code class="language-plaintext highlighter-rouge">//     }</code><br />
<code class="language-plaintext highlighter-rouge">//     Ok(())</code><br />
<code class="language-plaintext highlighter-rouge">// }</code></p>

<p>Downloaded Google Drive files should be stored in a designated temporary processing directory. It’s crucial to store their original Google Drive file IDs and paths for traceability. API rate limits and robust error handling for network issues or API errors must be implemented.</p>
  <h3 id="32-robust-file-type-identification-magic-numbers-libraries">
    
    
     <a href="#32-robust-file-type-identification-magic-numbers-libraries">#</a><a href="#" aria-label="Back to top"><strong>3.2. Robust File Type Identification (Magic Numbers, Libraries)</strong></a>
        
    
  </h3>
      

<p>Given that filenames are unreliable, content-based file type identification using “magic numbers” (the initial few bytes of a file that often uniquely identify its type) is essential. This step determines how each file will be subsequently parsed.</p>

<ul>
  <li><strong>Python:</strong>
    <ul>
      <li>python-magic: A wrapper around the libmagic library, widely used for identifying file types based on magic numbers.</li>
      <li>filetype: A lightweight, dependency-free Python package that infers file type and MIME type by checking magic numbers from the first 261 bytes of a file or buffer. It supports a wide range of types, including images, videos, archives, and documents.<br />
<code class="language-plaintext highlighter-rouge"># Conceptual Python snippet for file type identification using 'filetype'</code><br />
<code class="language-plaintext highlighter-rouge"># import filetype</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># def get_file_kind(file_path):</code><br />
<code class="language-plaintext highlighter-rouge">#     kind = filetype.guess(file_path)</code><br />
<code class="language-plaintext highlighter-rouge">#     if kind is None:</code><br />
<code class="language-plaintext highlighter-rouge">#         # print(f"Cannot guess file type for {file_path}")</code><br />
<code class="language-plaintext highlighter-rouge">#         return None, None</code><br />
<code class="language-plaintext highlighter-rouge">#     return kind.extension, kind.mime</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># # ext, mime = get_file_kind("path/to/your/file.pdf")</code></li>
    </ul>
  </li>
  <li><strong>Rust:</strong>
    <ul>
      <li>infer: A crate that, similar to Python’s filetype, infers file and MIME types by checking magic number signatures. It’s an adaptation of the Go filetype package and supports a broad array of file types without external dependencies.</li>
      <li>file_type: Another Rust crate for determining file type by examining file signatures and extensions, using data from sources like PRONOM, Apache HTTPD, and IANA.<br />
<code class="language-plaintext highlighter-rouge">// Conceptual Rust snippet for file type identification using 'infer'</code><br />
<code class="language-plaintext highlighter-rouge">// Add `infer = "0.19"` to Cargo.toml</code><br />
<code class="language-plaintext highlighter-rouge">// use infer;</code><br />
<code class="language-plaintext highlighter-rouge">//</code><br />
<code class="language-plaintext highlighter-rouge">// fn get_file_kind_rust(file_path: &amp;str) -&gt; Option&lt;(String, String)&gt; {</code><br />
<code class="language-plaintext highlighter-rouge">//     if let Ok(Some(kind)) = infer::get_from_path(file_path) {</code><br />
<code class="language-plaintext highlighter-rouge">//         Some((kind.extension().to_string(), kind.mime_type().to_string()))</code><br />
<code class="language-plaintext highlighter-rouge">//     } else {</code><br />
<code class="language-plaintext highlighter-rouge">//         // println!("Cannot guess file type for {}", file_path);</code><br />
<code class="language-plaintext highlighter-rouge">//         None</code><br />
<code class="language-plaintext highlighter-rouge">//     }</code><br />
<code class="language-plaintext highlighter-rouge">// }</code><br />
<code class="language-plaintext highlighter-rouge">// // let kind_info = get_file_kind_rust("path/to/your/file.pdf");</code></li>
    </ul>
  </li>
</ul>

<p>The identified file type should be logged, and this information will guide the selection of the appropriate content extraction module. Misidentification is possible for obscure or corrupted files, so error handling and logging are important here.</p>
  <h3 id="33-handling-archive-files-zip-rar-tar">
    
    
     <a href="#33-handling-archive-files-zip-rar-tar">#</a><a href="#" aria-label="Back to top"><strong>3.3. Handling Archive Files (ZIP, RAR, TAR)</strong></a>
        
    
  </h3>
      

<p>Files identified as archives (.zip,.rar,.tar) must have their contents extracted for individual processing. Extracted files should be placed into unique temporary subdirectories (e.g., named with a UUID) to prevent filename collisions and maintain a clear association with their parent archive. These extracted files will then re-enter the processing pipeline, starting from file type identification.</p>

<ul>
  <li><strong>Python:</strong>
    <ul>
      <li><strong>ZIP:</strong> The zipfile standard library provides comprehensive tools for reading and extracting ZIP archives.</li>
      <li><strong>TAR:</strong> The tarfile standard library handles TAR archives (.tar, .tar.gz, .tar.bz2).</li>
      <li><strong>RAR:</strong> The rarfile library can process RAR archives but typically requires the unrar command-line utility to be installed. patoolib is a higher-level library that wraps various archiver tools, including for RAR, and can simplify handling multiple archive formats.</li>
      <li><strong>Comprehensive Solution:</strong> The extractcode library is particularly noteworthy. It’s designed as a mostly universal archive extractor using 7zip, libarchive, and Python’s standard library. It excels at handling various formats, including nested archives, and addresses issues like problematic paths or damaged archives. It supports recursive extraction of archives-in-archives.<br />
<code class="language-plaintext highlighter-rouge"># Conceptual Python snippet for archive extraction using 'extractcode'</code><br />
<code class="language-plaintext highlighter-rouge"># from extractcode import extract # Check actual API for extract.extract or similar</code><br />
<code class="language-plaintext highlighter-rouge"># import tempfile</code><br />
<code class="language-plaintext highlighter-rouge"># import os</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># def extract_archive_contents(archive_path):</code><br />
<code class="language-plaintext highlighter-rouge">#     extracted_files_paths =</code><br />
<code class="language-plaintext highlighter-rouge">#     with tempfile.TemporaryDirectory() as tmpdir:</code><br />
<code class="language-plaintext highlighter-rouge">#         # Refer to extractcode documentation for precise API.</code><br />
<code class="language-plaintext highlighter-rouge">#         # Example using a hypothetical 'extract.extract_files_from_archive'</code><br />
<code class="language-plaintext highlighter-rouge">#         # for event in extract.extract(archive_path, tmpdir, recurse=True): # Placeholder from docs</code><br />
<code class="language-plaintext highlighter-rouge">#         #     if event.done and not event.errors and event.target and os.path.isfile(event.target):</code><br />
<code class="language-plaintext highlighter-rouge">#         #         extracted_files_paths.append(event.target)</code><br />
<code class="language-plaintext highlighter-rouge">#         pass # Replace with actual extractcode logic, ensuring extracted_files_paths is populated</code><br />
<code class="language-plaintext highlighter-rouge">#     return extracted_files_paths</code></li>
    </ul>
  </li>
  <li><strong>Rust:</strong>
    <ul>
      <li><strong>ZIP:</strong> The zip crate is commonly used for working with ZIP archives.</li>
      <li><strong>TAR:</strong> The tar crate provides functionalities for TAR archives.</li>
      <li><strong>RAR:</strong> Native Rust support for RAR is challenging due to the proprietary nature of the format and licensing restrictions of the UnRAR source code. While libarchive-rust bindings exist , libarchive itself has had historical limitations with full RAR support. The most reliable and recommended approach in Rust is shelling out to the unrar or 7z command-line utilities using std::process::Command.<br />
<code class="language-plaintext highlighter-rouge">// Conceptual Rust snippet for ZIP extraction</code><br />
<code class="language-plaintext highlighter-rouge">// Add `zip = "0.6"` to Cargo.toml</code><br />
<code class="language-plaintext highlighter-rouge">// use std::fs;</code><br />
<code class="language-plaintext highlighter-rouge">// use std::io;</code><br />
<code class="language-plaintext highlighter-rouge">// use std::path::Path;</code><br />
<code class="language-plaintext highlighter-rouge">//</code><br />
<code class="language-plaintext highlighter-rouge">// fn extract_zip_rust(archive_path: &amp;Path, output_dir: &amp;Path) -&gt; io::Result&lt;Vec&lt;String&gt;&gt; {</code><br />
<code class="language-plaintext highlighter-rouge">//     let file = fs::File::open(archive_path)?;</code><br />
<code class="language-plaintext highlighter-rouge">//     let mut archive = zip::ZipArchive::new(file)?;</code><br />
<code class="language-plaintext highlighter-rouge">//     let mut extracted_file_paths = Vec::new();</code><br />
<code class="language-plaintext highlighter-rouge">//</code><br />
<code class="language-plaintext highlighter-rouge">//     for i in 0..archive.len() {</code><br />
<code class="language-plaintext highlighter-rouge">//         let mut file = archive.by_index(i)?;</code><br />
<code class="language-plaintext highlighter-rouge">//         let outpath = match file.enclosed_name() {</code><br />
<code class="language-plaintext highlighter-rouge">//             Some(path) =&gt; output_dir.join(path),</code><br />
<code class="language-plaintext highlighter-rouge">//             None =&gt; continue,</code><br />
<code class="language-plaintext highlighter-rouge">//         };</code><br />
<code class="language-plaintext highlighter-rouge">//</code><br />
<code class="language-plaintext highlighter-rouge">//         if (*file.name()).ends_with('/') {</code><br />
<code class="language-plaintext highlighter-rouge">//             fs::create_dir_all(&amp;outpath)?;</code><br />
<code class="language-plaintext highlighter-rouge">//         } else {</code><br />
<code class="language-plaintext highlighter-rouge">//             if let Some(p) = outpath.parent() {</code><br />
<code class="language-plaintext highlighter-rouge">//                 if!p.exists() {</code><br />
<code class="language-plaintext highlighter-rouge">//                     fs::create_dir_all(p)?;</code><br />
<code class="language-plaintext highlighter-rouge">//                 }</code><br />
<code class="language-plaintext highlighter-rouge">//             }</code><br />
<code class="language-plaintext highlighter-rouge">//             let mut outfile = fs::File::create(&amp;outpath)?;</code><br />
<code class="language-plaintext highlighter-rouge">//             io::copy(&amp;mut file, &amp;mut outfile)?;</code><br />
<code class="language-plaintext highlighter-rouge">//             if let Some(path_str) = outpath.to_str() {</code><br />
<code class="language-plaintext highlighter-rouge">//                 extracted_file_paths.push(path_str.to_string());</code><br />
<code class="language-plaintext highlighter-rouge">//             }</code><br />
<code class="language-plaintext highlighter-rouge">//         }</code><br />
<code class="language-plaintext highlighter-rouge">//     }</code><br />
<code class="language-plaintext highlighter-rouge">//     Ok(extracted_file_paths)</code><br />
<code class="language-plaintext highlighter-rouge">// }</code></li>
    </ul>
  </li>
</ul>

<p>Key considerations include handling nested archives (archives within archives), potentially password-protected archives (though not specified by the user, this is a common real-world issue), and the very rare but possible “archive bomb” scenario (an archive designed to consume excessive resources upon extraction). Maintaining a clear mapping from extracted files back to their parent archive and original source file is crucial for traceability. The extractcode library’s ability to handle problematic paths and perform recursive extraction makes it a strong candidate, especially if the Python ecosystem is favored for this part of the pipeline.</p>
  <h3 id="34-core-content-extraction">
    
    
     <a href="#34-core-content-extraction">#</a><a href="#" aria-label="Back to top"><strong>3.4. Core Content Extraction</strong></a>
        
    
  </h3>
      

<p>Once individual, non-archived files are identified by type, their textual content must be extracted.</p>

<ul>
  <li><strong>PDFs (Portable Document Format):</strong>
    <ul>
      <li><strong>Python:</strong>
        <ul>
          <li>pypdf (formerly PyPDF2): Suitable for extracting text from text-based PDFs.</li>
          <li>PyMuPDF (Fitz): Generally more robust and faster. It can extract text, images, and metadata, and also identify if a PDF page is primarily image-based (indicating a need for OCR).</li>
        </ul>
      </li>
      <li><strong>Rust:</strong>
        <ul>
          <li>lopdf: Can load PDF documents and extract text from specific pages or all pages.</li>
          <li>pdf-extract: Another library focused on extracting content from PDF files.</li>
        </ul>
      </li>
      <li>Challenges: Encrypted or corrupted PDFs can cause errors. PyMuPDF can often identify these. Complex layouts with columns, tables, and embedded fonts can make text extraction difficult.</li>
    </ul>
  </li>
  <li><strong>DOCX (Office Open XML Document):</strong>
    <ul>
      <li><strong>Python:</strong>
        <ul>
          <li>python-docx: Allows reading and extracting text from paragraphs, tables, headers, and footers.</li>
          <li>docxpy: A utility to extract text, hyperlinks, and images from DOCX files.</li>
        </ul>
      </li>
      <li><strong>Rust:</strong>
        <ul>
          <li>docx-rust: A library for parsing DOCX files, allowing access to document content.</li>
          <li>dotext: A library aimed at extracting readable text from various document formats, including DOCX.</li>
        </ul>
      </li>
      <li>Challenges: Extracting text from complex tables or embedded objects (e.g., charts) in a meaningful way.</li>
    </ul>
  </li>
  <li><strong>TXT (Plain Text):</strong>
    <ul>
      <li><strong>Python:</strong> Standard file I/O operations (with open(…) as f: text = f.read()) are sufficient. Care must be taken with character encodings; attempting to decode as UTF-8 first, with fallbacks to other common encodings if necessary, is a good practice.</li>
      <li><strong>Rust:</strong> std::fs::read_to_string() is the standard way to read a file’s entire content into a string. Similar encoding considerations apply. The extractous crate also supports TXT file extraction.</li>
    </ul>
  </li>
</ul>

<p>For each file type, selecting the most robust and feature-rich library is important. Python libraries are often more mature and battle-tested for complex office formats. A hybrid approach, where Rust orchestrates the pipeline but calls Python scripts for specific parsing tasks (if Python libraries are demonstrably superior for a given format), is a viable strategy.</p>
  <h3 id="35-ocr-for-image-based-pdfs-and-scanned-documents">
    
    
     <a href="#35-ocr-for-image-based-pdfs-and-scanned-documents">#</a><a href="#" aria-label="Back to top"><strong>3.5. OCR for Image-Based PDFs and Scanned Documents</strong></a>
        
    
  </h3>
      

<p>If a PDF yields little or no extractable text (suggesting it’s image-based) or if image files containing text are found (e.g., extracted from archives), these must be processed by an Optical Character Recognition (OCR) engine.</p>

<ul>
  <li><strong>Recommended OCR Engines:</strong>
    <ul>
      <li><strong>Tesseract OCR:</strong> A widely-used, open-source engine with support for many languages. Python wrappers like pytesseract simplify its integration. It has shown good accuracy for various languages, including English (92% in one study).</li>
      <li><strong>PaddleOCR:</strong> An open-source toolkit from Baidu, known for strong performance, particularly with multilingual documents and complex layouts. It supports over 80 languages and offers tools for detection, recognition, and structure parsing.</li>
      <li><strong>docTR:</strong> A deep learning-based OCR developed by Mindee, available under an open-source license. It excels with structured documents and offers pre-trained models for text detection and recognition using TensorFlow and PyTorch.</li>
      <li><strong>EasyOCR:</strong> Known for its ease of integration and good performance on medium-quality or blurry images, supporting over 80 languages.</li>
      <li><strong>Kraken:</strong> A sophisticated OCR engine particularly well-suited for historical or complex documents, offering layout analysis and text recognition.</li>
    </ul>
  </li>
  <li><strong>Rust OCR Options:</strong>
    <ul>
      <li>ocrs: A Rust library and CLI tool for OCR that uses neural network models (trained in PyTorch, exported to ONNX) and the RTen inference engine. It aims for ease of compilation and cross-platform use, including WebAssembly. Currently recognizes Latin alphabet.</li>
      <li>extractous: This Rust library can integrate with Tesseract OCR, allowing Tesseract to be called from a Rust environment.</li>
    </ul>
  </li>
  <li><strong>Considerations for OCR:</strong>
    <ul>
      <li><strong>Accuracy:</strong> Academic documents often contain complex layouts, mathematical formulas, tables, and varied fonts. While Tesseract provides a strong open-source baseline , exploring modern alternatives like PaddleOCR or docTR is advisable. These engines feature advanced architectures and may offer benefits for complex layouts. However, direct comparative benchmarks for English academic documents were not available in the provided materials , so evaluation on a sample set is crucial.</li>
      <li><strong>Language Support:</strong> While English is likely predominant, the system should ideally support other languages if present in the corpus.</li>
      <li><strong>Performance:</strong> OCR is computationally intensive. Processing thousands of scanned pages will require significant time and CPU resources. The RK3588’s octa-core CPU or the Intel N97 can handle this.</li>
      <li><strong>ARM64 Compatibility:</strong> If running OCR locally on the RK3588, the chosen engine must be compatible. Tesseract can be compiled for ARM. PaddlePaddle (the framework behind PaddleOCR) has ARM support. ocrs (Rust) is inherently ARM-compatible if its dependencies are.</li>
      <li><strong>Image Pre-processing:</strong> To maximize OCR accuracy, input images should be pre-processed. This can include:
        <ul>
          <li><strong>Deskewing:</strong> Correcting tilted scans.</li>
          <li><strong>Binarization:</strong> Converting images to black and white.</li>
          <li><strong>Noise Removal:</strong> Eliminating speckles or unwanted marks.</li>
          <li><strong>Resolution Enhancement:</strong> Ensuring sufficient DPI (dots per inch), typically 300 DPI or higher for good OCR. Libraries like OpenCV (available for Python as opencv-python and for Rust via the opencv crate) are essential for these tasks.</li>
        </ul>
      </li>
      <li><strong>Handling Structural Noise:</strong> OCR can sometimes pick up repeated headers, footers, or page numbers. Strategies to identify and remove this “structural noise” post-OCR might be needed, though this can be challenging as such elements in academic papers might contain useful information (e.g., journal name, page range). Early detection of encrypted/corrupted files (e.g., using PyMuPDF) or low OCR confidence scores can help manage problematic documents by logging and skipping them.</li>
    </ul>
  </li>
</ul>

<p>For academic documents, accuracy is paramount. The RK3588’s Mali G610 GPU could potentially accelerate OCR if the chosen engine supports GPU acceleration via OpenCL or Vulkan and appropriate drivers/libraries are available and configured on Debian for the Mali GPU, but this significantly increases setup complexity; CPU-based OCR is more straightforward.<br />
The following table summarizes recommended libraries for file processing, which can serve as a quick reference:<br />
<strong>Table 1: Recommended File Processing Libraries</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">File Type</th>
      <th style="text-align: left">Python Library</th>
      <th style="text-align: left">Rust Crate</th>
      <th style="text-align: left">Key Features</th>
      <th style="text-align: left">Dependencies (Examples)</th>
      <th style="text-align: left">ARM64 Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">PDF (Text)</td>
      <td style="text-align: left">PyMuPDF (Fitz) , pypdf</td>
      <td style="text-align: left">lopdf , pdf-extract</td>
      <td style="text-align: left">Text extraction, metadata, image detection (PyMuPDF)</td>
      <td style="text-align: left">None</td>
      <td style="text-align: left">Python libs work. Rust crates are native.</td>
    </tr>
    <tr>
      <td style="text-align: left">PDF (Image/OCR)</td>
      <td style="text-align: left">pytesseract , paddleocr , doctr</td>
      <td style="text-align: left">ocrs , extractous (Tesseract wrapper)</td>
      <td style="text-align: left">Text recognition from images, layout analysis (PaddleOCR, docTR)</td>
      <td style="text-align: left">Tesseract, ONNX Runtime</td>
      <td style="text-align: left">Tesseract compiles on ARM. PaddleOCR/docTR models may run on ARM CPU/GPU. ocrs designed for Rust/ONNX.</td>
    </tr>
    <tr>
      <td style="text-align: left">DOCX</td>
      <td style="text-align: left">python-docx , docxpy</td>
      <td style="text-align: left">docx-rust , dotext</td>
      <td style="text-align: left">Text from paragraphs, tables, headers/footers</td>
      <td style="text-align: left">None</td>
      <td style="text-align: left">Python libs work. Rust crates are native.</td>
    </tr>
    <tr>
      <td style="text-align: left">TXT</td>
      <td style="text-align: left">Standard I/O (open().read())</td>
      <td style="text-align: left">std::fs::read_to_string()</td>
      <td style="text-align: left">Basic text reading</td>
      <td style="text-align: left">None</td>
      <td style="text-align: left">Native to both.</td>
    </tr>
    <tr>
      <td style="text-align: left">ZIP</td>
      <td style="text-align: left">zipfile (standard)</td>
      <td style="text-align: left">zip</td>
      <td style="text-align: left">Extraction, listing contents</td>
      <td style="text-align: left">None</td>
      <td style="text-align: left">Native to both.</td>
    </tr>
    <tr>
      <td style="text-align: left">TAR</td>
      <td style="text-align: left">tarfile (standard)</td>
      <td style="text-align: left">tar</td>
      <td style="text-align: left">Extraction, listing contents (supports.gz,.bz2)</td>
      <td style="text-align: left">None</td>
      <td style="text-align: left">Native to both.</td>
    </tr>
    <tr>
      <td style="text-align: left">RAR</td>
      <td style="text-align: left">rarfile , patoolib , extractcode</td>
      <td style="text-align: left">std::process::Command (to call unrar CLI) , potentially libarchive-rust (with caveats )</td>
      <td style="text-align: left">RAR extraction, including newer versions (v5+)</td>
      <td style="text-align: left">unrar CLI (for some)</td>
      <td style="text-align: left">unrar CLI has ARM64 versions. extractcode bundles dependencies.</td>
    </tr>
    <tr>
      <td style="text-align: left">File Type ID</td>
      <td style="text-align: left">python-magic , filetype</td>
      <td style="text-align: left">infer , file-type</td>
      <td style="text-align: left">Identification by magic numbers</td>
      <td style="text-align: left">libmagic (for some)</td>
      <td style="text-align: left">filetype (Python) and infer (Rust) are pure/native.</td>
    </tr>
    <tr>
      <td style="text-align: left">Archive (General)</td>
      <td style="text-align: left">extractcode</td>
      <td style="text-align: left">(Consider extractcode via Python interop or CLI tools)</td>
      <td style="text-align: left">Robust multi-format extraction, nested archives, error handling</td>
      <td style="text-align: left">Bundled (7z, libarchive)</td>
      <td style="text-align: left">extractcode aims for cross-platform, including ARM if its bundled tools support it.</td>
    </tr>
  </tbody>
</table>

<p>This table provides a consolidated view of tooling options, assisting in making informed choices based on language preference and specific file format needs, especially considering ARM64 compatibility for local processing on the RK3588.</p>
  <h2 id="4-phase-2-text-preparation-for-semantic-understanding">
    
    
     <a href="#4-phase-2-text-preparation-for-semantic-understanding">#</a><a href="#" aria-label="Back to top"><strong>4. Phase 2: Text Preparation for Semantic Understanding</strong></a>
        
    
  </h2>
      

<p>After raw text is extracted, it must be prepared for the embedding model. This involves cleaning, structuring, and selecting an appropriate model to convert text into meaningful numerical representations.</p>
  <h3 id="41-text-cleaning-and-normalization">
    
    
     <a href="#41-text-cleaning-and-normalization">#</a><a href="#" aria-label="Back to top"><strong>4.1. Text Cleaning and Normalization</strong></a>
        
    
  </h3>
      

<p>The quality of the text fed into the embedding model directly influences the quality of the resulting embeddings and, consequently, the search relevance.</p>

<ul>
  <li><strong>Standard Cleaning Steps:</strong>
    <ul>
      <li>Remove excessive whitespace (multiple spaces, leading/trailing spaces, redundant newlines).</li>
      <li>Eliminate or replace control characters that might have been introduced during extraction.</li>
      <li>Handle hyphenation: Attempt to rejoin words that were split across lines, especially if OCR was involved. This can be complex and might require dictionary lookups or sophisticated heuristics.</li>
      <li>Normalize Unicode characters to a consistent form (e.g., NFC - Normalization Form C) to handle different representations of the same character.</li>
    </ul>
  </li>
  <li><strong>Considerations for Academic Text:</strong>
    <ul>
      <li><strong>Case Preservation:</strong> Unlike general text processing where lowercasing is common, for academic documents, preserving case can be important for acronyms (e.g., “UNESCO,” “HIV”), proper nouns, and chemical formulas. Embedding models are often case-sensitive or have cased versions.</li>
      <li><strong>Boilerplate Removal:</strong> Headers, footers, and page numbers, if inconsistently extracted by OCR or parsers, can introduce noise. A careful strategy is needed; this might involve using layout analysis features from libraries like PyMuPDF to identify headers/footers based on their bounding boxes, or developing heuristics based on text recurrence, position on page, or distinct font styles, before applying NLP techniques to determine their utility, rather than blindly removing them.</li>
      <li><strong>Special Characters &amp; Formulas:</strong> Academic texts often contain mathematical symbols, Greek letters, and complex formulas. Ensure these are handled gracefully, typically by preserving them as Unicode characters within the text. While some specialized models might process LaTeX, most standard text embedding models will treat formulas as sequences of characters. The primary goal is to accurately embed the surrounding natural language text that explains or refers to these formulas.</li>
      <li><strong>Stemming/Lemmatization:</strong> Traditional NLP techniques like stemming (reducing words to their root form, e.g., “running” -&gt; “run”) or lemmatization (reducing words to their dictionary form, e.g., “ran” -&gt; “run”) might be considered. However, modern transformer-based embedding models are generally robust to inflectional variations and often perform better with full words, as they capture more contextual nuance. Their use should be evaluated carefully, as they can sometimes obscure meaning.</li>
    </ul>
  </li>
</ul>

<p>The goal is to produce clean, coherent text passages that accurately represent the document’s content. Over-aggressive cleaning can discard valuable information, so a balanced approach is necessary.</p>
  <h3 id="42-document-chunking-strategies">
    
    
     <a href="#42-document-chunking-strategies">#</a><a href="#" aria-label="Back to top"><strong>4.2. Document Chunking Strategies</strong></a>
        
    
  </h3>
      

<p>Large Language Models (LLMs) and embedding models have fixed context window sizes, meaning they can only process a limited amount of text at once. Therefore, long documents must be divided into smaller, semantically coherent segments or “chunks” before embedding. The choice of chunking strategy significantly impacts retrieval quality.</p>

<ul>
  <li><strong>Fixed-Size Chunking:</strong> The simplest method, dividing text into chunks of a predetermined character or token count, often with some overlap between chunks.
    <ul>
      <li><em>Advantage:</em> Easy to implement.</li>
      <li><em>Disadvantage:</em> Often splits sentences or paragraphs mid-thought, breaking semantic context and potentially reducing retrieval accuracy.</li>
    </ul>
  </li>
  <li><strong>Recursive Character Splitting:</strong> This method attempts to split text based on a predefined list of separators, trying them in order until the resulting chunks are small enough. A common default list of separators is [”\n\n”, “\n”, “ “, “”], which prioritizes keeping paragraphs together, then sentences, then words. LangChain recommends this for generic text. This approach is generally superior to fixed-size chunking for maintaining semantic coherence.</li>
  <li><strong>Semantic Chunking:</strong> This more advanced strategy involves splitting text by grouping semantically similar sentences. It typically requires an initial pass of embedding sentences and then clustering or splitting based on embedding similarity (e.g., splitting when the cosine distance between consecutive sentence embeddings exceeds a threshold).
    <ul>
      <li><em>Advantage:</em> Produces highly context-aware chunks.</li>
      <li><em>Disadvantage:</em> More computationally intensive during the preprocessing phase as it requires initial embedding generation for chunking decisions.</li>
    </ul>
  </li>
  <li><strong>Document-based / Layout-aware Chunking:</strong> This strategy leverages the inherent structure of documents, such as headings, sections, lists, and tables, to define chunk boundaries. For structured documents like academic papers (which typically have titles, abstracts, sections, subsections), this can be very effective. Vertex AI Search, for example, can use layout parsing for PDF, HTML, and DOCX files to identify elements like text blocks, tables, and headings to guide chunking. For academic documents, a strategy that combines layout awareness with recursive splitting is ideal. This could involve first using a library like PyMuPDF to parse the document into structural elements (e.g., paragraphs, sections based on headings, tables). Then, apply a recursive character splitter (like LangChain’s ) to these larger structural elements if they still exceed the desired chunk size. This approach respects natural semantic boundaries identified by the document’s layout.</li>
  <li><strong>Key Parameters for Chunking:</strong>
    <ul>
      <li>chunk_size: The maximum number of tokens or characters allowed in a single chunk. This should be determined based on the context window of the chosen embedding model and the desired granularity of information retrieval.</li>
      <li>chunk_overlap: The number of tokens or characters that overlap between adjacent chunks. This helps preserve context that might otherwise be lost at chunk boundaries.</li>
    </ul>
  </li>
</ul>

<p>For academic documents, a layout-aware recursive splitter would likely be the most effective strategy. If implementing full layout parsing is too complex initially, <strong>recursive character splitting</strong> using paragraph and sentence delimiters (\n\n, \n) is a strong alternative. Semantic chunking could be explored as a later optimization if the initial retrieval quality needs improvement. The chosen chunk size should be well within the embedding model’s maximum input token limit.</p>
  <h3 id="43-embedding-model-selection">
    
    
     <a href="#43-embedding-model-selection">#</a><a href="#" aria-label="Back to top"><strong>4.3. Embedding Model Selection</strong></a>
        
    
  </h3>
      

<p>The choice of embedding model is critical for the success of a semantic search system. The model transforms text chunks into numerical vectors, where semantically similar chunks have vectors that are close together in the vector space.</p>

<ul>
  <li><strong>Criteria for Selection:</strong>
    <ul>
      <li><strong>Accuracy on Domain-Specific Text:</strong> Models trained or fine-tuned on academic, scientific, or legal corpora are likely to perform better for “sociology of quantification” or “jurimetrics” than generic models.</li>
      <li><strong>Performance (Speed vs. Quality):</strong> Larger models often provide better embeddings but are slower and more resource-intensive.</li>
      <li><strong>Cost:</strong> API-based models incur costs per token/request , while local models have an upfront setup cost (time, compute for inference) but are “free” per inference thereafter.</li>
      <li><strong>Local Deployment (ARM64 Compatibility):</strong> For running on the RK3588, the model and its inference runtime must support ARM64. Many Hugging Face models can be converted to ONNX format and run using runtimes like ORT (ONNX Runtime), Candle, or RTen, which have varying degrees of ARM support. The RK3588’s NPU could offer acceleration if models are quantized (e.g., to INT8) and a compatible runtime (like RKNN-Toolkit or Tengine Lite) supports the specific ONNX operations, but this adds significant implementation complexity. For NPU acceleration on the RK3588, models typically need to be quantized (e.g., to INT8 format) and run using a compatible runtime like RKNN-Toolkit or Tengine Lite, which supports the specific ONNX operations in the quantized model. CPU-based inference on the RK3588’s octa-core processor is more straightforward.</li>
      <li><strong>Context Length:</strong> The model’s maximum input token limit must accommodate the chosen chunk_size.</li>
      <li><strong>Embedding Dimensionality:</strong> Higher dimensions can capture more nuance but increase storage requirements and can sometimes make similarity search slower or require more data for effective training/use. Common dimensions range from 384 to 1536 or even higher.</li>
    </ul>
  </li>
  <li><strong>Recommended Embedding Model Options:</strong>
    <ul>
      <li><strong>Open Source / Local Deployment (Potentially on RK3588):</strong>
        <ul>
          <li><strong>Sentence-Transformers (from Hugging Face):</strong> A widely used library providing access to many pre-trained models.
            <ul>
              <li>all-mpnet-base-v2: A strong general-purpose model, good baseline. Output dimension: 768.</li>
              <li>all-MiniLM-L6-v2: A smaller, faster model, good for resource-constrained environments or when speed is critical, though potentially less accurate than larger models. Output dimension: 384.</li>
              <li>BAAI/bge-large-en-v1.5: A high-performing model on various benchmarks, often a top choice for English text. Output dimension: 1024.</li>
              <li>Alibaba-NLP/gte-base-en-v1.5 or thenlper/gte-large: Other strong general-purpose models.</li>
              <li><strong>Domain-Specific Recommendation:</strong> NeuML/pubmedbert-base-embeddings. This model is fine-tuned on PubMed abstracts, making it particularly well-suited for biomedical and scientific literature. Its evaluation results show superior performance on PubMed-related tasks compared to general models like all-MiniLM-L6-v2 and bge-base-en-v1.5. Given the academic nature of the user’s documents, this model is a strong candidate for achieving high relevance, even if the topics are sociology/law rather than pure medicine, as academic writing styles share similarities. Output dimension: 768.</li>
            </ul>
          </li>
          <li><strong>Running on ARM64 (RK3588):</strong> Sentence Transformer models can be exported to ONNX format. Rust-based ONNX runtimes like rten or candle can then execute these models on the ARM CPU. Python’s onnxruntime also supports ARM64. While local deployment offers control, embedding a large corpus (“thousands of files”) on an SBC will be time-consuming compared to cloud APIs.</li>
        </ul>
      </li>
      <li><strong>Commercial API-based Models:</strong>
        <ul>
          <li><strong>OpenAI:</strong> OpenAI’s newer models like text-embedding-3-small (1536 dimensions, $0.02 / 1M tokens) and text-embedding-3-large (3072 dimensions, $0.13 / 1M tokens) offer strong performance and are recommended. The older text-embedding-ada-002 model (1536 dimensions, max input 8191 tokens) is also an option; its pricing is listed as $0.02/1M tokens in and $0.10/1M tokens in. Users should verify current pricing on OpenAI’s official site. Max input for all these models is 8191 tokens.</li>
          <li><strong>Cohere:</strong> Offers models like embed-english-v3.0 (Dimension: 1024), embed-multilingual-v3.0 (Dimension: 1024). Context length: 512 tokens. Direct API pricing is around $0.10 / 1M tokens for Embed 3. Alternatively, deployment via cloud marketplaces like Azure may offer different pricing structures, such as $0.0001 per 1000 tokens for embedding usage, plus any instance costs.</li>
          <li><strong>Jina AI:</strong> Offers models like jina-embeddings-v2-base-en (ColBERT-style late interaction, potentially good for search). Pricing: $0.18 / 1M tokens.</li>
        </ul>
      </li>
      <li><strong>General Guidance on Choosing:</strong> and provide general advice: consider accuracy for the specific domain, speed, scalability, and cost. For academic texts, a model with strong semantic understanding of formal language is key.</li>
    </ul>
  </li>
</ul>

<p>Given the user’s technical expertise and the capabilities of the RK3588, starting with a high-quality open-source Sentence Transformer model like NeuML/pubmedbert-base-embeddings or BAAI/bge-large-en-v1.5 deployed locally via ONNX is a strong recommendation. This offers control and avoids API costs. If local deployment proves too complex or performance on the ARM CPU is insufficient for the volume, then an API like OpenAI’s text-embedding-3-small (for balance) or text-embedding-3-large (for maximum quality) would be the next best option.</p>
  <h3 id="44-generating-text-embeddings">
    
    
     <a href="#44-generating-text-embeddings">#</a><a href="#" aria-label="Back to top"><strong>4.4. Generating Text Embeddings</strong></a>
        
    
  </h3>
      

<p>Once a model is selected and text is chunked, embeddings are generated for each chunk.</p>

<ul>
  <li><strong>Process:</strong> Each text chunk is fed to the chosen embedding model (whether local or API-based). The model outputs a dense vector (a list of floating-point numbers) representing that chunk’s semantic meaning.</li>
  <li><strong>Implementation:</strong>
    <ul>
      <li><strong>Python:</strong>
        <ul>
          <li>For local Sentence Transformers: Use the sentence-transformers library. model.encode(chunks) will return a list of embeddings.</li>
          <li>For ONNX models: Use onnxruntime to load the model and run inference.</li>
          <li>For APIs: Use the respective SDKs (e.g., openai.Embedding.create(…), cohere_client.embed(…)). Batch requests to APIs to improve efficiency and reduce the number of calls.</li>
        </ul>
      </li>
      <li><strong>Rust:</strong>
        <ul>
          <li>For local ONNX models: Use an ONNX runtime crate like rten, candle, or ort. This involves loading the ONNX model and its tokenizer, tokenizing the chunks, and then running inference.</li>
          <li>For APIs: Use an HTTP client like reqwest to make calls to the embedding endpoints, or use a dedicated Rust client crate if one exists for the chosen provider.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Metadata Association:</strong> It is critical to store each generated embedding alongside relevant metadata. This metadata should include:
    <ul>
      <li>A unique ID for the chunk.</li>
      <li>The original file’s path (or Google Drive ID).</li>
      <li>The position or ID of the chunk within the original document (e.g., chunk sequence number, character offset).</li>
      <li>The actual text of the chunk (useful for displaying search results without re-fetching from the original file).</li>
      <li>Source of the file (e.g., “local_hdd”, “gdrive”).</li>
    </ul>
  </li>
  <li><strong>Computational Load:</strong> This step is computationally intensive, especially with thousands of documents, each potentially yielding many chunks. The RK3588’s octa-core ARM CPU will be the primary workhorse for local embedding generation. If the workload is very large, distributing the embedding generation task (e.g., RK3588 processes chunks from local files, Intel N97 processes chunks from GDrive files, both writing to a central vector DB) could be considered.</li>
</ul>

<p>Error handling is important here, particularly for API calls (network issues, rate limits) or if a local model encounters an issue with a specific chunk (e.g., too long after tokenization, malformed input).</p>
  <h2 id="5-phase-3-vector-storage-and-indexing">
    
    
     <a href="#5-phase-3-vector-storage-and-indexing">#</a><a href="#" aria-label="Back to top"><strong>5. Phase 3: Vector Storage and Indexing</strong></a>
        
    
  </h2>
      

<p>After generating embeddings, they must be stored and indexed efficiently to enable fast similarity searches. This is the role of a vector database.</p>
  <h3 id="51-vector-database-selection">
    
    
     <a href="#51-vector-database-selection">#</a><a href="#" aria-label="Back to top"><strong>5.1. Vector Database Selection</strong></a>
        
    
  </h3>
      

<p>Choosing an appropriate vector database is a critical decision, impacting performance, scalability, and ease of deployment, especially on the user’s ARM64-based RK3588 hardware.</p>

<ul>
  <li><strong>Key Criteria for Selection:</strong>
    <ul>
      <li><strong>ARM64 Support:</strong> Essential for local deployment on the RK3588. This includes availability of ARM64 Docker images or native binaries.</li>
      <li><strong>Performance:</strong> Low query latency and high ingestion throughput are crucial.</li>
      <li><strong>Scalability:</strong> Ability to handle the current volume (“thousands of files,” translating to potentially tens or hundreds of thousands of vector embeddings) and future growth.</li>
      <li><strong>Persistence:</strong> The database must persist data to disk so that embeddings don’t need to be regenerated if the system restarts.</li>
      <li><strong>Ease of Use &amp; Deployment:</strong> Simple setup, clear API, good documentation. Docker deployment is often preferred for managing dependencies.</li>
      <li><strong>Client Libraries:</strong> Availability of robust Python and Rust client libraries.</li>
      <li><strong>Metadata Filtering:</strong> The ability to filter search results based on stored metadata (e.g., file source, original filename, date) alongside vector similarity.</li>
      <li><strong>License:</strong> Open-source options are plentiful, though the user is open to commercial solutions.</li>
      <li><strong>Resource Consumption:</strong> Memory and CPU usage, particularly important for deployment on an SBC like the RK3588.</li>
    </ul>
  </li>
  <li><strong>Recommended Local Vector Database Options (Considering RK3588 ARM64):</strong>
    <ul>
      <li><strong>Qdrant:</strong>
        <ul>
          <li><em>Features:</em> Written in Rust, performance-focused, supports HNSW indexing, filtering, on-disk persistence, scalar and product quantization.</li>
          <li><em>ARM64 Support:</em> Excellent. Official Docker images for ARM64 are available (qdrant/qdrant on DockerHub supports multiple architectures including arm64). Native compilation from source on ARM64 is also possible.</li>
          <li><em>Clients:</em> Official Python (qdrant-client) and Rust (qdrant-client) clients.</li>
          <li><em>Suitability:</em> Strong candidate due to Rust origins, explicit ARM64 support, performance, and feature set.</li>
        </ul>
      </li>
      <li><strong>Milvus Lite:</strong>
        <ul>
          <li><em>Features:</em> Lightweight version of Milvus bundled with the pymilvus Python SDK. Supports persistence to a local file. Good for up to ~1 million vectors.</li>
          <li><em>ARM64 Support:</em> Supported on Ubuntu ARM64 and macOS Apple Silicon. pip install pymilvus should handle this.</li>
          <li><em>Clients:</em> Primarily Python (pymilvus).</li>
          <li><em>Suitability:</em> Very easy to get started with for Python-centric projects on ARM64.</li>
        </ul>
      </li>
      <li><strong>ChromaDB:</strong>
        <ul>
          <li><em>Features:</em> Open-source, designed for ease of use and local development. Supports persistence. Uses HNSW for indexing.</li>
          <li><em>ARM64 Support:</em> OS-independent, pip install chromadb is expected to work on Linux ARM64.</li>
          <li><em>Clients:</em> Python client is primary.</li>
          <li><em>Suitability:</em> Good for rapid prototyping and smaller datasets.</li>
        </ul>
      </li>
      <li><strong>Weaviate:</strong>
        <ul>
          <li><em>Features:</em> Feature-rich open-source vector database, supports various vectorization modules, filtering, and GraphQL/REST APIs.</li>
          <li><em>ARM64 Support:</em> Official Docker images for ARM64 are available (e.g., cr.weaviate.io/semitechnologies/weaviate with arm64 tags or multi-arch images).</li>
          <li><em>Clients:</em> Official Python client. Rust clients may be community-supported.</li>
          <li><em>Suitability:</em> Viable for Docker deployment on RK3588, offers many advanced features.</li>
        </ul>
      </li>
      <li><strong>FAISS (Facebook AI Similarity Search):</strong>
        <ul>
          <li><em>Features:</em> A library for efficient similarity search and clustering of dense vectors, not a full-fledged database system. Requires manual setup for persistence, serving, and metadata handling.</li>
          <li><em>ARM64 Support:</em> faiss-cpu Python package provides precompiled wheels for aarch64 (ARM64) Linux on PyPI.</li>
          <li><em>Clients:</em> Python and C++.</li>
          <li><em>Suitability:</em> More DIY, but offers fine-grained control if building a custom solution.</li>
        </ul>
      </li>
      <li><strong>SahomeDB:</strong>
        <ul>
          <li><em>Features:</em> An embedded vector database written in Rust, using Sled for persistence and HNSW for indexing. Designed to be lightweight and easy to use.</li>
          <li><em>ARM64 Support:</em> As a Rust crate, it can be compiled for ARM64.</li>
          <li><em>Clients:</em> Native Rust API and Python bindings.</li>
          <li><em>Suitability:</em> An interesting Rust-native embedded option, potentially very efficient on the RK3588 if its feature set meets requirements.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Cloud/Managed Options (Fallback or Future Scaling):</strong>
    <ul>
      <li><strong>Pinecone:</strong> Fully managed, developer-friendly, strong performance, hybrid search.</li>
      <li><strong>Zilliz Cloud (Managed Milvus):</strong> Enterprise-grade managed Milvus service offering various tiers and features.</li>
      <li><strong>Google Cloud Vertex AI Vector Search:</strong> Integrated with Google Cloud, suitable if other GCP services are used.</li>
    </ul>
  </li>
</ul>

<p>For the user’s scenario, prioritizing local deployment on the RK3588, <strong>Qdrant</strong> stands out due to its Rust foundation (aligning with user preference for Rust’s efficiency), excellent ARM64 support (both Docker and native), robust feature set including persistence and filtering, and official clients for both Python and Rust. <strong>SahomeDB</strong> is a compelling Rust-native embedded alternative if a simpler, integrated solution is preferred. Milvus Lite and ChromaDB are strong Python-centric choices for ease of setup on ARM64.</p>
  <h3 id="52-setting-up-and-configuring-the-chosen-vector-database">
    
    
     <a href="#52-setting-up-and-configuring-the-chosen-vector-database">#</a><a href="#" aria-label="Back to top"><strong>5.2. Setting Up and Configuring the Chosen Vector Database</strong></a>
        
    
  </h3>
      

<p>Assuming <strong>Qdrant</strong> is selected as the primary candidate for local deployment on the RK3588:</p>

<ul>
  <li><strong>Installation (Docker Recommended):</strong>
    <ul>
      <li>Pull the official Qdrant Docker image: docker pull qdrant/qdrant</li>
      <li>Run the container, mapping ports and a volume for persistent storage:<br />
<code class="language-plaintext highlighter-rouge">docker run -d -p 6333:6333 -p 6334:6334 \</code><br />
    <code class="language-plaintext highlighter-rouge">-v $(pwd)/qdrant_storage:/qdrant/storage \</code><br />
    <code class="language-plaintext highlighter-rouge">qdrant/qdrant</code><br />
This command maps port 6333 for gRPC (used by clients) and 6334 for the REST API/Web UI. Data will be stored in the qdrant_storage directory in the current host path.</li>
    </ul>
  </li>
  <li><strong>Configuration:</strong>
    <ul>
      <li>Qdrant’s configuration can be managed via a configuration file (config/production.yaml if mounted into the container) or environment variables. For the RK3588 with 32GB RAM, default memory settings should be reasonable, but monitor resource usage.</li>
      <li>Ensure persistence is correctly configured so data survives container restarts.</li>
    </ul>
  </li>
  <li><strong>Schema Definition (Creating a Collection):</strong>
    <ul>
      <li>Using the Qdrant client (Python or Rust), create a “collection” to store the document embeddings.</li>
      <li>Specify:
        <ul>
          <li>vector_size: The dimensionality of the embeddings produced by the chosen embedding model (e.g., 768 for all-mpnet-base-v2 or NeuML/pubmedbert-base-embeddings).</li>
          <li>distance: The distance metric for similarity search. For sentence embeddings, Cosine similarity is standard. Qdrant supports Cosine, Euclidean, and Dot product.</li>
        </ul>
      </li>
      <li>
        <p>Conceptual Python client code for Qdrant:<br />
<code class="language-plaintext highlighter-rouge"># from qdrant_client import QdrantClient</code><br />
<code class="language-plaintext highlighter-rouge"># from qdrant_client.http.models import Distance, VectorParams # For older client versions</code><br />
<code class="language-plaintext highlighter-rouge"># from qdrant_client.models import Distance, VectorParams # For newer client versions (check Qdrant docs)</code></p>

        <p><code class="language-plaintext highlighter-rouge"># client = QdrantClient(host="localhost", port=6333) # Or use url="http://localhost:6333"</code><br />
<code class="language-plaintext highlighter-rouge"># collection_name = "academic_documents"</code><br />
<code class="language-plaintext highlighter-rouge"># embedding_dim = 768 # Example dimension</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># try:</code><br />
<code class="language-plaintext highlighter-rouge">#     client.get_collection(collection_name=collection_name)</code><br />
<code class="language-plaintext highlighter-rouge">#     # print(f"Collection '{collection_name}' already exists.")</code><br />
<code class="language-plaintext highlighter-rouge"># except Exception: # More specific exception handling is better (e.g., from qdrant_client.http.exceptions import UnexpectedResponse)</code><br />
<code class="language-plaintext highlighter-rouge">#     client.recreate_collection( # or client.create_collection for newer versions</code><br />
<code class="language-plaintext highlighter-rouge">#         collection_name=collection_name,</code><br />
<code class="language-plaintext highlighter-rouge">#         vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE)</code><br />
<code class="language-plaintext highlighter-rouge">#     )</code><br />
<code class="language-plaintext highlighter-rouge">#     # print(f"Collection '{collection_name}' created.")</code></p>
      </li>
      <li>Conceptual Rust client code for Qdrant:<br />
<code class="language-plaintext highlighter-rouge">// use qdrant_client::Qdrant;</code><br />
<code class="language-plaintext highlighter-rouge">// use qdrant_client::qdrant::{CreateCollection, VectorParams, Distance, VectorsConfig}; // Check specific imports for your client version</code><br />
<code class="language-plaintext highlighter-rouge">//</code><br />
<code class="language-plaintext highlighter-rouge">// async fn setup_qdrant_collection_rust(client: &amp;Qdrant, collection_name: &amp;str, embedding_dim: u64) -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {</code><br />
<code class="language-plaintext highlighter-rouge">//     // Check if collection exists, if not create it</code><br />
<code class="language-plaintext highlighter-rouge">//     match client.collection_info(collection_name).await {</code><br />
<code class="language-plaintext highlighter-rouge">//         Ok(_) =&gt; {</code><br />
<code class="language-plaintext highlighter-rouge">//             // println!("Collection '{}' already exists.", collection_name);</code><br />
<code class="language-plaintext highlighter-rouge">//         }</code><br />
<code class="language-plaintext highlighter-rouge">//         Err(_) =&gt; { // Simplified error handling, check actual error type</code><br />
<code class="language-plaintext highlighter-rouge">//             client.create_collection(&amp;CreateCollection {</code><br />
<code class="language-plaintext highlighter-rouge">//                 collection_name: collection_name.to_string(),</code><br />
<code class="language-plaintext highlighter-rouge">//                 vectors_config: Some(VectorsConfig::Params(VectorParams { // Structure might vary with client version</code><br />
<code class="language-plaintext highlighter-rouge">//                     size: embedding_dim,</code><br />
<code class="language-plaintext highlighter-rouge">//                     distance: Distance::Cosine.into(),</code><br />
<code class="language-plaintext highlighter-rouge">//                   ..Default::default() // for on_disk, hnsw_config etc.</code><br />
<code class="language-plaintext highlighter-rouge">//                 })),</code><br />
<code class="language-plaintext highlighter-rouge">//               ..Default::default()</code><br />
<code class="language-plaintext highlighter-rouge">//             }).await?;</code><br />
<code class="language-plaintext highlighter-rouge">//             // println!("Collection '{}' created.", collection_name);</code><br />
<code class="language-plaintext highlighter-rouge">//         }</code><br />
<code class="language-plaintext highlighter-rouge">//     }</code><br />
<code class="language-plaintext highlighter-rouge">//     Ok(())</code><br />
<code class="language-plaintext highlighter-rouge">// }</code></li>
    </ul>
  </li>
</ul>

<p>The collection will store points, where each point consists of an ID, its vector embedding, and an optional payload (metadata). The payload should store original_file_path, gdrive_file_id (if applicable), chunk_text, chunk_id_within_document, and source_location (local/GDrive).</p>
  <h3 id="53-indexing-strategies-for-efficient-search">
    
    
     <a href="#53-indexing-strategies-for-efficient-search">#</a><a href="#" aria-label="Back to top"><strong>5.3. Indexing Strategies for Efficient Search</strong></a>
        
    
  </h3>
      

<p>Once the vector database and collection are set up, the generated embeddings and their associated metadata are inserted (indexed).</p>

<ul>
  <li><strong>Indexing Algorithm:</strong> Most modern vector databases, including Qdrant, Weaviate, Milvus, and ChromaDB, primarily use or offer <strong>HNSW (Hierarchical Navigable Small World)</strong> as a key indexing algorithm for Approximate Nearest Neighbor (ANN) search. HNSW provides a good balance between search speed, accuracy (recall), and ingestion overhead.</li>
  <li><strong>HNSW Parameters:</strong>
    <ul>
      <li>m: The maximum number of bi-directional links created for every new element during construction. Higher m generally leads to better recall and faster search but increases index build time and memory usage. Typical values: 16-64.</li>
      <li>ef_construction: The size of the dynamic list for the nearest neighbors search during index construction. Higher values lead to a more accurate index but slower build times. Typical values: 100-500.</li>
      <li>ef (or ef_search): The size of the dynamic list for the nearest neighbors search at query time. Higher values increase recall and precision but also query latency. This can often be tuned at query time. Qdrant’s defaults are often a good starting point. For the scale of “thousands of files,” extensive HNSW tuning might not be critical initially but is an avenue for optimization if search performance or accuracy needs improvement. The optimal values for these parameters are dataset-dependent and often require experimentation to balance search speed, accuracy, and resource usage.</li>
    </ul>
  </li>
  <li><strong>Batching Insertions:</strong> When adding embeddings to the database, batch multiple points together in a single API call to the client. This is significantly more efficient than inserting points one by one, reducing network overhead and allowing the database to optimize ingestion.</li>
  <li><strong>Quantization (Optional for current scale):</strong> For very large datasets (millions to billions of vectors), vector quantization techniques (like Scalar Quantization or Product Quantization) can be used to compress embeddings, reducing memory and disk footprint at the cost of some precision. Qdrant supports scalar quantization. For the current scale, this is likely not necessary but is a future scalability option.</li>
  <li><strong>Persistence:</strong> Ensure the vector database is configured for on-disk persistence so that the index and data are not lost upon restart. Qdrant, when run with a mounted volume, persists data by default.</li>
</ul>

<p>The key is to ensure that the index is built correctly and can be efficiently queried. For the RK3588, memory usage of the HNSW index will be a factor; however, with 32GB of RAM, it should comfortably handle embeddings from thousands of documents, especially if only a portion of the index needs to be in active RAM for querying.<br />
The following table provides a comparative overview of potential vector database choices, focusing on aspects relevant to the user’s requirements:<br />
<strong>Table 2: Vector Database Comparison for Local Deployment</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: left">Qdrant</th>
      <th style="text-align: left">Milvus Lite</th>
      <th style="text-align: left">ChromaDB</th>
      <th style="text-align: left">Weaviate (Docker)</th>
      <th style="text-align: left">SahomeDB</th>
      <th style="text-align: left">FAISS (Library)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Type</strong></td>
      <td style="text-align: left">Standalone Server</td>
      <td style="text-align: left">Embedded (in Python)</td>
      <td style="text-align: left">Embedded/Server</td>
      <td style="text-align: left">Standalone Server</td>
      <td style="text-align: left">Embedded (in Rust)</td>
      <td style="text-align: left">Library</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Deployment (Local)</strong></td>
      <td style="text-align: left">Docker, Native Binary</td>
      <td style="text-align: left">Python package (pymilvus)</td>
      <td style="text-align: left">Python package (chromadb)</td>
      <td style="text-align: left">Docker</td>
      <td style="text-align: left">Rust crate, Python bindings</td>
      <td style="text-align: left">Python/C++ library</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>ARM64 Support</strong></td>
      <td style="text-align: left">Yes (Official Docker, Native)</td>
      <td style="text-align: left">Yes (Ubuntu, macOS)</td>
      <td style="text-align: left">Yes (OS-independent Python)</td>
      <td style="text-align: left">Yes (Official Docker arm64 images)</td>
      <td style="text-align: left">Yes (Compiles on ARM64)</td>
      <td style="text-align: left">Yes (faiss-cpu aarch64 wheels)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Key Indexing Algorithm</strong></td>
      <td style="text-align: left">HNSW, Full-text (planned)</td>
      <td style="text-align: left">HNSW, IVF_FLAT, etc.</td>
      <td style="text-align: left">HNSW</td>
      <td style="text-align: left">HNSW, Flat</td>
      <td style="text-align: left">HNSW</td>
      <td style="text-align: left">HNSW, IVF_PQ, LSH, etc.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Persistence</strong></td>
      <td style="text-align: left">Yes (On-disk)</td>
      <td style="text-align: left">Yes (Local file)</td>
      <td style="text-align: left">Yes (Local files)</td>
      <td style="text-align: left">Yes (Docker volume)</td>
      <td style="text-align: left">Yes (Sled disk storage)</td>
      <td style="text-align: left">Manual (save/load index)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Python Client</strong></td>
      <td style="text-align: left">Yes (Official)</td>
      <td style="text-align: left">Yes (pymilvus)</td>
      <td style="text-align: left">Yes (Official)</td>
      <td style="text-align: left">Yes (Official)</td>
      <td style="text-align: left">Yes (Bindings)</td>
      <td style="text-align: left">Yes (Official)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Rust Client</strong></td>
      <td style="text-align: left">Yes (Official)</td>
      <td style="text-align: left">No (gRPC possible)</td>
      <td style="text-align: left">No (HTTP API possible)</td>
      <td style="text-align: left">Community/HTTP API</td>
      <td style="text-align: left">Yes (Native)</td>
      <td style="text-align: left">C++ API, Rust bindings possible</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Metadata Filtering</strong></td>
      <td style="text-align: left">Yes (Rich filtering)</td>
      <td style="text-align: left">Yes</td>
      <td style="text-align: left">Yes</td>
      <td style="text-align: left">Yes (GraphQL-like)</td>
      <td style="text-align: left">Yes</td>
      <td style="text-align: left">Limited (ID-based, or via separate metadata store)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Notable Features</strong></td>
      <td style="text-align: left">Performance, Rust-native, Quantization, On-disk vectors</td>
      <td style="text-align: left">Easy setup for Python, Good for &lt;1M vectors</td>
      <td style="text-align: left">Developer-friendly, Simple API</td>
      <td style="text-align: left">Modular, Multi-modal support, Auto-vectorization options</td>
      <td style="text-align: left">Rust-native, Lightweight embedded, Incremental ops</td>
      <td style="text-align: left">Highly optimized ANN algorithms, GPU support (not faiss-cpu)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>License</strong></td>
      <td style="text-align: left">Apache 2.0</td>
      <td style="text-align: left">Apache 2.0</td>
      <td style="text-align: left">Apache 2.0</td>
      <td style="text-align: left">Apache 2.0</td>
      <td style="text-align: left">MIT / Apache 2.0</td>
      <td style="text-align: left">MIT</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Primary Use on RK3588</strong></td>
      <td style="text-align: left">Excellent choice, especially if Rust components are significant.</td>
      <td style="text-align: left">Good for Python-heavy pipeline if scale is moderate.</td>
      <td style="text-align: left">Good for Python-heavy pipeline, rapid prototyping.</td>
      <td style="text-align: left">Viable with Docker, offers more features if needed.</td>
      <td style="text-align: left">Excellent if a pure Rust, embedded solution is desired for efficiency.</td>
      <td style="text-align: left">Possible, but requires more infrastructure code around it.</td>
    </tr>
  </tbody>
</table>

<p>This table should aid in selecting the vector database that best fits the user’s hardware (RK3588), technical preferences (Rust/Python), and the scale of the project. Qdrant and SahomeDB are particularly appealing for a Rust-centric or high-performance local deployment on ARM64.</p>
  <h2 id="6-phase-4-implementing-the-search-and-retrieval-interface">
    
    
     <a href="#6-phase-4-implementing-the-search-and-retrieval-interface">#</a><a href="#" aria-label="Back to top"><strong>6. Phase 4: Implementing the Search and Retrieval Interface</strong></a>
        
    
  </h2>
      

<p>This phase focuses on enabling users to query the indexed documents and receive relevant results.</p>
  <h3 id="61-query-processing">
    
    
     <a href="#61-query-processing">#</a><a href="#" aria-label="Back to top"><strong>6.1. Query Processing</strong></a>
        
    
  </h3>
      

<p>To perform a semantic search, the user’s input query must be transformed into a vector embedding using the <em>exact same</em> embedding model and preprocessing steps (if any were applied to document chunks) that were used during the document indexing phase.</p>

<ul>
  <li><strong>Input:</strong> A natural language query from the user (e.g., “sociology of quantification and its impact on legal frameworks”).</li>
  <li><strong>Process:</strong>
    <ol>
      <li>(Optional, minimal) Clean the query text (e.g., trim whitespace). Extensive cleaning like stop-word removal is generally not needed for queries with modern embedding models.</li>
      <li>Generate an embedding for the query using the selected embedding model (e.g., NeuML/pubmedbert-base-embeddings locally, or OpenAI API).</li>
    </ol>
  </li>
  <li><strong>Output:</strong> A query vector.</li>
</ul>

<p>Consistency is paramount: if document chunks were, for example, prefixed with “passage: “ before embedding, queries should also be prefixed with “query: “ (or the appropriate prefix as per the model’s documentation) to ensure they are in a comparable part of the embedding space.</p>
  <h3 id="62-performing-similarity-search">
    
    
     <a href="#62-performing-similarity-search">#</a><a href="#" aria-label="Back to top"><strong>6.2. Performing Similarity Search</strong></a>
        
    
  </h3>
      

<p>The generated query vector is then used to search the vector database.</p>

<ul>
  <li><strong>Process:</strong>
    <ol>
      <li>Connect to the vector database using its client library (Python or Rust).</li>
      <li>Submit the query vector to the search/query endpoint of the relevant collection.</li>
      <li>Specify parameters:
        <ul>
          <li>k (or top_k, limit): The number of most similar results to retrieve (e.g., 10, 20).</li>
          <li>distance_metric: Ensure this matches the metric used when creating the collection (e.g., Cosine similarity).</li>
          <li>(Optional) Metadata filters: If the user wants to narrow the search (e.g., only files from Google Drive, or files processed after a certain date), these filters can be applied if supported by the vector DB.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li><strong>Output:</strong> The vector database will return a list of the k most similar document chunks. Each result typically includes:
    <ul>
      <li>The ID of the retrieved chunk.</li>
      <li>The similarity score (e.g., cosine similarity value, where higher is better, or a distance where lower is better, depending on the DB and metric).</li>
      <li>The stored metadata associated with that chunk (original file path, chunk text, etc.).</li>
    </ul>
  </li>
</ul>

<p>Vector databases like Qdrant, Milvus, Chroma, and Weaviate handle the complex Approximate Nearest Neighbor (ANN) search internally, abstracting this from the application developer.</p>
  <h3 id="63-presenting-search-results">
    
    
     <a href="#63-presenting-search-results">#</a><a href="#" aria-label="Back to top"><strong>6.3. Presenting Search Results</strong></a>
        
    
  </h3>
      

<p>Effective presentation of search results is crucial for user experience. The goal is to allow the user to quickly assess the relevance of each retrieved item.</p>

<ul>
  <li><strong>For each retrieved chunk/document:</strong>
    <ul>
      <li><strong>Original File Path/Identifier:</strong> Display the full path to the local file or a meaningful identifier for Google Drive files (e.g., GDrive name/path and ID). If a UI is developed, this could be a clickable link to open the file.</li>
      <li><strong>Text Snippet:</strong> Show the actual text of the retrieved chunk that matched the query. This provides immediate context. LangChain’s get_relevant_documents can retrieve relevant parts.</li>
      <li><strong>Relevance Score:</strong> Display the similarity score (e.g., “Cosine Similarity: 0.85”) to give the user an indication of how closely the chunk matches their query.</li>
      <li><strong>Highlighting (Optional):</strong> If feasible, highlight the query terms (or semantically similar terms if advanced techniques are used) within the displayed text snippet. For simple keyword highlighting, Python’s re.sub() can be used to wrap matched terms in HTML &lt;span&gt; tags for front-end display. More advanced semantic highlighting is complex. Python libraries like nltk can be used for sentence tokenization to create better snippets around keywords.</li>
    </ul>
  </li>
  <li><strong>Grouping Results:</strong> If multiple chunks from the same original document are retrieved, consider how to present them:
    <ul>
      <li>List each chunk individually with its score.</li>
      <li>Group chunks by the parent document, perhaps showing the document title once and then listing the relevant snippets from it.</li>
    </ul>
  </li>
  <li><strong>User Interface (UI) Considerations (Future Enhancement):</strong>
    <ul>
      <li>While the initial request implies a backend system, a simple CLI or a future web UI would be the interface for presenting these results.</li>
      <li>A web UI could allow sorting by relevance, filtering by metadata, and providing direct links to download/view the original files.</li>
    </ul>
  </li>
</ul>

<p>The aim is to provide enough information for the user to judge relevance without necessarily opening the full original document immediately.</p>
  <h3 id="64-optional-advanced-reranking-for-improved-precision">
    
    
     <a href="#64-optional-advanced-reranking-for-improved-precision">#</a><a href="#" aria-label="Back to top"><strong>6.4. (Optional) Advanced Reranking for Improved Precision</strong></a>
        
    
  </h3>
      

<p>The initial vector search is optimized for speed and recall (finding all potentially relevant items). To improve precision (the relevance of the top N results), a reranking step can be added. This involves taking the top M results (e.g., M=50 or M=100) from the vector search and re-evaluating their relevance using a more computationally intensive but potentially more accurate model.</p>

<ul>
  <li><strong>Cross-Encoders:</strong>
    <ul>
      <li><em>Concept:</em> Unlike bi-encoders (used for generating document/query embeddings independently), cross-encoders take a (query, document chunk) pair as input and output a single relevance score. They can capture finer-grained interactions between the query and the chunk.</li>
      <li><em>Usage:</em> Use a pre-trained cross-encoder model from libraries like sentence-transformers (e.g., cross-encoder/ms-marco-MiniLM-L-6-v2 is good for search relevance). Feed the query and each of the top M retrieved chunks to the cross-encoder. Sort the M results based on the new scores.</li>
      <li><em>Considerations:</em> Cross-encoders are significantly slower than bi-encoders because they recompute for every pair. Thus, they are only applied to a small subset of initial results.</li>
    </ul>
  </li>
  <li><strong>LLMs for Reranking:</strong>
    <ul>
      <li><em>Concept:</em> A powerful Large Language Model (LLM) can be prompted to assess the relevance of a document chunk to a query.</li>
      <li><em>Usage:</em> For each of the top M chunks, construct a prompt containing the user’s query and the chunk’s text. Ask the LLM to provide a relevance score (e.g., on a scale of 1-10) or a judgment (e.g., “highly relevant,” “somewhat relevant,” “not relevant”).</li>
      <li><em>Considerations:</em> This can be very effective due to the LLM’s deep understanding but can be slow and costly if using commercial LLM APIs. Prompt engineering is key to getting consistent and useful scores; prompts might ask the LLM to score relevance based on specific aspects like direct answer relevance, information completeness, and factual accuracy.</li>
    </ul>
  </li>
</ul>

<p>Reranking is an advanced optimization. It should be considered if the precision of the initial vector search results is insufficient for the user’s needs.</p>
  <h3 id="65-optional-enhancing-discoverability-with-result-diversification">
    
    
     <a href="#65-optional-enhancing-discoverability-with-result-diversification">#</a><a href="#" aria-label="Back to top"><strong>6.5. (Optional) Enhancing Discoverability with Result Diversification</strong></a>
        
    
  </h3>
      

<p>For broad queries, the top search results might all be very similar to each other, covering the same aspect of the topic. Result diversification aims to present a broader set of relevant results, covering different facets of the query.</p>

<ul>
  <li><strong>Techniques:</strong>
    <ul>
      <li><strong>Maximal Marginal Relevance (MMR):</strong> A common algorithm that iteratively selects results that are similar to the query but dissimilar to already selected results. This requires computing similarity between retrieved chunks themselves.</li>
      <li><strong>Clustering:</strong> Cluster the top M retrieved chunk embeddings. Then select one representative chunk from each of the top N clusters.</li>
    </ul>
  </li>
  <li><strong>Considerations:</strong> Diversification can improve user satisfaction for exploratory searches but might reduce precision if the user is looking for very specific information.</li>
</ul>

<p>This is also an advanced feature, typically implemented after the core search and reranking functionalities are stable.</p>
  <h2 id="7-implementation-details-tools-libraries-and-code">
    
    
     <a href="#7-implementation-details-tools-libraries-and-code">#</a><a href="#" aria-label="Back to top"><strong>7. Implementation Details: Tools, Libraries, and Code</strong></a>
        
    
  </h2>
      

<p>This section provides specific recommendations for libraries and conceptual code snippets to guide the implementation. The user’s preference for Rust for performance-critical components and Python for its rich ecosystem is a guiding principle.</p>
  <h3 id="71-table-recommended-python-libraries">
    
    
     <a href="#71-table-recommended-python-libraries">#</a><a href="#" aria-label="Back to top"><strong>7.1. Table: Recommended Python Libraries</strong></a>
        
    
  </h3>
      

<p>Python’s extensive libraries make it well-suited for many parts of this pipeline, especially for interacting with APIs, NLP tasks, and rapid prototyping.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Task Category</th>
      <th style="text-align: left">Library/Tool</th>
      <th style="text-align: left">Snippet ID(s) for Reference</th>
      <th style="text-align: left">Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>File System Ops</strong></td>
      <td style="text-align: left">os, pathlib</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Standard libraries for path manipulation and file system traversal. pathlib offers an object-oriented API.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Google Drive API</strong></td>
      <td style="text-align: left">google-api-python-client, google-auth-oauthlib</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Official Google libraries for interacting with Drive API v3 (listing, downloading files).</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>PDF Parsing</strong></td>
      <td style="text-align: left">PyMuPDF (Fitz), pypdf</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">PyMuPDF is highly recommended for robustness, speed, and ability to handle text, images, and detect image-based PDFs. pypdf is a pure-Python option.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>DOCX Parsing</strong></td>
      <td style="text-align: left">python-docx, docxpy</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">python-docx for reading content from paragraphs, tables. docxpy can also extract hyperlinks and images.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Archive Handling</strong></td>
      <td style="text-align: left">zipfile, tarfile (standard libs), rarfile, patoolib, extractcode</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">zipfile and tarfile are built-in. rarfile often needs unrar CLI. patoolib wraps many archivers. extractcode is highly robust for various formats and nested archives, recommended for comprehensive archive handling.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>File Type ID</strong></td>
      <td style="text-align: left">python-magic, filetype</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">filetype is dependency-free and uses magic numbers. python-magic wraps libmagic.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>OCR</strong></td>
      <td style="text-align: left">pytesseract, paddleocr, doctr, easyocr</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">pytesseract for Tesseract. paddleocr and doctr for advanced deep learning OCR. easyocr for simplicity. Choice depends on accuracy needs and setup complexity.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Embeddings (Local)</strong></td>
      <td style="text-align: left">sentence-transformers, onnxruntime</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">sentence-transformers for easy use of Hugging Face models. onnxruntime for running ONNX-exported models (potentially on ARM64).</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Embeddings (API)</strong></td>
      <td style="text-align: left">openai, cohere, jina-client</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Official SDKs for interacting with commercial embedding APIs.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Vector DB Clients</strong></td>
      <td style="text-align: left">qdrant-client, pymilvus, chromadb, weaviate-client, faiss-cpu</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Official or primary Python clients for the respective vector databases. faiss-cpu for FAISS library.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Orchestration</strong></td>
      <td style="text-align: left">LangChain, Prefect, Apache Airflow</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">LangChain for RAG-specific pipelines. Prefect for modern, Pythonic general workflow orchestration. Airflow for more traditional, complex DAGs.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Logging</strong></td>
      <td style="text-align: left">logging (standard), structlog</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Standard logging module. structlog for enhanced structured logging (e.g., JSON output, key-value pairs).</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Web Snippets</strong></td>
      <td style="text-align: left">nltk (for tokenization), re (for highlighting)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">nltk.sent_tokenize for splitting text into sentences to find relevant snippets around keywords. re.sub for simple keyword highlighting.</td>
    </tr>
  </tbody>
</table>
  <h3 id="72-table-recommended-rust-crates">
    
    
     <a href="#72-table-recommended-rust-crates">#</a><a href="#" aria-label="Back to top"><strong>7.2. Table: Recommended Rust Crates</strong></a>
        
    
  </h3>
      

<p>Rust can be employed for performance-sensitive parts of the pipeline, leveraging its speed and memory safety.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Task Category</th>
      <th style="text-align: left">Crate(s)</th>
      <th style="text-align: left">Snippet ID(s) for Reference</th>
      <th style="text-align: left">Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>File System Ops</strong></td>
      <td style="text-align: left">std::fs, walkdir</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">std::fs for basic operations. walkdir for efficient recursive directory traversal.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Google Drive API</strong></td>
      <td style="text-align: left">drive-v3, reqwest</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">drive-v3 crate for typed access to Drive API. reqwest for generic HTTP calls if direct API interaction is preferred or for services without dedicated crates.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>PDF Parsing</strong></td>
      <td style="text-align: left">lopdf, pdf-extract</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">lopdf for document manipulation and text extraction. pdf-extract specifically for text content.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>DOCX Parsing</strong></td>
      <td style="text-align: left">docx-rust, dotext</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">docx-rust for parsing and generating DOCX. dotext for extracting readable text from DOCX and other formats.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Archive Handling</strong></td>
      <td style="text-align: left">zip, tar, std::process::Command (for unrar/7z), libarchive-rust</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">zip and tar crates for their respective formats. For RAR, due to licensing, calling CLI unrar or 7z via std::process::Command is most reliable. libarchive-rust is an option but check RAR support status.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>File Type ID</strong></td>
      <td style="text-align: left">infer, file-type</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">infer for magic number based type detection (no external deps). file-type also uses signatures and extensions.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>OCR</strong></td>
      <td style="text-align: left">ocrs, extractous (Tesseract wrapper)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">ocrs for ONNX-based OCR. extractous can call Tesseract.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Embeddings (Local)</strong></td>
      <td style="text-align: left">rten, candle, ort (ONNX runtimes)</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Crates for running ONNX models on CPU (and potentially GPU/NPU with more setup). rten is a Rust-native ONNX runtime.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Vector DB Clients</strong></td>
      <td style="text-align: left">qdrant-client (Rust), sahomedb</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">Official Rust client for Qdrant. sahomedb is a Rust-native embedded vector DB. For others, gRPC/HTTP via tonic/reqwest.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Orchestration</strong></td>
      <td style="text-align: left">thepipelinetool, orchestrator, Custom logic</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">thepipelinetool for YAML/Rust pipeline definitions. orchestrator for sequencing functions. Custom async/await logic with tokio is also common.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Logging</strong></td>
      <td style="text-align: left">log (facade), env_logger, tracing</td>
      <td style="text-align: left"> </td>
      <td style="text-align: left">log as the facade. env_logger for simple, environment-variable configured logging. tracing for advanced structured and asynchronous logging with spans.</td>
    </tr>
  </tbody>
</table>
  <h3 id="73-conceptual-code-snippets">
    
    
     <a href="#73-conceptual-code-snippets">#</a><a href="#" aria-label="Back to top"><strong>7.3. Conceptual Code Snippets</strong></a>
        
    
  </h3>
      

<p>Below are conceptual snippets illustrating key operations. These are simplified and would require robust error handling, configuration management, and integration in a real implementation.</p>
  <h4 id="731-recursive-file-discovery-python-local--gdrive-placeholder">
    
    
     <a href="#731-recursive-file-discovery-python-local--gdrive-placeholder">#</a><a href="#" aria-label="Back to top"><strong>7.3.1. Recursive File Discovery (Python, Local + GDrive Placeholder)</strong></a>
        
    
  </h4>
      

<p><code class="language-plaintext highlighter-rouge"># Python: File Discovery</code><br />
<code class="language-plaintext highlighter-rouge">import os</code><br />
<code class="language-plaintext highlighter-rouge"># from googleapiclient.discovery import build #... and other Google imports</code></p>

<p><code class="language-plaintext highlighter-rouge">def discover_files(local_paths_roots, gdrive_service_object): # Changed gdrive_config to service object</code><br />
    <code class="language-plaintext highlighter-rouge">all_file_metadata = # Store dicts: {'path': str, 'source': 'local'/'gdrive', 'gdrive_id': optional_str, 'name': str}</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>`# Local files`  
`for root_path in local_paths_roots:`  
    `for dirpath, _, filenames in os.walk(root_path):`  
        `for filename in filenames:`  
            `full_path = os.path.join(dirpath, filename)`  
            `all_file_metadata.append({'path': full_path, 'source': 'local', 'gdrive_id': None, 'name': filename})`  
  
`# Google Drive files (conceptual - requires auth and full listing logic)`  
`# gdrive_items = list_all_gdrive_files(gdrive_service_object) # Recursive listing, defined elsewhere`  
`# for item in gdrive_items:`  
`#     # Download item to a temporary local path`  
`#     # temp_local_path = download_gdrive_item(gdrive_service_object, item['id'], "/tmp/gdrive_downloads/") # Defined elsewhere`  
`#     if temp_local_path: # Check if download was successful`  
`#         all_file_metadata.append({'path': temp_local_path, 'source': 'gdrive',`   
`#                                   'gdrive_id': item['id'], 'name': item.get('name', 'UnknownGdriveFile')}) # Use.get for name`  
`return all_file_metadata`
</code></section></div></div>
  <h4 id="732-archive-extraction-loop-python-using-extractcode">
    
    
     <a href="#732-archive-extraction-loop-python-using-extractcode">#</a><a href="#" aria-label="Back to top"><strong>7.3.2. Archive Extraction Loop (Python, using extractcode)</strong></a>
        
    
  </h4>
      

<p><code class="language-plaintext highlighter-rouge"># Python: Archive Extraction (Conceptual with extractcode)</code><br />
<code class="language-plaintext highlighter-rouge"># from extractcode import extract # Check actual API for extract.extract or similar</code><br />
<code class="language-plaintext highlighter-rouge"># import tempfile</code><br />
<code class="language-plaintext highlighter-rouge"># import os</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># def process_file_or_extract_archive(file_path, identified_type_extension, processing_function, get_file_kind_function):</code><br />
<code class="language-plaintext highlighter-rouge">#     archive_extensions = ["zip", "rar", "tar", "gz", "bz2", "7z"] # More comprehensive list</code><br />
<code class="language-plaintext highlighter-rouge">#     if identified_type_extension and identified_type_extension.lower() in archive_extensions:</code><br />
<code class="language-plaintext highlighter-rouge">#         # print(f"Extracting archive: {file_path}")</code><br />
<code class="language-plaintext highlighter-rouge">#         # with tempfile.TemporaryDirectory() as tmpdir:</code><br />
<code class="language-plaintext highlighter-rouge">#             # extracted_items = # This should be populated by extractcode</code><br />
<code class="language-plaintext highlighter-rouge">#             # # Example:</code><br />
<code class="language-plaintext highlighter-rouge">#             # for event in extract.extract(archive_path=file_path, target_dir=tmpdir, recurse=True): # Placeholder based on extractcode docs</code><br />
<code class="language-plaintext highlighter-rouge">#             #    if event.done and not event.errors and event.target and os.path.isfile(event.target):</code><br />
<code class="language-plaintext highlighter-rouge">#             #        extracted_items.append(event.target)</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge">#             # for item_path in extracted_items:</code><br />
<code class="language-plaintext highlighter-rouge">#                 # item_type_ext, _ = get_file_kind_function(item_path) # Re-identify type</code><br />
<code class="language-plaintext highlighter-rouge">#                 # process_file_or_extract_archive(item_path, item_type_ext, processing_function, get_file_kind_function) # Recursive call</code><br />
<code class="language-plaintext highlighter-rouge">#         pass # Replace with actual extractcode logic and error handling</code><br />
<code class="language-plaintext highlighter-rouge">#     else:</code><br />
<code class="language-plaintext highlighter-rouge">#         # This is a non-archive file, process its content</code><br />
<code class="language-plaintext highlighter-rouge">#         processing_function(file_path, identified_type_extension)</code></p>

<p><code class="language-plaintext highlighter-rouge"># def my_document_processor(file_path, file_type_ext):</code><br />
<code class="language-plaintext highlighter-rouge">#     # print(f"Processing document: {file_path} of type {file_type_ext}")</code><br />
<code class="language-plaintext highlighter-rouge">#     # Add to content extraction, chunking, embedding queue</code><br />
<code class="language-plaintext highlighter-rouge">#     pass</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># def get_file_kind_placeholder(file_path): # Placeholder for the actual get_file_kind function</code><br />
<code class="language-plaintext highlighter-rouge">#   return "unknown", "unknown"</code></p>
  <h4 id="733-content-extraction-and-ocr-python-conceptual">
    
    
     <a href="#733-content-extraction-and-ocr-python-conceptual">#</a><a href="#" aria-label="Back to top"><strong>7.3.3. Content Extraction and OCR (Python, Conceptual)</strong></a>
        
    
  </h4>
      

<p><code class="language-plaintext highlighter-rouge"># Python: Content Extraction (Conceptual)</code><br />
<code class="language-plaintext highlighter-rouge"># import fitz # PyMuPDF</code><br />
<code class="language-plaintext highlighter-rouge"># from docx import Document as DocxDocument # Renamed to avoid conflict</code><br />
<code class="language-plaintext highlighter-rouge"># import pytesseract # requires Tesseract install</code><br />
<code class="language-plaintext highlighter-rouge"># from PIL import Image</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># def extract_text_from_file(file_path, file_type_ext):</code><br />
<code class="language-plaintext highlighter-rouge">#     text_content = ""</code><br />
<code class="language-plaintext highlighter-rouge">#     if file_type_ext == "pdf":</code><br />
<code class="language-plaintext highlighter-rouge">#         try:</code><br />
<code class="language-plaintext highlighter-rouge">#             doc = fitz.open(file_path)</code><br />
<code class="language-plaintext highlighter-rouge">#             for page_num in range(len(doc)):</code><br />
<code class="language-plaintext highlighter-rouge">#                 page = doc.load_page(page_num)</code><br />
<code class="language-plaintext highlighter-rouge">#                 text_content += page.get_text()</code><br />
<code class="language-plaintext highlighter-rouge">#             if not text_content.strip() and len(doc) &gt; 0: # Potentially image-based PDF and has pages</code><br />
<code class="language-plaintext highlighter-rouge">#                 # print(f"PDF {file_path} has no text, attempting OCR...")</code><br />
<code class="language-plaintext highlighter-rouge">#                 text_content = ocr_pdf_conceptual(doc, file_path) # Pass file_path for logging</code><br />
<code class="language-plaintext highlighter-rouge">#             doc.close()</code><br />
<code class="language-plaintext highlighter-rouge">#         except Exception as e:</code><br />
<code class="language-plaintext highlighter-rouge">#             # print(f"Error processing PDF {file_path}: {e}")</code><br />
<code class="language-plaintext highlighter-rouge">#             return None</code><br />
<code class="language-plaintext highlighter-rouge">#     elif file_type_ext == "docx":</code><br />
<code class="language-plaintext highlighter-rouge">#         try:</code><br />
<code class="language-plaintext highlighter-rouge">#             doc = DocxDocument(file_path)</code><br />
<code class="language-plaintext highlighter-rouge">#             for para in doc.paragraphs:</code><br />
<code class="language-plaintext highlighter-rouge">#                 text_content += para.text + "\n"</code><br />
<code class="language-plaintext highlighter-rouge">#             # Add table extraction if needed</code><br />
<code class="language-plaintext highlighter-rouge">#         except Exception as e:</code><br />
<code class="language-plaintext highlighter-rouge">#             # print(f"Error processing DOCX {file_path}: {e}")</code><br />
<code class="language-plaintext highlighter-rouge">#             return None</code><br />
<code class="language-plaintext highlighter-rouge">#     elif file_type_ext == "txt":</code><br />
<code class="language-plaintext highlighter-rouge">#         try:</code><br />
<code class="language-plaintext highlighter-rouge">#             with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:</code><br />
<code class="language-plaintext highlighter-rouge">#                 text_content = f.read()</code><br />
<code class="language-plaintext highlighter-rouge">#         except Exception as e:</code><br />
<code class="language-plaintext highlighter-rouge">#             # print(f"Error processing TXT {file_path}: {e}")</code><br />
<code class="language-plaintext highlighter-rouge">#             return None</code><br />
<code class="language-plaintext highlighter-rouge">#     #... handle other types or log unknown</code><br />
<code class="language-plaintext highlighter-rouge">#     return text_content.strip() if text_content else None</code></p>

<p><code class="language-plaintext highlighter-rouge"># def ocr_pdf_conceptual(pdf_document, file_path_for_log): # Conceptual</code><br />
<code class="language-plaintext highlighter-rouge">#     ocr_text = ""</code><br />
<code class="language-plaintext highlighter-rouge">#     # for page_num in range(len(pdf_document)):</code><br />
<code class="language-plaintext highlighter-rouge">#     #     page = pdf_document.load_page(page_num)</code><br />
<code class="language-plaintext highlighter-rouge">#     #     pix = page.get_pixmap() # default DPI, consider increasing for better OCR</code><br />
<code class="language-plaintext highlighter-rouge">#     #     img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)</code><br />
<code class="language-plaintext highlighter-rouge">#     #     try:</code><br />
<code class="language-plaintext highlighter-rouge">#     #         ocr_text += pytesseract.image_to_string(img) + "\n"</code><br />
<code class="language-plaintext highlighter-rouge">#     #     except Exception as ocr_error:</code><br />
<code class="language-plaintext highlighter-rouge">#     #         # print(f"OCR error on page {page_num} of {file_path_for_log}: {ocr_error}")</code><br />
<code class="language-plaintext highlighter-rouge">#     #         pass # Continue with other pages</code><br />
<code class="language-plaintext highlighter-rouge">#     return ocr_text</code></p>
  <h4 id="734-text-chunking-python-langchain-style-recursive">
    
    
     <a href="#734-text-chunking-python-langchain-style-recursive">#</a><a href="#" aria-label="Back to top"><strong>7.3.4. Text Chunking (Python, LangChain Style Recursive)</strong></a>
        
    
  </h4>
      

<p><code class="language-plaintext highlighter-rouge"># Python: Text Chunking</code><br />
<code class="language-plaintext highlighter-rouge"># from langchain_text_splitters import RecursiveCharacterTextSplitter</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># def chunk_text_content(text_content, chunk_size=1000, chunk_overlap=200):</code><br />
<code class="language-plaintext highlighter-rouge">#     if not text_content: # Check if text_content is None or empty</code><br />
<code class="language-plaintext highlighter-rouge">#         return # Return empty list if no content</code><br />
<code class="language-plaintext highlighter-rouge">#     text_splitter = RecursiveCharacterTextSplitter(</code><br />
<code class="language-plaintext highlighter-rouge">#         chunk_size=chunk_size,</code><br />
<code class="language-plaintext highlighter-rouge">#         chunk_overlap=chunk_overlap,</code><br />
<code class="language-plaintext highlighter-rouge">#         length_function=len,</code><br />
<code class="language-plaintext highlighter-rouge">#         is_separator_regex=False,</code><br />
<code class="language-plaintext highlighter-rouge">#         separators=["\n\n", "\n", ". ", " ", ""] # Common separators</code><br />
<code class="language-plaintext highlighter-rouge">#     )</code><br />
<code class="language-plaintext highlighter-rouge">#     chunks = text_splitter.split_text(text_content)</code><br />
<code class="language-plaintext highlighter-rouge">#     return chunks</code></p>
  <h4 id="735-embedding-generation-python-sentence-transformers-local--openai-api">
    
    
     <a href="#735-embedding-generation-python-sentence-transformers-local--openai-api">#</a><a href="#" aria-label="Back to top"><strong>7.3.5. Embedding Generation (Python, Sentence Transformers Local &amp; OpenAI API)</strong></a>
        
    
  </h4>
      

<p><code class="language-plaintext highlighter-rouge"># Python: Embedding Generation</code><br />
<code class="language-plaintext highlighter-rouge"># from sentence_transformers import SentenceTransformer # For local</code><br />
<code class="language-plaintext highlighter-rouge"># import openai # For API</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># # Local model example</code><br />
<code class="language-plaintext highlighter-rouge"># local_embedding_model_instance = None # Renamed to avoid conflict</code><br />
<code class="language-plaintext highlighter-rouge"># def get_local_st_model(model_name='all-MiniLM-L6-v2'): # Or NeuML/pubmedbert-base-embeddings</code><br />
<code class="language-plaintext highlighter-rouge">#     global local_embedding_model_instance</code><br />
<code class="language-plaintext highlighter-rouge">#     if local_embedding_model_instance is None:</code><br />
<code class="language-plaintext highlighter-rouge">#         local_embedding_model_instance = SentenceTransformer(model_name)</code><br />
<code class="language-plaintext highlighter-rouge">#     return local_embedding_model_instance</code></p>

<p><code class="language-plaintext highlighter-rouge"># def generate_embeddings_local(text_chunks, model_name='all-MiniLM-L6-v2'):</code><br />
<code class="language-plaintext highlighter-rouge">#     if not text_chunks: return # Handle empty input</code><br />
<code class="language-plaintext highlighter-rouge">#     model = get_local_st_model(model_name)</code><br />
<code class="language-plaintext highlighter-rouge">#     embeddings = model.encode(text_chunks, show_progress_bar=False) # Set to True for progress</code><br />
<code class="language-plaintext highlighter-rouge">#     return embeddings.tolist() # Convert numpy arrays to lists</code></p>

<p><code class="language-plaintext highlighter-rouge"># # OpenAI API example</code><br />
<code class="language-plaintext highlighter-rouge"># # openai.api_key = "YOUR_OPENAI_API_KEY" # Should be set via environment variable</code><br />
<code class="language-plaintext highlighter-rouge"># def generate_embeddings_openai(text_chunks, model_name="text-embedding-3-small"):</code><br />
<code class="language-plaintext highlighter-rouge">#     if not text_chunks: return # Handle empty input</code><br />
<code class="language-plaintext highlighter-rouge">#     # Ensure API key is configured, e.g., openai.api_key = os.getenv("OPENAI_API_KEY")</code><br />
<code class="language-plaintext highlighter-rouge">#     try:</code><br />
<code class="language-plaintext highlighter-rouge">#         response = openai.embeddings.create(input=text_chunks, model=model_name)</code><br />
<code class="language-plaintext highlighter-rouge">#         embeddings = [item.embedding for item in response.data]</code><br />
<code class="language-plaintext highlighter-rouge">#         return embeddings</code><br />
<code class="language-plaintext highlighter-rouge">#     except Exception as e:</code><br />
<code class="language-plaintext highlighter-rouge">#         # print(f"OpenAI API error: {e}")</code><br />
<code class="language-plaintext highlighter-rouge">#         return [None] * len(text_chunks) # Return list of Nones or handle error appropriately</code></p>
  <h4 id="736-vector-db-indexing-python-qdrant-client">
    
    
     <a href="#736-vector-db-indexing-python-qdrant-client">#</a><a href="#" aria-label="Back to top"><strong>7.3.6. Vector DB Indexing (Python, Qdrant Client)</strong></a>
        
    
  </h4>
      

<p><code class="language-plaintext highlighter-rouge"># Python: Qdrant Indexing</code><br />
<code class="language-plaintext highlighter-rouge"># from qdrant_client import QdrantClient, models # For newer versions, 'models' might be 'qdrant_client.http.models' or just 'qdrant_client.models'</code><br />
<code class="language-plaintext highlighter-rouge"># import uuid</code><br />
<code class="language-plaintext highlighter-rouge">#</code><br />
<code class="language-plaintext highlighter-rouge"># qdrant_cli = QdrantClient(host="localhost", port=6333) # Or url="http://localhost:6333"</code><br />
<code class="language-plaintext highlighter-rouge"># QDRANT_COLLECTION_NAME = "academic_documents"</code></p>

<p><code class="language-plaintext highlighter-rouge"># def index_embeddings_in_qdrant(embeddings_list, text_chunks_list, metadata_list_of_dicts): # Ensure metadata is a list of dicts</code><br />
<code class="language-plaintext highlighter-rouge">#     points_to_upsert =</code><br />
<code class="language-plaintext highlighter-rouge">#     for i, emb in enumerate(embeddings_list):</code><br />
<code class="language-plaintext highlighter-rouge">#         if emb is None: # Skip if embedding generation failed for this chunk</code><br />
<code class="language-plaintext highlighter-rouge">#             # print(f"Skipping chunk {i} due to missing embedding.")</code><br />
<code class="language-plaintext highlighter-rouge">#             continue</code><br />
<code class="language-plaintext highlighter-rouge">#         # Ensure metadata_list_of_dicts[i] is a flat dictionary of JSON-serializable types</code><br />
<code class="language-plaintext highlighter-rouge">#         # Example: {'original_path': 'path/to/doc', 'chunk_text': text_chunks_list[i], 'source': 'local'}</code><br />
<code class="language-plaintext highlighter-rouge">#         payload_data = metadata_list_of_dicts[i]</code> <br />
<code class="language-plaintext highlighter-rouge">#         point_id = str(uuid.uuid4()) # Generate unique ID for each chunk</code></p>

<p><code class="language-plaintext highlighter-rouge">#         points_to_upsert.append(</code><br />
<code class="language-plaintext highlighter-rouge">#             models.PointStruct( # or qdrant_client.http.models.PointStruct for older versions</code><br />
<code class="language-plaintext highlighter-rouge">#                 id=point_id,</code><br />
<code class="language-plaintext highlighter-rouge">#                 vector=emb,</code><br />
<code class="language-plaintext highlighter-rouge">#                 payload=payload_data</code> <br />
<code class="language-plaintext highlighter-rouge">#             )</code><br />
<code class="language-plaintext highlighter-rouge">#         )</code><br />
<code class="language-plaintext highlighter-rouge">#     if points_to_upsert:</code><br />
<code class="language-plaintext highlighter-rouge">#         try:</code><br />
<code class="language-plaintext highlighter-rouge">#             qdrant_cli.upsert( # or client.upsert for newer versions</code><br />
<code class="language-plaintext highlighter-rouge">#                 collection_name=QDRANT_COLLECTION_NAME,</code><br />
<code class="language-plaintext highlighter-rouge">#                 points=points_to_upsert,</code><br />
<code class="language-plaintext highlighter-rouge">#                 wait=True # Wait for operation to complete</code><br />
<code class="language-plaintext highlighter-rouge">#             )</code><br />
<code class="language-plaintext highlighter-rouge">#             # print(f"Indexed {len(points_to_upsert)} points into Qdrant.")</code><br />
<code class="language-plaintext highlighter-rouge">#         except Exception as e:</code><br />
<code class="language-plaintext highlighter-rouge">#             # print(f"Error indexing points in Qdrant: {e}")</code><br />
<code class="language-plaintext highlighter-rouge">#             pass # Or raise</code></p>
  <h4 id="737-vector-db-querying-python-qdrant-client">
    
    
     <a href="#737-vector-db-querying-python-qdrant-client">#</a><a href="#" aria-label="Back to top"><strong>7.3.7. Vector DB Querying (Python, Qdrant Client)</strong></a>
        
    
  </h4>
      

<p><code class="language-plaintext highlighter-rouge"># Python: Qdrant Querying</code><br />
<code class="language-plaintext highlighter-rouge"># def search_qdrant(query_text, embedding_function_for_query, top_k=5): # Renamed embedding_function</code><br />
<code class="language-plaintext highlighter-rouge">#     query_vector_list = embedding_function_for_query([query_text]) # embedding_function takes list, returns</code></p>
  <h4 id="works-cited">
    
    
     <a href="#works-cited">#</a><a href="#" aria-label="Back to top"><strong>Works cited</strong></a>
        
    
  </h4>
      

<table>
  <tbody>
    <tr>
      <td>1. AI Document Indexing Explained - Botpress, https://botpress.com/blog/ai-document-indexing 2. (PDF) An Integrated Content and Metadata Based Retrieval System for Art - ResearchGate, https://www.researchgate.net/publication/8337794_An_Integrated_Content_and_Metadata_Based_Retrieval_System_for_Art 3. How to Build a Search Engine - Packt, https://www.packtpub.com/en-us/learning/how-to-tutorials/how-build-search-engine 4. Comparing Popular Embedding Models: Choosing the Right One for Your Use Case, https://dev.to/simplr_sh/comparing-popular-embedding-models-choosing-the-right-one-for-your-use-case-43p1 5. Semantic Text Search Using LangChain (OpenAI) and Redis, https://redis.io/learn/howtos/solutions/vector/semantic-text-search 6. How to Implement Semantic Search in Python Step by Step - TiDB, https://www.pingcap.com/article/semantic-search-python-step-by-step/ 7. docx_rust - Rust - Docs.rs, https://docs.rs/docx-rust 8. dotext — Rust parser // Lib.rs, https://lib.rs/crates/dotext 9. extractous - Rust - Docs.rs, https://docs.rs/extractous 10. Docker</td>
      <td>Weaviate, https://weaviate.io/developers/weaviate/installation/docker-compose 11. Python Virtual Environments: A Primer, https://realpython.com/python-virtual-environments-a-primer/ 12. The definitive guide to Python virtual environments with conda</td>
      <td>WhiteBox Blog, https://www.whiteboxml.com/en/blog/the-definitive-guide-to-python-virtual-environments-with-conda 13. Understanding the Rust Ecosystem: A Deep Dive into Cargo and Crates - Java Code Geeks, https://www.javacodegeeks.com/2024/11/understanding-the-rust-ecosystem-a-deep-dive-into-cargo-and-crates.html 14. When should a dependency be in the workspace vs crate, best practices? : r/rust - Reddit, https://www.reddit.com/r/rust/comments/1i4c1x5/when_should_a_dependency_be_in_the_workspace_vs/ 15. How to GET folders from the Google Drive API in python - Merge.dev, https://www.merge.dev/blog/get-folders-google-drive-api 16. Download and export files</td>
      <td>Google Drive, https://developers.google.com/workspace/drive/api/guides/manage-downloads 17. drive-v3 - crates.io: Rust Package Registry, https://crates.io/crates/drive-v3 18. drive_v3 - Rust - Docs.rs, https://docs.rs/drive-v3 19. API Pricing - OpenAI, https://openai.com/api/pricing/ 20. How to choose the best model for semantic search - Meilisearch, https://www.meilisearch.com/blog/choosing-the-best-model-for-semantic-search 21. Embedding API - Jina AI, https://jina.ai/embeddings/ 22. How do I access files on an external hard drive? [closed] - Unix &amp; Linux Stack Exchange, https://unix.stackexchange.com/questions/116375/how-do-i-access-files-on-an-external-hard-drive 23. How do I get a complete list of files in my hard drive in a convenient format? - Ask Ubuntu, https://askubuntu.com/questions/431181/how-do-i-get-a-complete-list-of-files-in-my-hard-drive-in-a-convenient-format 24. Analyzing Your File System and Folder Structures with Python - Nikolai Janakiev, https://janakiev.com/blog/python-filesystem-analysis/ 25. How can I list files of a directory in Rust? - Stack Overflow, https://stackoverflow.com/questions/26076005/how-can-i-list-files-of-a-directory-in-rust 26. File Magic Numbers - GitHub Gist, https://gist.github.com/leommoore/f9e57ba2aa4bf197ebc5 27. Determining file format using Python</td>
      <td>GeeksforGeeks, https://www.geeksforgeeks.org/determining-file-format-using-python/ 28. filetype · PyPI, https://pypi.org/project/filetype/ 29. infer - crates.io: Rust Package Registry, https://crates.io/crates/infer 30. infer - Rust - Docs.rs, https://docs.rs/infer 31. Introducing file-type: detects thousands of file types using signatures/extensions/media-types : r/rust - Reddit, https://www.reddit.com/r/rust/comments/1i24esb/introducing_filetype_detects_thousands_of_file/ 32. file_type - Rust - Docs.rs, https://docs.rs/file_type 33. zipfile — Work with ZIP archives — Python 3.13.3 documentation, https://docs.python.org/3/library/zipfile.html 34. S3 bucket RAR file extraction using Python script and AWS Lambda, https://discuss.python.org/t/s3-bucket-rar-file-extraction-using-python-script-and-aws-lambda/49634 35. Python Rarfile Module - Tutorialspoint, https://www.tutorialspoint.com/python/python_rarfile_module.htm 36. patool - PyPI, https://pypi.org/project/patool/ 37. Create RAR Files in Python Using patool Package - YouTube, https://www.youtube.com/watch?v=06WaW5eLtnE 38. extractcode - PyPI, https://pypi.org/project/extractcode/ 39. extractcode · PyPI, https://pypi.org/project/extractcode/21.1.15/ 40. Which library is most commonly used to read and write to archive files? - Rust Users Forum, https://users.rust-lang.org/t/which-library-is-most-commonly-used-to-read-and-write-to-archive-files/129644 41. tar - Rust - Docs.rs, https://docs.rs/tar 42. Support for RAR · Issue #151 · libarchive/libarchive - GitHub, https://github.com/libarchive/libarchive/issues/151 43. libarchive - Rust Package Registry - Crates.io, https://crates.io/crates/libarchive 44. Extract text from PDF File using Python - GeeksforGeeks, https://www.geeksforgeeks.org/extract-text-from-pdf-file-using-python/ 45. How to extract text from PDF file in Rust? - Ahmad Rosid, https://ahmadrosid.com/blog/extract-text-from-pdf-in-rust 46. pdf-extract - crates.io: Rust Package Registry, https://crates.io/crates/pdf-extract 47. Extracting Information from a DOCX File Using Python - ByteScrum Technologies, https://blog.bytescrum.com/extracting-information-from-a-docx-file-using-python 48. docxpy - PyPI, https://pypi.org/project/docxpy/ 49. Extract numbers from a text file and add them using Python</td>
      <td>GeeksforGeeks, https://www.geeksforgeeks.org/extract-numbers-from-a-text-file-and-add-them-using-python/ 50. How to extract text, line by line from a txt file in python - Stack Overflow, https://stackoverflow.com/questions/21651661/how-to-extract-text-line-by-line-from-a-txt-file-in-python 51. Top 8 OCR Libraries in Python to Extract Text from Image - Analytics Vidhya, https://www.analyticsvidhya.com/blog/2024/04/ocr-libraries-in-python/ 52. Open-Source OCR Libraries: A Comprehensive Study for Low Resource Language - ACL Anthology, https://aclanthology.org/2024.icon-1.48.pdf 53. Best OCR Software in 2025</td>
      <td>PDF OCR Tool Comparison Guide - Unstract, https://unstract.com/blog/best-pdf-ocr-software/ 54. GitHub - PaddlePaddle/PaddleOCR: Awesome multilingual OCR …, https://github.com/PaddlePaddle/PaddleOCR 55. 10 Open Source OCR Tools You Should Know About - Koncile, https://www.koncile.ai/en/ressources/10-open-source-ocr-tools-you-should-know-about 56. docTR - Open Source OCR - Mindee, https://www.mindee.com/platform/doctr 57. docTR documentation - GitHub Pages, https://mindee.github.io/doctr/ 58. robertknight/ocrs: Rust library and CLI tool for OCR (extracting text from images) - GitHub, https://github.com/robertknight/ocrs 59. Build an unstructured data pipeline for RAG - Databricks Documentation, https://docs.databricks.com/aws/en/generative-ai/tutorials/ai-cookbook/quality-data-pipeline-rag 60. Chunking strategies for RAG tutorial using Granite - IBM, https://www.ibm.com/think/tutorials/chunking-strategies-for-rag-with-langchain-watsonx-ai 61. How to Split Text For Vector Embeddings in Snowflake - phData, https://www.phdata.io/blog/how-to-split-text-for-vector-embeddings-in-snowflake/ 62. Chunking Strategies for RAG in Generative AI - Association of Data Scientists, https://adasci.org/chunking-strategies-for-rag-in-generative-ai/ 63. Mastering Chunking Strategies for RAG: Best Practices &amp; Code Examples - Databricks Community, https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chunking-strategies-for-rag-applications/ba-p/113089 64. How to recursively split text by characters</td>
      <td>🦜️ LangChain, https://python.langchain.com/docs/how_to/recursive_text_splitter/ 65. Parse and chunk documents</td>
      <td>AI Applications - Google Cloud, https://cloud.google.com/generative-ai-app-builder/docs/parse-chunk-documents 66. NeuML/pubmedbert-base-embeddings - Hugging Face, https://huggingface.co/NeuML/pubmedbert-base-embeddings 67. Word Embedding for Social Sciences: An Interdisciplinary Survey - arXiv, https://arxiv.org/html/2207.03086v2 68. A Comparative Analysis of Sentence Transformer Models for Automated Journal Recommendation Using PubMed Metadata - MDPI, https://www.mdpi.com/2504-2289/9/3/67 69. Cohere Embed v3 - Multilingual - Microsoft Azure Marketplace, https://azuremarketplace.microsoft.com/en-us/marketplace/apps/cohere.cohere-embed-v3-multilingual-offer?tab=PlansAndPrice 70. Running sentence transformers model in Rust? - Reddit, https://www.reddit.com/r/rust/comments/1hyfex8/running_sentence_transformers_model_in_rust/ 71. Running Qwen3-30B-A3B on ARM CPU of Single-board computer : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1kapjwa/running_qwen330ba3b_on_arm_cpu_of_singleboard/ 72. What is an example of using Sentence Transformers for an academic purpose, such as finding related research papers or publications on a topic? - Milvus, https://milvus.io/ai-quick-reference/what-is-an-example-of-using-sentence-transformers-for-an-academic-purpose-such-as-finding-related-research-papers-or-publications-on-a-topic 73. Embedding models</td>
      <td>🦜️ LangChain, https://python.langchain.com/docs/integrations/text_embedding/ 74. Best Open Source Sentence Embedding Models in August 2024 - Codesphere, https://codesphere.com/articles/best-open-source-sentence-embedding-models 75. A Guide to Using OpenAI Text Embedding Models for NLP Tasks - Zilliz Learn, https://zilliz.com/learn/guide-to-using-openai-text-embedding-models 76. AWS Marketplace: Cohere Embed Model v3 - English, https://aws.amazon.com/marketplace/pp/prodview-qd64mji3pbnvk 77. 9 Best Embedding Models for Semantic Search - Graft, https://www.graft.com/blog/text-embeddings-for-search-semantic 78. Feature Request: Support smart AM60 RK3588 · Issue #1215 · Joshua-Riek/ubuntu-rockchip - GitHub, https://github.com/Joshua-Riek/ubuntu-rockchip/issues/1215 79. ARMv7 and ARM64 Support on Linux - Vector, https://vector.dev/highlights/2019-11-19-arm-support-on-linux/ 80. Vector Database Comparison 2025: Features, Performance &amp; Use Cases - Turing, https://www.turing.com/resources/vector-database-comparison 81. Pgvector vs. Qdrant: Open-Source Vector Database Comparison - Timescale, https://www.timescale.com/blog/pgvector-vs-qdrant 82. What Exactly is a Vector Database and How Does It Work - Milvus Blog, https://milvus.io/blog/what-is-a-vector-database.md 83. Vector Database - Product Documentation - NetApp, https://docs.netapp.com/us-en/netapp-solutions/ai/vector-database-vector-database.html 84. The Ultimate Guide to Vector Databases - KX, https://kx.com/vector-database/ 85. How to Install and Use Chroma DB - DatabaseMart AI, https://www.databasemart.com/blog/how-to-install-and-use-chromadb 86. Package qdrant - GitHub, https://github.com/orgs/qdrant/packages/container/package/qdrant 87. Qdrant - Docker Image, https://hub.docker.com/r/qdrant/qdrant 88. qdrant/docs/DEVELOPMENT.md at master - GitHub, https://github.com/qdrant/qdrant/blob/master/docs/DEVELOPMENT.md 89. Installation - Qdrant, https://qdrant.tech/documentation/guides/installation/ 90. How to Get Started with Milvus, https://milvus.io/blog/how-to-get-started-with-milvus.md 91. Run Milvus Lite Locally, https://milvus.io/docs/milvus_lite.md 92. milvus - PyPI, https://pypi.org/project/milvus/2.2.4/ 93. Getting Started - Chroma Docs, https://docs.trychroma.com/getting-started 94. www.truefoundry.com, https://www.truefoundry.com/blog/best-vector-databases#:~:text=Chroma,Python%20environments%20with%20minimal%20configuration. 95. 7 Best Vector Databases in 2025 - TrueFoundry, https://www.truefoundry.com/blog/best-vector-databases 96. Quickstart (with cloud resources) - Weaviate, https://weaviate.io/developers/weaviate/quickstart 97. Image Layer Details - semitechnologies/weaviate:1.31.0-dev-1dd636c.arm64</td>
      <td>Docker Hub, https://hub.docker.com/layers/semitechnologies/weaviate/1.31.0-dev-1dd636c.arm64/images/sha256-ac77a64a5bb16dcb844e04de9c3ca3fa6a9d605ace0e442b9053fd354159cb57 98. semitechnologies/weaviate:1.30.0-dev-396f9f8-arm64 - Docker Hub, https://hub.docker.com/layers/semitechnologies/weaviate/1.30.0-dev-396f9f8-arm64/images/sha256-ac81aebbdf4d46e23a7dbbcff6733ceaeaf28164a9694acdbfbc98e06518d612 99. semitechnologies/weaviate Tags</td>
      <td>Docker Hub, https://hub.docker.com/r/semitechnologies/weaviate/tags 100. Create a local Docker instance - Weaviate, https://weaviate.io/developers/academy/py/starter_multimodal_data/setup_weaviate/create_docker 101. Python</td>
      <td>Weaviate, https://weaviate.io/developers/weaviate/client-libraries/python 102. faiss-cpu 1.8.0 - PyPI, https://pypi.org/project/faiss-cpu/1.8.0/ 103. How can I install faiss-gpu? - Stack Overflow, https://stackoverflow.com/questions/78200859/how-can-i-install-faiss-gpu 104. sahomedb - Rust - Docs.rs, https://docs.rs/sahomedb 105. AWS Marketplace: Pinecone Vector Database - Pay As You Go Pricing - Amazon.com, https://aws.amazon.com/marketplace/pp/prodview-xhgyscinlz4jk 106. Pricing - Pinecone, https://www.pinecone.io/pricing/ 107. Zilliz Cloud Pricing - Fully Managed Vector Database for AI &amp; Machine Learning, https://zilliz.com/pricing 108. Milvus Vector Database Pricing: Cloud vs Self-Hosted Cost Guide - Airbyte, https://airbyte.com/data-engineering-resources/milvus-database-pricing 109. Perform semantic search and retrieval-augmented generation</td>
      <td>BigQuery - Google Cloud, https://cloud.google.com/bigquery/docs/vector-index-text-search-tutorial 110. Vector database - Microsoft Fabric, https://learn.microsoft.com/en-us/fabric/real-time-intelligence/vector-database 111. How a vector index works and 5 critical best practices - Instaclustr, https://www.instaclustr.com/education/vector-database/how-a-vector-index-works-and-5-critical-best-practices/ 112. Vector Indexing</td>
      <td>Weaviate, https://weaviate.io/developers/weaviate/concepts/vector-index 113. Optimize Performance - Qdrant, https://qdrant.tech/documentation/guides/optimize/ 114. Optimize HNSW Parameters in FAISS for Better Searches - BakingAI Blog, https://bakingai.com/blog/optimize-hnsw-parameters-faiss/ 115. weaviate.io, https://weaviate.io/blog/vector-embeddings-explained#:~:text=The%20embeddings%20are%20placed%20into,vector%20computed%20for%20the%20query. 116. Searching existing ChromaDB database using cosine similarity - Stack Overflow, https://stackoverflow.com/questions/77794024/searching-existing-chromadb-database-using-cosine-similarity 117. Evaluating Semantic Search Algorithms: Key Metrics &amp; Techniques for Optimal Performance, https://hakia.com/evaluating-semantic-search-algorithms-metrics-and-techniques-for-performance-assessment/ 118. Text Search with Semantic Kernel (Preview)</td>
      <td>Microsoft Learn, https://learn.microsoft.com/en-us/semantic-kernel/concepts/text-search/ 119. understanding retriever.get_relevant_documents #16033 - GitHub, https://github.com/langchain-ai/langchain/discussions/16033 120. Retrieval - LangChain, https://www.langchain.com/retrieval 121. Highlight search result match text in Python - GitHub Gist, https://gist.github.com/5935726472c3823d1c45 122. Is there a way to highlight where in the text the match was found? - Oracle Forums, https://forums.oracle.com/ords/apexds/post/is-there-a-way-to-highlight-where-in-the-text-the-match-was-5927 123. 9 Best Python Natural Language Processing (NLP) Libraries - Sunscrapers, https://sunscrapers.com/blog/9-best-python-natural-language-processing-nlp/ 124. Results snippets - Stanford NLP Group, https://nlp.stanford.edu/IR-book/html/htmledition/results-snippets-1.html 125. Sentence Embeddings. Cross-encoders and Re-ranking – hackerllama - GitHub Pages, https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/ 126. Reranking in RAG: Enhancing Accuracy with Cross-Encoders - EY/KA Lab, https://eyka.com/blog/reranking-in-rag-enhancing-accuracy-with-cross-encoders/ 127. What is the process to use a cross-encoder from the Sentence Transformers library for re-ranking search results? - Milvus, https://milvus.io/ai-quick-reference/what-is-the-process-to-use-a-crossencoder-from-the-sentence-transformers-library-for-reranking-search-results 128. How could you use the LLM itself to improve retrieval — for example, by generating a better search query or re-ranking the retrieved results? How would you measure the impact of such techniques? - Milvus, https://milvus.io/ai-quick-reference/how-could-you-use-the-llm-itself-to-improve-retrieval-for-example-by-generating-a-better-search-query-or-reranking-the-retrieved-results-how-would-you-measure-the-impact-of-such-techniques 129. Using LLM as a Reranker - Blog by Jason Kang, https://jasonkang14.github.io/llm/how-to-use-llm-as-a-reranker/ 130. CONTEXT BASED SEMANTIC SEARCH DIVERSIFICATION MODEL - IJCRT.org, https://ijcrt.org/papers/IJCRT2112050.pdf 131. DIVERSIFYING SEMANTIC ENTITY SEARCH: INDEPENDENT COMPONENT ANALYSIS APPROACH - World Scientific Publishing, https://worldscientific.com/doi/abs/10.1142/S1793351X13400138</td>
    </tr>
  </tbody>
</table>

        </div>
        
          URL: https://ib.bsb.br/content-based-retrieval-system
        
      </article>
      <nav class="post-navigation-combined" aria-label="Post navigation">
        <!-- Chronological Navigation (always visible at 40vh) -->
        
          <div class="nav-arrow chronological prev">
            <a href="/lightrag-extracting-relevant-information-from-mixed-data/" title="LightRAG - extracting relevant information from mixed data" rel="prev">
              <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ+lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="Previous article" width="45" height="45" loading="lazy">
            </a>
          </div>
        
        
          <div class="nav-arrow chronological next">
            <a href="/diagnostic-script-for-disks-and-mount-with-exfat-fuse/" title="Diagnostic script for disks and mount with `exfat-fuse`" rel="next">
              <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAwdJREFUaEPtl01rG1cUht9z7p070ijKWLVS3I9YULtNMBRn2UIXxZtuvCjedNWfoKVXBvsHeOUfoIUx5AeY0pVXXSR052UxBJSi0kDiRplII83HvadI9CMtOFjCmBjubC7cc8+Zc555eYdLuIEP3cCe4Zu+rq/mSXvSbyHg5eHl4eVxXRrwpD3p+Qh4n56P2+xZnvTszC7MCAAUF0XfOdJa62+IaCeO40GSJA/zPP8RwB9vDjBtWmv9JRE5ImpZawdVpbo58CkAIyJnIrLAzEvOuXNm/h3AfQDOAGdj5z5m5gXn3G9ENCKilckaiDwpiFZEpCoiTyYrM3/knOtXmHs58BkABvCLc+4DZl6cxIjoh4ODA728vIzt7W2sra09PT4+fgTgQES4LMvHZIy5LyI/f/dJvfK6FKMJqAdqmBS2ZgWINY9zJyZ1whUiRJrTfmGjybSxUemwcFEmghqT1UxlUrpQEXD7jRq3NWelEz10okIi1AJOX+U2EgALgUrT0kVnmcKvQ+uazSZvbW2hWq2i3++j3W5jf38f6+vrebvdHovI5pT09+vN7teLYev990K8GpQYjS1u1TQqhnHez6EU4cM7FTw7z5AXDo16ACcyPRsaxp2FEM9ejKd7zUaIYVpiOCpRq2rUIo0XLzMwEZaaFTzvZ8hyh/iWnu69fF1AAoOfFr/A5w8eYDQaIY5jJEmC09NTNBoNdLtd7O3t4ejoCIeHh8k/pL8KUSkCZcg6hCLDjLk2ufYGpRuXAuOMYiosQiDNNEcQILQuzYgi0QxVWKuAMtcqJPdXDaKaMMOUNrOAtoFSVE5jaaY4mtQPS5dmQPS8WkdfGReYgDe/3cTdpbvo9XrY2NhAp9NBq9XKO53Ov6T/r2mlVBeX0DSAMzeHppm5h7doemdvR99buYfd3V2srq4+PTk5+a+mr9CmrqTU3+5Rr9cHg8HgYve4krddbZGb5dOXmf2d+7n4pi9D4LrOeHl40v42fl0a8KQ96fkIeJ+ej9vsWZ707Mzmy/Ck5+M2e9afhmuAz1BhwewAAAAASUVORK5CYII=" alt="Next article" width="45" height="45" loading="lazy">
            </a>
          </div>
        
            
            
              
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                  
                  
              
              
              
              <!-- Navigation block for tag "scratchpad"; default display for the first tag -->
              <div class="nav-group tags" id="tag-nav-scratchpad" style="display: block;">
                <div class="nav-arrow tags prev">
                  
                    <a href="/diagnostic-script-for-disks-and-mount-with-exfat-fuse/" title="Diagnostic script for disks and mount with `exfat-fuse`" rel="prev">
                      <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ+lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="Previous in Tag" width="45" height="45" loading="lazy">
                    </a>
                  
                </div>
                <div class="nav-arrow tags next">
                  
                    <a href="/ugrep/" title="Extract information from disorganized data with `ugrep` bash script" rel="next">
                      <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAwdJREFUaEPtl01rG1cUht9z7p070ijKWLVS3I9YULtNMBRn2UIXxZtuvCjedNWfoKVXBvsHeOUfoIUx5AeY0pVXXSR052UxBJSi0kDiRplII83HvadI9CMtOFjCmBjubC7cc8+Zc555eYdLuIEP3cCe4Zu+rq/mSXvSbyHg5eHl4eVxXRrwpD3p+Qh4n56P2+xZnvTszC7MCAAUF0XfOdJa62+IaCeO40GSJA/zPP8RwB9vDjBtWmv9JRE5ImpZawdVpbo58CkAIyJnIrLAzEvOuXNm/h3AfQDOAGdj5z5m5gXn3G9ENCKilckaiDwpiFZEpCoiTyYrM3/knOtXmHs58BkABvCLc+4DZl6cxIjoh4ODA728vIzt7W2sra09PT4+fgTgQES4LMvHZIy5LyI/f/dJvfK6FKMJqAdqmBS2ZgWINY9zJyZ1whUiRJrTfmGjybSxUemwcFEmghqT1UxlUrpQEXD7jRq3NWelEz10okIi1AJOX+U2EgALgUrT0kVnmcKvQ+uazSZvbW2hWq2i3++j3W5jf38f6+vrebvdHovI5pT09+vN7teLYev990K8GpQYjS1u1TQqhnHez6EU4cM7FTw7z5AXDo16ACcyPRsaxp2FEM9ejKd7zUaIYVpiOCpRq2rUIo0XLzMwEZaaFTzvZ8hyh/iWnu69fF1AAoOfFr/A5w8eYDQaIY5jJEmC09NTNBoNdLtd7O3t4ejoCIeHh8k/pL8KUSkCZcg6hCLDjLk2ufYGpRuXAuOMYiosQiDNNEcQILQuzYgi0QxVWKuAMtcqJPdXDaKaMMOUNrOAtoFSVE5jaaY4mtQPS5dmQPS8WkdfGReYgDe/3cTdpbvo9XrY2NhAp9NBq9XKO53Ov6T/r2mlVBeX0DSAMzeHppm5h7doemdvR99buYfd3V2srq4+PTk5+a+mr9CmrqTU3+5Rr9cHg8HgYve4krddbZGb5dOXmf2d+7n4pi9D4LrOeHl40v42fl0a8KQ96fkIeJ+ej9vsWZ707Mzmy/Ck5+M2e9afhmuAz1BhwewAAAAASUVORK5CYII=" alt="Next in Tag" width="45" height="45" loading="lazy">
                    </a>
                  
                </div>
              </div>
            
          
          <!-- JavaScript to Toggle the Tag-based Navigation -->
          <script>
            document.addEventListener("DOMContentLoaded", function(){
              var tagLinks = document.querySelectorAll('.tag-option');
              tagLinks.forEach(function(link){
                link.addEventListener('click', function(event){
                  event.preventDefault();
                  // Remove "active" class from all tag options.
                  tagLinks.forEach(function(el){ el.classList.remove('active'); });
                  // Add active class to the clicked tag option.
                  this.classList.add('active');
                  // Hide all tag navigation blocks.
                  document.querySelectorAll('.nav-group.tags').forEach(function(block){
                    block.style.display = 'none';
                  });
                  // Show the navigation block corresponding to the selected tag.
                  var tagSlug = this.getAttribute('data-tag');
                  var target = document.getElementById('tag-nav-' + tagSlug);
                  if(target) {
                    target.style.display = 'block';
                  }
                });
              });
            });
          </script>
      </nav>
      
    </main>
    <footer id="bottom-of-page" class="site-footer">
      <div class="footer-content">
        <!-- Back to top link -->
        <a href="#" aria-label="Back to top" class="back2top-link">
          <span class="sronly">Back to top</span>
        </a>
    
        <!-- Liquid Time Calculation and Display -->
        
        
        
        <a href="https://ib.bsb.br/404" aria-label="404">
          2025-07-17 16:49:39
        </a>
        &#x23;
    
        <!-- Tag Selector -->
        <ul class="tag-selector">
          
            
            
              <li>
                <a href="#" class="tag-option active" data-tag="scratchpad">
                  scratchpad
                </a>
              </li>
            
          
        </ul>
        &hArr;
    
        <!-- GitHub Link -->
        <a href="https://github.com/ib-bsb-br/ib-bsb-br.github.io" aria-label="GitHub">
          &#8505;
        </a>
    
        <!-- Homepage Link -->
        <a href="/" aria-label="Homepage">
          infoBAG
        </a>
    
        <!-- Copy All Code Button -->
        <button id="copyAllButton" aria-label="Copy all code">
          &copy;
        </button>
      </div>
    </footer>
    <style>
      .back2top-link {
        display: inline-block;
        width: 32px;
        height: 32px;
        background: url("/assets/Rope_(Old).gif") center center no-repeat;
        background-size: contain;
        text-decoration: none;
        vertical-align: middle;
      }
      .sronly {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
      }
    </style>
    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "Article",
        "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "https://ib.bsb.br/content-based-retrieval-system/"
        },
        "headline": "Content-Based Retrieval System",
        "description": "",
        "datePublished": "2025-05-22T00:00:00+00:00",
        "dateModified": "2025-05-22T19:34:45+00:00",
        "author": {
          "@type": "Person",
          "name": "Author"
        },
        "publisher": {
          "@type": "Organization",
          "name": "infoBAG"
          
        }
        
      }
    </script>
    <script src="/assets/js/prism.js" defer></script>
    <script src="/assets/js/copy-all-code.js"></script>
  </body>
</html>
