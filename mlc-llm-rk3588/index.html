<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
      
        MLC-LLM for rk3588 - infoBAG
      
    </title>
    <meta name="title" content="MLC-LLM for rk3588 - infoBAG" />
    <meta name="description" content="">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://ib.bsb.br/mlc-llm-rk3588/">
    <meta property="og:title" content="MLC-LLM for rk3588 - infoBAG">
    <meta property="og:description" content="">
    <meta property="og:image" content="/favicon.ico">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://ib.bsb.br/mlc-llm-rk3588/">
    <meta name="twitter:title" content="MLC-LLM for rk3588 - infoBAG">
    <meta name="twitter:description" content="">
    <meta name="twitter:image" content="/favicon.ico">
    <link rel="canonical" href="https://ib.bsb.br/mlc-llm-rk3588/">
    <link rel="alternate" type="application/rss+xml" title="infoBAG" href="https://ib.bsb.br/rss.xml">
    
      <meta name="keywords" content="scratchpad">
      
        <meta property="article:tag" content="scratchpad">
      
    
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    
    <link rel="stylesheet" href="/style.css">
  </head>
  <body class="post-content-body">
    <header class="header-container">
      <nav aria-label="Main navigation" class="header-content">
        <a href="/" aria-label="Home">
          <img src="/favicon.ico" alt="Home" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/tags" aria-label="Tags">
          <img src="/assets/Label.gif" alt="Tags" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/send" aria-label="send">
          <img src="/assets/rot.gif" alt="send" class="favicon search-link" width="32" height="32" loading="lazy">
        </a> 
        <a href="/created" aria-label="archive created">
          <img src="/assets/Loose_Stone_Pile.gif" alt="archive created" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/events" aria-label="Events">
          <img src="/assets/Paralyse_Rune.gif" alt="Events" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/modified" aria-label="archive modified">
          <img src="/assets/Hole_(Rock).gif" alt="archive modified" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
    </nav>
      <h5 class="post-title">
        <a href="#bottom-of-page" aria-label="Go to bottom">
          MLC-LLM for rk3588
        </a>
      </h5>
      <div class="post-meta">
        <time datetime="2026-01-14T00:00:00+00:00" class="post-date">
          14 Jan 2026
        </time>
        
          <span class="post-updated">
            ↣
            <time datetime="2026-01-14T17:38:54+00:00">
              14 Jan 2026
            </time>
          </span>
        
        
          <p class="post-slug">
            Slug: <a href="https://ib.bsb.br/mlc-llm_rk3588" class="tag">mlc-llm_rk3588</a>
          </p>
        
        
          <p class="post-tags">
            Tags:
            
              <a href="https://ib.bsb.br/tags/#scratchpad" class="tag">scratchpad</a>
            
          </p>
        
      </div>
      <div class="post-actions">
        <div class="page-stats mt-3" role="status" aria-label="Page statistics">
      
      <span class="badge bg-primary">
        17733 characters
      </span>
        <span class="separator mx-2" aria-hidden="true">•</span>
        <span class="badge bg-primary">
        2486 words
      </span>
      </div>
        <div class="action-buttons d-flex flex-wrap gap-2">
          
            
              <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/edit/main/_posts/2026-01-14-mlc-llm_rk3588.md"
                    method="GET"
                    target="_blank"
                    rel="noopener noreferrer"
                    class="d-inline-block">
                <button type="submit" class="btn btn-danger" aria-label="Edit page content">
                  Improve this page
                </button>
              </form>
            
            <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/commits/main/_posts/2026-01-14-mlc-llm_rk3588.md"
                  method="GET"
                  target="_blank"
                  rel="noopener noreferrer"
                  class="d-inline-block">
              <button type="submit" class="btn btn-danger" aria-label="View page revision history">
                View revision history
              </button>
            </form>
          
        </div>
      </div>
    </header>
    <main class="content">
      <article class="post-wrapper">
        <div class="post-content-body">
          
<ul><li><a href="#prepare-the-hardware">Prepare the hardware</a></li><li><a href="#install-ubuntu-for-rockchip">Install Ubuntu for Rockchip</a></li><li><a href="#install-os-to-ssd-or-emmc-optional-but-recommended">Install OS to SSD or eMMC (Optional but recommended)</a></li><li><a href="#install-opencl-gpu-drivers">Install OpenCL GPU Drivers</a></li><li><a href="#install-conda---miniconda-optional-but-recommended">Install Conda - miniconda (optional but recommended)</a></li><li><a href="#compile-tvm-unity-optional-but-recommended">Compile TVM-Unity (Optional but recommended)</a><ul><li><a href="#some-prerequisites">Some prerequisites</a><ul><li><a href="#pip-prerequisites">PIP prerequisites</a></li></ul></li><li><a href="#using-conda">Using conda</a></li><li><a href="#verify-tvm-compiled-correct">Verify TVM compiled correct</a></li></ul></li><li><a href="#compile-mlc-llm-from-source">Compile mlc-llm from source</a><ul><li><a href="#package-prerequisites">Package prerequisites</a><ul><li><a href="#pip-prerequisites-1">PIP prerequisites</a></li></ul></li><li><a href="#conda-environment-optional-but-recommended">Conda environment (optional but recommended)</a></li><li><a href="#clone-and-compile">Clone and compile</a></li></ul></li><li><a href="#running-mlc-llm">Running mlc-llm</a><ul><li><a href="#selecting-a-device">Selecting a device</a></li><li><a href="#getting-pre-compiled-llms">Getting pre-compiled LLMs</a></li></ul></li><li><a href="#extras">Extras</a><ul><li><a href="#adding-variables-to-your-shell-configuration">Adding variables to your shell configuration</a></li><li><a href="#compiling-your-own-models">Compiling your own models</a></li><li><a href="#using-mlc-llm-with-homeassistant">Using mlc-llm with HomeAssistant</a></li></ul></li></ul>
          <p>bibref https://llm.mlc.ai/docs/install/gpu.html
bibref https://blog.mlc.ai/2024/04/20/GPU-Accelerated-LLM-on-Orange-Pi
bibref https://github.com/Chrisz236/llm-rk3588</p>

<p>Please open an issue if you find something that can be improved in this guide</p>

<p>See benchmarks for various models I’ve tested <a href="https://github.com/serialscriptr/Orange-PI-5-Pro-MLC-LLM/blob/main/mlc-llm%20benchmarks.md">here</a></p>
  <h2 id="prepare-the-hardware">
    
    
     <a href="#prepare-the-hardware">#</a><a href="#" aria-label="Back to top">Prepare the hardware</a>
        
    
  </h2>
      
<p>While this guide is tailored to the rk3588 it could also be used as a generic guide for other boards using rockchip. Make sure your hardware is properly cooled with a fan and heatsinks as the hardware may overheat or thermal throttle.</p>
  <h2 id="install-ubuntu-for-rockchip">
    
    
     <a href="#install-ubuntu-for-rockchip">#</a><a href="#" aria-label="Back to top">Install Ubuntu for Rockchip</a>
        
    
  </h2>
      
<ul>
  <li>Download the latest version of ubuntu for your device <a href="https://joshua-riek.github.io/ubuntu-rockchip-download/">here</a></li>
  <li>Burn the OS image to a micro SD card using balenaetcher or any other image writing software</li>
  <li>Boot the OS on your device. Optionally connect via SSH for convivence. The default username and password is ubuntu/ubuntu. Once you login for the first time you will be forced to change the password</li>
</ul>
  <h2 id="install-os-to-ssd-or-emmc-optional-but-recommended">
    
    
     <a href="#install-os-to-ssd-or-emmc-optional-but-recommended">#</a><a href="#" aria-label="Back to top">Install OS to SSD or eMMC (Optional but recommended)</a>
        
    
  </h2>
      
<p>In order to not experience disk read/write speed bottlenecks when using mlc-llm I highly recommend using a M.2 SSD or eMMC storage module.</p>
<ul>
  <li>Once booted to the OS off the micro SD card run</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">sudo </span>fdisk <span class="nt">-l</span>
</code></section></div></div>
<p>to find the storage you will be installing the OS on (Highlighted in red is the eMMC module I will be installing my OS on)
   <img width="539" alt="Pasted image 20240910125904" src="https://github.com/user-attachments/assets/c1fbc06f-944f-465b-bade-50967160d4f0" /></p>

<ul>
  <li>Once you found the storage to install the OS on run the following command
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">sudo </span>ubuntu-rockchip-install /dev/mmcblk0
</code></section></div>    </div>
    <p>Where “/dev/mmcblk0” is the storage you wish to install the OS on as shown by fdisk. After the process is done you will need to power off the device, remove the micro SD card and turn the system back on.</p>
  </li>
</ul>
  <h2 id="install-opencl-gpu-drivers">
    
    
     <a href="#install-opencl-gpu-drivers">#</a><a href="#" aria-label="Back to top">Install OpenCL GPU Drivers</a>
        
    
  </h2>
      
<ul>
  <li>Download and install <code class="language-plaintext highlighter-rouge">libmali-g610.so</code>
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">cd</span> /usr/lib <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>wget https://github.com/JeffyCN/mirrors/raw/libmali/lib/aarch64-linux-gnu/libmali-valhall-g610-g6p0-x11-wayland-gbm.so
</code></section></div>    </div>
  </li>
  <li>Check if file <code class="language-plaintext highlighter-rouge">mali_csffw.bin</code> exist under path <code class="language-plaintext highlighter-rouge">/lib/firmware</code>, if not download it with command:
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">cd</span> /lib/firmware <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>wget https://github.com/JeffyCN/mirrors/raw/libmali/firmware/g610/mali_csffw.bin
</code></section></div>    </div>
  </li>
  <li>Download OpenCL ICD loader and manually add libmali to ICD
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>mesa-opencl-icd
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> /etc/OpenCL/vendors
<span class="nb">echo</span> <span class="s2">"/usr/lib/libmali-valhall-g610-g6p0-x11-wayland-gbm.so"</span> | <span class="nb">sudo tee</span> /etc/OpenCL/vendors/mali.icd
</code></section></div>    </div>
  </li>
  <li>Download and install <code class="language-plaintext highlighter-rouge">libOpenCL</code>
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">sudo </span>apt <span class="nb">install </span>ocl-icd-opencl-dev
</code></section></div>    </div>
  </li>
  <li>Download and install dependencies for Mali OpenCL
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">sudo </span>apt <span class="nb">install </span>libxcb-dri2-0 libxcb-dri3-0 libwayland-client0 libwayland-server0 libx11-xcb1
</code></section></div>    </div>
  </li>
  <li>Download and install clinfo to check if OpenCL successfully installed
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">sudo </span>apt <span class="nb">install </span>clinfo
</code></section></div>    </div>
  </li>
  <li>Run clinfo and check the results. We are looking for a mali device like shown below.<img src="https://github.com/user-attachments/assets/e9913f73-8d7b-4186-83e2-859bfd2e15f6" alt="Pasted image 20240910132233" /></li>
</ul>
  <h2 id="install-conda---miniconda-optional-but-recommended">
    
    
     <a href="#install-conda---miniconda-optional-but-recommended">#</a><a href="#" aria-label="Back to top">Install Conda - miniconda (optional but recommended)</a>
        
    
  </h2>
      
<p>While not necessary using conda will make managing dependencies for your build environment much easier.</p>
<ul>
  <li>Grab the latest installer script for your hardware <a href="https://docs.anaconda.com/miniconda/">here</a> for the rk3588 I will be using “Miniconda3 Linux-aarch64 64-bit” as the device is using an arm64 CPU.</li>
  <li>Run the following, change the installer script url accordingly
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">mkdir</span> <span class="nt">-p</span> ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh <span class="nt">-O</span> ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh <span class="nt">-b</span> <span class="nt">-u</span> <span class="nt">-p</span> ~/miniconda3
<span class="nb">rm</span> ~/miniconda3/miniconda.sh
</code></section></div>    </div>
  </li>
  <li>add/initialize conda in your shell
 for bash:
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code>~/miniconda3/bin/conda init bash
</code></section></div>    </div>
    <p>for zsh:</p>
    <div class="language-zsh highlighter-rouge"><div class="highlight"><section><code>~/miniconda3/bin/conda init zsh
</code></section></div>    </div>
  </li>
  <li>
    <p>close and restart your shell to activate and start using conda. You should see (base) next to your username <img src="https://github.com/user-attachments/assets/fe7c8885-55a2-417c-9318-69942e9a9fce" alt="Pasted image 20240910134503" /></p>
  </li>
  <li>Update conda
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code>conda update <span class="nt">--yes</span> <span class="nt">-n</span> base <span class="nt">-c</span> defaults conda
</code></section></div>    </div>
  </li>
  <li><strong>Set libmamba as the dependency solver.</strong> The default dependency solver in conda could be slow in certain scenarios, and it is always recommended to upgrade it to libmamba, a faster solver. Install:
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code>conda <span class="nb">install</span> <span class="nt">--yes</span> <span class="nt">-n</span> base conda-libmamba-solver
</code></section></div>    </div>
    <p>set as default dependency solver:</p>
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code>conda config <span class="nt">--set</span> solver libmamba
</code></section></div>    </div>
  </li>
</ul>
  <h2 id="compile-tvm-unity-optional-but-recommended">
    
    
     <a href="#compile-tvm-unity-optional-but-recommended">#</a><a href="#" aria-label="Back to top">Compile TVM-Unity (Optional but recommended)</a>
        
    
  </h2>
      
<p>See instructions here for up to date instructions: https://llm.mlc.ai/docs/install/tvm.html</p>

<p>mlc-llm provides pre-compiles tvm packages which should be fine for our uses but in my personal testing I was unable to get them to work (specifically on the rk3588). If you skip this section and encounter errors related to the “tvm” module I recommend attempting to compile it yourself.</p>
  <h3 id="some-prerequisites">
    
    
     <a href="#some-prerequisites">#</a><a href="#" aria-label="Back to top">Some prerequisites</a>
        
    
  </h3>
      
<p>Since these packages are required when building mlc-llm I install them with apt install but you could optionally install them to your build environment with conda. Either way make sure these packages are install prior to attempting to compile TVM-Unity</p>
<ul>
  <li>doxygen</li>
  <li>graphviz</li>
  <li>build-essential (ubuntu specific package. Install the equivalent for your OS)</li>
  <li>zlib1g-dev</li>
  <li>libfl-dev</li>
  <li>clang</li>
  <li>libxml2-dev</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">sudo </span>apt <span class="nb">install </span>doxygen graphviz build-essential zlib1g-dev libfl-dev clang libxml2-dev
</code></section></div></div>
  <h4 id="pip-prerequisites">
    
    
     <a href="#pip-prerequisites">#</a><a href="#" aria-label="Back to top">PIP prerequisites</a>
        
    
  </h4>
      
<p>The following python modules are required in order for us to run mlc-llm and test our tvm-unity build. If you are not using conda you will need to install and use python3-pip</p>
<ul>
  <li>numpy</li>
  <li>decorator</li>
  <li>psutil</li>
  <li>typing_extensions</li>
  <li>packaging</li>
  <li>attrs</li>
  <li>cython</li>
</ul>

<p>Using pip</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>pip <span class="nb">install </span>numpy decorator psutil typing_extensions packaging attrs cython
</code></section></div></div>
<p>Using conda</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>conda <span class="nb">install </span>numpy decorator psutil typing_extensions packaging attrs cython
</code></section></div></div>
  <h3 id="using-conda">
    
    
     <a href="#using-conda">#</a><a href="#" aria-label="Back to top">Using conda</a>
        
    
  </h3>
      
<p>The following commands are for setting up a build environment for compiling TVM-Unity. If you skipped the conda installation you can attempt to install these dependencies using apt install. If manually installing these packages with apt install make sure you are installing the correct versions as by default it will install the latest version of each package.</p>

<p>Run the following to create and setup the conda environment</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="c"># make sure to start with a fresh environment</span>
conda <span class="nb">env </span>remove <span class="nt">-n</span> tvm-build-venv
<span class="c"># create the conda environment with build dependency</span>
conda create <span class="nt">-n</span> tvm-build-venv <span class="nt">-c</span> conda-forge <span class="se">\</span>
    <span class="s2">"llvmdev&gt;=15"</span> <span class="se">\</span>
    <span class="s2">"cmake&gt;=3.24"</span> <span class="se">\</span>
    git <span class="se">\</span>
    <span class="nv">python</span><span class="o">=</span>3.11
<span class="c"># enter the build environment</span>
conda activate tvm-build-venv
</code></section></div></div>

<ul>
  <li>Clone the github repo for tvm-unity (actually mlc-ai/relax temporarily, check link above to see if tvm-unity is recommended now) I recommend cloning in the home directory (~)
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="c"># clone from GitHub</span>
git clone <span class="nt">--recursive</span> https://github.com/mlc-ai/relax.git tvm-unity <span class="o">&amp;&amp;</span> <span class="nb">cd </span>tvm-unity
<span class="c"># create the build directory</span>
<span class="nb">rm</span> <span class="nt">-rf</span> build <span class="o">&amp;&amp;</span> <span class="nb">mkdir </span>build <span class="o">&amp;&amp;</span> <span class="nb">cd </span>build
<span class="c"># specify build requirements in `config.cmake`</span>
<span class="nb">cp</span> ../cmake/config.cmake <span class="nb">.</span>
</code></section></div>    </div>
  </li>
  <li>configure the cmake.config file which instructs cmake how we want to compile the project. Ensure you are in the build directory created earlier when running these commands.
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">echo</span> <span class="s2">"set(CMAKE_BUILD_TYPE RelWithDebInfo)"</span> <span class="o">&gt;&gt;</span> config.cmake
<span class="nb">echo</span> <span class="s2">"set(USE_LLVM </span><span class="se">\"</span><span class="s2">llvm-config --ignore-libllvm --link-static</span><span class="se">\"</span><span class="s2">)"</span> <span class="o">&gt;&gt;</span> config.cmake
<span class="nb">echo</span> <span class="s2">"set(HIDE_PRIVATE_SYMBOLS ON)"</span> <span class="o">&gt;&gt;</span> config.cmake
</code></section></div>    </div>
  </li>
  <li>Set how mlc-llm will run the llm. The options are Cuda (Nvidia), Metal (Apple), Vulkan (AMD) and OpenCL which is what we will be using. If you are using something other than OpenCL make sure you set that to “ON”
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">echo</span> <span class="s2">"set(USE_OPENCL ON)"</span> <span class="o">&gt;&gt;</span> config.cmake
</code></section></div>    </div>
  </li>
  <li>once the config.cmake file is created and configured to our liking we can run the following to build the project. Again ensure you are in the build folder from earlier:
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code>cmake .. <span class="o">&amp;&amp;</span> cmake <span class="nt">--build</span> <span class="nb">.</span> <span class="nt">--parallel</span> <span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span>
</code></section></div>    </div>
  </li>
</ul>
  <h3 id="verify-tvm-compiled-correct">
    
    
     <a href="#verify-tvm-compiled-correct">#</a><a href="#" aria-label="Back to top">Verify TVM compiled correct</a>
        
    
  </h3>
      
<p>You may see several warnings or errors during compilation but this should be ok as long as the build doesn’t end with several error messages like the screenshot below which is due to missing libfl-dev <img src="https://github.com/user-attachments/assets/6eac5357-e851-4396-ae45-d554505053b3" alt="Pasted image 20240910152125" /></p>

<p>if you encounter any errors such as the error shown above remediate the errors by installing the required packages and restart the process including and following this command in the guide above. Make sure you are in the tvm-unity directory when running the command below.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">rm</span> <span class="nt">-rf</span> build <span class="o">&amp;&amp;</span> <span class="nb">mkdir </span>build <span class="o">&amp;&amp;</span> <span class="nb">cd </span>build
</code></section></div></div>

<p>Example of output when built successfully: <img src="https://github.com/user-attachments/assets/c51bd9fc-f5b1-425e-9495-1088dc34b98c" alt="Pasted image 20240910170713" /></p>

<p>Add the tvm-unity path to your python path variable. Edit the path if your tvm-unity folder is located somewhere else:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">export </span><span class="nv">PYTHONPATH</span><span class="o">=</span>/home/ubuntu/tvm-unity/python:<span class="nv">$PYTHONPATH</span>
</code></section></div></div>
<p>After adding the path to your python path variable run the following. Make sure you have the python modules installed from the pip prerequisites section!</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>python <span class="nt">-c</span> <span class="s2">"import tvm; print(tvm.__file__)"</span>
</code></section></div></div>
<p>which should output something similar to: <img src="https://github.com/user-attachments/assets/ed4880f6-1e75-4f95-bc7f-244c4a4e838d" alt="Pasted image 20240910173413" /></p>

<p>10/10/24: You may see errors about “-mcpu=apple-latest” this is safe to ignore provided the final output is similar to the output shown above. Ex:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="o">[</span>20:09:46] /home/ubuntu/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>apple-latest<span class="sb">`</span> is not valid <span class="k">in</span> <span class="sb">`</span><span class="nt">-mtriple</span><span class="o">=</span>arm64-apple-macos<span class="sb">`</span>, using default <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>generic<span class="sb">`</span>
<span class="o">[</span>20:09:46] /home/ubuntu/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>apple-latest<span class="sb">`</span> is not valid <span class="k">in</span> <span class="sb">`</span><span class="nt">-mtriple</span><span class="o">=</span>arm64-apple-macos<span class="sb">`</span>, using default <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>generic<span class="sb">`</span>
<span class="o">[</span>20:09:46] /home/ubuntu/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>apple-latest<span class="sb">`</span> is not valid <span class="k">in</span> <span class="sb">`</span><span class="nt">-mtriple</span><span class="o">=</span>arm64-apple-macos<span class="sb">`</span>, using default <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>generic<span class="sb">`</span>
</code></section></div></div>

<p>Now run the following to check the build options used:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>python <span class="nt">-c</span> <span class="s2">"import tvm; print('</span><span class="se">\n</span><span class="s2">'.join(f'{k}: {v}' for k, v in tvm.support.libinfo().items()))"</span>
</code></section></div></div>
<p>We should then be able to see opencl set to “ON”</p>

<p><img width="304" alt="Pasted image 20240910173823" src="https://github.com/user-attachments/assets/e9097e30-664b-4a76-b80e-e13ce060a6f9" /></p>
  <h2 id="compile-mlc-llm-from-source">
    
    
     <a href="#compile-mlc-llm-from-source">#</a><a href="#" aria-label="Back to top">Compile mlc-llm from source</a>
        
    
  </h2>
      
  <h3 id="package-prerequisites">
    
    
     <a href="#package-prerequisites">#</a><a href="#" aria-label="Back to top">Package prerequisites</a>
        
    
  </h3>
      
<p>There is only one other package required here compared to the tvm-unity section. If you installed the packages included in the tvm-unity section then you only need to install git-lfs</p>
<ul>
  <li>git-lfs</li>
  <li>Doxygen</li>
  <li>Graphviz</li>
  <li>build-essential (ubuntu specific package. Install the equivalent for your OS)</li>
  <li>zlib1g-dev</li>
  <li>libfl-dev</li>
  <li>clang
  <h4 id="pip-prerequisites-1">
    
    
     <a href="#pip-prerequisites-1">#</a><a href="#" aria-label="Back to top">PIP prerequisites</a>
        
    
  </h4>
      
    <p>The following python modules are required in order for us to run mlc-llm. If you are not using conda you will need to install and use python3-pip.</p>
  </li>
  <li>numpy</li>
  <li>decorator</li>
  <li>psutil</li>
  <li>typing_extensions</li>
  <li>packaging</li>
  <li>attrs</li>
  <li>pydantic</li>
  <li>shortuuid</li>
  <li>fastapi</li>
  <li>requests</li>
  <li>tqdm</li>
  <li>prompt_toolkit</li>
  <li>uvicorn</li>
  <li>cython</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>pip <span class="nb">install </span>numpy decorator psutil typing_extensions packaging attrs pydantic shortuuid fastapi requests tqdm prompt_toolkit uvicorn cython
</code></section></div></div>
  <h3 id="conda-environment-optional-but-recommended">
    
    
     <a href="#conda-environment-optional-but-recommended">#</a><a href="#" aria-label="Back to top">Conda environment (optional but recommended)</a>
        
    
  </h3>
      
<p>If you have conda installed from earlier perform the following to setup a build environment for mlc-llm. Note that if you aren’t using conda you need to have the packages installed on your OS with the recommended versions.</p>
<ul>
  <li>Create the environment
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code>conda create <span class="nt">-n</span> mlc-chat-venv <span class="nt">-c</span> conda-forge <span class="se">\</span>
  <span class="s2">"cmake&gt;=3.24"</span> <span class="se">\</span>
  rust <span class="se">\</span>
  git <span class="se">\</span>
  <span class="nv">python</span><span class="o">=</span>3.11
</code></section></div>    </div>
  </li>
  <li>activate the environment
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code>conda activate mlc-chat-venv
</code></section></div>    </div>
  </li>
</ul>
  <h3 id="clone-and-compile">
    
    
     <a href="#clone-and-compile">#</a><a href="#" aria-label="Back to top">Clone and compile</a>
        
    
  </h3>
      
<ul>
  <li>clone the mlc-llm github repo. I recommend cloning it to your home directory to make things easy:
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code>git clone <span class="nt">--recursive</span> https://github.com/mlc-ai/mlc-llm.git <span class="o">&amp;&amp;</span> <span class="nb">cd </span>mlc-llm/
</code></section></div>    </div>
  </li>
  <li>create the build directory
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">mkdir</span> <span class="nt">-p</span> build <span class="o">&amp;&amp;</span> <span class="nb">cd </span>build
</code></section></div>    </div>
  </li>
  <li>generate the config.cmake with the following script. Note if you are not using conda you will likely have to replace “python” with “python3”
    <div class="language-bash highlighter-rouge"><div class="highlight"><section><code>python ../cmake/gen_cmake_config.py
</code></section></div>    </div>
  </li>
  <li>
    <p>for the first option of the script, the tvm location, if you followed the compile tvm-unity instructions from earlier you should use that directory here. Otherwise you can attempt to use the default provided and just leave this option blank. Here in the example below I am using a custom compiled tvm <img src="https://github.com/user-attachments/assets/91853fcf-bf9c-4b5d-8688-3bae6e57a229" alt="Pasted image 20240910175631" /></p>
  </li>
  <li>Use CUDA should be no</li>
  <li>Use CUTLASS should be no</li>
  <li>Use CUBLAS should be no</li>
  <li>Use ROCm should be no</li>
  <li>Use Vulkan should be no</li>
  <li>Use Metal should be no</li>
  <li>Use OpenCL should be Yes</li>
  <li>Use OpenCLHostPtr can be yes or no. From what I understand this has something to do with using storage for opencl cache of some kind but dont have a great understanding of what exactly this means/does.</li>
  <li>Use FlashInfer should be no
Here is my output after running the script:
<img src="https://github.com/user-attachments/assets/bb2e8efb-5ca3-4dfb-a824-ce8ae9c987b3" alt="Pasted image 20240910180223" /></li>
</ul>

<p>Now you are ready to compile. Run the following to build the project. Again keep an eye out for errors and warnings . Warnings and errors may not prevent the project from compiling</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>cmake .. <span class="o">&amp;&amp;</span> cmake <span class="nt">--build</span> <span class="nb">.</span> <span class="nt">--parallel</span> <span class="si">$(</span><span class="nb">nproc</span><span class="si">)</span> <span class="o">&amp;&amp;</span> <span class="nb">cd</span> ..
</code></section></div></div>
<p>Here is my output after a successful build:
<img src="https://github.com/user-attachments/assets/18289722-ec4c-4bb3-967a-d75ccb0c60b8" alt="Pasted image 20240910181146" /></p>

<p>After successfully building lets add the mlc-llm path to our pythonpath variable. Change the following to match your paths if you used something different:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">export </span><span class="nv">MLC_LLM_SOURCE_DIR</span><span class="o">=</span>/home/ubuntu/mlc-llm
<span class="nb">export </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$MLC_LLM_SOURCE_DIR</span>/python:<span class="nv">$PYTHONPATH</span>
</code></section></div></div>
<p>And since mlc-llm is ran within python we will make an alias for calling it:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">alias </span><span class="nv">mlc_llm</span><span class="o">=</span><span class="s2">"python -m mlc_llm"</span>
</code></section></div></div>

<p>now provided you have all the pre-requisites installed you can run the following to test mlc-llm:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>python <span class="nt">-c</span> <span class="s2">"import mlc_llm; print(mlc_llm)"</span>
</code></section></div></div>
<p>if successful you should see something along the lines of:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>&lt;module 'mlc_llm' from '/home/ubuntu/mlc-llm/python/mlc_llm/__init__.py'&gt;
</code></section></div></div>

<p>10/10/24: You may see errors about “-mcpu=apple-latest” this is safe to ignore provided the final output is similar to the output shown above. Ex:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="o">[</span>20:09:46] /home/ubuntu/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>apple-latest<span class="sb">`</span> is not valid <span class="k">in</span> <span class="sb">`</span><span class="nt">-mtriple</span><span class="o">=</span>arm64-apple-macos<span class="sb">`</span>, using default <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>generic<span class="sb">`</span>
<span class="o">[</span>20:09:46] /home/ubuntu/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>apple-latest<span class="sb">`</span> is not valid <span class="k">in</span> <span class="sb">`</span><span class="nt">-mtriple</span><span class="o">=</span>arm64-apple-macos<span class="sb">`</span>, using default <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>generic<span class="sb">`</span>
<span class="o">[</span>20:09:46] /home/ubuntu/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>apple-latest<span class="sb">`</span> is not valid <span class="k">in</span> <span class="sb">`</span><span class="nt">-mtriple</span><span class="o">=</span>arm64-apple-macos<span class="sb">`</span>, using default <span class="sb">`</span><span class="nt">-mcpu</span><span class="o">=</span>generic<span class="sb">`</span>
</code></section></div></div>
  <h2 id="running-mlc-llm">
    
    
     <a href="#running-mlc-llm">#</a><a href="#" aria-label="Back to top">Running mlc-llm</a>
        
    
  </h2>
      
<p>To open the command help information run the following:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>mlc_llm chat <span class="nt">-h</span>
</code></section></div></div>
  <h3 id="selecting-a-device">
    
    
     <a href="#selecting-a-device">#</a><a href="#" aria-label="Back to top">Selecting a device</a>
        
    
  </h3>
      
<p>When attempting to run mlc-llm using opencl on the rk3588 I found that it would default to a generic ARM opencl device rather than the Mali GPU which we setup drivers for earlier in this guide. Because of this I had to specify the device to run mlc-llm on otherwise I would encounter an error preventing mlc-llm from running. I believe the order of the devices is reflected by clinfo but this may just be a coincidence in my case. See examples of specifying the device in the command examples below</p>
  <h3 id="getting-pre-compiled-llms">
    
    
     <a href="#getting-pre-compiled-llms">#</a><a href="#" aria-label="Back to top">Getting pre-compiled LLMs</a>
        
    
  </h3>
      
<p>To download and utilize some pre-comipled LLM models for mlc-llm we can visit the mlc-ai organization on huggingface https://huggingface.co/mlc-ai
Available quantization codes are: <code class="language-plaintext highlighter-rouge">q3f16_0</code>, <code class="language-plaintext highlighter-rouge">q4f16_1</code>, <code class="language-plaintext highlighter-rouge">q4f16_2</code>, <code class="language-plaintext highlighter-rouge">q4f32_0</code>, <code class="language-plaintext highlighter-rouge">q0f32</code>, and <code class="language-plaintext highlighter-rouge">q0f16</code></p>

<p>for testing I will be using SmolLM-1.7B-Instruct-q4f16_1-MLC as its a pretty small download and I’ve found it runs decent. To run it as a chat run the following (note that in the future you may need to select a different model or an updated version of this model):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>mlc_llm chat HF://mlc-ai/SmolLM-1.7B-Instruct-q4f16_1-MLC <span class="nt">--device</span> opencl:1
</code></section></div></div>

<p>If you wanted to run the model as a server you would do the following:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code>mlc_llm serve HF://mlc-ai/SmolLM-1.7B-Instruct-q4f16_1-MLC <span class="nt">--device</span> opencl:1 <span class="nt">--mode</span> server <span class="nt">--host</span> 0.0.0.0
</code></section></div></div>
<p>This will run mlc-llm as a REST server on opencl device 1 with server mode specified and the host changed from 127.0.0.1 to allow access outside of localhost</p>

<p>You can test if the server is accessible outside of localhost using curl or invoke-restmethod on windows<img src="https://github.com/user-attachments/assets/7a1fda4c-aff6-4e72-bc64-d1c4e927a4b8" alt="Pasted image 20240910185054" /></p>
  <h2 id="extras">
    
    
     <a href="#extras">#</a><a href="#" aria-label="Back to top">Extras</a>
        
    
  </h2>
      
  <h3 id="adding-variables-to-your-shell-configuration">
    
    
     <a href="#adding-variables-to-your-shell-configuration">#</a><a href="#" aria-label="Back to top">Adding variables to your shell configuration</a>
        
    
  </h3>
      
<p>To make the variables and alias we added earlier permanent we can add them to our bash shell config. Note this is unique to the user and not systemwide:
Modify your shell config (use your text editor of choice):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">sudo </span>nano ~/.bashrc
</code></section></div></div>
<p>At the bottom of the file add the following and change accordingly based on the paths you used and if you compiled tvm-unity:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><section><code><span class="nb">export </span><span class="nv">PYTHONPATH</span><span class="o">=</span>/home/ubuntu/mlc-llm/python:/home/ubuntu/tvm-unity/python
<span class="nb">alias </span><span class="nv">mlc_llm</span><span class="o">=</span><span class="s2">"python -m mlc_llm"</span>
</code></section></div></div>
<p>Once saved you need to reload your shell in order for the variables to be loaded</p>
  <h3 id="compiling-your-own-models">
    
    
     <a href="#compiling-your-own-models">#</a><a href="#" aria-label="Back to top">Compiling your own models</a>
        
    
  </h3>
      
<p>See the bottom of the following guide for basic instructions on building your own model:
https://github.com/Chrisz236/llm-rk3588</p>
  <h3 id="using-mlc-llm-with-homeassistant">
    
    
     <a href="#using-mlc-llm-with-homeassistant">#</a><a href="#" aria-label="Back to top">Using mlc-llm with HomeAssistant</a>
        
    
  </h3>
      
<p>At the time of writing this HomeAssistant has been adding lots of support for LLMs to utilize as a “Conversation agent” within a custom voice assistant. This allows mlc-llm to act as the brains for whatever you are requesting via your HomeAssistant voice assistant. Currently there is no official integration which allows using mlc-llm with HomeAssistant but there is a custom integration which we can use thanks to mlc-llm Open-AI REST compatibility https://github.com/jekalmin/extended_openai_conversation</p>

<p>The Extended Open-AI conversation integration is a fork of the official Open-AI integration but allows for specifying a custom server address etc. Here is an example of the config options I used when using mlc-llm with HomeAssistant:
<img src="https://github.com/user-attachments/assets/89a91b00-ff81-41bf-89d0-10a7a56127ff" alt="Pasted image 20240910190953" /></p>

<p>Also make sure you have the correct chat model selected. I entered the model as shown by the mlc-llm server when querying its models.
<img src="https://github.com/user-attachments/assets/aaddc08d-0305-4539-8405-8bc0c1f3f439" alt="Pasted image 20240910191005" /></p>

<p>Do not expect this to perform well. The config options of this extension are vague and have no documentation so I am not sure if there is anything in HomeAssistant that can be tweaked for better performance. I expect we will need an official implementation of Open-AI compatible servers before we see better performance. This integration also attempts to use functions rather than tools for controlling entities in HomeAssistant, I haven’t had much luck using either and I expect more work is needed to fine tune this.</p>

        </div>
        
          URL: https://ib.bsb.br/mlc-llm_rk3588
        
      </article>
      <nav class="post-navigation-combined" aria-label="Post navigation">
        <!-- Chronological Navigation (always visible at 40vh) -->
        
          <div class="nav-arrow chronological prev">
            <a href="/agentsmd/" title="AGENTS.md" rel="prev">
              <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ+lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="Previous article" width="45" height="45" loading="lazy">
            </a>
          </div>
        
        
          <div class="nav-arrow chronological next">
            <a href="/rk3588-npu/" title="rk3588 npu" rel="next">
              <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAwdJREFUaEPtl01rG1cUht9z7p070ijKWLVS3I9YULtNMBRn2UIXxZtuvCjedNWfoKVXBvsHeOUfoIUx5AeY0pVXXSR052UxBJSi0kDiRplII83HvadI9CMtOFjCmBjubC7cc8+Zc555eYdLuIEP3cCe4Zu+rq/mSXvSbyHg5eHl4eVxXRrwpD3p+Qh4n56P2+xZnvTszC7MCAAUF0XfOdJa62+IaCeO40GSJA/zPP8RwB9vDjBtWmv9JRE5ImpZawdVpbo58CkAIyJnIrLAzEvOuXNm/h3AfQDOAGdj5z5m5gXn3G9ENCKilckaiDwpiFZEpCoiTyYrM3/knOtXmHs58BkABvCLc+4DZl6cxIjoh4ODA728vIzt7W2sra09PT4+fgTgQES4LMvHZIy5LyI/f/dJvfK6FKMJqAdqmBS2ZgWINY9zJyZ1whUiRJrTfmGjybSxUemwcFEmghqT1UxlUrpQEXD7jRq3NWelEz10okIi1AJOX+U2EgALgUrT0kVnmcKvQ+uazSZvbW2hWq2i3++j3W5jf38f6+vrebvdHovI5pT09+vN7teLYev990K8GpQYjS1u1TQqhnHez6EU4cM7FTw7z5AXDo16ACcyPRsaxp2FEM9ejKd7zUaIYVpiOCpRq2rUIo0XLzMwEZaaFTzvZ8hyh/iWnu69fF1AAoOfFr/A5w8eYDQaIY5jJEmC09NTNBoNdLtd7O3t4ejoCIeHh8k/pL8KUSkCZcg6hCLDjLk2ufYGpRuXAuOMYiosQiDNNEcQILQuzYgi0QxVWKuAMtcqJPdXDaKaMMOUNrOAtoFSVE5jaaY4mtQPS5dmQPS8WkdfGReYgDe/3cTdpbvo9XrY2NhAp9NBq9XKO53Ov6T/r2mlVBeX0DSAMzeHppm5h7doemdvR99buYfd3V2srq4+PTk5+a+mr9CmrqTU3+5Rr9cHg8HgYve4krddbZGb5dOXmf2d+7n4pi9D4LrOeHl40v42fl0a8KQ96fkIeJ+ej9vsWZ707Mzmy/Ck5+M2e9afhmuAz1BhwewAAAAASUVORK5CYII=" alt="Next article" width="45" height="45" loading="lazy">
            </a>
          </div>
        
            
            
              
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                  
                  
              
              
              
              <!-- Navigation block for tag "scratchpad"; default display for the first tag -->
              <div class="nav-group tags" id="tag-nav-scratchpad" style="display: block;">
                <div class="nav-arrow tags prev">
                  
                    <a href="/rk3588-npu/" title="rk3588 npu" rel="prev">
                      <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ+lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="Previous in Tag" width="45" height="45" loading="lazy">
                    </a>
                  
                </div>
                <div class="nav-arrow tags next">
                  
                    <a href="javascript:void(0)" class="disabled" aria-disabled="true" title="No next in tag">
                      <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ+lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="No next in Tag" width="45" height="45" loading="lazy">
                    </a>
                  
                </div>
              </div>
            
          
          <!-- JavaScript to Toggle the Tag-based Navigation -->
          <script>
            document.addEventListener("DOMContentLoaded", function(){
              var tagLinks = document.querySelectorAll('.tag-option');
              tagLinks.forEach(function(link){
                link.addEventListener('click', function(event){
                  event.preventDefault();
                  // Remove "active" class from all tag options.
                  tagLinks.forEach(function(el){ el.classList.remove('active'); });
                  // Add active class to the clicked tag option.
                  this.classList.add('active');
                  // Hide all tag navigation blocks.
                  document.querySelectorAll('.nav-group.tags').forEach(function(block){
                    block.style.display = 'none';
                  });
                  // Show the navigation block corresponding to the selected tag.
                  var tagSlug = this.getAttribute('data-tag');
                  var target = document.getElementById('tag-nav-' + tagSlug);
                  if(target) {
                    target.style.display = 'block';
                  }
                });
              });
            });
          </script>
      </nav>
      
    <div class="comment-box">
      Ref. 
      <a href="https://llm.mlc.ai/docs" title="https://llm.mlc.ai/docs">https://llm.mlc.ai/docs</a>
    </div>
    
    </main>
    <footer id="bottom-of-page" class="site-footer">
      <div class="footer-content">
        <!-- Back to top link -->
        <a href="#" aria-label="Back to top" class="back2top-link">
          <span class="sronly">Back to top</span>
        </a>
    
        <!-- Liquid Time Calculation and Display -->
        
        
        
        <a href="https://ib.bsb.br/404" aria-label="404">
          2026-01-14 15:34:45
        </a>
        &#x23;
    
        <!-- Tag Selector -->
        <ul class="tag-selector">
          
            
            
              <li>
                <a href="#" class="tag-option active" data-tag="scratchpad">
                  scratchpad
                </a>
              </li>
            
          
        </ul>
        &hArr;
    
        <!-- GitHub Link -->
        <a href="https://github.com/ib-bsb-br/ib-bsb-br.github.io" aria-label="GitHub">
          &#8505;
        </a>
    
        <!-- Homepage Link -->
        <a href="/" aria-label="Homepage">
          infoBAG
        </a>
    
        <!-- Copy All Code Button -->
        <button id="copyAllButton" aria-label="Copy all code">
          &copy;
        </button>
      </div>
    </footer>
    <style>
      .back2top-link {
        display: inline-block;
        width: 32px;
        height: 32px;
        background: url("/assets/Rope_(Old).gif") center center no-repeat;
        background-size: contain;
        text-decoration: none;
        vertical-align: middle;
      }
      .sronly {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
      }
    </style>
    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "Article",
        "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "https://ib.bsb.br/mlc-llm-rk3588/"
        },
        "headline": "MLC-LLM for rk3588",
        "description": "",
        "datePublished": "2026-01-14T00:00:00+00:00",
        "dateModified": "2026-01-14T17:38:54+00:00",
        "author": {
          "@type": "Person",
          "name": "Author"
        },
        "publisher": {
          "@type": "Organization",
          "name": "infoBAG"
          
        }
        
      }
    </script>
    <script src="/assets/js/prism.js" defer></script>
    <script src="/assets/js/copy-all-code.js"></script>
  </body>
</html>
