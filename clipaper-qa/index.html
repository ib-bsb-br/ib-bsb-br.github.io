<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
      
        cliPaperQA - infoBAG
      
    </title>
    <meta name="title" content="cliPaperQA - infoBAG" />
    <meta name="description" content="">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://ib.bsb.br/clipaper-qa/">
    <meta property="og:title" content="cliPaperQA - infoBAG">
    <meta property="og:description" content="">
    <meta property="og:image" content="/favicon.ico">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://ib.bsb.br/clipaper-qa/">
    <meta name="twitter:title" content="cliPaperQA - infoBAG">
    <meta name="twitter:description" content="">
    <meta name="twitter:image" content="/favicon.ico">
    <link rel="canonical" href="https://ib.bsb.br/clipaper-qa/">
    <link rel="alternate" type="application/rss+xml" title="infoBAG" href="https://ib.bsb.br/rss.xml">
    
      <meta name="keywords" content="linux,scripts">
      
        <meta property="article:tag" content="linux">
      
        <meta property="article:tag" content="scripts">
      
    
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    
    <link rel="stylesheet" href="/style.css">
  </head>
  <body class="post-content-body">
    <header class="header-container">
      <nav aria-label="Main navigation" class="header-content">
        <a href="/" aria-label="Home">
          <img src="/favicon.ico" alt="Home" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/tags" aria-label="Tags">
          <img src="/assets/Label.gif" alt="Tags" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/send" aria-label="send">
          <img src="/assets/rot.gif" alt="send" class="favicon search-link" width="32" height="32" loading="lazy">
        </a> 
        <a href="/created" aria-label="archive created">
          <img src="/assets/Loose_Stone_Pile.gif" alt="archive created" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/events" aria-label="Events">
          <img src="/assets/Paralyse_Rune.gif" alt="Events" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
        <a href="/modified" aria-label="archive modified">
          <img src="/assets/Hole_(Rock).gif" alt="archive modified" class="favicon search-link" width="32" height="32" loading="lazy">
        </a>
    </nav>
      <h5 class="post-title">
        <a href="#bottom-of-page" aria-label="Go to bottom">
          cliPaperQA
        </a>
      </h5>
      <div class="post-meta">
        <time datetime="2024-06-16T00:00:00+00:00" class="post-date">
          16 Jun 2024
        </time>
        
          <span class="post-updated">
            ↣
            <time datetime="2025-02-01T21:29:53+00:00">
              01 Feb 2025
            </time>
          </span>
        
        
          <p class="post-slug">
            Slug: <a href="https://ib.bsb.br/clipaper-qa" class="tag">clipaper-qa</a>
          </p>
        
        
          <p class="post-tags">
            Tags:
            
              <a href="https://ib.bsb.br/tags/#linux" class="tag">linux</a>
            
              <a href="https://ib.bsb.br/tags/#scripts" class="tag">scripts</a>
            
          </p>
        
      </div>
      <div class="post-actions">
        <div class="page-stats mt-3" role="status" aria-label="Page statistics">
      
      <span class="badge bg-primary">
        26009 characters
      </span>
        <span class="separator mx-2" aria-hidden="true">•</span>
        <span class="badge bg-primary">
        2580 words
      </span>
      </div>
        <div class="action-buttons d-flex flex-wrap gap-2">
          
            
              <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/edit/main/_posts/2024-06-16-clipaper-qa.md"
                    method="GET"
                    target="_blank"
                    rel="noopener noreferrer"
                    class="d-inline-block">
                <button type="submit" class="btn btn-danger" aria-label="Edit page content">
                  <span class="button-text">Improve this page?</span>
                  <span class="info-text">aberto.</span>
                </button>
              </form>
            
            <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/commits/main/_posts/2024-06-16-clipaper-qa.md"
                  method="GET"
                  target="_blank"
                  rel="noopener noreferrer"
                  class="d-inline-block">
              <button type="submit" class="btn btn-danger" aria-label="View page revision history">
                View revision history
              </button>
            </form>
          
        </div>
      </div>
    </header>
    <main class="content">
      <article class="post-wrapper">
        <div class="post-content-body">
          
<ul><li><a href="#paperqa-cli-script-documentation">PaperQA CLI Script Documentation</a><ul><li><a href="#introduction">Introduction</a></li><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#installation">Installation</a></li><li><a href="#usage">Usage</a><ul><li><a href="#command-line-arguments">Command Line Arguments</a></li><li><a href="#favorites">Favorites</a></li><li><a href="#examples">Examples</a></li><li><a href="#list-of-questions-template">List of questions template:</a></li></ul></li><li><a href="#detailed-explanation">Detailed Explanation</a><ul><li><a href="#loading-custom-prompts">Loading Custom Prompts</a></li><li><a href="#creating-docs-object">Creating Docs Object</a></li><li><a href="#loading-existing-embeddings">Loading Existing Embeddings</a></li><li><a href="#adding-pdf-documents">Adding PDF Documents</a></li><li><a href="#getting-questions">Getting Questions</a></li><li><a href="#answering-questions">Answering Questions</a></li><li><a href="#saving-data">Saving Data</a></li></ul></li><li><a href="#error-handling">Error Handling</a></li></ul></li><li><a href="#script">Script</a></li><li><a href="#2add">2add</a></li></ul>
          <h1 id="paperqa-cli-script-documentation">
    
    
     <a href="#paperqa-cli-script-documentation">#</a><a href="#" aria-label="Back to top">PaperQA CLI Script Documentation</a>
        
    
  </h1>
      

<p>This documentation provides a comprehensive guide on how to use the provided PaperQA CLI script. The script is designed to process PDF documents, generate embeddings, and answer questions using OpenAI’s language models.</p>
  <h2 id="introduction">
    
    
     <a href="#introduction">#</a><a href="#" aria-label="Back to top">Introduction</a>
        
    
  </h2>
      

<p>The PaperQA CLI script is a command-line tool that allows users to process PDF documents, generate embeddings, and answer questions using OpenAI’s language models. It supports various configurations and options to customize the processing and querying of documents.</p>
  <h2 id="prerequisites">
    
    
     <a href="#prerequisites">#</a><a href="#" aria-label="Back to top">Prerequisites</a>
        
    
  </h2>
      

<p>Before using the script, ensure you have the following:</p>

<ul>
  <li>Python 3.7 or higher</li>
  <li>Required Python packages: <code class="language-plaintext highlighter-rouge">argparse</code>, <code class="language-plaintext highlighter-rouge">glob</code>, <code class="language-plaintext highlighter-rouge">json</code>, <code class="language-plaintext highlighter-rouge">pickle</code>, <code class="language-plaintext highlighter-rouge">sys</code>, <code class="language-plaintext highlighter-rouge">pathlib</code>, <code class="language-plaintext highlighter-rouge">typing</code>, <code class="language-plaintext highlighter-rouge">paperqa</code></li>
</ul>
  <h2 id="installation">
    
    
     <a href="#installation">#</a><a href="#" aria-label="Back to top">Installation</a>
        
    
  </h2>
      

<ol>
  <li>Clone the repository or download the script.</li>
  <li>Install the required Python packages using pip:</li>
</ol>

<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="pip3 install paperqa" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">pip3 install paperqa</code>
</section>

<p>Note: The other required packages (<code class="language-plaintext highlighter-rouge">argparse</code>, <code class="language-plaintext highlighter-rouge">glob</code>, <code class="language-plaintext highlighter-rouge">json</code>, <code class="language-plaintext highlighter-rouge">pickle</code>, <code class="language-plaintext highlighter-rouge">sys</code>, <code class="language-plaintext highlighter-rouge">pathlib</code>, <code class="language-plaintext highlighter-rouge">typing</code>) are part of the Python standard library and do not need to be installed separately.</p>
  <h2 id="usage">
    
    
     <a href="#usage">#</a><a href="#" aria-label="Back to top">Usage</a>
        
    
  </h2>
      
  <h3 id="command-line-arguments">
    
    
     <a href="#command-line-arguments">#</a><a href="#" aria-label="Back to top">Command Line Arguments</a>
        
    
  </h3>
      

<p>The script accepts several command-line arguments to customize its behavior. Below is a list of available arguments:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">--pdf_dir</code>: Directory containing PDF files</li>
  <li><code class="language-plaintext highlighter-rouge">--question</code>: Question to ask (use quotes for multi-word questions)</li>
  <li><code class="language-plaintext highlighter-rouge">--questions_file</code>: Path to a text file containing a list of questions, one per line</li>
  <li><code class="language-plaintext highlighter-rouge">--save_embeddings</code>: Path to save the Docs object with embeddings (default: <code class="language-plaintext highlighter-rouge">paperqa_embeddings.pkl</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">--load_embeddings</code>: Path to load a pre-saved Docs object with embeddings</li>
  <li><code class="language-plaintext highlighter-rouge">--save_answers</code>: Path to save the answers to a text file (default: <code class="language-plaintext highlighter-rouge">answers.txt</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">--save_data</code>: Path to save all data (answers, Docs object) to a pickle file (default: <code class="language-plaintext highlighter-rouge">paperqa_data.pkl</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">--save_embeddings_txt</code>: Path to save embeddings in a text file (for debugging/analysis)</li>
  <li><code class="language-plaintext highlighter-rouge">--llm</code>: OpenAI LLM model name (default: <code class="language-plaintext highlighter-rouge">gpt-4o</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">--embedding</code>: OpenAI embedding model name (default: <code class="language-plaintext highlighter-rouge">text-embedding-3-large</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">--summary_llm</code>: OpenAI LLM model name for summarization (defaults to same as <code class="language-plaintext highlighter-rouge">--llm</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">--k</code>: Number of top-k results to retrieve for each query (default: 50)</li>
  <li><code class="language-plaintext highlighter-rouge">--max_sources</code>: Maximum number of sources to use in the final answer (default: 50)</li>
  <li><code class="language-plaintext highlighter-rouge">--chunk_chars</code>: Number of characters per chunk when splitting documents (default: 3200)</li>
  <li><code class="language-plaintext highlighter-rouge">--overlap</code>: Number of overlapping characters between chunks (default: 1600)</li>
  <li><code class="language-plaintext highlighter-rouge">--json_summary</code>: Use JSON format for summarization (requires GPT-3.5-turbo or later)</li>
  <li><code class="language-plaintext highlighter-rouge">--detailed_citations</code>: Include full citations in the context</li>
  <li><code class="language-plaintext highlighter-rouge">--disable_vector_search</code>: Disable vector search and use all text chunks</li>
  <li><code class="language-plaintext highlighter-rouge">--key_filter</code>: Filter evidence by document keys based on question similarity</li>
  <li><code class="language-plaintext highlighter-rouge">--custom_prompt_file</code>: Path to a JSON file containing custom prompts</li>
  <li><code class="language-plaintext highlighter-rouge">--batch_size</code>: Batch size for processing documents (default: 10)</li>
  <li><code class="language-plaintext highlighter-rouge">--max_concurrent</code>: Maximum number of concurrent requests (default: 4)</li>
  <li><code class="language-plaintext highlighter-rouge">--strip_citations</code>: Strip citations from the generated answers</li>
  <li><code class="language-plaintext highlighter-rouge">--jit_texts_index</code>: Enable just-in-time text indexing</li>
  <li><code class="language-plaintext highlighter-rouge">--answer_length</code>: Specify the desired length of the answer</li>
</ul>
  <h3 id="favorites">
    
    
     <a href="#favorites">#</a><a href="#" aria-label="Back to top">Favorites</a>
        
    
  </h3>
      

<p><code class="language-plaintext highlighter-rouge">--question</code>
“Question to ask (use quotes for multi-word questions)”</p>

<p><code class="language-plaintext highlighter-rouge">--questions_file</code>
“Path to a text file containing a list of questions, one per line”</p>

<p><code class="language-plaintext highlighter-rouge">--load_embeddings</code>
“Path to load a pre-saved Docs object with embeddings”</p>

<p><code class="language-plaintext highlighter-rouge">--save_embeddings_txt</code>
“Path to save embeddings in a text file (for debugging/analysis)”</p>

<p><code class="language-plaintext highlighter-rouge">--detailed_citations</code>
“Include full citations in the context”</p>

<p><code class="language-plaintext highlighter-rouge">--custom_prompt_file</code>
“Path to a JSON file containing custom prompts”</p>

<p><code class="language-plaintext highlighter-rouge">--answer_length</code>
“Specify the desired length of the answer (e.g., ‘about 200 words’)”</p>
  <h3 id="examples">
    
    
     <a href="#examples">#</a><a href="#" aria-label="Back to top">Examples</a>
        
    
  </h3>
      

<ol>
  <li>Ask a single question:</li>
</ol>

<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="python script.py --pdf_dir /path/to/pdfs --question &quot;What is the impact of climate change on agriculture?&quot;" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">python script.py --pdf_dir /path/to/pdfs --question &quot;What is the impact of climate change on agriculture?&quot;</code>
</section>

<ol>
  <li>Ask multiple questions from a file:</li>
</ol>

<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="python script.py --pdf_dir /path/to/pdfs --questions_file questions.txt" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">python script.py --pdf_dir /path/to/pdfs --questions_file questions.txt</code>
</section>
  <h3 id="list-of-questions-template">
    
    
     <a href="#list-of-questions-template">#</a><a href="#" aria-label="Back to top">List of questions template:</a>
        
    
  </h3>
      

<section class="code-block-container" role="group" aria-label=" Code Block" data-filename="_code_block.txt" data-code="questions = [
    &quot;Q1?&quot;,
    &quot;Q2?&quot;,
    # ... add your 28 other questions here
]" data-download-link="" data-download-label="Download ">
  <code class="language-">questions = [
    &quot;Q1?&quot;,
    &quot;Q2?&quot;,
    # ... add your 28 other questions here
]</code>
</section>

<ol>
  <li>Load existing embeddings and ask a question:</li>
</ol>

<section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="python script.py --load_embeddings embeddings.pkl --question &quot;What are the latest advancements in AI?&quot;" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">python script.py --load_embeddings embeddings.pkl --question &quot;What are the latest advancements in AI?&quot;</code>
</section>
  <h2 id="detailed-explanation">
    
    
     <a href="#detailed-explanation">#</a><a href="#" aria-label="Back to top">Detailed Explanation</a>
        
    
  </h2>
      
  <h3 id="loading-custom-prompts">
    
    
     <a href="#loading-custom-prompts">#</a><a href="#" aria-label="Back to top">Loading Custom Prompts</a>
        
    
  </h3>
      

<p>If a custom prompt file is provided using <code class="language-plaintext highlighter-rouge">--custom_prompt_file</code>, the script loads the prompts from the specified JSON file. This allows users to customize the prompts used for querying the documents.</p>
  <h3 id="creating-docs-object">
    
    
     <a href="#creating-docs-object">#</a><a href="#" aria-label="Back to top">Creating Docs Object</a>
        
    
  </h3>
      

<p>The script creates a <code class="language-plaintext highlighter-rouge">Docs</code> object with the specified configurations, including the LLM model, embedding model, and other parameters. This object is used to manage the documents and perform queries.</p>
  <h3 id="loading-existing-embeddings">
    
    
     <a href="#loading-existing-embeddings">#</a><a href="#" aria-label="Back to top">Loading Existing Embeddings</a>
        
    
  </h3>
      

<p>If a pre-saved Docs object with embeddings is provided using <code class="language-plaintext highlighter-rouge">--load_embeddings</code>, the script loads the object from the specified file. This allows users to reuse previously generated embeddings.</p>
  <h3 id="adding-pdf-documents">
    
    
     <a href="#adding-pdf-documents">#</a><a href="#" aria-label="Back to top">Adding PDF Documents</a>
        
    
  </h3>
      

<p>The script adds PDF documents from the specified directory to the <code class="language-plaintext highlighter-rouge">Docs</code> object. It ensures that only new documents are added to avoid duplication.</p>
  <h3 id="getting-questions">
    
    
     <a href="#getting-questions">#</a><a href="#" aria-label="Back to top">Getting Questions</a>
        
    
  </h3>
      

<p>The script retrieves questions from the command line, a file, or standard input. It supports multiple sources for questions to provide flexibility in querying the documents.</p>
  <h3 id="answering-questions">
    
    
     <a href="#answering-questions">#</a><a href="#" aria-label="Back to top">Answering Questions</a>
        
    
  </h3>
      

<p>The script processes each question using the <code class="language-plaintext highlighter-rouge">Docs</code> object and generates answers. It supports various configurations for querying, including the number of top-k results, maximum sources, and answer length.</p>
  <h3 id="saving-data">
    
    
     <a href="#saving-data">#</a><a href="#" aria-label="Back to top">Saving Data</a>
        
    
  </h3>
      

<p>The script saves the generated embeddings, answers, and other data to the specified files. This allows users to persist the results and reuse them later.</p>
  <h2 id="error-handling">
    
    
     <a href="#error-handling">#</a><a href="#" aria-label="Back to top">Error Handling</a>
        
    
  </h2>
      

<p>The script includes error handling for various scenarios, such as file not found errors and JSON decoding errors. It prints appropriate error messages and exits gracefully in case of errors.</p>
  <h1 id="script">
    
    
     <a href="#script">#</a><a href="#" aria-label="Back to top">Script</a>
        
    
  </h1>
      

<section class="code-block-container" role="group" aria-label="Python Code Block" data-filename="python_code_block.py" data-code="from paperqa import Answer, Docs, PromptCollection, OpenAILLMModel, OpenAIEmbeddingModel
import argparse
import sys
import pickle
import json
import glob
from pathlib import Path
from typing import List, Dict

def main():
    parser = argparse.ArgumentParser(description=&quot;PaperQA CLI&quot;)
    parser.add_argument(
        &quot;--pdf_dir&quot;,
        type=str,
        default=&quot;/home/mario/gpt-researcher/BIBTEX&quot;,
        help=&quot;Directory containing PDF files&quot;,
    )
    parser.add_argument(
        &quot;--question&quot;, type=str, help=&quot;Question to ask (use quotes for multi-word questions)&quot;
    )
    parser.add_argument(
        &quot;--questions_file&quot;,
        type=str,
        help=&quot;Path to a text file containing a list of questions, one per line&quot;,
    )
    parser.add_argument(
        &quot;--save_embeddings&quot;,
        type=str,
        default=&quot;paperqa_embeddings.pkl&quot;,
        help=&quot;Path to save the Docs object with embeddings&quot;,
    )
    parser.add_argument(
        &quot;--load_embeddings&quot;,
        type=str,
        help=&quot;Path to load a pre-saved Docs object with embeddings&quot;,
    )
    parser.add_argument(
        &quot;--save_answers&quot;,
        type=str,
        default=&quot;answers.txt&quot;,
        help=&quot;Path to save the answers to a text file&quot;,
    )
    parser.add_argument(
        &quot;--save_data&quot;,
        type=str,
        default=&quot;paperqa_data.pkl&quot;,
        help=&quot;Path to save all data (answers, Docs object) to a pickle file&quot;,
    )
    parser.add_argument(
        &quot;--save_embeddings_txt&quot;,
        type=str,
        help=&quot;Path to save embeddings in a text file (for debugging/analysis)&quot;,
    )
    parser.add_argument(
        &quot;--llm&quot;,
        type=str,
        default=&quot;gpt-4o&quot;,
        help=&quot;OpenAI LLM model name (e.g., &#39;gpt-4o&#39;, &#39;gpt-4o&#39;)&quot;,
    )
    parser.add_argument(
        &quot;--embedding&quot;,
        type=str,
        default=&quot;text-embedding-3-large&quot;,
        help=&quot;OpenAI embedding model name (e.g., &#39;text-embedding-3-large&#39;)&quot;,
    )
    parser.add_argument(
        &quot;--summary_llm&quot;,
        type=str,
        help=&quot;OpenAI LLM model name for summarization (defaults to same as --llm)&quot;,
    )
    parser.add_argument(
        &quot;--k&quot;,
        type=int,
        default=50,
        help=&quot;Number of top-k results to retrieve for each query&quot;,
    )
    parser.add_argument(
        &quot;--max_sources&quot;,
        type=int,
        default=50,
        help=&quot;Maximum number of sources to use in the final answer&quot;,
    )
    parser.add_argument(
        &quot;--chunk_chars&quot;,
        type=int,
        default=2400,
        help=&quot;Number of characters per chunk when splitting documents&quot;,
    )
    parser.add_argument(
        &quot;--overlap&quot;,
        type=int,
        default=1200,
        help=&quot;Number of overlapping characters between chunks&quot;,
    )
    parser.add_argument(
        &quot;--json_summary&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Use JSON format for summarization (requires GPT-3.5-turbo or later)&quot;,
    )
    parser.add_argument(
        &quot;--detailed_citations&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Include full citations in the context&quot;,
    )
    parser.add_argument(
        &quot;--disable_vector_search&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Disable vector search and use all text chunks&quot;,
    )
    parser.add_argument(
        &quot;--key_filter&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Filter evidence by document keys based on question similarity&quot;,
    )
    parser.add_argument(
        &quot;--custom_prompt_file&quot;,
        type=str,
        help=&quot;Path to a JSON file containing custom prompts&quot;,
    )
    parser.add_argument(
        &quot;--batch_size&quot;,
        type=int,
        default=10,  # Increased batch size for efficiency
        help=&quot;Batch size for processing documents (adjust for performance)&quot;,
    )
    parser.add_argument(
        &quot;--max_concurrent&quot;,
        type=int,
        default=4,
        help=&quot;Maximum number of concurrent requests (adjust for performance)&quot;,
    )
    parser.add_argument(
        &quot;--strip_citations&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Strip citations from the generated answers&quot;,
    )
    parser.add_argument(
        &quot;--jit_texts_index&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Enable just-in-time text indexing&quot;,
    )
    parser.add_argument(
        &quot;--answer_length&quot;,
        type=str,
        default=&quot;about 100 words&quot;,
        help=&quot;Specify the desired length of the answer (e.g., &#39;about 200 words&#39;)&quot;,
    )
    args = parser.parse_args()

    # Load custom prompts from JSON file if provided
    custom_prompts: Dict[str, str] = {}
    if args.custom_prompt_file:
        try:
            with open(args.custom_prompt_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
                custom_prompts = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError) as e:
            print(f&quot;Error loading custom prompts: {e}&quot;, file=sys.stderr)
            sys.exit(1)
    # Initialize PromptCollection without arguments
    prompts = PromptCollection()

    # If custom prompts are needed, set them directly
    if custom_prompts:
        for key, value in custom_prompts.items():
            setattr(prompts, key, value)

    llm_model = OpenAILLMModel(config={&quot;model&quot;: args.llm, &quot;temperature&quot;: 0.1})
    embedding_model = OpenAIEmbeddingModel(config={&quot;model&quot;: args.embedding})
    summary_llm_model = (
        OpenAILLMModel(config={&quot;model&quot;: args.summary_llm, &quot;temperature&quot;: 0.1})
        if args.summary_llm
        else llm_model
    )

    # Create Docs object with supported parameters
    docs = Docs(
        llm=args.llm,  # Pass the model name as a string
        embedding=args.embedding,  # Pass the model name as a string
        summary_llm=args.summary_llm if args.summary_llm else args.llm,  # Pass the model name as a string
        prompts=prompts,  # Use the initialized and updated PromptCollection
        max_concurrent=args.max_concurrent,
        jit_texts_index=args.jit_texts_index,
    )

    # Load existing embeddings if provided
    if args.load_embeddings:
        try:
            with open(args.load_embeddings, &quot;rb&quot;) as f:
                docs = pickle.load(f)
            docs.set_client()  # Required after loading from pickle
        except FileNotFoundError as e:
            print(f&quot;Error loading embeddings: {e}&quot;, file=sys.stderr)
            sys.exit(1)

    # Add PDF documents
    pdf_dir = Path(args.pdf_dir)
    pdf_files = glob.glob(str(pdf_dir / &quot;*.pdf&quot;))
    for pdf_file in pdf_files:
        if pdf_file not in [doc.dockey for doc in docs.docs.values()]:
            docs.add(pdf_file)

    # Get questions from command line, file, or standard input
    questions: List[str] = []
    if args.question:
        questions.append(args.question)
    if args.questions_file:
        try:
            with open(args.questions_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
                questions.extend([line.strip() for line in f])
        except FileNotFoundError as e:
            print(f&quot;Error reading questions file: {e}&quot;, file=sys.stderr)
            sys.exit(1)
    if not questions:
        questions = [line.strip() for line in sys.stdin]

    # Get answers for each question
    answers = []
    for question in questions:
        answer = docs.query(question, k=args.k, max_sources=args.max_sources)
        print(f&quot;Answer object: {answer}&quot;)  # Debug print to inspect the Answer object
        answers.append(str(answer))  # Use str(answer) to store the entire Answer object

    # Save answers to file
    with open(args.save_answers, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        for answer in answers:
            f.write(answer + &quot;\n&quot;)

    # Save embeddings to file
    if args.save_embeddings:
        with open(args.save_embeddings, &quot;wb&quot;) as f:
            pickle.dump(docs, f)

    # Save all data to file
    if args.save_data:
        with open(args.save_data, &quot;wb&quot;) as f:
            pickle.dump({&quot;docs&quot;: docs, &quot;answers&quot;: answers}, f)

    # Save embeddings to text file (for debugging/analysis)
    if args.save_embeddings_txt:
        with open(args.save_embeddings_txt, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            for doc in docs.docs.values():
                f.write(f&quot;{doc.dockey}\t{doc.embedding}\n&quot;)

if __name__ == &quot;__main__&quot;:
    main()" data-download-link="" data-download-label="Download Python">
  <code class="language-python">from paperqa import Answer, Docs, PromptCollection, OpenAILLMModel, OpenAIEmbeddingModel
import argparse
import sys
import pickle
import json
import glob
from pathlib import Path
from typing import List, Dict

def main():
    parser = argparse.ArgumentParser(description=&quot;PaperQA CLI&quot;)
    parser.add_argument(
        &quot;--pdf_dir&quot;,
        type=str,
        default=&quot;/home/mario/gpt-researcher/BIBTEX&quot;,
        help=&quot;Directory containing PDF files&quot;,
    )
    parser.add_argument(
        &quot;--question&quot;, type=str, help=&quot;Question to ask (use quotes for multi-word questions)&quot;
    )
    parser.add_argument(
        &quot;--questions_file&quot;,
        type=str,
        help=&quot;Path to a text file containing a list of questions, one per line&quot;,
    )
    parser.add_argument(
        &quot;--save_embeddings&quot;,
        type=str,
        default=&quot;paperqa_embeddings.pkl&quot;,
        help=&quot;Path to save the Docs object with embeddings&quot;,
    )
    parser.add_argument(
        &quot;--load_embeddings&quot;,
        type=str,
        help=&quot;Path to load a pre-saved Docs object with embeddings&quot;,
    )
    parser.add_argument(
        &quot;--save_answers&quot;,
        type=str,
        default=&quot;answers.txt&quot;,
        help=&quot;Path to save the answers to a text file&quot;,
    )
    parser.add_argument(
        &quot;--save_data&quot;,
        type=str,
        default=&quot;paperqa_data.pkl&quot;,
        help=&quot;Path to save all data (answers, Docs object) to a pickle file&quot;,
    )
    parser.add_argument(
        &quot;--save_embeddings_txt&quot;,
        type=str,
        help=&quot;Path to save embeddings in a text file (for debugging/analysis)&quot;,
    )
    parser.add_argument(
        &quot;--llm&quot;,
        type=str,
        default=&quot;gpt-4o&quot;,
        help=&quot;OpenAI LLM model name (e.g., &#39;gpt-4o&#39;, &#39;gpt-4o&#39;)&quot;,
    )
    parser.add_argument(
        &quot;--embedding&quot;,
        type=str,
        default=&quot;text-embedding-3-large&quot;,
        help=&quot;OpenAI embedding model name (e.g., &#39;text-embedding-3-large&#39;)&quot;,
    )
    parser.add_argument(
        &quot;--summary_llm&quot;,
        type=str,
        help=&quot;OpenAI LLM model name for summarization (defaults to same as --llm)&quot;,
    )
    parser.add_argument(
        &quot;--k&quot;,
        type=int,
        default=50,
        help=&quot;Number of top-k results to retrieve for each query&quot;,
    )
    parser.add_argument(
        &quot;--max_sources&quot;,
        type=int,
        default=50,
        help=&quot;Maximum number of sources to use in the final answer&quot;,
    )
    parser.add_argument(
        &quot;--chunk_chars&quot;,
        type=int,
        default=2400,
        help=&quot;Number of characters per chunk when splitting documents&quot;,
    )
    parser.add_argument(
        &quot;--overlap&quot;,
        type=int,
        default=1200,
        help=&quot;Number of overlapping characters between chunks&quot;,
    )
    parser.add_argument(
        &quot;--json_summary&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Use JSON format for summarization (requires GPT-3.5-turbo or later)&quot;,
    )
    parser.add_argument(
        &quot;--detailed_citations&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Include full citations in the context&quot;,
    )
    parser.add_argument(
        &quot;--disable_vector_search&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Disable vector search and use all text chunks&quot;,
    )
    parser.add_argument(
        &quot;--key_filter&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Filter evidence by document keys based on question similarity&quot;,
    )
    parser.add_argument(
        &quot;--custom_prompt_file&quot;,
        type=str,
        help=&quot;Path to a JSON file containing custom prompts&quot;,
    )
    parser.add_argument(
        &quot;--batch_size&quot;,
        type=int,
        default=10,  # Increased batch size for efficiency
        help=&quot;Batch size for processing documents (adjust for performance)&quot;,
    )
    parser.add_argument(
        &quot;--max_concurrent&quot;,
        type=int,
        default=4,
        help=&quot;Maximum number of concurrent requests (adjust for performance)&quot;,
    )
    parser.add_argument(
        &quot;--strip_citations&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Strip citations from the generated answers&quot;,
    )
    parser.add_argument(
        &quot;--jit_texts_index&quot;,
        action=&quot;store_true&quot;,
        help=&quot;Enable just-in-time text indexing&quot;,
    )
    parser.add_argument(
        &quot;--answer_length&quot;,
        type=str,
        default=&quot;about 100 words&quot;,
        help=&quot;Specify the desired length of the answer (e.g., &#39;about 200 words&#39;)&quot;,
    )
    args = parser.parse_args()

    # Load custom prompts from JSON file if provided
    custom_prompts: Dict[str, str] = {}
    if args.custom_prompt_file:
        try:
            with open(args.custom_prompt_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
                custom_prompts = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError) as e:
            print(f&quot;Error loading custom prompts: {e}&quot;, file=sys.stderr)
            sys.exit(1)
    # Initialize PromptCollection without arguments
    prompts = PromptCollection()

    # If custom prompts are needed, set them directly
    if custom_prompts:
        for key, value in custom_prompts.items():
            setattr(prompts, key, value)

    llm_model = OpenAILLMModel(config={&quot;model&quot;: args.llm, &quot;temperature&quot;: 0.1})
    embedding_model = OpenAIEmbeddingModel(config={&quot;model&quot;: args.embedding})
    summary_llm_model = (
        OpenAILLMModel(config={&quot;model&quot;: args.summary_llm, &quot;temperature&quot;: 0.1})
        if args.summary_llm
        else llm_model
    )

    # Create Docs object with supported parameters
    docs = Docs(
        llm=args.llm,  # Pass the model name as a string
        embedding=args.embedding,  # Pass the model name as a string
        summary_llm=args.summary_llm if args.summary_llm else args.llm,  # Pass the model name as a string
        prompts=prompts,  # Use the initialized and updated PromptCollection
        max_concurrent=args.max_concurrent,
        jit_texts_index=args.jit_texts_index,
    )

    # Load existing embeddings if provided
    if args.load_embeddings:
        try:
            with open(args.load_embeddings, &quot;rb&quot;) as f:
                docs = pickle.load(f)
            docs.set_client()  # Required after loading from pickle
        except FileNotFoundError as e:
            print(f&quot;Error loading embeddings: {e}&quot;, file=sys.stderr)
            sys.exit(1)

    # Add PDF documents
    pdf_dir = Path(args.pdf_dir)
    pdf_files = glob.glob(str(pdf_dir / &quot;*.pdf&quot;))
    for pdf_file in pdf_files:
        if pdf_file not in [doc.dockey for doc in docs.docs.values()]:
            docs.add(pdf_file)

    # Get questions from command line, file, or standard input
    questions: List[str] = []
    if args.question:
        questions.append(args.question)
    if args.questions_file:
        try:
            with open(args.questions_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
                questions.extend([line.strip() for line in f])
        except FileNotFoundError as e:
            print(f&quot;Error reading questions file: {e}&quot;, file=sys.stderr)
            sys.exit(1)
    if not questions:
        questions = [line.strip() for line in sys.stdin]

    # Get answers for each question
    answers = []
    for question in questions:
        answer = docs.query(question, k=args.k, max_sources=args.max_sources)
        print(f&quot;Answer object: {answer}&quot;)  # Debug print to inspect the Answer object
        answers.append(str(answer))  # Use str(answer) to store the entire Answer object

    # Save answers to file
    with open(args.save_answers, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
        for answer in answers:
            f.write(answer + &quot;\n&quot;)

    # Save embeddings to file
    if args.save_embeddings:
        with open(args.save_embeddings, &quot;wb&quot;) as f:
            pickle.dump(docs, f)

    # Save all data to file
    if args.save_data:
        with open(args.save_data, &quot;wb&quot;) as f:
            pickle.dump({&quot;docs&quot;: docs, &quot;answers&quot;: answers}, f)

    # Save embeddings to text file (for debugging/analysis)
    if args.save_embeddings_txt:
        with open(args.save_embeddings_txt, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
            for doc in docs.docs.values():
                f.write(f&quot;{doc.dockey}\t{doc.embedding}\n&quot;)

if __name__ == &quot;__main__&quot;:
    main()</code>
</section>
  <h1 id="2add">
    
    
     <a href="#2add">#</a><a href="#" aria-label="Back to top">2add</a>
        
    
  </h1>
      <hr />
<p>Automating a list of prompts to be queried:</p>
<ul>
  <li>You can create a list of prompts and iterate over them, similar to the example in point 1.</li>
  <li>Here’s an example:
&lt;section class=”code-block-container” role=”group” aria-label=”Python Code Block” data-filename=”python_code_block.py” data-code=”prompts = [
    "Prompt 1",
    "Prompt 2",
    # …
    "Prompt N"
]</li>
</ul>

<p>for prompt in prompts:
    answer = loaded_docs.query(prompt, k=50)
    print(f"Prompt: {prompt}")
    print(f"Answer: {answer.formatted_answer}\n")” data-download-link data-download-label=”Download Python”&gt;
  <code class="language-python">prompts = [
    &quot;Prompt 1&quot;,
    &quot;Prompt 2&quot;,
    # ...
    &quot;Prompt N&quot;
]</code></p>

<p>for prompt in prompts:
    answer = loaded_docs.query(prompt, k=50)
    print(f"Prompt: {prompt}")
    print(f"Answer: {answer.formatted_answer}\n")&lt;/code&gt;
&lt;/section&gt;</p><hr />
<p>Piping from command line stdout into the Python script:</p>
<ul>
  <li>You can use the <code class="language-plaintext highlighter-rouge">sys</code> module to read input from the command line.</li>
  <li>Here’s an example:
&lt;section class=”code-block-container” role=”group” aria-label=”Python Code Block” data-filename=”python_code_block.py” data-code=”import sys</li>
</ul>

<p>question = sys.stdin.read().strip()
answer = loaded_docs.query(question, k=50)
print(answer.formatted_answer)” data-download-link data-download-label=”Download Python”&gt;
  <code class="language-python">import sys</code></p>

<p>question = sys.stdin.read().strip()
answer = loaded_docs.query(question, k=50)
print(answer.formatted_answer)&lt;/code&gt;
&lt;/section&gt;</p>

<ul>
  <li>You can then pipe the question from the command line:
    <section class="code-block-container" role="group" aria-label="Bash Code Block" data-filename="bash_code_block.sh" data-code="echo &quot;What is the meaning of life?&quot; | python3 researcher.py" data-download-link="" data-download-label="Download Bash">
  <code class="language-bash">echo &quot;What is the meaning of life?&quot; | python3 researcher.py</code>
</section>
  </li>
</ul>
<p>&lt;/section&gt;</p><hr />
<p>Using the response of a request/query as context for subsequent prompts:</p>
<ul>
  <li>You can use the <code class="language-plaintext highlighter-rouge">answer.context</code> attribute to access the context used for generating the answer.</li>
  <li>Here’s an example:
    <section class="code-block-container" role="group" aria-label="Python Code Block" data-filename="python_code_block.py" data-code="previous_context = &quot;&quot;
for prompt in prompts:
    answer = loaded_docs.query(prompt, k=50, context=previous_context)
    previous_context = answer.context
    print(f&quot;Prompt: {prompt}&quot;)
    print(f&quot;Answer: {answer.formatted_answer}\n&quot;)" data-download-link="" data-download-label="Download Python">
  <code class="language-python">previous_context = &quot;&quot;
for prompt in prompts:
    answer = loaded_docs.query(prompt, k=50, context=previous_context)
    previous_context = answer.context
    print(f&quot;Prompt: {prompt}&quot;)
    print(f&quot;Answer: {answer.formatted_answer}\n&quot;)</code>
</section>
  </li>
</ul>
<p>&lt;/section&gt;</p><hr />

<section class="code-block-container" role="group" aria-label=" Code Block" data-filename="_code_block.txt" data-code="Implement adversarial prompting. #131
This adds a method adversarial_query. Adversarial queries first generate an answer, then asks the LLM to find problems with the answer, then finally generates the final response so that it addresses th
 1 commit
 2 files changed
 1 contributor
Commits on Jun 2, 2023
Add method for adversarial prompting. Also work around a problem with… 

davidbrodrick committed on Jun 2, 2023
 Showing  with 114 additions and 4 deletions.
  87 changes: 83 additions &amp; 4 deletions87  
paperqa/docs.py
Original file line number	Diff line number	Diff line change
@@ -2,6 +2,7 @@
import os
import re
import sys
import copy
from datetime import datetime
from functools import reduce
from pathlib import Path
@@ -18,12 +19,15 @@
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.llms.base import LLM
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate

from .paths import CACHE_PATH
from .qaprompts import (
    citation_prompt,
    make_chain,
    qa_prompt,
    adversarial_prompt,
    revision_prompt,
    search_prompt,
    select_paper_prompt,
    summary_prompt,
@@ -257,7 +261,10 @@ async def adoc_match(
        papers = [f&quot;{d.metadata[&#39;key&#39;]}: {d.page_content}&quot; for d in docs]
        result = await chain.arun(
            question=query, papers=&quot;\n&quot;.join(papers), callbacks=callbacks
        )
        )        
        if result==&quot;None.&quot;:
            #Something in that call stack returns &quot;None.&quot; as a string!
            result=[]
        return result

    def doc_match(
@@ -378,9 +385,10 @@ async def aget_evidence(
            docs = self._faiss_index.similarity_search(
                answer.question, k=_k, fetch_k=5 * _k
            )

        # ok now filter
        if key_filter is not None:
            docs = [doc for doc in docs if doc.metadata[&quot;dockey&quot;] in key_filter][:k]
        if (key_filter is not None):
            docs = [doc for doc in docs if doc.metadata[&quot;key&quot;] in key_filter][:k]

        async def process(doc):
            if doc.metadata[&quot;dockey&quot;] in self._deleted_keys:
@@ -468,6 +476,74 @@ def generate_search_query(self, query: str) -&gt; List[str]:
        queries = [re.sub(r&quot;^\d+\.\s*&quot;, &quot;&quot;, q) for q in queries]
        return queries

    def adversarial_query(
        self,
        query: str,
        k: int = 10,
        max_sources: int = 5,
        length_prompt: str = &quot;about 100 words&quot;,
        marginal_relevance: bool = True,
        answer: Optional[Answer] = None,
        key_filter: Optional[bool] = None,
        get_callbacks: Callable[[str], AsyncCallbackHandler] = lambda x: [],
        recontextualise: bool = False
    ) -&gt; List[Answer]:
        #Get an answer to the question
        orig_answer=self.query(
                query,
                k=k,
                max_sources=max_sources,
                length_prompt=length_prompt,
                marginal_relevance=marginal_relevance,
                answer=answer,
                key_filter=key_filter,
                get_callbacks=get_callbacks,
                prompt_template=qa_prompt)
        if &quot;I cannot answer this question due to insufficient information.&quot; in orig_answer.answer:
            #We can&#39;t do an adversarial challenge if there was no answer
            return [orig_answer]

        #Ask the LLM to critique the original answer
        adversarial_query=&quot;Original Question: %s\n\nAnswer to be reviewed: %s&quot;%(query,orig_answer.answer)
        if recontextualise:
            #We will search for new context strings based on the original answer
            critique=None
        else:            
            #Use the original context strings
            critique=copy.copy(orig_answer)
            critique.question=query
        critique=self.query(
                adversarial_query,
                k=k,
                max_sources=max_sources,
                length_prompt=&quot;less than 500 words&quot;,
                marginal_relevance=marginal_relevance,
                answer=critique,
                key_filter=key_filter,
                get_callbacks=get_callbacks,
                prompt_template=adversarial_prompt)

        #Generate a new answer which addresses the criticism
        revision_query=&quot;Original Question: %s\n\nYour original answer: %s\n\nFeedback from the reviewer: %s&quot;%(query,orig_answer.answer,critique.answer)
        if recontextualise:
            final_answer=None
        else:
            final_answer=copy.copy(critique)
            critique.question=query
        final_answer=self.query(
                revision_query,
                k=k,
                max_sources=max_sources,
                length_prompt=length_prompt,
                marginal_relevance=marginal_relevance,
                answer=final_answer,
                key_filter=key_filter,
                get_callbacks=get_callbacks,
                prompt_template=revision_prompt)
        #Return all three parts of the process
        return [orig_answer, critique, final_answer]


    def query(
        self,
        query: str,
@@ -478,6 +554,7 @@ def query(
        answer: Optional[Answer] = None,
        key_filter: Optional[bool] = None,
        get_callbacks: Callable[[str], AsyncCallbackHandler] = lambda x: [],
        prompt_template: PromptTemplate = qa_prompt
    ) -&gt; Answer:
        # special case for jupyter notebooks
        if &quot;get_ipython&quot; in globals() or &quot;google.colab&quot; in sys.modules:
@@ -499,6 +576,7 @@ def query(
                answer=answer,
                key_filter=key_filter,
                get_callbacks=get_callbacks,
                prompt_template=prompt_template
            )
        )

@@ -512,6 +590,7 @@ async def aquery(
        answer: Optional[Answer] = None,
        key_filter: Optional[bool] = None,
        get_callbacks: Callable[[str], AsyncCallbackHandler] = lambda x: [],
        prompt_template: PromptTemplate = qa_prompt
    ) -&gt; Answer:
        if k &lt; max_sources:
            raise ValueError(&quot;k should be greater than max_sources&quot;)
@@ -542,7 +621,7 @@ async def aquery(
        else:
            cb = OpenAICallbackHandler()
            callbacks = [cb] + get_callbacks(&quot;answer&quot;)
            qa_chain = make_chain(qa_prompt, self.llm)
            qa_chain = make_chain(prompt_template, self.llm)
            answer_text = await qa_chain.arun(
                question=query,
                context_str=context_str,
  31 changes: 31 additions &amp; 0 deletions31  
paperqa/qaprompts.py
Original file line number	Diff line number	Diff line change
@@ -25,6 +25,7 @@
    &quot;Relevant Information Summary:&quot;,
)


qa_prompt = prompts.PromptTemplate(
    input_variables=[&quot;question&quot;, &quot;context_str&quot;, &quot;length&quot;],
    template=&quot;Write an answer ({length}) &quot;
@@ -42,6 +43,36 @@
)


adversarial_prompt = prompts.PromptTemplate(
    input_variables=[&quot;question&quot;, &quot;context_str&quot;],
    template=&quot;You are an adversarial and critical scientific reviewer. &quot;
    &quot;Your task is to find deficiencies and shortcomings in the following answer to the original question. &quot;
    &quot;Please be specific about the shortcomings of the answer and offer suggestions which would make the answer more complete and accurate.&quot;
    &quot;For each sentence in your critique, indicate which sources most support it &quot;
    &quot;via valid citation markers at the end of sentences, like (Example2012).\n&quot;
    &quot;{context_str}\n&quot;
    &quot;{question}\n&quot;
    &quot;Your Critique: &quot;,
)


revision_prompt = prompts.PromptTemplate(
    input_variables=[&quot;question&quot;, &quot;context_str&quot;],
    template=&quot;For each sentence in your answer, indicate which sources most support it &quot;
    &quot;via valid citation markers at the end of sentences, like (Example2012).\n&quot;
    &quot;You are a scientist and use a scholarly tone. &quot;
    &quot;You want to rewrite and improve your original answer to the original question to &quot;
    &quot;to include better grammatical structure and logical reasoning. &quot;
    &quot;You have also received critical feedback provided by a reviewer. Please address the &quot;
    &quot;feedback from the reviewer so that your new answer is more comprehensive. &quot;
    &quot;You can use dot points and paragraphs in your response where appropriate. &quot;
    #&quot;You may conclude with one or two opinionated sentences to summarise your answer. &quot;
    &quot;{context_str}\n&quot;
    &quot;{question}\n&quot;
    &quot;Your Improved Answer: &quot;,
)


search_prompt = prompts.PromptTemplate(
    input_variables=[&quot;question&quot;],
    template=&quot;We want to answer the following question: {question} \n&quot;" data-download-link="" data-download-label="Download ">
  <code class="language-">Implement adversarial prompting. #131
This adds a method adversarial_query. Adversarial queries first generate an answer, then asks the LLM to find problems with the answer, then finally generates the final response so that it addresses th
 1 commit
 2 files changed
 1 contributor
Commits on Jun 2, 2023
Add method for adversarial prompting. Also work around a problem with… 

davidbrodrick committed on Jun 2, 2023
 Showing  with 114 additions and 4 deletions.
  87 changes: 83 additions &amp; 4 deletions87  
paperqa/docs.py
Original file line number	Diff line number	Diff line change
@@ -2,6 +2,7 @@
import os
import re
import sys
import copy
from datetime import datetime
from functools import reduce
from pathlib import Path
@@ -18,12 +19,15 @@
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.llms.base import LLM
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate

from .paths import CACHE_PATH
from .qaprompts import (
    citation_prompt,
    make_chain,
    qa_prompt,
    adversarial_prompt,
    revision_prompt,
    search_prompt,
    select_paper_prompt,
    summary_prompt,
@@ -257,7 +261,10 @@ async def adoc_match(
        papers = [f&quot;{d.metadata[&#39;key&#39;]}: {d.page_content}&quot; for d in docs]
        result = await chain.arun(
            question=query, papers=&quot;\n&quot;.join(papers), callbacks=callbacks
        )
        )        
        if result==&quot;None.&quot;:
            #Something in that call stack returns &quot;None.&quot; as a string!
            result=[]
        return result

    def doc_match(
@@ -378,9 +385,10 @@ async def aget_evidence(
            docs = self._faiss_index.similarity_search(
                answer.question, k=_k, fetch_k=5 * _k
            )

        # ok now filter
        if key_filter is not None:
            docs = [doc for doc in docs if doc.metadata[&quot;dockey&quot;] in key_filter][:k]
        if (key_filter is not None):
            docs = [doc for doc in docs if doc.metadata[&quot;key&quot;] in key_filter][:k]

        async def process(doc):
            if doc.metadata[&quot;dockey&quot;] in self._deleted_keys:
@@ -468,6 +476,74 @@ def generate_search_query(self, query: str) -&gt; List[str]:
        queries = [re.sub(r&quot;^\d+\.\s*&quot;, &quot;&quot;, q) for q in queries]
        return queries

    def adversarial_query(
        self,
        query: str,
        k: int = 10,
        max_sources: int = 5,
        length_prompt: str = &quot;about 100 words&quot;,
        marginal_relevance: bool = True,
        answer: Optional[Answer] = None,
        key_filter: Optional[bool] = None,
        get_callbacks: Callable[[str], AsyncCallbackHandler] = lambda x: [],
        recontextualise: bool = False
    ) -&gt; List[Answer]:
        #Get an answer to the question
        orig_answer=self.query(
                query,
                k=k,
                max_sources=max_sources,
                length_prompt=length_prompt,
                marginal_relevance=marginal_relevance,
                answer=answer,
                key_filter=key_filter,
                get_callbacks=get_callbacks,
                prompt_template=qa_prompt)
        if &quot;I cannot answer this question due to insufficient information.&quot; in orig_answer.answer:
            #We can&#39;t do an adversarial challenge if there was no answer
            return [orig_answer]

        #Ask the LLM to critique the original answer
        adversarial_query=&quot;Original Question: %s\n\nAnswer to be reviewed: %s&quot;%(query,orig_answer.answer)
        if recontextualise:
            #We will search for new context strings based on the original answer
            critique=None
        else:            
            #Use the original context strings
            critique=copy.copy(orig_answer)
            critique.question=query
        critique=self.query(
                adversarial_query,
                k=k,
                max_sources=max_sources,
                length_prompt=&quot;less than 500 words&quot;,
                marginal_relevance=marginal_relevance,
                answer=critique,
                key_filter=key_filter,
                get_callbacks=get_callbacks,
                prompt_template=adversarial_prompt)

        #Generate a new answer which addresses the criticism
        revision_query=&quot;Original Question: %s\n\nYour original answer: %s\n\nFeedback from the reviewer: %s&quot;%(query,orig_answer.answer,critique.answer)
        if recontextualise:
            final_answer=None
        else:
            final_answer=copy.copy(critique)
            critique.question=query
        final_answer=self.query(
                revision_query,
                k=k,
                max_sources=max_sources,
                length_prompt=length_prompt,
                marginal_relevance=marginal_relevance,
                answer=final_answer,
                key_filter=key_filter,
                get_callbacks=get_callbacks,
                prompt_template=revision_prompt)
        #Return all three parts of the process
        return [orig_answer, critique, final_answer]


    def query(
        self,
        query: str,
@@ -478,6 +554,7 @@ def query(
        answer: Optional[Answer] = None,
        key_filter: Optional[bool] = None,
        get_callbacks: Callable[[str], AsyncCallbackHandler] = lambda x: [],
        prompt_template: PromptTemplate = qa_prompt
    ) -&gt; Answer:
        # special case for jupyter notebooks
        if &quot;get_ipython&quot; in globals() or &quot;google.colab&quot; in sys.modules:
@@ -499,6 +576,7 @@ def query(
                answer=answer,
                key_filter=key_filter,
                get_callbacks=get_callbacks,
                prompt_template=prompt_template
            )
        )

@@ -512,6 +590,7 @@ async def aquery(
        answer: Optional[Answer] = None,
        key_filter: Optional[bool] = None,
        get_callbacks: Callable[[str], AsyncCallbackHandler] = lambda x: [],
        prompt_template: PromptTemplate = qa_prompt
    ) -&gt; Answer:
        if k &lt; max_sources:
            raise ValueError(&quot;k should be greater than max_sources&quot;)
@@ -542,7 +621,7 @@ async def aquery(
        else:
            cb = OpenAICallbackHandler()
            callbacks = [cb] + get_callbacks(&quot;answer&quot;)
            qa_chain = make_chain(qa_prompt, self.llm)
            qa_chain = make_chain(prompt_template, self.llm)
            answer_text = await qa_chain.arun(
                question=query,
                context_str=context_str,
  31 changes: 31 additions &amp; 0 deletions31  
paperqa/qaprompts.py
Original file line number	Diff line number	Diff line change
@@ -25,6 +25,7 @@
    &quot;Relevant Information Summary:&quot;,
)


qa_prompt = prompts.PromptTemplate(
    input_variables=[&quot;question&quot;, &quot;context_str&quot;, &quot;length&quot;],
    template=&quot;Write an answer ({length}) &quot;
@@ -42,6 +43,36 @@
)


adversarial_prompt = prompts.PromptTemplate(
    input_variables=[&quot;question&quot;, &quot;context_str&quot;],
    template=&quot;You are an adversarial and critical scientific reviewer. &quot;
    &quot;Your task is to find deficiencies and shortcomings in the following answer to the original question. &quot;
    &quot;Please be specific about the shortcomings of the answer and offer suggestions which would make the answer more complete and accurate.&quot;
    &quot;For each sentence in your critique, indicate which sources most support it &quot;
    &quot;via valid citation markers at the end of sentences, like (Example2012).\n&quot;
    &quot;{context_str}\n&quot;
    &quot;{question}\n&quot;
    &quot;Your Critique: &quot;,
)


revision_prompt = prompts.PromptTemplate(
    input_variables=[&quot;question&quot;, &quot;context_str&quot;],
    template=&quot;For each sentence in your answer, indicate which sources most support it &quot;
    &quot;via valid citation markers at the end of sentences, like (Example2012).\n&quot;
    &quot;You are a scientist and use a scholarly tone. &quot;
    &quot;You want to rewrite and improve your original answer to the original question to &quot;
    &quot;to include better grammatical structure and logical reasoning. &quot;
    &quot;You have also received critical feedback provided by a reviewer. Please address the &quot;
    &quot;feedback from the reviewer so that your new answer is more comprehensive. &quot;
    &quot;You can use dot points and paragraphs in your response where appropriate. &quot;
    #&quot;You may conclude with one or two opinionated sentences to summarise your answer. &quot;
    &quot;{context_str}\n&quot;
    &quot;{question}\n&quot;
    &quot;Your Improved Answer: &quot;,
)


search_prompt = prompts.PromptTemplate(
    input_variables=[&quot;question&quot;],
    template=&quot;We want to answer the following question: {question} \n&quot;</code>
</section><hr />

        </div>
        
          URL: https://ib.bsb.br/clipaper-qa
        
      </article>
      <nav class="post-navigation-combined" aria-label="Post navigation">
        <!-- Chronological Navigation (always visible at 40vh) -->
        
          <div class="nav-arrow chronological prev">
            <a href="/bulk-gpt/" title="bulkGPT: multithreading for OpenAI's output limits" rel="prev">
              <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ+lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="Previous article" width="45" height="45" loading="lazy">
            </a>
          </div>
        
        
          <div class="nav-arrow chronological next">
            <a href="/gingko-toc-outliner/" title="Gingko JSON ToC outliner" rel="next">
              <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAwdJREFUaEPtl01rG1cUht9z7p070ijKWLVS3I9YULtNMBRn2UIXxZtuvCjedNWfoKVXBvsHeOUfoIUx5AeY0pVXXSR052UxBJSi0kDiRplII83HvadI9CMtOFjCmBjubC7cc8+Zc555eYdLuIEP3cCe4Zu+rq/mSXvSbyHg5eHl4eVxXRrwpD3p+Qh4n56P2+xZnvTszC7MCAAUF0XfOdJa62+IaCeO40GSJA/zPP8RwB9vDjBtWmv9JRE5ImpZawdVpbo58CkAIyJnIrLAzEvOuXNm/h3AfQDOAGdj5z5m5gXn3G9ENCKilckaiDwpiFZEpCoiTyYrM3/knOtXmHs58BkABvCLc+4DZl6cxIjoh4ODA728vIzt7W2sra09PT4+fgTgQES4LMvHZIy5LyI/f/dJvfK6FKMJqAdqmBS2ZgWINY9zJyZ1whUiRJrTfmGjybSxUemwcFEmghqT1UxlUrpQEXD7jRq3NWelEz10okIi1AJOX+U2EgALgUrT0kVnmcKvQ+uazSZvbW2hWq2i3++j3W5jf38f6+vrebvdHovI5pT09+vN7teLYev990K8GpQYjS1u1TQqhnHez6EU4cM7FTw7z5AXDo16ACcyPRsaxp2FEM9ejKd7zUaIYVpiOCpRq2rUIo0XLzMwEZaaFTzvZ8hyh/iWnu69fF1AAoOfFr/A5w8eYDQaIY5jJEmC09NTNBoNdLtd7O3t4ejoCIeHh8k/pL8KUSkCZcg6hCLDjLk2ufYGpRuXAuOMYiosQiDNNEcQILQuzYgi0QxVWKuAMtcqJPdXDaKaMMOUNrOAtoFSVE5jaaY4mtQPS5dmQPS8WkdfGReYgDe/3cTdpbvo9XrY2NhAp9NBq9XKO53Ov6T/r2mlVBeX0DSAMzeHppm5h7doemdvR99buYfd3V2srq4+PTk5+a+mr9CmrqTU3+5Rr9cHg8HgYve4krddbZGb5dOXmf2d+7n4pi9D4LrOeHl40v42fl0a8KQ96fkIeJ+ej9vsWZ707Mzmy/Ck5+M2e9afhmuAz1BhwewAAAAASUVORK5CYII=" alt="Next article" width="45" height="45" loading="lazy">
            </a>
          </div>
        
            
            
              
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                  
                  
              
              
              
              <!-- Navigation block for tag "linux"; default display for the first tag -->
              <div class="nav-group tags" id="tag-nav-linux" style="display: block;">
                <div class="nav-arrow tags prev">
                  
                    <a href="/bulk-gpt/" title="bulkGPT: multithreading for OpenAI's output limits" rel="prev">
                      <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ+lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="Previous in Tag" width="45" height="45" loading="lazy">
                    </a>
                  
                </div>
                <div class="nav-arrow tags next">
                  
                    <a href="/linux-dd2boot/" title="How to use the `dd` command to create a bootable USB drive from an ISO file on a Linux-based system" rel="next">
                      <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAwdJREFUaEPtl01rG1cUht9z7p070ijKWLVS3I9YULtNMBRn2UIXxZtuvCjedNWfoKVXBvsHeOUfoIUx5AeY0pVXXSR052UxBJSi0kDiRplII83HvadI9CMtOFjCmBjubC7cc8+Zc555eYdLuIEP3cCe4Zu+rq/mSXvSbyHg5eHl4eVxXRrwpD3p+Qh4n56P2+xZnvTszC7MCAAUF0XfOdJa62+IaCeO40GSJA/zPP8RwB9vDjBtWmv9JRE5ImpZawdVpbo58CkAIyJnIrLAzEvOuXNm/h3AfQDOAGdj5z5m5gXn3G9ENCKilckaiDwpiFZEpCoiTyYrM3/knOtXmHs58BkABvCLc+4DZl6cxIjoh4ODA728vIzt7W2sra09PT4+fgTgQES4LMvHZIy5LyI/f/dJvfK6FKMJqAdqmBS2ZgWINY9zJyZ1whUiRJrTfmGjybSxUemwcFEmghqT1UxlUrpQEXD7jRq3NWelEz10okIi1AJOX+U2EgALgUrT0kVnmcKvQ+uazSZvbW2hWq2i3++j3W5jf38f6+vrebvdHovI5pT09+vN7teLYev990K8GpQYjS1u1TQqhnHez6EU4cM7FTw7z5AXDo16ACcyPRsaxp2FEM9ejKd7zUaIYVpiOCpRq2rUIo0XLzMwEZaaFTzvZ8hyh/iWnu69fF1AAoOfFr/A5w8eYDQaIY5jJEmC09NTNBoNdLtd7O3t4ejoCIeHh8k/pL8KUSkCZcg6hCLDjLk2ufYGpRuXAuOMYiosQiDNNEcQILQuzYgi0QxVWKuAMtcqJPdXDaKaMMOUNrOAtoFSVE5jaaY4mtQPS5dmQPS8WkdfGReYgDe/3cTdpbvo9XrY2NhAp9NBq9XKO53Ov6T/r2mlVBeX0DSAMzeHppm5h7doemdvR99buYfd3V2srq4+PTk5+a+mr9CmrqTU3+5Rr9cHg8HgYve4krddbZGb5dOXmf2d+7n4pi9D4LrOeHl40v42fl0a8KQ96fkIeJ+ej9vsWZ707Mzmy/Ck5+M2e9afhmuAz1BhwewAAAAASUVORK5CYII=" alt="Next in Tag" width="45" height="45" loading="lazy">
                    </a>
                  
                </div>
              </div>
            
          
            
            
              
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                  
                  
              
              
              
              <!-- Navigation block for tag "scripts"; default display for the first tag -->
              <div class="nav-group tags" id="tag-nav-scripts" style="display: none;">
                <div class="nav-arrow tags prev">
                  
                    <a href="/bulk-gpt/" title="bulkGPT: multithreading for OpenAI's output limits" rel="prev">
                      <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ+lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="Previous in Tag" width="45" height="45" loading="lazy">
                    </a>
                  
                </div>
                <div class="nav-arrow tags next">
                  
                    <a href="/gingko-toc-outliner/" title="Gingko JSON ToC outliner" rel="next">
                      <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAwdJREFUaEPtl01rG1cUht9z7p070ijKWLVS3I9YULtNMBRn2UIXxZtuvCjedNWfoKVXBvsHeOUfoIUx5AeY0pVXXSR052UxBJSi0kDiRplII83HvadI9CMtOFjCmBjubC7cc8+Zc555eYdLuIEP3cCe4Zu+rq/mSXvSbyHg5eHl4eVxXRrwpD3p+Qh4n56P2+xZnvTszC7MCAAUF0XfOdJa62+IaCeO40GSJA/zPP8RwB9vDjBtWmv9JRE5ImpZawdVpbo58CkAIyJnIrLAzEvOuXNm/h3AfQDOAGdj5z5m5gXn3G9ENCKilckaiDwpiFZEpCoiTyYrM3/knOtXmHs58BkABvCLc+4DZl6cxIjoh4ODA728vIzt7W2sra09PT4+fgTgQES4LMvHZIy5LyI/f/dJvfK6FKMJqAdqmBS2ZgWINY9zJyZ1whUiRJrTfmGjybSxUemwcFEmghqT1UxlUrpQEXD7jRq3NWelEz10okIi1AJOX+U2EgALgUrT0kVnmcKvQ+uazSZvbW2hWq2i3++j3W5jf38f6+vrebvdHovI5pT09+vN7teLYev990K8GpQYjS1u1TQqhnHez6EU4cM7FTw7z5AXDo16ACcyPRsaxp2FEM9ejKd7zUaIYVpiOCpRq2rUIo0XLzMwEZaaFTzvZ8hyh/iWnu69fF1AAoOfFr/A5w8eYDQaIY5jJEmC09NTNBoNdLtd7O3t4ejoCIeHh8k/pL8KUSkCZcg6hCLDjLk2ufYGpRuXAuOMYiosQiDNNEcQILQuzYgi0QxVWKuAMtcqJPdXDaKaMMOUNrOAtoFSVE5jaaY4mtQPS5dmQPS8WkdfGReYgDe/3cTdpbvo9XrY2NhAp9NBq9XKO53Ov6T/r2mlVBeX0DSAMzeHppm5h7doemdvR99buYfd3V2srq4+PTk5+a+mr9CmrqTU3+5Rr9cHg8HgYve4krddbZGb5dOXmf2d+7n4pi9D4LrOeHl40v42fl0a8KQ96fkIeJ+ej9vsWZ707Mzmy/Ck5+M2e9afhmuAz1BhwewAAAAASUVORK5CYII=" alt="Next in Tag" width="45" height="45" loading="lazy">
                    </a>
                  
                </div>
              </div>
            
          
          <!-- JavaScript to Toggle the Tag-based Navigation -->
          <script>
            document.addEventListener("DOMContentLoaded", function(){
              var tagLinks = document.querySelectorAll('.tag-option');
              tagLinks.forEach(function(link){
                link.addEventListener('click', function(event){
                  event.preventDefault();
                  // Remove "active" class from all tag options.
                  tagLinks.forEach(function(el){ el.classList.remove('active'); });
                  // Add active class to the clicked tag option.
                  this.classList.add('active');
                  // Hide all tag navigation blocks.
                  document.querySelectorAll('.nav-group.tags').forEach(function(block){
                    block.style.display = 'none';
                  });
                  // Show the navigation block corresponding to the selected tag.
                  var tagSlug = this.getAttribute('data-tag');
                  var target = document.getElementById('tag-nav-' + tagSlug);
                  if(target) {
                    target.style.display = 'block';
                  }
                });
              });
            });
          </script>
      </nav>
      
    <div class="comment-box">
      Ref. 
      <a href="https://github.com/whitead/paper-qa" title="https://github.com/whitead/paper-qa">https://github.com/whitead/paper-qa</a>
    </div>
    
    </main>
    <footer id="bottom-of-page" class="site-footer">
      <div class="footer-content">
        <!-- Back to top link -->
        <a href="#" aria-label="Back to top" class="back2top-link">
          <span class="sronly">Back to top</span>
        </a>
    
        <!-- Liquid Time Calculation and Display -->
        
        
        
        <a href="https://ib.bsb.br/404" aria-label="404">
          2025-10-21 17:38:54
        </a>
        &#x23;
    
        <!-- Tag Selector -->
        <ul class="tag-selector">
          
            
            
              <li>
                <a href="#" class="tag-option active" data-tag="linux">
                  linux
                </a>
              </li>
            
          
            
            
              <li>
                <a href="#" class="tag-option " data-tag="scripts">
                  scripts
                </a>
              </li>
            
          
        </ul>
        &hArr;
    
        <!-- GitHub Link -->
        <a href="https://github.com/ib-bsb-br/ib-bsb-br.github.io" aria-label="GitHub">
          &#8505;
        </a>
    
        <!-- Homepage Link -->
        <a href="/" aria-label="Homepage">
          infoBAG
        </a>
    
        <!-- Copy All Code Button -->
        <button id="copyAllButton" aria-label="Copy all code">
          &copy;
        </button>
      </div>
    </footer>
    <style>
      .back2top-link {
        display: inline-block;
        width: 32px;
        height: 32px;
        background: url("/assets/Rope_(Old).gif") center center no-repeat;
        background-size: contain;
        text-decoration: none;
        vertical-align: middle;
      }
      .sronly {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
      }
    </style>
    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "Article",
        "mainEntityOfPage": {
          "@type": "WebPage",
          "@id": "https://ib.bsb.br/clipaper-qa/"
        },
        "headline": "cliPaperQA",
        "description": "",
        "datePublished": "2024-06-16T00:00:00+00:00",
        "dateModified": "2025-02-01T21:29:53+00:00",
        "author": {
          "@type": "Person",
          "name": "Author"
        },
        "publisher": {
          "@type": "Organization",
          "name": "infoBAG"
          
        }
        
      }
    </script>
    <script src="/assets/js/prism.js" defer></script>
    <script src="/assets/js/copy-all-code.js"></script>
  </body>
</html>
