<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>
    
      URLs list text scraper — infoBAG
    
  </title>
  <meta name="title" content="URLs list text scraper">
  <meta name="description" content="can't steer unless already moving">
  <meta property="og:title" content="URLs list text scraper — infoBAG">
  <meta property="og:description" content="can't steer unless already moving">
  <meta property="og:url" content="https://ib.bsb.br/urls-list-text-scraper/">
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="infoBAG">
  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="URLs list text scraper — infoBAG">
  <meta name="twitter:description" content="can't steer unless already moving">
  

  <link rel="canonical" href="https://ib.bsb.br/urls-list-text-scraper/">
  <link rel="alternate" type="application/rss+xml" title="infoBAG" href="https://ib.bsb.br/rss.xml">

  
    <meta name="keywords" content="scripts>powershell,,software>windows">
    
      <meta property="article:tag" content="scripts>powershell,">
    
      <meta property="article:tag" content="software>windows">
    
  
  <link rel="icon" href="/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  

  <link href="/style.css" rel="stylesheet">
</head>
<body class="post-content-body">
  <header class="header-container">
    <nav aria-label="Main navigation" class="header-content">
      <a href="/" aria-label="Home">
        <img src="/favicon.ico" alt="Home" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/tags" aria-label="Tags">
        <img src="/assets/Label.gif" alt="Tags" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/archive" aria-label="Archive">
        <img src="/assets/Loose_Stone_Pile.gif" alt="Archive" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/events" aria-label="Events">
        <img src="/assets/Paralyse_Rune.gif" alt="Events" class="favicon search-link" width="45" height="45" loading="lazy">
      </a>
      <a href="/archive-created" aria-label="Archive created">
          <img src="/assets/Hole_(Rock).gif" alt="Archive created" class="favicon search-link" width="32" height="32" loading="lazy">
      </a>
      <a href="/rot64" aria-label="rot">
          <img src="/assets/rot.gif" alt="rot" class="favicon search-link" width="32" height="32" loading="lazy">
      </a>
    </nav>
    <h5 class="post-title">
      <a href="#bottom-of-page" aria-label="Go to bottom">
        URLs list text scraper
      </a>
    </h5>
    <div class="post-meta">
      <time datetime="2024-09-12T00:00:00+00:00" class="post-date">
        12 Sep 2024
      </time>
      
        <span class="post-updated">
          ↣
          <time datetime="2024-12-29T18:40:23+00:00">29 Dec 2024</time>
        </span>
      
      
        <p class="post-slug">
          Slug: <a href="https://ib.bsb.br/urls-list-text-scraper" class="tag">urls-list-text-scraper</a>
        </p>
      
      
        <p class="post-tags">
          Tags:
          
            <a href="https://ib.bsb.br/tags/#scripts-powershell" class="tag">scripts>powershell,</a>
          
            <a href="https://ib.bsb.br/tags/#software-windows" class="tag">software>windows</a>
          
        </p>
      
    </div>
    <div class="post-actions">
      <div class="page-stats mt-3" role="status" aria-label="Page statistics">
        <span class="badge bg-primary">
          12631 characters
        </span>
        <span class="separator mx-2" aria-hidden="true">•</span>
        <span class="badge bg-primary">
          893 words
        </span>
      </div>
      <div class="action-buttons d-flex flex-wrap gap-2">
        
          
            <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/edit/main/_posts/2024-09-12-urls-list-text-scraper.md"
                  method="GET"
                  target="_blank"
                  rel="noopener noreferrer"
                  class="d-inline-block">
              <button type="submit" class="btn btn-danger" aria-label="Edit page content">
                <span class="button-text">Improve this page?</span>
                <span class="info-text">aberto.</span>
              </button>
            </form>
          
          <form action="https://github.com/ib-bsb-br/ib-bsb-br.github.io/commits/main/_posts/2024-09-12-urls-list-text-scraper.md"
                method="GET"
                target="_blank"
                rel="noopener noreferrer"
                class="d-inline-block">
            <button type="submit" class="btn btn-danger" aria-label="View page revision history">
              View revision history
            </button>
          </form>
        
      </div>
    </div>
  </header>
  <main class="content">
    <article class="post-wrapper">
      <div class="post-content-body">
        

        <p>Reference: <code class="language-plaintext highlighter-rouge">https://github.com/kitsuyui/scraper</code></p>

<p>To download the text content of multiple URLs from a list on Windows 11, we’ll create a PowerShell script that’s more robust and flexible than the previously suggested batch file. This approach leverages PowerShell’s strengths and provides better error handling and output formatting.</p>

<ol>
  <li>First, ensure you have <code class="language-plaintext highlighter-rouge">scraper.exe</code> set up:
    <ul>
      <li>Download the latest Windows executable from https://github.com/kitsuyui/scraper/releases/latest</li>
      <li>Rename it to <code class="language-plaintext highlighter-rouge">scraper.exe</code> and place it in a directory that’s in your system PATH</li>
    </ul>
  </li>
  <li>Create a file named <code class="language-plaintext highlighter-rouge">scraper-config.json</code> with the following content:
    <div class="language-json highlighter-rouge"><div class="highlight"><section><code><span class="p">[</span><span class="w">
  </span><span class="p">{</span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"xpath"</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BodyText"</span><span class="p">,</span><span class="w"> </span><span class="nl">"query"</span><span class="p">:</span><span class="w"> </span><span class="s2">"//body//text()"</span><span class="p">}</span><span class="w">
</span><span class="p">]</span><span class="w">
</span></code></section></div>    </div>
  </li>
  <li>Create a text file named <code class="language-plaintext highlighter-rouge">urls.txt</code> with one URL per line:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>https://example.com
https://another-example.com
https://third-example.com
</code></section></div>    </div>
  </li>
  <li>
    <p>Create a new file named <code class="language-plaintext highlighter-rouge">Scrape-Urls.ps1</code> with the following PowerShell script:</p>

    <div class="language-powershell highlighter-rouge"><div class="highlight"><section><code><span class="c"># Scrape-Urls.ps1</span><span class="w">
</span><span class="kr">param</span><span class="p">(</span><span class="w">
    </span><span class="p">[</span><span class="n">string</span><span class="p">]</span><span class="nv">$UrlFile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"urls.txt"</span><span class="p">,</span><span class="w">
    </span><span class="p">[</span><span class="n">string</span><span class="p">]</span><span class="nv">$ConfigFile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"scraper-config.json"</span><span class="p">,</span><span class="w">
    </span><span class="p">[</span><span class="n">string</span><span class="p">]</span><span class="nv">$OutputDir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"scraped_content"</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c"># Ensure the output directory exists</span><span class="w">
</span><span class="n">New-Item</span><span class="w"> </span><span class="nt">-ItemType</span><span class="w"> </span><span class="nx">Directory</span><span class="w"> </span><span class="nt">-Force</span><span class="w"> </span><span class="nt">-Path</span><span class="w"> </span><span class="nv">$OutputDir</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Out-Null</span><span class="w">

</span><span class="c"># Read URLs from file</span><span class="w">
</span><span class="nv">$urls</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Get-Content</span><span class="w"> </span><span class="nv">$UrlFile</span><span class="w">

</span><span class="kr">foreach</span><span class="w"> </span><span class="p">(</span><span class="nv">$url</span><span class="w"> </span><span class="kr">in</span><span class="w"> </span><span class="nv">$urls</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="kr">try</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">Write-Host</span><span class="w"> </span><span class="s2">"Processing: </span><span class="nv">$url</span><span class="s2">"</span><span class="w">
           
        </span><span class="c"># Generate a safe filename</span><span class="w">
        </span><span class="nv">$filename</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="nv">$url</span><span class="w"> </span><span class="o">-replace</span><span class="w"> </span><span class="s2">"https?://"</span><span class="p">,</span><span class="w"> </span><span class="s2">""</span><span class="w"> </span><span class="o">-replace</span><span class="w"> </span><span class="s2">"[^a-zA-Z0-9]+"</span><span class="p">,</span><span class="w"> </span><span class="s2">"_"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s2">".txt"</span><span class="w">
        </span><span class="nv">$outputPath</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Join-Path</span><span class="w"> </span><span class="nv">$OutputDir</span><span class="w"> </span><span class="nv">$filename</span><span class="w">

        </span><span class="c"># Download and scrape content</span><span class="w">
        </span><span class="nv">$content</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Invoke-WebRequest</span><span class="w"> </span><span class="nt">-Uri</span><span class="w"> </span><span class="nv">$url</span><span class="w"> </span><span class="nt">-UseBasicParsing</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Select-Object</span><span class="w"> </span><span class="nt">-ExpandProperty</span><span class="w"> </span><span class="nx">Content</span><span class="w">
        </span><span class="nv">$scrapedContent</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">$content</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">scraper</span><span class="w"> </span><span class="nt">-c</span><span class="w"> </span><span class="nv">$ConfigFile</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ConvertFrom-Json</span><span class="w">

        </span><span class="c"># Extract text from JSON and save</span><span class="w">
        </span><span class="nv">$bodyText</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">$scrapedContent</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Where-Object</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="bp">$_</span><span class="o">.</span><span class="nf">label</span><span class="w"> </span><span class="o">-eq</span><span class="w"> </span><span class="s2">"BodyText"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Select-Object</span><span class="w"> </span><span class="nt">-ExpandProperty</span><span class="w"> </span><span class="nx">results</span><span class="w">
        </span><span class="nv">$bodyText</span><span class="w"> </span><span class="o">-join</span><span class="w"> </span><span class="s2">" "</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Out-File</span><span class="w"> </span><span class="nt">-FilePath</span><span class="w"> </span><span class="nv">$outputPath</span><span class="w">

        </span><span class="n">Write-Host</span><span class="w"> </span><span class="s2">"Saved to: </span><span class="nv">$outputPath</span><span class="s2">"</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="kr">catch</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">Write-Host</span><span class="w"> </span><span class="s2">"Error processing </span><span class="nv">$url</span><span class="s2"> : </span><span class="bp">$_</span><span class="s2">"</span><span class="w"> </span><span class="nt">-ForegroundColor</span><span class="w"> </span><span class="nx">Red</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="n">Write-Host</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">Write-Host</span><span class="w"> </span><span class="s2">"All URLs processed."</span><span class="w"> </span><span class="nt">-ForegroundColor</span><span class="w"> </span><span class="nx">Green</span><span class="w">
</span></code></section></div>    </div>
  </li>
  <li>
    <p>Open PowerShell and navigate to the directory containing your script and files.</p>
  </li>
  <li>Run the script:
    <div class="language-powershell highlighter-rouge"><div class="highlight"><section><code><span class="o">.</span><span class="n">\Scrape-Urls.ps1</span><span class="w">
</span></code></section></div>    </div>
  </li>
</ol>

<p>This improved solution offers several advantages:</p>

<ul>
  <li>It uses PowerShell, which is more powerful and flexible than batch scripts on Windows.</li>
  <li>It includes error handling to manage issues with individual URLs without stopping the entire process.</li>
  <li>It creates a separate output directory for scraped content, keeping things organized.</li>
  <li>It generates safe filenames based on the URLs, avoiding potential naming conflicts or invalid characters.</li>
  <li>It extracts the actual text content from the JSON output, providing clean text files.</li>
  <li>It’s more customizable, allowing you to specify different input files, config files, or output directories.</li>
</ul>

<p>Additional notes:</p>

<ol>
  <li>
    <p>This script respects rate limiting by processing URLs sequentially. For a large number of URLs, consider adding a delay between requests.</p>
  </li>
  <li>
    <p>Some websites may block or behave differently with automated requests. You might need to add user-agent headers or other modifications for certain sites:</p>

    <div class="language-powershell highlighter-rouge"><div class="highlight"><section><code><span class="nv">$headers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">@{</span><span class="w">
    </span><span class="s2">"User-Agent"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="nv">$content</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Invoke-WebRequest</span><span class="w"> </span><span class="nt">-Uri</span><span class="w"> </span><span class="nv">$url</span><span class="w"> </span><span class="nt">-UseBasicParsing</span><span class="w"> </span><span class="nt">-Headers</span><span class="w"> </span><span class="nv">$headers</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Select-Object</span><span class="w"> </span><span class="nt">-ExpandProperty</span><span class="w"> </span><span class="nx">Content</span><span class="w">
</span></code></section></div>    </div>
  </li>
  <li>
    <p>Always ensure you have permission to scrape the websites you’re targeting and that you’re complying with their terms of service and robots.txt files.</p>
  </li>
  <li>
    <p>For very large lists of URLs, consider implementing parallel processing or breaking the list into smaller batches to improve efficiency.</p>
  </li>
  <li>
    <p>You may want to add more robust URL validation and error checking, depending on your specific needs and the reliability of your URL list.</p>
  </li>
</ol>

      </div>
      
        URL: https://ib.bsb.br/urls-list-text-scraper
      
    </article>
        <nav class="post-navigation same-tag-navigation" aria-label="Posts sharing the same tag">
          
          
        </nav>
    
    
      
      
      
        
          
          
      
    
    
  </main>
  <footer id="bottom-of-page" class="site-footer">
    <p>
      <a
        href="#"
        aria-label="Back to top"
        class="back-to-top-link">
        <span class="sr-only">Back to top</span>
      </a>
      
      
      
      <a href="https://ib.bsb.br/404" aria-label="404">
        2025-01-28 21:27:44
      </a>
      &hArr;
      <a href="https://github.com/ib-bsb-br/ib-bsb-br.github.io" aria-label="GitHub">&#8505;</a>
      <a href="/" aria-label="Homepage">
        infoBAG
      </a>
      <button id="copyAllButton" aria-label="Copy all code">
        &copy;
      </button>
    </p>
  </footer>
  <style>
  .back-to-top-link {
    display: inline-block;
    width: 32px;
    height: 32px;
    background: url("/assets/Rope_(Old).gif") center center no-repeat;
    background-size: contain; /* or cover */
    text-decoration: none;
    vertical-align: middle;
  }
  .sr-only {
    /* Utility class to hide text visually but keep it accessible */
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    white-space: nowrap;
    border: 0;
  }
  </style>
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://ib.bsb.br/urls-list-text-scraper/"
    },
    "headline": "URLs list text scraper",
    "description": "",
    "datePublished": "2024-09-12T00:00:00+00:00",
    "dateModified": "2024-12-29T18:40:23+00:00",
    "author": {
      "@type": "Person",
      "name": "Author"
    },
    "publisher": {
      "@type": "Organization",
      "name": "infoBAG"
      
    }
    
  }
  </script>
  <script src="/assets/js/prism.js" defer></script>
  <script src="/assets/js/copy-all-code.js"></script>
</body>
</html>
