<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>
    
      URLs list text scraper — infoBAG
    
  </title>
  <meta name="title" content="URLs list text scraper" />
  <meta name="description" content="A website about..." />

  <!-- Open Graph Meta Tags -->
  <meta property="og:title" content="URLs list text scraper — infoBAG" />
  <meta property="og:description" content="A website about..." />
  <meta property="og:url" content="https://ib.bsb.br/urls-list-text-scraper/" />
  <meta property="og:type" content="article" />
  <meta property="og:site_name" content="infoBAG" />
  

  <!-- Twitter Card Meta Tags -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="URLs list text scraper — infoBAG">
  <meta name="twitter:description" content="A website about...">
  

  <!-- Structured Data (JSON-LD) -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://ib.bsb.br/urls-list-text-scraper/"
    },
    "headline": "URLs list text scraper",
    "description": "",
    "datePublished": "2024-09-12T00:00:00+00:00",
    "dateModified": "2024-09-20T02:37:06+00:00",
    "author": {
      "@type": "Person",
      "name": "infoBAG"
    },
    "publisher": {
      "@type": "Organization",
      "name": "infoBAG"
    }
  }
  </script>

  <link rel="canonical" href="https://ib.bsb.br/urls-list-text-scraper/">
  <link rel="alternate" type="application/rss+xml" title="infoBAG" href="https://ib.bsb.br/rss.xml">
  
  <!-- Favicons and Icons -->
  
  <link rel="icon" href="/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">

  
    <meta name="keywords" content="scripts>powershell, software>windows">
    
      <meta property="article:tag" content="scripts>powershell, software>windows">
    
  

  <!-- Stylesheets -->
  <link href="/style.css" rel="stylesheet">
  <link href="/pagefind/pagefind-ui.css" rel="stylesheet">

  <!-- JavaScript Files with 'defer' for Non-blocking -->
  <script src="/pagefind/pagefind-ui.js" defer></script>
  <script type="module" defer>
    import PagefindHighlight from '/pagefind/pagefind-highlight.js';
    document.addEventListener('DOMContentLoaded', () => {
      new PagefindHighlight({ highlightParam: 'highlight' });
    });
  </script>
  <script src="/assets/js/search.js" defer></script>
  <script src="/assets/js/prism.js" defer></script>
</head>
<body class="post-content-body">
  <header class="header-container">
    <div id="search" class="search-input-block"></div>
    <nav aria-label="Main navigation" class="header-content">
      <a href="/" aria-label="Home">
        <img src="/assets/Sudden_Death_Rune.gif" alt="Home" class="favicon search-link" width="32" height="32" loading="lazy">
      </a>
      <a href="/archive" aria-label="Archive">
        <img src="/favicon.ico" alt="Archive" class="favicon search-link" width="32" height="32" loading="lazy">
      </a>
      <a href="/tags" aria-label="Tags">
        <img src="/assets/Label.gif" alt="Tags" class="favicon search-link" width="32" height="32" loading="lazy">
      </a>
      <a href="/events" aria-label="Events">
        <img src="/assets/Paralyse_Rune.gif" alt="Events" class="favicon search-link" width="32" height="32" loading="lazy">
      </a>
    </nav>
  </header>

  <main class="content">
    <article class="post-wrapper">
      <header class="post-heading">
        <h1 class="post-title">URLs list text scraper</h1>
        <div class="post-meta">
          <time datetime="2024-09-12T00:00:00+00:00" class="post-date">September 12, 2024</time>
          
            <span class="post-updated"> &rightarrowtail; Updated: <time datetime="2024-09-20T02:37:06+00:00">September 20, 2024</time></span>
          
          
            <div class="post-info">Edit: aberto.</div>
          
          
            <div class="post-tags">
              Tags:
              
                <a href="/tags-scripts-powershell-software-windows" class="tag">scripts>powershell, software>windows</a>
              
            </div>
          
          <div class="post-actions">
            <a href="https://github.com/ib-bsb-br/ib-bsb-br.github.io/edit/main/_posts/2024-09-12-urls-list-text-scraper.md" target="_blank" rel="noopener noreferrer nofollow" class="btn-primary">Improve this page</a> ⇔
            <a href="https://github.com/ib-bsb-br/ib-bsb-br.github.io/commits/main/_posts/2024-09-12-urls-list-text-scraper.md" target="_blank" rel="noopener noreferrer nofollow" class="btn-secondary">View revision history</a>
          </div>
        </div>
      </header>
      <div class="post-content">
        

        <p>Reference: <code class="language-plaintext highlighter-rouge">https://github.com/kitsuyui/scraper</code></p>

<p>To download the text content of multiple URLs from a list on Windows 11, we’ll create a PowerShell script that’s more robust and flexible than the previously suggested batch file. This approach leverages PowerShell’s strengths and provides better error handling and output formatting.</p>

<ol>
  <li>First, ensure you have <code class="language-plaintext highlighter-rouge">scraper.exe</code> set up:
    <ul>
      <li>Download the latest Windows executable from https://github.com/kitsuyui/scraper/releases/latest</li>
      <li>Rename it to <code class="language-plaintext highlighter-rouge">scraper.exe</code> and place it in a directory that’s in your system PATH</li>
    </ul>
  </li>
  <li>Create a file named <code class="language-plaintext highlighter-rouge">scraper-config.json</code> with the following content:
    <div class="language-json highlighter-rouge"><div class="highlight"><section><code><span class="p">[</span><span class="w">
  </span><span class="p">{</span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"xpath"</span><span class="p">,</span><span class="w"> </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"BodyText"</span><span class="p">,</span><span class="w"> </span><span class="nl">"query"</span><span class="p">:</span><span class="w"> </span><span class="s2">"//body//text()"</span><span class="p">}</span><span class="w">
</span><span class="p">]</span><span class="w">
</span></code></section></div>    </div>
  </li>
  <li>Create a text file named <code class="language-plaintext highlighter-rouge">urls.txt</code> with one URL per line:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>https://example.com
https://another-example.com
https://third-example.com
</code></section></div>    </div>
  </li>
  <li>
    <p>Create a new file named <code class="language-plaintext highlighter-rouge">Scrape-Urls.ps1</code> with the following PowerShell script:</p>

    <div class="language-powershell highlighter-rouge"><div class="highlight"><section><code><span class="c"># Scrape-Urls.ps1</span><span class="w">
</span><span class="kr">param</span><span class="p">(</span><span class="w">
    </span><span class="p">[</span><span class="n">string</span><span class="p">]</span><span class="nv">$UrlFile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"urls.txt"</span><span class="p">,</span><span class="w">
    </span><span class="p">[</span><span class="n">string</span><span class="p">]</span><span class="nv">$ConfigFile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"scraper-config.json"</span><span class="p">,</span><span class="w">
    </span><span class="p">[</span><span class="n">string</span><span class="p">]</span><span class="nv">$OutputDir</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"scraped_content"</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c"># Ensure the output directory exists</span><span class="w">
</span><span class="n">New-Item</span><span class="w"> </span><span class="nt">-ItemType</span><span class="w"> </span><span class="nx">Directory</span><span class="w"> </span><span class="nt">-Force</span><span class="w"> </span><span class="nt">-Path</span><span class="w"> </span><span class="nv">$OutputDir</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Out-Null</span><span class="w">

</span><span class="c"># Read URLs from file</span><span class="w">
</span><span class="nv">$urls</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Get-Content</span><span class="w"> </span><span class="nv">$UrlFile</span><span class="w">

</span><span class="kr">foreach</span><span class="w"> </span><span class="p">(</span><span class="nv">$url</span><span class="w"> </span><span class="kr">in</span><span class="w"> </span><span class="nv">$urls</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="kr">try</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">Write-Host</span><span class="w"> </span><span class="s2">"Processing: </span><span class="nv">$url</span><span class="s2">"</span><span class="w">
           
        </span><span class="c"># Generate a safe filename</span><span class="w">
        </span><span class="nv">$filename</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="nv">$url</span><span class="w"> </span><span class="o">-replace</span><span class="w"> </span><span class="s2">"https?://"</span><span class="p">,</span><span class="w"> </span><span class="s2">""</span><span class="w"> </span><span class="o">-replace</span><span class="w"> </span><span class="s2">"[^a-zA-Z0-9]+"</span><span class="p">,</span><span class="w"> </span><span class="s2">"_"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s2">".txt"</span><span class="w">
        </span><span class="nv">$outputPath</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Join-Path</span><span class="w"> </span><span class="nv">$OutputDir</span><span class="w"> </span><span class="nv">$filename</span><span class="w">

        </span><span class="c"># Download and scrape content</span><span class="w">
        </span><span class="nv">$content</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Invoke-WebRequest</span><span class="w"> </span><span class="nt">-Uri</span><span class="w"> </span><span class="nv">$url</span><span class="w"> </span><span class="nt">-UseBasicParsing</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Select-Object</span><span class="w"> </span><span class="nt">-ExpandProperty</span><span class="w"> </span><span class="nx">Content</span><span class="w">
        </span><span class="nv">$scrapedContent</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">$content</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">scraper</span><span class="w"> </span><span class="nt">-c</span><span class="w"> </span><span class="nv">$ConfigFile</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">ConvertFrom-Json</span><span class="w">

        </span><span class="c"># Extract text from JSON and save</span><span class="w">
        </span><span class="nv">$bodyText</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">$scrapedContent</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Where-Object</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="bp">$_</span><span class="o">.</span><span class="nf">label</span><span class="w"> </span><span class="o">-eq</span><span class="w"> </span><span class="s2">"BodyText"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Select-Object</span><span class="w"> </span><span class="nt">-ExpandProperty</span><span class="w"> </span><span class="nx">results</span><span class="w">
        </span><span class="nv">$bodyText</span><span class="w"> </span><span class="o">-join</span><span class="w"> </span><span class="s2">" "</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Out-File</span><span class="w"> </span><span class="nt">-FilePath</span><span class="w"> </span><span class="nv">$outputPath</span><span class="w">

        </span><span class="n">Write-Host</span><span class="w"> </span><span class="s2">"Saved to: </span><span class="nv">$outputPath</span><span class="s2">"</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="kr">catch</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">Write-Host</span><span class="w"> </span><span class="s2">"Error processing </span><span class="nv">$url</span><span class="s2"> : </span><span class="bp">$_</span><span class="s2">"</span><span class="w"> </span><span class="nt">-ForegroundColor</span><span class="w"> </span><span class="nx">Red</span><span class="w">
    </span><span class="p">}</span><span class="w">
    </span><span class="n">Write-Host</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">Write-Host</span><span class="w"> </span><span class="s2">"All URLs processed."</span><span class="w"> </span><span class="nt">-ForegroundColor</span><span class="w"> </span><span class="nx">Green</span><span class="w">
</span></code></section></div>    </div>
  </li>
  <li>
    <p>Open PowerShell and navigate to the directory containing your script and files.</p>
  </li>
  <li>Run the script:
    <div class="language-powershell highlighter-rouge"><div class="highlight"><section><code><span class="o">.</span><span class="n">\Scrape-Urls.ps1</span><span class="w">
</span></code></section></div>    </div>
  </li>
</ol>

<p>This improved solution offers several advantages:</p>

<ul>
  <li>It uses PowerShell, which is more powerful and flexible than batch scripts on Windows.</li>
  <li>It includes error handling to manage issues with individual URLs without stopping the entire process.</li>
  <li>It creates a separate output directory for scraped content, keeping things organized.</li>
  <li>It generates safe filenames based on the URLs, avoiding potential naming conflicts or invalid characters.</li>
  <li>It extracts the actual text content from the JSON output, providing clean text files.</li>
  <li>It’s more customizable, allowing you to specify different input files, config files, or output directories.</li>
</ul>

<p>Additional notes:</p>

<ol>
  <li>
    <p>This script respects rate limiting by processing URLs sequentially. For a large number of URLs, consider adding a delay between requests.</p>
  </li>
  <li>
    <p>Some websites may block or behave differently with automated requests. You might need to add user-agent headers or other modifications for certain sites:</p>

    <div class="language-powershell highlighter-rouge"><div class="highlight"><section><code><span class="nv">$headers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">@{</span><span class="w">
    </span><span class="s2">"User-Agent"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="nv">$content</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Invoke-WebRequest</span><span class="w"> </span><span class="nt">-Uri</span><span class="w"> </span><span class="nv">$url</span><span class="w"> </span><span class="nt">-UseBasicParsing</span><span class="w"> </span><span class="nt">-Headers</span><span class="w"> </span><span class="nv">$headers</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">Select-Object</span><span class="w"> </span><span class="nt">-ExpandProperty</span><span class="w"> </span><span class="nx">Content</span><span class="w">
</span></code></section></div>    </div>
  </li>
  <li>
    <p>Always ensure you have permission to scrape the websites you’re targeting and that you’re complying with their terms of service and robots.txt files.</p>
  </li>
  <li>
    <p>For very large lists of URLs, consider implementing parallel processing or breaking the list into smaller batches to improve efficiency.</p>
  </li>
  <li>
    <p>You may want to add more robust URL validation and error checking, depending on your specific needs and the reliability of your URL list.</p>
  </li>
</ol>

      </div>

      <nav class="post-navigation" aria-label="Post navigation">
        
          <div class="nav-arrow prev">
            <a href="/mlo-ratz/" title="MLO setup using Ratz Computed-Score Algorithm" rel="prev">
              <span aria-hidden="true">&larr;</span> Previous: MLO setup using Ratz Computed-Score Algorithm
            </a>
          </div>
        
        
          <div class="nav-arrow next">
            <a href="/daily-coffee-for-the-heart/" title="Daily coffee for the heart" rel="next">
              Next: Daily coffee for the heart <span aria-hidden="true">&rarr;</span>
            </a>
          </div>
        
      </nav>

      
    </article>
  </main>

  <footer class="site-footer">
    <p>&copy; 2024 infoBAG. All rights reserved.</p>
  </footer>
</body>
</html>
