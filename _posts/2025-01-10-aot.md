---
tags: [AI>prompt]
info: aberto.
date: 2025-01-10
type: post
layout: post
published: true
slug: aot
title: 'AoT prompt enhancer'
---
{% codeblock %}
### Introduction: The Need for Structured Prompting

This document outlines a comprehensive, structured approach to designing and enhancing prompts for Large Language Models (LLMs) using a defined XML-based format. As LLM capabilities advance, moving beyond simple prompts is crucial for achieving consistent, reliable, and controllable results. This structured methodology emphasizes an iterative design process, focusing deeply on clarity, comprehensive context, explicit instructions, and rigorous evaluation. Adopting such a structure facilitates better collaboration, simplifies debugging, improves reproducibility, and ultimately allows for more sophisticated and effective interaction with LLMs.

*(**Note on Iteration:** This process takes your initial prompt, **`[[user-original-prompt]]`**, and guides its enhancement into a structured XML format. This is inherently iterative. Insights from later steps, particularly Step 6: Evaluate and Refine, frequently necessitate revisiting and adjusting earlier steps like Step 2: Provide Context or Step 3: Identify XML Components. Expect to cycle through these steps. This section introduces the iterative nature of the prompt enhancement process.)*

---

**1. `Problem Statement (AoT - Define Goal & <purpose>):`**

* **Elaboration:** Starting with **`[[user-original-prompt]]`**, this foundational step establishes unambiguous direction, scope, and success criteria for the *enhanced* prompt. It transforms vague ideas into a precise articulation of the prompt's core function and the LLM's expected role. A weak purpose is a primary source of prompt failure, leading to irrelevant outputs, wasted resources, and difficult evaluation. A clear purpose is the bedrock of an effective prompt.
* **Pitfall:** Overly broad purpose (e.g., "Improve text," "Write about topic X"). This forces the LLM to make too many assumptions.
* **Tip:** Define the goal clearly.
    * Apply SMART criteria (Specific, Measurable, Achievable, Relevant, Time-bound) to the *intended outcome*.
    * Define the primary action verb (Generate, Analyze, Convert, Summarize, Classify, Refactor, etc.).
    * Specify the expected output's nature, format, and key characteristics.
    * Ask: What specific task? What does success look like? How is success measured? What are the task boundaries?
    * Consider LLM limitations: Factor in known strengths/weaknesses (e.g., reasoning vs. creativity), knowledge cutoffs, and potential biases. This includes being realistic about tasks prone to hallucination if high factuality is required, or tasks needing knowledge beyond the LLM's cutoff date.
* **Evolution Example (Code Analysis):**
    * Vague: "Make code better." -> Initial: "Review Python code." -> Refined: "Analyze Python code snippets for PEP 8 violations and potential runtime errors." -> Final XML: `<purpose>You are an expert code reviewer specializing in identifying PEP 8 style guide violations and common runtime error patterns (e.g., NameError, TypeError) in Python 3.9 code snippets provided in [[code-snippet]].</purpose>`
* **Evolution Example (Content Generation):**
    * Vague: "Write a blog post." -> Initial: "Write a blog post about sustainable gardening." -> Refined: "Generate a 500-word introductory blog post about sustainable gardening techniques for urban environments, targeting beginner gardeners." -> Final XML: `<purpose>You are a content writer creating an engaging and informative 500-word blog post introduction to sustainable gardening practices suitable for small urban spaces (balconies, patios). The target audience is novice gardeners with limited space. Use an encouraging and accessible tone. The output should be in Markdown format.</purpose>`
* **Task:** Distill the core intent of **`[[user-original-prompt]]`** into a specific, actionable goal. Factor in LLM capabilities/limitations. Define success criteria.
    * Examples: "Generate valid Mermaid sequence diagrams from `[[process-description]]`...", "Analyze `[[code-input]]` for OWASP Top 10 vulnerabilities...", "Convert `[[user-input]]` math problems to MathML...", "Summarize `[[article-text]]` into a 3-bullet executive summary (<150 words)..."
* **Output:** The specific goal populates the `<purpose>` tag, setting the LLM's persona, objective, and context for subsequent elements.

---

**2. Provide Context:**

* **Elaboration:** Context grounds the prompt with background, constraints, assumptions, and implicit knowledge needed for correct interpretation. Omitting context forces risky LLM assumptions, leading to outputs that might be technically valid but situationally inappropriate. Eliciting context requires understanding users, systems, the domain, and the task nuances derived from **`[[user-original-prompt]]`**.
* **2a. Key Context Types & Importance:**
    * ***Technical Constraints:***
        * *Examples:* Language versions (Python 3.9), libraries (React 18), output formats (JSON schema compliance), performance needs (latency limits), system integrations (API details).
        * *Importance:* Ensures functional code, machine-readable outputs, and performance targets are met.
    * ***Audience Assumptions:***
        * *Examples:* Expertise (novice vs. expert), role (manager vs. developer), language, cultural background.
        * *Importance:* Tailors output language, complexity, and tone for the intended recipient.
    * ***Stylistic Requirements:***
        * *Examples:* Tone (formal vs. informal), voice (active vs. passive), length limits, formatting (Markdown tables, citation style), branding guidelines.
        * *Importance:* Ensures output aligns with communication standards, brand identity, and presentation needs.
    * ***Ethical Guardrails:***
        * *Examples:* Content prohibitions (no harmful/biased content), privacy rules (PII handling), fairness considerations (avoid stereotypes), safety protocols.
        * *Importance:* Crucial for responsible AI, preventing harm, and ensuring compliance.
    * ***Domain-Specific Knowledge:***
        * *Examples:* Subject matter standards (medical terms, legal formats), field conventions (financial acronyms), required ontologies, references to external documents.
        * *Importance:* Enables accurate, relevant outputs using correct domain terminology.
* **2b. Eliciting Context:**
    * *Techniques:* Stakeholder interviews, reviewing requirements/user stories, analyzing personas/journey maps, examining existing workflows/documentation, consulting domain experts, analyzing good/bad output examples, competitive analysis.
    * *Probing Questions:* "What implicit assumptions are we making?", "What could go wrong if X is misunderstood?", "What external factors influence output use?"
* **Task:**
        * Gather all relevant context.
        * Analyze the gathered context.
        * Document the context systematically.
        * Ensure thoroughness to prevent failures.
    * Examples: "`[[user_description]]` is informal feedback (expect errors)," "Output Mermaid must render in GitLab Markdown (v16.x)," "Prioritize code review: 1st=Security, 2nd=Bugs, 3rd=Performance, 4th=Style," "Summary must be factual, cite sections from `[[article-text]]`, avoid external info," "Generated Python needs error handling (FileNotFound, ValueError) and standard logging."
* **Output:** A clear understanding of the operational environment. This informs `<instructions>`, `<examples>`, and the content of `<context>` / `<constraints>`. (*Distinction:* `<constraints>` typically define hard limits like "max 100 words," while `<context>` provides broader background like "audience is non-technical").

---

**3. Identify XML Components:**

* **Elaboration:** This phase translates the abstract task, purpose, and context into concrete XML elements based on the `raw_data` specification. Plan *what* information the LLM needs, *how* it should be presented (variables, instructions, examples), and *how* it should be structured for optimal interpretation.
* **`3a. Identifying Variables ([[variable-name]]):`**
    * *Task:* Identify dynamic inputs. Assign clear, descriptive names (e.g., `[[customer_query]]`, `[[product_database_json]]`). Consider data types/formats. Plan for nested data if needed/supported. Define mandatory/optional variables. *Note:* These placeholders are typically replaced with actual data by an external system before the prompt is sent to the LLM.
    * *Tip:* Review `<purpose>`/`<context>` for data entities. Use consistent naming (snake_case, camelCase). Avoid generic names (`[[input]]`). Document each variable's purpose/format.
* **`3b. Crafting Instructions (<instruction>):`**
    * *Task:* Decompose the task into the smallest, logical, sequential, actionable steps.
        * Use clear, imperative verbs (Analyze, Generate, List, Format, Extract).
        * Reference variables explicitly (`Analyze [[customer_query]]`).
        * Frame positively ("Generate output in JSON") but use negative constraints for boundaries ("Do not include PII").
        * Consider *meta-instructions* ("Think step-by-step," "Refer to all constraints...").
        * Handle conditionality explicitly ("If [[input_type]] is 'email', do X...").
    * *Pitfall:* Ambiguous verbs ("Consider"), complex multi-part instructions, assumed knowledge, implicit steps.
    * *Tip:* Write for a literal-minded assistant. Test clarity with others. Keep instructions atomic (one action per instruction). Ensure logical sequence.
    * *Example (Poor vs. Good):*
        * Poor: `<instruction>Process the user info.</instruction>`
        * Good: `<instruction>Extract the user's full name and email address from [[user_profile_text]].</instruction>`
        * Poor: `<instruction>Write a summary, make it short and use JSON.</instruction>`
        * Good: `<instruction>Identify the 3 key findings in [[research_paper_text]].</instruction>` -> `<instruction>Summarize the 3 key findings in one paragraph (<75 words).</instruction>` -> `<instruction>Format the summary as a JSON object with key "summary".</instruction>`
* **`3c. Determining Sections (<section-tag>):`**
    * *Task:* Group related information logically using allowed tags (e.g., `<examples>`, `<input_data>`, `<context>`, `<constraints>`, `<persona>`, `<output_format_specification>`). Evaluate semantic vs. generic tags based on LLM interpretation.
    * *Goal:* Improve readability and potentially guide LLM attention. Logically separate information types. Ensure clear roles for information.
* **`3d. Curating Examples (<examples>):`**
    * *Task:* Create high-quality, diverse examples (3-5+, quality > quantity) demonstrating the desired input-to-output transformation.
        * *Example Considerations:*
            * *Representativeness:* Show typical use cases.
            * *Diversity:* Include variations in input/output.
            * *Clarity:* Directly illustrate instructions.
            * *Edge Cases:* Test boundaries, ambiguities, less common scenarios.
            * *(Optional) Contrasting/Negative Examples:* Show errors to avoid.
    * *Pitfall:* Simplistic examples, lack of diversity (overfitting), inconsistency with instructions/format, introducing bias.
    * *Tip:* Develop examples *with* instructions. Ensure outputs *perfectly* match specifications. Verify examples for bias. Ensure coverage of different logical paths/conditions.

* **Output:** A detailed inventory: list of variables, sequence of instructions, chosen section structure, and curated input/output example pairs.

---

**4. Structure as XML:**

* **Elaboration:** Assemble components into the formal XML structure per the `raw_data` specification. Precision is key; syntax errors break prompts. Use XML editors/linters. Use XML comments (``) judiciously for maintainability (mindful of potential token cost).
* **Task:**
        * Map variables to `[[placeholders]]` in correct sections.
        * Place `<instruction>` tags sequentially under `<instructions>`.
        * Format `<examples>` correctly with consistent input/output structure.
        * Perform consistency/validity checks:
            * Check XML well-formedness and validity.
            * Check placeholder correctness.
            * Check instruction-example alignment.
            * Check context/constraint reflection.
            * Check purpose alignment.
    * *Tip:* Validate frequently during construction. Automate if possible. Proofread placeholders/tags.

* **Output:** A well-formed, syntactically valid draft XML prompt. Example:

    ```xml
    <prompt>
      <purpose>Generate concise, factual summaries of technical articles provided in [[article_text]], tailored for a non-technical managerial audience.</purpose>

      <context>
        <audience_profile>
          <role>Manager</role>
          <technical_expertise>Low</technical_expertise>
          <goal>Quickly understand key findings and business implications.</goal>
        </audience_profile>
        <style_guide>
          <tone>Formal but accessible</tone>
          <voice>Active</voice>
          <jargon>Avoid technical jargon; explain if essential.</jargon>
        </style_guide>
        <constraints>
          <constraint>Summary must not exceed 100 words.</constraint>
          <constraint>Focus only on information present in [[article_text]]. Do not add external knowledge.</constraint>
          <constraint>Output format must be plain text, suitable for email.</constraint>
          <constraint>Begin summary with "Key takeaways:".</constraint>
        </constraints>
      </context>

      <instructions>
        <instruction>1. Read the input technical article provided in [[article-text]] carefully.</instruction>
        <instruction>2. Identify the main topic, key findings, and explicitly mentioned business implications based *only* on the provided text.</instruction>
        <instruction>3. Synthesize these points into a concise summary paragraph.</instruction>
        <instruction>4. Ensure the summary adheres to *all* constraints listed in the <constraints> section (word count, factual basis, format, starting phrase).</instruction>
        <instruction>5. Refine language for clarity and directness, suitable for the specified non-technical audience (see <audience_profile>).</instruction>
        <instruction>6. Output *only* the final summary text.</instruction>
      </instructions>

      <input_data>
        <article_text>[[article-text]]</article-text>
      </input_data>

      <examples>
        <example>
          <input_data>
            <article-text>A study on Quantum Computing advancements reveals a new qubit stabilization technique using targeted magnetic pulses. This could potentially reduce error rates by 30%, making fault-tolerant quantum computers more feasible. Business implications include faster drug discovery and complex system modeling, though widespread use is likely 5-10 years away.</article-text>
          </input_data>
          <output>Key takeaways: Recent research introduced a method using magnetic pulses to stabilize qubits, potentially cutting quantum computing error rates by 30%. This advance could accelerate drug discovery and complex modeling in the future, although practical application is estimated to be 5-10 years off. The focus is on making reliable quantum computers more achievable.</output>
        </example>
        <example>
          <input_data>
            <article-text>Our analysis of renewable energy adoption shows solar panel efficiency increased by 5% last year due to perovskite cell improvements. Wind turbine deployment grew 15%. Challenges remain in grid storage and transmission infrastructure. The primary business impact is decreasing long-term energy costs for adopters.</article-text>
          </input_data>
          <output>Key takeaways: Renewable energy saw progress with a 5% increase in solar panel efficiency (due to perovskite cells) and 15% growth in wind turbine use. Key challenges involve energy storage and grid infrastructure. The main business benefit noted is lower long-term energy expenses for those adopting these technologies.</output>
        </example>
      </examples>
    </prompt>
    ```

---

**5. Propose Final XML:**

* **Elaboration:** A formal checkpoint before extensive testing. Consolidate components into the complete candidate XML. This is your testable hypothesis. Documenting design rationale aids future understanding.
* **Task:**
        * Assemble the complete XML structure.
        * Present the XML structure formally.
        * State your hypothesis about its effectiveness.
* **Output:** The complete candidate XML prompt structure, ready for review and evaluation.

---

**6. Evaluate and Refine:**

* **Elaboration:** The critical, iterative empirical validation phase. Systematically test the prompt against objectives to uncover weaknesses before deployment. Expect multiple cycles. Rigorous testing builds confidence.
* **6a. Evaluation Methodologies:**
    * ***Peer Review:***
        * *Action:* Have others review the XML for clarity, logic, completeness, ambiguity.
        * *Questions:* Is purpose clear? Instructions unambiguous? Examples accurate? Any blind spots?
    * ***Mental Simulation:***
        * *Action:* Manually trace logic with diverse inputs (typical, edge cases, invalid, ambiguous).
        * *Check:* Does logic hold? Instructions cover scenarios?
    * ***LLM Testing (Systematic):***
        * *Action:* Design test suite with realistic inputs. Execute prompt against target LLM(s). Analyze outputs. Automate if possible (e.g., using `pytest`).
        * *Checks:* Correctness (fulfills task?), Consistency (similar inputs -> similar outputs?), Robustness (handles noise?), Adherence (follows format/constraints?), Safety/Bias (no harmful content?).
    * ***A/B Testing:***
        * *Action:* Compare two prompt versions (e.g., different instructions) against the same inputs. Analyze results quantitatively/qualitatively.
    * ***Checklist Comparison:***
        * *Action:* Use prompt quality checklists (clarity, specificity, bias, safety, etc.) for systematic assessment.
    * ***Metrics:***
        * *Action:* Define and measure relevant metrics.
        * *Examples:* ROUGE/BLEU (summarization), F1/Accuracy (classification), Code Quality Scores (code gen), Task Success Rate (pass/fail), Factual Consistency Checks, Human Ratings.
* **6b. Common Refinement Strategies:**
    * ***Instruction Tuning:*** Rephrasing, adding/removing/reordering steps, adding explicit constraints (e.g., `<constraint>Output MUST be valid JSON.</constraint>`). *Example:* If format fails, add `<instruction>Format output strictly per JSON schema in [[output_schema]].</instruction>`.
    * ***Example Curation:*** Adding/improving/removing examples, targeting failure modes. Quality > quantity. *Example:* If summaries are too long, ensure examples meet length and add one showing concise summary of long input.
    * ***Context/Constraint Modification:*** Adjusting rules/background based on tests (e.g., add tone constraint if output is too informal).
    * ***Structural Reorganization:*** Modifying XML structure/tags (if spec allows) to improve interpretation.
    * ***Persona/Role Adjustment:*** Refining `<purpose>` or `<persona>` tag for better guidance.
* **Task:**
        * Evaluate the prompt against criteria (Clarity, Completeness, etc.).
        * Identify weaknesses and failure modes.
        * Apply targeted refinements based on findings.
        * *Document evaluation findings and link them to refinements.*
        * Repeat until performance meets requirements.
* **Pitfall:** Confirmation bias; inadequate test coverage (missing edge cases); stopping refinement too early.
* **Tip:** Design test cases *before* refinement. Aim for diverse coverage. Be prepared for multiple cycles. Track changes and rationale systematically.

* **Output:** An evaluated, tested, iterated, and refined XML prompt structure, with documented evaluation results and high confidence in its effectiveness.

---

**7. Finalize XML Prompt:**

* **Elaboration:** Conclude the design/refinement phase. Lock down the validated structure. Establish version control and documentation for ongoing use and maintenance.
* **Task:**
        * Finalize the XML.
        * Implement version control (e.g., Git):
            * Commit final XML.
            * Use meaningful tags (e.g., v1.0).
            * Consider branching.
        * Create documentation:
            * \[ ] Purpose: Clear statement of the prompt's goal.
            * \[ ] Variables: List of `[[placeholders]]`, purpose, format, mandatory/optional status.
            * \[ ] Context/Constraints: Explanation of key assumptions and rules.
            * \[ ] Output Specification: Description of expected output format/characteristics.
            * \[ ] Usage Guide: How to execute the prompt correctly.
            * \[ ] Examples: Curated examples used for testing.
            * \[ ] Evaluation Results Summary: Brief overview of testing findings and key refinements made.
            * \[ ] Known Limitations: Documented edge cases, failure modes, or areas of weakness found during evaluation.
            * \[ ] Changelog: Record of significant changes across versions.
            * \[ ] (Optional) Versioning/Management Strategy: Briefly describe the approach to managing different prompt versions and deployments (e.g., using a prompt registry, environment variables for switching between versions).
            * \[ ] (Optional) Monitoring and Logging: Suggest strategies for monitoring prompt performance in production and logging inputs/outputs for debugging.
* **Output:** The final, validated, version-controlled, production-ready XML prompt, accompanied by essential documentation enabling effective deployment, monitoring, maintenance, and future development.
{% endcodeblock %}
